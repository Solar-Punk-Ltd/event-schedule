{
  "Day 1": [
    {
      "id": "opening-ceremony",
      "sourceId": "X3JSYF",
      "title": "Opening Ceremony",
      "description": "Don’t miss the Devcon Opening Ceremony, where we’ll set the stage for an incredible event ahead, with talks from Vitalik Buterin (Founder of Ethereum), Aya Miyaguchi (Executive Director of the Ethereum Foundation), Josh Stark (Ethereum Foundation Leadership), Skylar Weaver (Devcon Team Lead), and more surprise guests.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "b4b199e383bcf161d7da28671901d39434e7456159cd822eaf6ccf1d802635ab",
      "sources_youtubeId": "dMLeSMcBskU",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T03:00:00.000Z",
      "slot_end": "2024-11-12T03:15:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1VG1PST0liQiPWvaWsw3TB7LQkH7HEEXqam66Ds4rCHw",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "keynote-this-year-in-ethereum",
      "sourceId": "MFBX7X",
      "title": "Keynote: This year in Ethereum",
      "description": "Josh Stark from EF Leadership opens Devcon with an overview of the Ethereum ecosystem's progress and status in 2024",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "42b2f958a6ad4ec1fc91b8dd669da09457cace9ae38b40d9772bcc6a5851ab4a",
      "sources_youtubeId": "YyK8i2-0aPk",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T03:15:00.000Z",
      "slot_end": "2024-11-12T03:40:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1jnpwsT-B0lnVYIbUt5XuDZoqqTEjj666EzfAz3-aSZY",
      "resources_slides": null,
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "keynote-redefining-boundaries-in-the-infinite-garden",
      "sourceId": "FUZDNX",
      "title": "Keynote: Redefining boundaries in the Infinite Garden",
      "description": "Aya Miyaguchi, Executive Director of the Ethereum Foundation, opens Devcon by emphasizing the importance of nurturing Ethereum as an infinite garden—a decentralized, long-term ecosystem built on shared values, overlapping spheres of influence, and collaborative growth beyond singular control.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "ad356189d2834782997d461c0a3e4b34ad700af2998a1d88369a28e185b406d0",
      "sources_youtubeId": "SE15rsPVHz0",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T03:40:00.000Z",
      "slot_end": "2024-11-12T04:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1K5z-RKHIToQZFNAHUKMDOmuOsAKkNs4VPNUiIqAAe9M",
      "resources_slides": null,
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "keynote-ethereum-in-30-minutes",
      "sourceId": "GAJPCN",
      "title": "Keynote: Ethereum in 30 minutes",
      "description": "Vitalik Buterin, Founder of Ethereum, opens Devcon with a comprehensive overview of Ethereum’s evolution as a decentralized “world computer,” explaining its layer 1 trust machine and layer 2 scaling solutions for security and scalability, emphasizing improvements in decentralization and client diversity, and encouraging developers to build innovative applications that leverage Ethereum’s robust and evolving ecosystem.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "d4b974f86276f34632b9a6361a60ff2c85d5da50b1aa85c09829c824eb97c5a9",
      "sources_youtubeId": "ei3tDRMjw6k",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T04:00:00.000Z",
      "slot_end": "2024-11-12T04:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1c4kXKhLTBksDY0GKRITW1Zog1_t1FjxKAJm7icOjg3I",
      "resources_slides": null,
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "devcon-sea-overview",
      "sourceId": "HXNYDR",
      "title": "Devcon SEA Overview",
      "description": "Don’t miss the Devcon Opening Ceremony, where we’ll set the stage for an incredible event ahead, with talks from Vitalik Buterin (Founder of Ethereum), Aya Miyaguchi (Executive Director of the Ethereum Foundation), Josh Stark (Ethereum Foundation Leadership), Skylar Weaver (Devcon Team Lead), and more surprise guests.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "6579559384c3752ff4c849eff1d48dbe1dfc815f78de54dc6debbd2a36b5e991",
      "sources_youtubeId": "c8suX-_PTo8",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T04:30:00.000Z",
      "slot_end": "2024-11-12T05:15:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1Qo0Nhlnmak6ecCzF_nhTStunc63frayP5RYA5bLD3TQ",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "build-and-deploy-an-onchain-app-in-80-minutes",
      "sourceId": "ZQZCUM",
      "title": "Build and deploy an onchain app in 80 minutes!",
      "description": "We will tinker with Solidity, build out a frontend, deploy the contract, and ship an app onchain—all in 80 minutes with state-of-the-art tooling.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,DevEx,Public good,app,DevEx,Public good,Tooling",
      "keywords": "Scaffold-ETH,Dapp,App",
      "duration": 4561,
      "language": "en",
      "sources_swarmHash": "434525c487510a834e0a2b0663c78bbe606c6bf1fa15e4bbb8984faf43525dd9",
      "sources_youtubeId": "BXimn0tony8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1918G58t4sIlc_rPC4YAPTZbQ0BQzE8SIevAi5yPsMaA",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "crypto-is-the-real-world-understanding-the-cryptonative-economy",
      "sourceId": "UCZ83J",
      "title": "Crypto is the Real World: Understanding the Cryptonative Economy",
      "description": "Ethereum has often been viewed as a separate, speculative space detached from the \"real world.\" However, recent developments and analyses reveal that the cryptonative economy is not only substantial but also comparable to traditional economies in its scope and dynamics. This talk will delve into the findings of the Cryptonative Economy Reports, highlighting the significant economic demand for Ethereum and showcasing where has Ethereum become the real world already.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Digital Sovereignty,Use Cases,Economics,cryptonative,Digital Sovereignty,Economics,Use Cases",
      "keywords": "Real world,Ecosystem,Cryptonativism",
      "duration": 991,
      "language": "en",
      "sources_swarmHash": "9a3b05187a79c6bb70f1acb84dcc9c89cdbac55482d6f2a8c022454a40a42a1c",
      "sources_youtubeId": "Y0u6J1OmPXc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T05:50:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1I8L_RL8n3RI4PQDkpmQfboZc2IVdS6GLh-psdPM4k5s",
      "resources_slides": "https://drive.google.com/file/d/1LzVukBA7im392myDzTOJpZUffVG5z5Nl/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "eips-simplified-history-and-process-explained",
      "sourceId": "TBY8MK",
      "title": "EIPs Simplified: History and Process Explained",
      "description": "It is planned to be an easy-to-understand session about Ethereum Improvement Proposals (EIPs). We'll explore the interesting history of EIPs and the important moments that have shaped different types and categories of proposals. Learn how EIPs go from an idea to becoming part of the Ethereum network, and see how editors help improve the standardization process. This talk is perfect for anyone who wants to learn about EIPs without getting into technical details.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": true,
      "doNotRecord": false,
      "tags": "Core Protocol,ACD,Coordination,Governance,improvement,eip,processes,ACD,Coordination,Core Protocol,Governance",
      "keywords": "EIP,Process,Improvement",
      "duration": 125,
      "language": "en",
      "sources_swarmHash": "801854695e0493469f2bb74493e3223d05e9df3dfa70606f5dfd12f0a17381bc",
      "sources_youtubeId": "xycI1vbxJo8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T06:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1kJKEZ4wRwEX_SUXgxNa4xYGnxsnpoukmIzmPr2XQ64A",
      "resources_slides": "https://drive.google.com/file/d/1wR2iCzSPkrV3tMb5x2cZ2HsX0jqLE_zn/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "ens-war-stories-securing-web3-from-web2-based-attacks",
      "sourceId": "P9U9Q3",
      "title": "ENS War Stories: Securing Web3 from Web2-Based Attacks",
      "description": "Web3 is not an island. Every day, threat actors try to exploit web2 domains to target web3 entities. This talk recounts ENS' war stories / lessons of battling threats in the DNS, including:\r\n- Detecting early-stage attacks on web3 entities in the DNS\r\n- How we unraveled a campaign of over 2,500+ web2 domains targeting web3 and defi entities \r\n- Legal and technical remedies to combat web2-based threats (and their limitations)\r\n- Why the ecosystem must come together to share intel and resources",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Collective Intelligence,Security,Best Practices,user,safety,Best Practices,Collective Intelligence,Security",
      "keywords": "threat actors,legal process,user safety",
      "duration": 781,
      "language": "en",
      "sources_swarmHash": "ddf9a9beb6a6cc606ece80ac2cfa5b7ea1dc15ed7e74f90748997b8a953b8574",
      "sources_youtubeId": "ht_Szqvtx8w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T05:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1TPTt3DvIJCvQfAzoDGb3Ea32KlVjG7UCxB3UUz4JC4I",
      "resources_slides": "https://drive.google.com/file/d/1hNnpW-0g_JQUwESvzh2lNn2a4KP9SkCq/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "improving-the-user-experience-by-user-research",
      "sourceId": "ZVUFEY",
      "title": "Improving the User Experience by User Research.",
      "description": "This workshop will help you understand your users and their needs, motivations and problems because this is a critical stage in product development.\r\nThis will help reduce development risks and costs through improved user experience, decision validity, increased user loyalty, etc.\r\nWe will practice in-depth interviews at the workshop, analyze its results and create a Customer Journey Map.",
      "track": "Usability",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "User Experience,Interface,Accessibility,User Research,adoption,blockchain,mass,Accessibility,Interface,User Experience,User Research",
      "keywords": "Customer Journey Map,In-depth interviews,Blockchain Mass Adoption.",
      "duration": 4254,
      "language": "en",
      "sources_swarmHash": "e32e0f241560e407a1450d712a6db039fb681b3956e5c8e6798b4616c6a901b3",
      "sources_youtubeId": "1PpaGPkCfpY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673457ee9dbb7a90e1199e95",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1FKJnGwx0Fa6M46QKoFqfn0W7-iZIbFqvnLkxjd-Pct0",
      "resources_slides": "https://drive.google.com/file/d/1xTs3bDUS3LfpXJ3fcPXyM1_bZT38VR6D/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "keynote-infinite-diversity-in-infinite-combinations",
      "sourceId": "3MNMHA",
      "title": "Keynote: ⿻ Infinite Diversity in Infinite Combinations",
      "description": "This talk explores the evolving relationship between freedom, wisdom, and technology, centered on ⿻ Plurality—a philosophy that promotes collaborative diversity.\r\n\r\nDrawing on experiences from Taiwan and beyond, we’ll examine how decentralized governance can scale to bridge divides, empower autonomy, and co-create innovative solutions for the challenges of the 21st century.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": true,
      "doNotRecord": false,
      "tags": "Decentralization,Governance,Political systems",
      "keywords": "Plurality",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "7b57f594e589cebcc14cb04fcc90c7201ef214a347ba31c146c0fbe984a280ae",
      "sources_youtubeId": "n3R4ze2hesk",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T06:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1hyqMQ-ALTG3QKpk5SkiuUcDNN1L0Z_UuyGNml54Xc60",
      "resources_slides": "https://drive.google.com/file/d/1jBpgzgzMBMJo3TlLYH3CyBxqiwnVrJ4l/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "speedrun-rollups-a-beginners-guide-to-l2s-zk-and-wtf-people-are-talking-about-on-panels",
      "sourceId": "L3Z78Q",
      "title": "Speedrun Rollups: A Beginner's Guide to L2s, ZK, and WTF People are Talking About on Panels",
      "description": "The L2 landscape has grown, both in terms of size, but also the development of the tech and the new problems that need to be solved.\r\n\r\nThis talk aims to take you from zero to hero, equipping you with the history, development, and current state of L2s, so you can maximize your Devcon experience without having to carry around a dictionary to understand WTF people are talking about.",
      "track": "Layer 2",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Scalability,ZK-EVMs,eli5,Layer 2s,Scalability,ZK-EVMs",
      "keywords": "ELI5",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "HDV-WoMb3U4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "",
      "transcript_text": "",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/17fKWm64cWJz5zLVi9Av7ZypNBcbMuJYxb55zQcDbVJ8",
      "resources_slides": "https://drive.google.com/file/d/1RM25t1m49nZWSMP9IV8DBxjeB_rcIX7g/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "why-vpns-are-scams-and-what-to-do-about-it",
      "sourceId": "TRMC3L",
      "title": "Why VPNs are scams and what to do about it",
      "description": "Existing VPNs are essentially scams. Free VPNs and most centralized VPNs (such as ExpressVPN, owned by Kape) are effectively data harvesting companies.  Decentralized VPNs usually have a few large servers and offer barely any more privacy than centralized VPNs. What is missing is 1) onion-routing packets like Tor 2)  adding noise (fake traffic) 3) censorship-resistance and 4) mixing packets from different users together. We'll explore how technologies work to defeat even AI adversaries.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "censorship,resistance,Decentralization,Privacy,Use Cases",
      "keywords": "VPNs,mixnets,censorship-resistance",
      "duration": 538,
      "language": "en",
      "sources_swarmHash": "a1d36033bf5ebaf4e1f8ed35812948388d4ba3cb56f13648118d2e9ba837ede6",
      "sources_youtubeId": "4Ir-fptXPr8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T05:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1X40WVD7E27evrL1uMb90tNX_OrjLhOmaw9pd-qrbFB4",
      "resources_slides": "https://drive.google.com/file/d/1YHaGta0yxyKPo2DImTbCg7zUcFeEhQZG/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "your-intuition-antoine-flute-and-didgeridoo",
      "sourceId": "B8SMVZ",
      "title": "Your intuition Antoine flute and didgeridoo",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:30:00.000Z",
      "slot_end": "2024-11-12T06:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1y6uMrtpD3uRb_lrG6TXEsSK_UJ8-x7X4UM7zvdFJaIY",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "merkle-proofs-when-leaves-leave-you-vulnerable",
      "sourceId": "LAKCG3",
      "title": "Merkle Proofs: When Leaves Leave You Vulnerable",
      "description": "A Merkle proof is a cryptographically authenticated data structure widely used to minimize on-chain data storage. The Merkle algorithm is neat yet non-trivial to implement correctly and securely; its leaves may leave you vulnerable if not handled properly.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Auditing,Bug,merkle,Auditing,Bug",
      "keywords": "Merkle",
      "duration": 334,
      "language": "en",
      "sources_swarmHash": "2acec7178510ddfaad6efbb63c85e3282df87ac7932d16fc39fed44b7ce8b8df",
      "sources_youtubeId": "TEBV4hPNm3k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6732face80d989c5b7aebead",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6732face80d989c5b7aebead.vtt",
      "transcript_text": " I'm a security engineer from China Security. I'm very happy to share my insights about Merkle proofs. Today, especially, I'm going to talk about when leaves will leave you vulnerable. So, I believe in your life you have seen many trees with different shapes, different structures. For example, it's a very simple tree. It could be bigger and more complex. And in addition, this is good, like it doesn't look like a tree. It could also be the Earth tree in the game Elden Ring. And for Merkle tree, it's very similar. It could be a very balanced and simple Merkle tree, where data is at the bottom and it's balanced. It could also be very imbalanced in this case. And be sure to distinguish Merkle tree from Merkle tree. It's very different. And in addition, we all know the Merkle Patricia tree, which is a very important data structure in the Ethereum global storage. So with all these trees, there are very different Merkle tree algorithms. However, I believe you can find all the important checklists that you should use to ensure a security proof in your protocols. But even though we have this checklist, we cannot ensure that we can find all the vulnerabilities in very different Merkle algorithms. And that is when the leaves can leave your protocol vulnerable. The answer is very clear. The answer is when the leafs can leave your protocol vulnerable. The answer is very clear. The answer is about the beginning. If you don't recognize a node as a leaf, you will never ensure the property from the checklist against it and make it secure. So today, I'm just going to show you one example of this kind of attack that is about Merkle-Monte range. I'm not sure if you have all heard about this algorithm. So let me first introduce what is Merkle monorange. It is a very simple data structure. It's just like a group of sub-Merkle trees. You just add the data at the bottom of this Merkle monorange, and once you keep adding new data, if there are two siblings, we add a parent. If there are two parents, we add a parent. If there are two parents, we add another parent. You do this recursively, and eventually you will get several Merkle trees in this Merkle mountain range with different depths and different size. And we call the top node of this sub-tree peaks. And why do we use Merkle mountain range? As I tell you, if you are adding a new node here, it's very easy to build a tree. And you don't need to re-compute all the nodes in this tree. You just append only. And now we have this sub-trade. If we only want to store a constant size of commitment on the blockchain, what we do is we compute a route. The route will be a nested hash of all the subtree peaks and this process we call it a bagging process. So once you have the route, you can do a normal existence proof of any data within this Merkle monorange. How do you do that? That's also very simple. We first do a proof of any of the leaf within the subtree. And once we have that proof, we can also add more pigs within this proof so we can reconstruct the root at the top. And just take a minute to think about what can go wrong in this algorithm if you want to prove any data within any of the leaves. So we must have some assumptions. So let's assume the people who is adding data to this Merkle-Moner range is trusted. So he won't add any malicious data. He won't add a subtree into these leaves. In addition, let's assume you have sufficient validation of the index and the depth of the leaves when you do the proof so that you can easily use another intermediate node as a leaf here. Like for example, the P1 and D1, they are very different nodes. The P1 has depth one, but D1 has depth two. So you can't use the intermediate node in this proof. Even with these constraints, can you break this system? The answer is yes. If we just take one step back and look at this Merkle-Marner range again, we'll find there is actually a hidden tree when you build the route. That makes up of these peaks. So we have three peaks of the subtrees here. And now we are building another Merkle tree with these peaks to the route. And as you can see, we have no validation of the peaks in the previous assumptions. And that means you can easily find a vulnerability here. And here is one example of this attack. So on the left side, this is the Merkle-Marner range. On the right side, I do the attack by just removing the D5 and D6 leave node. And now, as you can see, the P4, the intermediate node, becomes a leave. And more surprisingly, they have the same depth as the node D5. So now if I'm going to prove P4 in the subtree of N1, it will have depth one. That satisfies all the properties we have validated before, but we have never validated the peaks. So this is a trick that now you can prove something non-existing in your Merkle tree. So that's the example I want to show you today, and the summary is very simple. A non-leaf with leaf could be a leaf. So please ensure that you check all the input and validate all the properties you desire in your protocol. Otherwise it could become vulnerable easily. And thank you very much for listening. If you want to know more about chain security and our work",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:40:00.000Z",
      "slot_end": "2024-11-12T05:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1_G-GfGgNMUn5tiiaH-Srat0PLHtYYRNtiVjZwWlxU_c",
      "resources_slides": "https://drive.google.com/file/d/106Up4ALt8E_9hozdiFMGaUJ3oX0zs51D/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "the-combination-of-zkp-mpc-fhe",
      "sourceId": "XPLVT8",
      "title": "The combination of ZKP +/- MPC +/- FHE",
      "description": "This talk will provide you with the necessary intuition to understand when you should use ZKP, MPC or FHE, or any combination of them.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,MPC,fhe,MPC,ZKP",
      "keywords": "FHE",
      "duration": 521,
      "language": "en",
      "sources_swarmHash": "7724dd5759a7e9323aa0eff8393fff2e9afee7739808254312ba965d6a194a18",
      "sources_youtubeId": "Tq7CVqDE_P4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:40:00.000Z",
      "slot_end": "2024-11-12T05:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1iRVxAm1tYqEBlFNUqErTPQ1GCnhG1txvgCWdfQbSgpk",
      "resources_slides": "https://drive.google.com/file/d/1-64x8IsSYka-eRl4bwL--25gw7uB38Y9/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "mpc-tooling-or-how-to-create-mpc-apps",
      "sourceId": "QLMYBD",
      "title": "MPC Tooling or How to create MPC apps",
      "description": "Let's get into the state of the art of MPC development: we'll discuss different MPC schemes, current MPC tooling & how you can create MPC apps today.\r\nWe'll cover the tech stack from a frontend level (e.g. MPC compilers) to a backend - and of course how we can combine them.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Cryptography,MPC,Cryptography,MPC,Tooling",
      "keywords": "Circom-MPC,MPC tooling",
      "duration": 489,
      "language": "en",
      "sources_swarmHash": "8b8ee46fd9725a4ea9ca521e0da429005bef6925bc6bb11dae9ee6fc11c803aa",
      "sources_youtubeId": "eKpcf1JMNak",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:50:00.000Z",
      "slot_end": "2024-11-12T06:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1F2EhWXcgf32_Gh77ty0p18d2rnEPMZymHL7KX7iwSdE",
      "resources_slides": "https://drive.google.com/file/d/1NGZv7lW_PwYWoB_O_shLOwvSq1os6-CH/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "reimagining-layer-0-new-worlds-and-ancient-philosophies",
      "sourceId": "JPHQYQ",
      "title": "Reimagining Layer 0: New Worlds and Ancient Philosophies",
      "description": "Where the early internet was an expression of freedom, liberty, and democratising virtual spaces, etc. Today, our digital spaces are breaking and have not met that promise. The Web3 space also faces scams, degen behaviour, and capture by centralised actors.  How do we guide Ethereum to stay aligned with human values as we build a new world?  Revisiting ancient Asian philosophies can help us reimagine a new world from Layer0.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Political systems,Solarpunk,Regenative Ethereum,value,asian,Coordination,Political systems,Regenative Ethereum,Solarpunk",
      "keywords": "asian,values",
      "duration": 1568,
      "language": "en",
      "sources_swarmHash": "3d225064900625c44f6ace62cf5e21ef0505517583e3365f6e57b9cebb8ddb67",
      "sources_youtubeId": "rhDemdcnVVE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:50:00.000Z",
      "slot_end": "2024-11-12T06:20:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1hKiZ-7BNfUDp8MUrH21ufSaRDdB7UK0-A4X85CDWHvg",
      "resources_slides": "https://drive.google.com/file/d/1A5UDE-8kpzerV4h2aQHSwIF0K4avv00j/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "running-wargames-to-prepare-protocol-teams-for-incident-response",
      "sourceId": "N3DBC3",
      "title": "Running Wargames to Prepare Protocol Teams for Incident Response",
      "description": "SEAL (Security Alliance) Wargames: cybersecurity exercises designed to enhance Web3 protocol resilience. We'll share experiences from running these with major Ethereum protocols, covering:\r\n-Exercise structure: OSINT, tabletops, and live simulations on forked networks\r\n-Scenario designs and common vulnerabilities\r\n-Infrastructure and open-source tooling\r\n-Key learnings and best practices\r\n-Scaling strategies and the importance of regular security drills in the evolving Web3 landscape",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Security,incident,response,Coordination,Security",
      "keywords": "Incident,Response",
      "duration": 1350,
      "language": "en",
      "sources_swarmHash": "702aabf1c42143159cad1c657c1247f880937f9ed6f493fa9a9bdf4323e70723",
      "sources_youtubeId": "mIOEkVh6aGM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6732febd80d989c5b7b49fc9",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T05:50:00.000Z",
      "slot_end": "2024-11-12T06:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1Vl9aDLrFn0_bNTA3ddPbHqxDjrCLUyNEIUn4eBlSNzE",
      "resources_slides": "https://drive.google.com/file/d/1jvvbrebU3zmid3XxLVhEdMAb1rR17Tv7/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "bridging-tradfi-and-defi-through-ethereum-and-evm-a-pathway-to-innovation",
      "sourceId": "8N3GEZ",
      "title": "Bridging TradFi and DeFi through Ethereum and EVM: A Pathway to Innovation",
      "description": "This workshop will explore how Ethereum and EVM-based environments are becoming the critical intersection between traditional finance (TradFi) and decentralized finance (DeFi). It will focus on Ethereum’s maturity for real-world use cases, the role of EVM in regulated environments, and how  institutions, governments, and central banks can foster innovation by leveraging permissionless blockchains while maintaining compliance with regulations.",
      "track": "Real World Ethereum",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Public good,RWA",
      "keywords": "Tokenization,CBDC,RWA",
      "duration": 7255,
      "language": "en",
      "sources_swarmHash": "4e2df7cab1327c5c38a630718f021a8e6ed933319908184e757d3dc88dc49e97",
      "sources_youtubeId": "BrzFVv1aWfA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673310173a168eb535f5a4e4",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:00:00.000Z",
      "slot_end": "2024-11-12T08:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1L-zUhR7NnvpMbCgqVyQBGzA1iXGeJqSDxOxg2F323yI",
      "resources_slides": "https://drive.google.com/file/d/1BAjQDnKcTgLH8M8xmRRCCLJxmJvSxfKt/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "introduction-to-multilateral-trade-credit-set-off-in-mpc",
      "sourceId": "VYD38F",
      "title": "Introduction to Multilateral Trade Credit Set-off in MPC",
      "description": "Multilateral Trade Credit Set-off is a process for collecting outstanding invoices from a network of firms and detecting cycles. A cycle is a circular pattern of due payments that connects businesses. Removing a cycle yields liquidity savings for the firms involved. This process is done by a central agency that collects the invoices and performs the netting. Instead, we leverage MPC to perform the set-ff while preserving the privacy of sensitive financial data of the firms",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "finance",
      "keywords": "MPC,cryptography,finance",
      "duration": 680,
      "language": "en",
      "sources_swarmHash": "7a26e690c86585c39a8f2060e0df78edb94d20dc82bf22ba67b3c85cdc3d2bcb",
      "sources_youtubeId": "OCEEe8azbR8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:00:00.000Z",
      "slot_end": "2024-11-12T06:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1uaHx0jU0Bz-S7lJarLkDXQgyJwYi9XQaoCd5IniQ4ls",
      "resources_slides": "https://drive.google.com/file/d/1noMU0_SkCnjQZcFA2oTGg2KzY3GbO4Tz/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "onchain-capital-allocation-from-current-mechanisms-to-future-possbilities",
      "sourceId": "BEWPLY",
      "title": "Onchain Capital Allocation: From current mechanisms to future possbilities",
      "description": "Capital allocation, from paying bills to complex organizational funding, often suffers from inefficiencies and lack of transparency. Web3 has the potential to revolutionize this by enabling more efficient, effective, and transparent capital distribution. By addressing coordination failures and introducing new onchain strategies, crypto could transform how society allocates resources.\r\n\r\nGitcoin founder Kevin Owocki will articulate this design space in this 20 minute talk.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": true,
      "doNotRecord": false,
      "tags": "Quadratic Voting,Public good,Regenerative Applications,mycofi,Public good,Quadratic Voting,Regenerative Applications",
      "keywords": "Mycofi",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "cd812bc83c5fc28fa42061376bf270b2ffff2f325328e88579375f3fd212fc7b",
      "sources_youtubeId": "3R7ehJ6OGw8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:00:00.000Z",
      "slot_end": "2024-11-12T06:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1-hdTt4ELigY4Pe3nCr4vnQFCDtQaHLB_e-UaHGdXucE",
      "resources_slides": "https://drive.google.com/file/d/1xd41vnH6elifzDdtu1ulcLNd5K8lNcxT/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "remix-jazz-and-blues-jam",
      "sourceId": "P8DPWB",
      "title": "Remix Jazz and Blues Jam",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:00:00.000Z",
      "slot_end": "2024-11-12T08:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1gubgQAcUzwO-7G0PuYDVVhtDoG62piEJsVzXp4Pfrgw",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "whats-going-into-the-pectra-upgrade",
      "sourceId": "9WTJRX",
      "title": "What’s Going Into the Pectra Upgrade?",
      "description": "A talk explaining the core EIPs going into the Pectra upgrade and the core EIPs still TBD for inclusion in Pectra. The talk will also touch on Pectra timing and fork scoping for the next hard fork after Pectra. Finally, the talk will share insights about the governance process of Ethereum in light of Pectra and takeaways about the priorities of Ethereum protocol developers.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "fork,hard",
      "keywords": "Pectra,Governance,Hard forks",
      "duration": 1515,
      "language": "en",
      "sources_swarmHash": "9c19d1c251eda5ae03524a901f817d1fb823edb289430285e2f1c606f649b80f",
      "sources_youtubeId": "ufIDBCgdGwY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:00:00.000Z",
      "slot_end": "2024-11-12T06:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1aEeDer7GTTFvo4hdDKqx3zqCVAtFdk2XqVNuiRomMTc",
      "resources_slides": "https://drive.google.com/file/d/13HFL9PFs43pDYmamE0T0Pi_eMAOwDr0F/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "mp-fhe-experiments-our-learnings-trying-to-find-the-next-big-tech-to-focus-on",
      "sourceId": "9JYWVP",
      "title": "MP-FHE experiments. Our learnings trying to find the next big tech to focus on.",
      "description": "This talk mainly focuses on showcasing the work that some PSE members did while starting to dive into MPC-FHE during Q2 2024. This work is composed by various explorations within the MPC-FHE realm that move towards different directions and goals.\r\n\r\nFrom FHE compilers to FFT Bootstrapping GPU optimization proposals, passing by FHE Game demos and many application level implementations, the talk aims to reach beginner-advanced audience on the research/product paths that we have explored so far.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Homomorphic Encryption,Use cases of cryptography,exploration,Homomorphic Encryption,Use cases of cryptography",
      "keywords": "FHE,MPC,Explorations",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "16b50097b34925bd9c17633e4b231fd78b57d4e01d8c707eca5bc13e7d0b475a",
      "sources_youtubeId": "Didnvmet5Ng",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:10:00.000Z",
      "slot_end": "2024-11-12T06:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12k_WqxuHHHeL-ozPhNdmibpCzBNzvOlF-4z0chDHOyY",
      "resources_slides": "https://drive.google.com/file/d/1J-VtH2lLo-CkZ9ufw7BOdkTUXdksUDoh/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "bringing-an-open-music-protocol-into-the-real-world-onboarding-millions",
      "sourceId": "EJXTFL",
      "title": "Bringing an Open Music Protocol into The Real World: Onboarding Millions",
      "description": "Every month Audius serves millions of artists & fans and hundreds of millions of API requests while growing a fully-decentralized and fully-licensed open music protocol. This talk is about how we achieve real world usage where most users are non-crypto native in a complex industry full of legacy software systems, red tape, and multi-party complexity. It boils down to simplicity, choosing the right tech at the right time, and uniting Web2 and Web3 philosophies.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Sufficient decentralization,Product-market fit,User Experience,adoption,mass,Product-market fit,Sufficient decentralization,User Experience",
      "keywords": "Music,Mass Adoption",
      "duration": 441,
      "language": "en",
      "sources_swarmHash": "ce2604d6133c8452dee58f52c72413f9bf8be020ed9eeb8ffbf5bf634749c74c",
      "sources_youtubeId": "MbjUqblMpsY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:20:00.000Z",
      "slot_end": "2024-11-12T06:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1y9pSagBtZCT-PX46JT90ABDD2x8ACdZNVrHy2b9NaOQ",
      "resources_slides": "https://drive.google.com/file/d/1QZBdeNgRzJqteWLxvECgZQLAJ8Q57tM1/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "introduction-to-hash-based-proof-systems",
      "sourceId": "EUAERD",
      "title": "Introduction to hash-based proof systems",
      "description": "Over the last decade, ZK has been gaining attention due to its applications in verifiable private computation and the scalability of blockchains. The development of general-purpose zkvms powered with STARK/hash-based proof systems have made writing provable applications simpler, abstracting developers from the details of ZK. In this talk, we will explain the basics of hash-based proof systems, different arithmetization schemes and how to prove computations without needing a trusted setup.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Scalability,ZKP,STARK,reed-solomon,Scalability,STARK,ZKP",
      "keywords": "Binius,Reed-Solomon",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "142f2355c580bfe903ae33829f5d32180bcb83382f60682077b05a498df10096",
      "sources_youtubeId": "hd8uYCXUwa4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:20:00.000Z",
      "slot_end": "2024-11-12T06:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/13SZq6cgLNu-xaLH6s8Xx4zOAbocLGeK_vQMElFVIUtU",
      "resources_slides": "https://drive.google.com/file/d/1SzCLj48a7CfRuHIde2kNdS3olPwqUaPR/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "bringing-ethereum-to-1b-telegram-users",
      "sourceId": "GZQ7PE",
      "title": "Bringing Ethereum to 1B Telegram Users",
      "description": "Learn how we use account abstraction to build an invisible wallet for 1 billion Telegram users. We will share our methods and learnings, showing how users and dapp developers will soon seamlessly interact with Ethereum, enjoying a smooth experience without ever realizing they are using a wallet. Features like biometric authentication will enhance user experience, while session keys make transactions invisible, ensuring secure and effortless interactions.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,User Experience,Account Abstraction,integrations,Account Abstraction,Developer Infrastructure,User Experience",
      "keywords": "Session Keys,Invisible Wallets,Integration",
      "duration": 558,
      "language": "en",
      "sources_swarmHash": "4260dbd123f3448c32e94796b2c75769a70901b3a5bf4ec8e0eba1a04e20f4d0",
      "sources_youtubeId": "ul1D6N8qVJU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:30:00.000Z",
      "slot_end": "2024-11-12T06:40:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1zCoYHO3MyfIPj-uA_nWYvHwd9lM6-v7ZDScGGdme-Fg",
      "resources_slides": "https://drive.google.com/file/d/15SiSxz7ZQa7RIaT4DeokgTwZ39QsWu6Q/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "daos-unmasked-the-hard-truths-behind-the-hype",
      "sourceId": "ZSSKBL",
      "title": "DAOs Unmasked: The Hard Truths Behind the Hype",
      "description": "In this talk we will see what a DAO is, what its not and face some hard truths about DAOs and how they are used today.\r\n\r\nDoes a DAO stand for Discord Administered Organization? Is a DAO just a discord chat and a multisig?\r\n\r\nIs a DAO a way for your company to have 2 cap tables, one for your and your investors and one for your community?\r\n\r\nAre DAOs a face for a Cayman Islands foundation which uses decentralization theater to shift liability?\r\n\r\nAre DAOs a way to sidestep regulations?\r\n\r\nLet's find out!",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Governance,Regulation,Coordination,DAO,Governance",
      "keywords": "Decentralization Theater,Regulation",
      "duration": 1670,
      "language": "en",
      "sources_swarmHash": "ecced45caf16ee62282cfbd6014f1bbf67c2b02c548c9d1fee650b8fc8ba5a2c",
      "sources_youtubeId": "pdlMrmUxpSg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:30:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1m4BLa2dYtnZhDK4AVI7x-ufBecnidvE3pGBZchBa82k",
      "resources_slides": "https://drive.google.com/file/d/1mS2lnCqkBjQit39J_YAyDojxS_RJF09M/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "eip-7702-a-technical-deep-dive",
      "sourceId": "NNNPLC",
      "title": "EIP-7702: a technical deep dive",
      "description": "We'll discuss some of the design goals that lead to EIP-7702, how it works, and what will be possible for users after the Pectra network upgrade.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Account Abstraction,eip,Account Abstraction,Core Protocol",
      "keywords": "EIP",
      "duration": 1299,
      "language": "en",
      "sources_swarmHash": "d4c1051f49830760c82a47ec5d0413b0d5fef571e4c09d5a7a0c76f69753c619",
      "sources_youtubeId": "_k5fKlKBWV4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:30:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/15huammnvrT8ljoiAi9Bnn4jcV_r6L0sm3_gBK-LqQ-4",
      "resources_slides": "https://drive.google.com/file/d/1iV1j274j_y7KL7GOTk2YJcE-qQ-WxjoE/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "enter-the-war-room-a-black-swan-simulation",
      "sourceId": "HQSNWQ",
      "title": "Enter the War Room: A Black Swan Simulation",
      "description": "BREAKING: A key Layer 2 sequencer has suffered a complete outage for a brief period! As a consequence, many loans from the protocol DevaLend could not be paid, leading to liquidations and bad debt.\r\n\r\nIn this workshop, you will assume the role of one of the key players in this exciting simulation, and explore how to navigate through it. Propose how to navigate through the DevaLend situation and react as new scenarios evolve and respond to your ideas. Good Luck!",
      "track": "Coordination",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": true,
      "tags": "Layer 2s,Governance,Emergency Plan,conflict,Emergency Plan,Governance,Layer 2s",
      "keywords": "Conflict",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:30:00.000Z",
      "slot_end": "2024-11-12T08:30:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1QJBCSyIk_2YgSpZlJQsuZo3ymwCCG1XhQXN6zxlSBoQ",
      "resources_slides": "https://drive.google.com/file/d/1Z8PjyJpz88NAxOzQkenIcl3BcNBqqTb4/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "modern-zkp-compilers",
      "sourceId": "CV7QXP",
      "title": "Modern ZKP compilers",
      "description": "At PSE we have done much ZKP advanced development. From that learning we are building a language and compiler, that is summarizing much of this learning.\r\nWe answer questions like: Are compilers necessary in a zkVM world? What is the role of a compiler in ZKP development?  What are its most common components? How different ways can this problem be approached?\r\nIn this advanced talk, we will learn how we compile arbitrary boolean expressions, or how the Schwartz–Zippel lemma can be used to optimize",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Languages,ZKP,education,Developer Infrastructure,Languages,ZKP",
      "keywords": "education",
      "duration": 645,
      "language": "en",
      "sources_swarmHash": "ff06f4ba851b1ea9b39cae607b1ef0d62e19962feb32f62ee1611e236c5b5a1c",
      "sources_youtubeId": "JX9YtcG_EHk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:30:00.000Z",
      "slot_end": "2024-11-12T06:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1XmimA6xYE2Wr9c4tzpc9e9P7XDxysFx2QT8rBsA-piQ",
      "resources_slides": "https://drive.google.com/file/d/1NKw7iFLRw_UYR5UCEQQLqExrMwgp4oQx/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "an-introduction-to-post-quantum-signature-schemes-for-ethereum",
      "sourceId": "UDQRWN",
      "title": "An introduction to post quantum signature schemes for Ethereum",
      "description": "In this lightning talk, we will give attendees the opportunity to understand the various post-quantum signature schemes proposed to make Ethereum post-quantum ready.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,Signatures,Quantum resistance,scheme,Cryptography,Quantum resistance",
      "keywords": "Signature,schemes",
      "duration": 575,
      "language": "en",
      "sources_swarmHash": "21ab9203390537d2ac46194c71f63c394cc6151c9464a574c67049bf8f07dcf3",
      "sources_youtubeId": "S3yf7c4GCbk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:40:00.000Z",
      "slot_end": "2024-11-12T06:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1BcDHTCrVf5XicbPd1sVJ8RzyhsCOuPE4DuSSGBO6AA8",
      "resources_slides": "https://drive.google.com/file/d/1osBmPqHTWApFGBOGclj60vOpO_9emG7k/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "who-needs-a-wallet-anyway",
      "sourceId": "ZZKKRZ",
      "title": "Who needs a wallet anyway?",
      "description": "This talk confronts the community’s obsession with decentralization purity at the cost of usability. This session explores how to hide the complexities of crypto, enabling seamless integration for users who may not even realize they are using a wallet. We’ll cover simplifying user interactions, making wallets function invisibly, maintaining benefits like permissionless innovation, managing thousands of wallets, and real-world applications. It’s time to push for real, user-friendly innovation.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Permissionless,Developer Infrastructure,Decentralization,Environment,User Experience,trusted,wallet,execution,Developer Infrastructure,Permissionless,User Experience",
      "keywords": "Trusted,Execution,Environments",
      "duration": 555,
      "language": "en",
      "sources_swarmHash": "dcba0214c791f887977ae84378b09be85862162e256dc4fea0db787f53e98d83",
      "sources_youtubeId": "iNLHWc5toYo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:40:00.000Z",
      "slot_end": "2024-11-12T06:50:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1pVk3HgI3jY_eVj3C7F4jVkcdwrwbVFi9NzWDCgBBUFg",
      "resources_slides": "https://drive.google.com/file/d/1G8Xm4-jRcL7DkEuY_Vffwk-nLnNBELVd/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "building-with-intention-achieving-system-qualities-through-design-choices",
      "sourceId": "DSXM73",
      "title": "Building with Intention: Achieving System Qualities through Design Choices",
      "description": "Technical and design decisions should be viewed as means to achieving broader system qualities rather than ends in themselves. This talk reorients our focus on the underlying goals of these decisions, exploring why we build the way we do, what we aim to achieve, and whether there are better ways to reach comparable outcomes. Through examples and case studies, attendees will learn to critically evaluate their design choices and understand the broader implications of their technical strategies.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Design",
      "featured": true,
      "doNotRecord": false,
      "tags": "Protocol Design,Design,Design Thinking,Mechanism design,system,Design Thinking,Mechanism design,Protocol Design",
      "keywords": "system,design",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "b69873dbca0293348685caa12fbec0dbf646359f0a4b622f237780c174f9d7f6",
      "sources_youtubeId": "ZPVXghNFj1I",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:50:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1o2XT-zv9XnvPAfdhkvLLqa8zJtsbRxJ_E9wOKaoe6WU",
      "resources_slides": "https://drive.google.com/file/d/1rJQFDAy9EjPIKCbbzRsAyARjAivymAfT/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "stark-proofs-eli5",
      "sourceId": "BKTYWY",
      "title": "STARK proofs ELI5",
      "description": "Let's face it, ZK proofs are intimidating. But they don't have to be!\r\nZK proofs are complex not because of the depth math they use, but because of the large number of fields of mathematics they leverage features from.\r\nIn this talk, we'll break down STARK proofs into simple blocks and colorful analogies so that you get a good high level overview of how they work",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Use cases of cryptography,STARK,eli5,STARK,Use cases of cryptography,ZKP",
      "keywords": "ELI5",
      "duration": 496,
      "language": "en",
      "sources_swarmHash": "69d7d8817a7c0b608f741bd14a6d7e15b142dcc69b50fdaa2c91f7cf3ff65161",
      "sources_youtubeId": "eHPp8mFCS6E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T06:50:00.000Z",
      "slot_end": "2024-11-12T07:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1wuFB_JXv5HWJjXdbPmQNAk43TRxm_cDU9haSzPCxKco",
      "resources_slides": "https://drive.google.com/file/d/1LOYGnKxMC1rdJV_RHo560EiTRV7PuTYl/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "cls-formal-verification-hangout",
      "sourceId": "ZTHE3N",
      "title": "[CLS] Formal Verification Hangout",
      "description": "A low key, informal, self-organized event hosted within the Devcon venue* to explore interesting topics in Formal Verification.\r\n\r\nThe event will be casual, with minimal talks/programming, and geared towards facilitating discussions and allowing researchers to connect with others in the field.\r\n\r\n​Agenda\r\n\r\n​2:00 - 2:15 – Welcome\r\n2:15 - 3:30 – Fishbowl Panel\r\n3:30 - 4:00 – Break\r\n4:00 - 5:00 – Lightning Talks**\r\n5:00 - 6:00 – Discussion Groups\r\n6:00 onwards – Informal Discussions",
      "track": "[CLS] Formal Verification Hangout, FV Team",
      "type": "Mixed Formats",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Formal,Verification",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "breakout-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1KG701RsIoq1QyT_uxs6WdIDJVZLJz0WL2Zcdl1b-gzg",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-2",
        "name": "Breakout 2",
        "description": "",
        "info": "2",
        "capacity": 100,
        "youtubeStreamUrl_1": "",
        "youtubeStreamUrl_2": "",
        "youtubeStreamUrl_3": "",
        "youtubeStreamUrl_4": "",
        "translationUrl": "https://stm.live/Breakout-2"
      }
    },
    {
      "id": "do-you-really-know-your-web3-users",
      "sourceId": "YRDFDY",
      "title": "Do you really know your web3 users?",
      "description": "Product discovery is to understand users' problems and using that knowledge to build a product. In the world of Web3, where anonymity & privacy prevail, how can teams identify user segments & collect relevant data to understand behaviours behind accounts? As we aim to onboard the next billion web3 users, how should we approach activation & growth, considering best practices and emerging trends? This panel will explore strategies for effective product discovery in a privacy-centric ecosystem.",
      "track": "Usability",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Product-market fit,User Experience,UI/UX,User Research,product,discovery,Product-market fit,UI/UX,User Experience,User Research",
      "keywords": "Product Management,Strategy,Product Discovery",
      "duration": 3425,
      "language": "en",
      "sources_swarmHash": "ae5d589708b7deb49fa418329e2b03c6b8b14d698f9e4c29bd4e4a97b2b285b0",
      "sources_youtubeId": "KSfkX-dGskg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673420ce9dbb7a90e172caa6",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T08:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1NT9-QOOV4dbn06g_FMOVREI8em-zEVjMVNnJ2DBkCuc",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "eip-7251-maximum-effective-balance-overview",
      "sourceId": "BBFNLG",
      "title": "EIP-7251 - Maximum effective balance overview",
      "description": "An overview of the  maximum effective balance change coming in Electra.\r\nAt a high level, other considerations that were required to allow the maximum effective balance increase in Electra, and ensure that it delivers value.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Staking,Pectra,Core Protocol,Staking",
      "keywords": "Pectra",
      "duration": 1218,
      "language": "en",
      "sources_swarmHash": "7232962eceb9c9b07027a3ebb1759835c57a4c1aacf89e245dbceaca4a6ae4dc",
      "sources_youtubeId": "EwW6dNi9VCY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T07:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Q5srMGhMm8grwI_O0CFKN_QN1QRx24-AxIwgbDha6U0",
      "resources_slides": "https://drive.google.com/file/d/1S13Yqk1IjwpO7lSpWk2sO4VyGjL2CHVx/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "l1sload-in-action-write-l2-dapps-that-read-from-l1-state",
      "sourceId": "ERQ7N3",
      "title": "L1SLOAD in Action: Write L2 Dapps that Read from L1 State",
      "description": "In this workshop we will explore some interesting new use cases unlocked by the newly proposed L1SLOAD precompile (RIP-7728). We will develop and deploy L2 dapps that read from L1 state using this precompile.",
      "track": "Layer 2",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,DevEx,Rollups",
      "keywords": "RIP,L1SLOAD,Precompile",
      "duration": 5050,
      "language": "en",
      "sources_swarmHash": "4eb65f6c60b1496a7551b229ed23560c241cd3229d0222453f48e4d1c7d143fc",
      "sources_youtubeId": "FhvJPUcD6go",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733123b3a168eb535f7b817",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T08:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1bocSfX9_K930B6knXp5J9HUDPwUP0hIP5UeWRegKZ_E",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "liquid-staking-for-daos",
      "sourceId": "ZV39SQ",
      "title": "Liquid Staking for DAOs",
      "description": "DAOs face a critical challenge: aligning token holder interests with long-term success while maintaining effective governance. This talk explores the tension between governance participation and financial gains, as well as the dangers and opportunities posed by restaking protocols using DAO tokens. We'll examine how misaligned incentives can compromise DAOs and discuss innovative solutions like liquid staking and token splitting.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Best Practices,Mechanism design,Best Practices,Coordination,Mechanism design",
      "keywords": "DAOs",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "af4dfb7196b3f28598932bfa68f572203f1aa20534f0498a56d483f964c3c0ba",
      "sources_youtubeId": "94oHo7AORak",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T07:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1o6QVDTmx3Wki_7YsSzzIkIb_lvDouMYuQyMpQ98lqww",
      "resources_slides": "https://drive.google.com/file/d/1egL6My2goPKpL3mjWJx3OgN9B7Wg_HWs/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "maci-why-do-we-need-private-voting-and-what-are-we-up-to",
      "sourceId": "TCJJW3",
      "title": "MACI - Why do we need private voting and what are we up to",
      "description": "MACI is a protocol that can be used to run private on chain polls. This talk will introduce the protocol, dive into some of the technical aspects. Finally we will talk about the team's plans for the future and how the community can get involved to help improve the project.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Quadratic Voting,Public good,voting,Coordination,Public good,Quadratic Voting",
      "keywords": "Privacy,Voting",
      "duration": 606,
      "language": "en",
      "sources_swarmHash": "3e12944268e30652d72b931cdbdd1bf68e19741b4d3f57dd9daf2464127f2dd6",
      "sources_youtubeId": "18KFAia72Ww",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T07:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1paq5inxTY__nUEseJKES2bwcdoZZSvs-h5ZpEXOfwsg",
      "resources_slides": "https://drive.google.com/file/d/1si1cJtSDiv7B1C8w4Z03ivCb46ikn9hq/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "opening-circle",
      "sourceId": "T7THRV",
      "title": "Opening Circle",
      "description": "By master Zoe\r\n(Opening Session)\r\n- Nervous system check-in (to communicate safety and help people settle into the space)\r\n- Short check-in: guided meditation, breathwork, and gentle stretches (approx. 5 minutes) to bring everyone into the present moment\r\n- Intention setting for the conference, guiding participants to align their energy and time with their vision\r\n- Sharing intentions in small groups (3-5 people) to build community connection\r\n- Closing with a gratitude practice\r\n\r\n12 Nov 14:00 - 14:45",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:00:00.000Z",
      "slot_end": "2024-11-12T07:45:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/1n226DY0rUYiKnECT9xm9IZ_yu2qSeuhOfgg63eVqUM0",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "the-blind-mans-elephant-a-product-vision-towards-private-identities",
      "sourceId": "GSZKVK",
      "title": "The Blind Man's Elephant: a product vision towards private identities",
      "description": "A short talk introducing the concepts of key properties we want to achieve in private ZK identities. Sparkling concepts like SSI and DIDs and why blockchains are the best way to ensure that.\r\n\r\nFinally it concludes with simple ZK and data-structure constructions and different alternatives that are seeking to provide this characteristics.\r\n\r\nIn short, this is a lightning overview of the space, it's desired features and different approaches to achieve them.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Identity,ZKP,Use Cases,selective,disclosure,Identity,Privacy,Use Cases,ZKP",
      "keywords": "Selective-disclosure",
      "duration": 706,
      "language": "en",
      "sources_swarmHash": "849d3e4fd5ed45afc927a10bae59624aead23e6e86dad6d8ff724046c4df13b9",
      "sources_youtubeId": "-BESF3MUM20",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:10:00.000Z",
      "slot_end": "2024-11-12T07:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1OM2zZQsD8haiBnMdAS98Oz90Cmk3F2nH7dY0H_hjKTA",
      "resources_slides": "https://drive.google.com/file/d/1cBG-vGfpn9lGNM3Q02luBsmrhfOuSU3h/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "mpcstats",
      "sourceId": "ND3S9R",
      "title": "MPCStats",
      "description": "MPCStats is a framework allowing data consumers to query statistical computation from either one or multiple data providers while preserving privacy to those raw data. We support standard statistical operations, including nested and filter ones. Data providers do not leak their data and data consumers can be convinced the computation is done correctly.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Privacy,MPC,Public good,verification,computation,MPC,Privacy,Public good,Tooling",
      "keywords": "privacy-preserving,data analysis,MPC,statistics,verifiable computation",
      "duration": 508,
      "language": "en",
      "sources_swarmHash": "9b8211a5308190cf41598cd33cefed8af79e239f4d4c5a6648a32a2cbcf77f51",
      "sources_youtubeId": "wCp7Zsjou7w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:20:00.000Z",
      "slot_end": "2024-11-12T07:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/10sZNPm9ETDOiRts7vDo9aVWovdRE2PpqvKAxR6_9Lv8",
      "resources_slides": "https://drive.google.com/file/d/1-bybulVBrLQLhna46XkTSP5yVxlCF_zJ/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "privacy-preserving-groups",
      "sourceId": "LSA3JK",
      "title": "Privacy-Preserving Groups",
      "description": "This talk will explore the concept of privacy-preserving groups and the challenges associated with managing them. It will cover different ideas to add anti-sybil mechanisms to enhance group security and trust. The presentation will also highlight real-world projects working on it and provide practical use cases to illustrate their application and impact.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,DAO,Privacy,Anonymity,Identity,Open Source Software,ZKP,Zero-Knowledge,Use cases of cryptography,Public good,User Experience,groups,Anonymity,DAO,Identity,Open Source Software,Privacy,Public good,Tooling,Use cases of cryptography,User Experience,Zero-Knowledge,ZKP",
      "keywords": "Groups",
      "duration": 464,
      "language": "en",
      "sources_swarmHash": "18b4db550bcad65fa27ad24340bd8c75da7a5d04c9944d8303de3e690ebbdaf8",
      "sources_youtubeId": "dWQWoqJVfn8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:30:00.000Z",
      "slot_end": "2024-11-12T07:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/13v7xDojqK_R5sq5GZJLvGNitJNJ0JqrztXhZYzs0pXM",
      "resources_slides": "https://drive.google.com/file/d/1xrvM_TgYGCccvpk-3V-y0JVz5DxZp-bE/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "understanding-eip-7002-and-eip-6110",
      "sourceId": "KPD8HB",
      "title": "Understanding EIP-7002 and EIP-6110",
      "description": "The first part will be an overview of EIP-7002, explaining how it works, why adding this extra option to exit validators is important, and addressing some of the UX challenges of this approach. The second part will be a technical overview of EIP-6110, explaining the UX improvements for validators depositing on the beacon chain, the removal of pre-merge technical debt as well as a quick look at the EIP implementation in Teku.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking",
      "keywords": "EIP,validator,staking",
      "duration": 1495,
      "language": "en",
      "sources_swarmHash": "5e5addf0da8b7cde13a38f9d5bf27a477cb4b61980091c63038ec72253663a34",
      "sources_youtubeId": "EyDChjFQEkQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:30:00.000Z",
      "slot_end": "2024-11-12T08:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/13NjraDw6-VLGwVGpYUmZprFK68Rq7uVHZ7yVIgSx7Q0",
      "resources_slides": "https://drive.google.com/file/d/1-FGeFm390sMVKLb3dMTYJ_tY3OspsSkS/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "semaphore-v4",
      "sourceId": "ZU9D8U",
      "title": "Semaphore V4",
      "description": "Semaphore is a protocol enabling individuals to prove group membership and send messages (such as votes or endorsements) anonymously. The latest version enhances efficiency and simplifies the use of libraries and contracts. This presentation will cover the new features, project vision, and the importance and challanges of zero-knowledge technologies.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Zero-Knowledge,User Experience,proof-of,membership,Privacy,User Experience,Zero-Knowledge",
      "keywords": "semaphore,anonymity sets,proof of membership",
      "duration": 1035,
      "language": "en",
      "sources_swarmHash": "619dc838e91326f82a78ebd1207f07fa45e9941e162c7999de38f6d08fee6691",
      "sources_youtubeId": "OErC2MyIKjY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:40:00.000Z",
      "slot_end": "2024-11-12T07:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12uKp51aS4tQMokLfQJRDQlh518PRLNinkH3148Cq9Do",
      "resources_slides": "https://drive.google.com/file/d/1Nj0EM_QtP6Fbu2pGvbgXkl-l2cmQ1nSH/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "mopro-make-client-side-proving-on-mobile-easy",
      "sourceId": "BZWFEM",
      "title": "Mopro: Make Client-side Proving on Mobile Easy",
      "description": "Mopro is a toolkit for ZK app development on mobile. Mopro makes client-side proving on mobile simple. Mopro aims to connect different adapters with different platforms. In this talk, we will share:\r\n- How to use Mopro to develop your own ZK mobile app.\r\n- What is the current development progress, including the current supported proving systems, supported platforms, and mobile GPU exploration results. \r\n- Moreover, we will share the challenges that Mopro faces and our future roadmap.",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Cryptography,Mobile,android,Cryptography,Mobile,ZKP",
      "keywords": "iOS,Android",
      "duration": 958,
      "language": "en",
      "sources_swarmHash": "83f2fcfab64a4052bdaa28b2c9f33ae4f5a4bccdd8fdc70865019c8ab568a649",
      "sources_youtubeId": "0ziKiYwhJHk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T07:50:00.000Z",
      "slot_end": "2024-11-12T08:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1usTBzr557w8yMObkzJBvScKjnAoHQFztqym-wk6b1dk",
      "resources_slides": "https://drive.google.com/file/d/1xM2-Vbj6dDtLfGIGo9bx72Z30hArdWIg/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "a-new-cypherpunk-generation",
      "sourceId": "3H39WT",
      "title": "A New Cypherpunk Generation",
      "description": "How does the 30 year old cypherpunk manifesto compare to our culture now? Is it the safe, or has it lost its relevance?\r\n\r\nThis talk is giving an insight into the current hacking/cracking/pirating culture that is thriving on the internet alongside culture that is cypherpunk. \r\n\r\nA visibility inside a (white&blackhat) environment where people are moving, and what do they do to \"make\" a living. \r\n\r\nWhat can we learn from them? And how can make sure that we do not become the goliath we all hate,",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": true,
      "tags": "Anonymity,Free Speech,Use cases of cryptography,cracking,Anonymity,Free Speech,Use cases of cryptography",
      "keywords": "Grayhat/Blackhat,Social Engineering,Cracking,Cypherpunk",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T08:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1dJRUdeeSAm3IMKjIUvUMfYutPBeDhcB5ai34k_UGutY",
      "resources_slides": "https://drive.google.com/file/d/1S6u5VfaRmN5SbE5lfAYzEqUB_oWZPMFs/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "austin-griffith-intro-to-scaffold-eth-2-and-speedrunethereum",
      "sourceId": "MCVDPP",
      "title": "Austin Griffith –  Intro to Scaffold-ETH 2 and SpeedRunEthereum!",
      "description": "In this session, we’re going to dive into Scaffold-ETH v2, a toolkit we built to make developing on Ethereum way easier. We’ll take a look at how it works, why it’s super useful for quickly spinning up projects, and how we can use it to tinker with smart contracts and hook up the frontend. \r\n\r\n⚙️ Scaffold-ETH 2 is built using NextJS, RainbowKit, Foundry/Hardhat, Wagmi, Viem, and Typescript.\r\n\r\nWhether you’re new or experienced, we’ll get you to SpeedRunEthereum!",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Open Source Software,Public good,Tooling",
      "keywords": "Developer,onboarding",
      "duration": 5979,
      "language": "en",
      "sources_swarmHash": "9f682f049726c2298f59c88f7e57aad30cf83d73869108354243c000485f2aeb",
      "sources_youtubeId": "_VWF2fiXDPQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1obbj2bv-axqq0YxZpjvXmMW2LkNzMztiVyIBfdu3Z6Q",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "crypto-twitter-is-wrong-this-is-how-rollups-really-work",
      "sourceId": "YNBTQR",
      "title": "Crypto Twitter is Wrong: This is How Rollups *Really* Work",
      "description": "It's 2024, L2s are a critical part of the Ethereum scaling roadmap, and everyone *still* gets Rollups completely wrong. If you think that Optimistic Rollups and ZK Rollups are real things, that Rollups need sequencers to create blocks, or that Rollups need proofs to be secure, you've been completely and utterly bamboozled by the Crypto Twitter intelligentsia. It's time we take back the truth - this is How Rollups *Really* Work.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Layer 2s,Rollups,explainer,Layer 2s,Protocol Design,Rollups",
      "keywords": "Explainer",
      "duration": 1311,
      "language": "en",
      "sources_swarmHash": "6ef1847de49946125226649f6d7f43cc8bb2186fc9f0880a95d2fb7cceaf091d",
      "sources_youtubeId": "c1IbglrscSU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T08:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1Zq5DAdb9ha3cFF-gOzk6L82ORlY9uvzFl7T5sV1W2mg",
      "resources_slides": "https://drive.google.com/file/d/1NV0mr4e5x0vJ-BznUx5j8Xvb3iqQvidK/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "keynote-programmable-cryptography-and-ethereum",
      "sourceId": "MQ8T8Z",
      "title": "Keynote: Programmable Cryptography and Ethereum",
      "description": "Programmable Cryptography is a \"second generation\" of cryptographic primitives - primitives that allow arbitrary programs to be executed \"inside of\" or \"on top of\" cryptographic objects. Programmable cryptography provides three key affordances that complement and amplify the affordances of Ethereum--verifiability, confidentiality, and non-interactivity. We'll discuss how these technologies can reshape the Internet over the next 50 years.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "Cryptography,Use cases of cryptography",
      "keywords": "Programmable,Cryptography",
      "duration": 1517,
      "language": "en",
      "sources_swarmHash": "e13e6bd7be8fffa7336eb9daa88cf857ddb07345077867d9a45fa4fda0586ac9",
      "sources_youtubeId": "UWPg_AmWtlw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T08:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1xCnHIn3N6_CE75tyV-Jo2eMU07wZIBXFedFxwrk7xf4",
      "resources_slides": "https://drive.google.com/file/d/1FsDYkbfv0MstTDS-NlxxkvtPyiR-Fq2V/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "scaling-ethereum-with-das-an-iterative-approach",
      "sourceId": "JFWPRG",
      "title": "Scaling Ethereum with DAS: an iterative approach",
      "description": "In this time between the launch of 4844 and the possible launch of a first version of PeerDAS, we explore and explain the iterative approach that has been employed in the rollout of blobs and DAS to Ethereum, and discuss the past and future steps.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Blobspace,Data Availability,Ethereum Roadmap,Scalability",
      "keywords": "PeerDAS",
      "duration": 1522,
      "language": "en",
      "sources_swarmHash": "567a45d310dc81275e061b69797f55ce5386ac2d95acdaf5d71076c274539d71",
      "sources_youtubeId": "toR2UKzE_zA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T08:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1AIOGsICQD3wWyrBZ5kDP7FX-hHDQ53lT_n8M7Jdl_kI",
      "resources_slides": "https://drive.google.com/file/d/1DlRtJr-dFvuzL-C1inJa5SY6_xs7x-vB/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "web3-poetry-day-1",
      "sourceId": "VDMFMR",
      "title": "Web3 Poetry - Day 1",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1YlWqriBn80NWKxkgkyOqcJWz0Dtul50_teTrfXcFHJA",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "web3-user-research-101",
      "sourceId": "7YZGVW",
      "title": "Web3 User Research 101",
      "description": "Everything you’ve wanted to know about talking to users in web3 and were too afraid to ask! This workshop will give participants a crash course in user research and UX first principles, then guide them through the process of conducting a research project from start to finish - with a focus on web3 users specifically.",
      "track": "Usability",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Design",
      "featured": true,
      "doNotRecord": false,
      "tags": "Best Practices,User Experience,UI/UX,User Research,Design Thinking,101,Best Practices,Design Thinking,UI/UX,User Experience,User Research",
      "keywords": "101",
      "duration": 6420,
      "language": "en",
      "sources_swarmHash": "27a77bf4fed4058eaa45474e58169a82061e00054ba64ee5ceaf5efbec8dd25e",
      "sources_youtubeId": "--SIpz6SfAo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733f1f63a168eb5353dea86",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:00:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1WDegVtKo7rojZIBJT9EVkbEcih7LrcH0QIwcJFOGr6Y",
      "resources_slides": "https://drive.google.com/file/d/1vPKxU0_5aZ_feGt6TJfEr2htRifMHvBA/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "a-practical-introduction-to-prediction-markets",
      "sourceId": "BECMXA",
      "title": "A practical introduction to prediction markets",
      "description": "In this workshop, attendees will participate in \"play money\" prediction markets on fun events which will happen during the workshop. They will discover how prediction markets work and how they can be used as an information aggregation or decision tools.\r\n- Mechanisms of conditional tokens\r\n- Categorical markets\r\n- Scalar markets\r\n- Multiscalar markets\r\n- Revelation loss problem\r\n- Conditional markets\r\n- Futarchy",
      "track": "Real World Ethereum",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Futarchy,Collective Intelligence,Economics,market,prediction,Collective Intelligence,Economics,Futarchy",
      "keywords": "Prediction,Markets",
      "duration": 6725,
      "language": "en",
      "sources_swarmHash": "647dac106bf5a731dbf576fdf876b6ca959feb907dddca93ad2c9b2489c5ea2f",
      "sources_youtubeId": "N3xal0a1-Q8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d0069dbb7a90e121ba96",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1CNiy8pyXPgFrGk4YOJEIIwWPxSTEpivis_7GsXcdpdw",
      "resources_slides": "https://drive.google.com/file/d/1b-21Mv2xFk1qbcVoliQNps5i2zQDai45/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "building-developer-communities-101",
      "sourceId": "JXTVQA",
      "title": "Building developer communities 101",
      "description": "Even though there are a large number of communities in the web3 space, most of them get into the events trap and lose out on building an organic builder community.\r\nIn my lightning talk, I will cover a series of tips and tricks from my experience of building developer communities.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevRel,Best Practices,builder,community,Best Practices,DevRel",
      "keywords": "community,building",
      "duration": 464,
      "language": "en",
      "sources_swarmHash": "c40c5e3977516472e2a3c6acc41741648c8ab34684462e462f11d84b20326bea",
      "sources_youtubeId": "2og2d0Xxc3I",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T08:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1jkv8pKTuI6m3JNnCg4Deeyb6gn7FhIcRIC_N_qPtTsY",
      "resources_slides": "https://drive.google.com/file/d/1d1m2uDgsdfnZCd9GCNBFmp2eqngHNs5p/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "cultivating-the-understory-building-resilient-daos",
      "sourceId": "NRHH9B",
      "title": "Cultivating the Understory : Building Resilient DAOs",
      "description": "Let's explore the overlooked \"understory\" of DAOs and teams: the human layer that forms the foundation of successful decentralized governance. While much attention is given to the technical and structural aspects of DAOs (the \"overstory\"), we'll dive into the cultural, social, and distributed leadership elements that are crucial for the longevity and effectiveness of anything we build.\r\n\r\nThemes: DAO Ecology, Decentralized leadership, Coding culture DNA, Biomimicry for Governance",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": true,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Vision,resiliency,Coordination,DAO,Vision",
      "keywords": "Culture,Resilience",
      "duration": 1569,
      "language": "en",
      "sources_swarmHash": "0536080797b46eb6a52445d16070f665d69e68664ce0253bfed99069d746be0d",
      "sources_youtubeId": "274Uyrxv6uI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/15uqb6bZbGBerAG0KgTCVf2KHzFimQ1D5YSJ8jUna96c",
      "resources_slides": "https://drive.google.com/file/d/1xK1Ig499KHKUofOKpVgT2W_xD1biJh61/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "evm-object-format-eof-history-and-motivation",
      "sourceId": "SEUZGU",
      "title": "EVM Object Format (EOF) - History and motivation",
      "description": "EOF is one of the important parts of the upcoming Pectra upgrade, delivering long-standing feature requests to the EVM. This talk aims to provide insight into its history, significance, and role in Ethereum and EVM improvement, and explore the rationale for including it in the next upgrade, its potential impacts and implications, as well as long-term advantages and possible challenges.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,developer,experience,Core,Protocol",
      "keywords": "EVM,Developer Experience",
      "duration": 1503,
      "language": "en",
      "sources_swarmHash": "f05d6e3e2b2f1bbc704ce9e98664b8dac849778797f474d69fa7dd09e007a496",
      "sources_youtubeId": "X2mlptWzphc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1V-NCCshtl60AkHuWhlJDZ4XszkThsUvFQ_L8gM-hAro",
      "resources_slides": "https://drive.google.com/file/d/1zJLa4mjTTFCHx2tL0Xu-pAbtfy2GyRiV/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "financial-nihilism-vs-foss-culture-the-battle-for-ethereums-soul",
      "sourceId": "SSXAMG",
      "title": "Financial Nihilism vs FOSS Culture: The Battle for Ethereum’s Soul",
      "description": "In recent years, the Ethereum ecosystem has witnessed a stark dichotomy: the rise of financial nihilism through memecoins and rampant speculation on one side, and the foundational principles of the FOSS (Free and Open Source Software) community, emphasising public goods, interdependence, and intrinsic rewards, on the other. \r\n\r\nThis talk will delve into the experiences of interacting with FOSS developers, shedding light on their views and concerns regarding Ethereum’s current trajectory.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Values,FOSS,Decentralization,culture,Decentralization,FOSS,Values",
      "keywords": "Culture",
      "duration": 1584,
      "language": "en",
      "sources_swarmHash": "d2fa049d664484b158c36db2c05b9ff461267f4ee44787a45a7ba182dabe07fc",
      "sources_youtubeId": "Q_bsYpVfhHs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Qlvu4fLzJTTaotuNmKf4QNZRg5u7Im3WjKq6D2kS0Vw",
      "resources_slides": "https://drive.google.com/file/d/1i7CzB6NvxJVj1UN__gl7M6rTgJ3PVW6U/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "open-challenges-in-mini-apps-and-frames",
      "sourceId": "TZDRPY",
      "title": "Open challenges in Mini-apps and Frames",
      "description": "There are a number of open challenges we've run into with trying to make interoperable mini-apps work at Open Frames. I'll run through some of them and what I think it'll take to get great UX via Mini-apps.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Social,UI/UX,frames,Social,UI/UX",
      "keywords": "frames",
      "duration": 480,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "-LlOzt951z8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T08:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/10NeCTKHHZ_IznsD0BVvBmKLhLozti5XPFkZHUhhk45M",
      "resources_slides": "https://drive.google.com/file/d/1MA6kr1rqx0MMROyZjFUwScQiP_APVOfK/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "privacy-first-cbdcs",
      "sourceId": "TWMAWN",
      "title": "Privacy-First CBDCs",
      "description": "This talk explores how central bank digital currencies (CBDCs) can leverage zero-knowledge proofs (ZKPs) and Ethereum to create privacy-centric monetary systems. We'll examine how ZKPs enable robust AML/CTF compliance while preserving user privacy, discuss the benefits of Ethereum deployment for financial inclusion and innovation, and showcase how these technologies could revolutionize digital currency design. Future CBDCs can and should offer unparalleled privacy, security, and functionality.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Lobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "Payment,Privacy,Zero-Knowledge",
      "keywords": "CBDC",
      "duration": 1538,
      "language": "en",
      "sources_swarmHash": "2fcc5d2003328d6a5974f449d4235f5bb8901d112e31c10318763719e26e7e96",
      "sources_youtubeId": "zNgmBX0c8yo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673321023a168eb5354cf6cb",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1yAUh-BkJ1oE5n2L_-NknKAtAJ9okKkjhrA-_VvME4rw",
      "resources_slides": "https://drive.google.com/file/d/1-V9S_6Mf_j8rlh7h6L0cjPVl9xUoSG5T/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "programmable-cryptography-and-ethereum-panel",
      "sourceId": "MWKMBQ",
      "title": "Programmable Cryptography and Ethereum, Panel",
      "description": "One of the core themes of this panel is how Programmable Cryptography synergizes with Ethereum. Panelists will discuss questions such as ''Why have we not been able to do everything we've wanted with Ethereum?'' and ''Why have certain kinds of applications - from decentralized social to decentralized games to decentralized finance - not been able to reach their full potential with only consensus technology?''",
      "track": "Applied Cryptography",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "Programmable Cryptography,ZKP,MPC,FHE,ORAM,Obfuscation,Panel,0xPARC",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "yCOCZhkDmnc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/17ZRAYhS4Uh4J1-UKAL-2OFQwRR2N0dQ4bBZOVPIoYQU",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "public-private-hybrid-rollups",
      "sourceId": "YUFEJK",
      "title": "Public-Private Hybrid Rollups",
      "description": "We posit that it is a best practice that rollups have privacy capabilities. We'll focus on zero-knowledge and its role in enhancing privacy and how to deal with the need for public state for shared use cases. We'll delve into the interaction between public and private execution environments, detailing how such disparate execution environments can be combined.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zk Rollups,Token bridging,Privacy,best,practice,Privacy,Token bridging,Zk Rollups",
      "keywords": "hybrid rollups,privacy as a best practice",
      "duration": 1396,
      "language": "en",
      "sources_swarmHash": "2e6b811ad2567c4e1aca22ebd687a47279b5f1ce00313a27a958d3092402370e",
      "sources_youtubeId": "0mDlVkzde_M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:30:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/11nsntpn_PkweY9PIGZYHntFGei0Pk5LLe9J12awK9K4",
      "resources_slides": "https://drive.google.com/file/d/1Zx3-C0WpnelZq1FoCgIgLQi0SY0UCyqG/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "building-a-developer-empire-unitas-et-innovatio",
      "sourceId": "UPLSUN",
      "title": "Building a Developer Empire: Unitas et Innovatio",
      "description": "Friends, Romans, builders! We're on the cusp of a new era in developer collaboration. Let's focus on:\r\n\r\nDocumentation cult: Aiming for Stripe-level docs in Web3\r\nOpen Source: Democracy in Action\r\nTrials of the Hackathon: Lessons from blockchain assemblies\r\nDevRel metrics: Tracking what truly matters in Web3\r\n\r\nOur goal: Build a lasting developer empire through creation and community. Together, we'll shape the future of decentralized technology.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,DevRel,Best Practices,developer,community,Best Practices,DevEx,DevRel",
      "keywords": "documentation,developer community",
      "duration": 354,
      "language": "en",
      "sources_swarmHash": "a0388763c7d4f9679072784395aa6173df82ec8b9de2b5f41688cb91bea675cf",
      "sources_youtubeId": "6FXFJxyjGUI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:40:00.000Z",
      "slot_end": "2024-11-12T08:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1D7C6aTr2ZEkqegToHCcpPBpSCVDYOFShDiu3eia6D9U",
      "resources_slides": "https://drive.google.com/file/d/1HCWFa9gJcLD1Fv5nJ0yX9rYiijt8ie9p/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "farcaster-frames-building-embeddable-ethereum-apps",
      "sourceId": "NPGET3",
      "title": "Farcaster frames: building embeddable Ethereum apps",
      "description": "Frames are an open standard for creating embeddable, interactive apps in social media feeds and on the web. They help solve one of the hardest problems for Ethereum dapp developers: distribution. Although frames originated on Farcaster, it's now possible to build cross-platform frames that work on Farcaster, Lens, XMTP, and the open web. In this hands on workshop we'll introduce the core concepts behind frames and build a simple frame app that interacts with a smart contract.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Social,farcaster,Developer Infrastructure,Social",
      "keywords": "Farcaster",
      "duration": 5086,
      "language": "en",
      "sources_swarmHash": "8e0e0c17254242e8c66955524eb158e4655137ffbc89bd6592179981209be316",
      "sources_youtubeId": "LnEpR575FRA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:40:00.000Z",
      "slot_end": "2024-11-12T10:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1liuvnLXBUAB0kNGDh3VePfZNkfZ-ECHpzPYsSrv_d-M",
      "resources_slides": "https://drive.google.com/file/d/1JiBcsKGvRN0eGITIl7aEACqQ5QDOhBJv/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "scaling-community-lessons-from-building-base",
      "sourceId": "P73W8S",
      "title": "Scaling Community: Lessons from Building Base",
      "description": "Drawing from experiences as a Base core contributor and Base community lead, this talk is about building scalable Ethereum communities. Learn strategies for engagement, growth, best practices, and key insights.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,community,Layer 1,Layer 2s,Values",
      "keywords": "Community,Discord,Farcaster,Building Community,Community Management,Community Security",
      "duration": 574,
      "language": "en",
      "sources_swarmHash": "4e8c46194a8800b5909aeb01cc6c0324728e82519f7c0ad031cb1149411d564e",
      "sources_youtubeId": "7T9YaSIAk2s",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T08:50:00.000Z",
      "slot_end": "2024-11-12T09:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Z6KNA8npIjlvXcTWwPrFhWHFQ9A2gd2wkiaNRb-bwuQ",
      "resources_slides": "https://drive.google.com/file/d/1pHNzfZgE2qzPXp91mudAu0-VW7oMeqHJ/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "a-revenue-model-for-based-rollups",
      "sourceId": "LEFXQV",
      "title": "A Revenue Model for Based Rollups",
      "description": "We outline the costs that a based rollup has, followed by the paths it has to revenue generation, revenue capture and finally revenue redistribution. We demonstrate that based rollup costs are front-loaded and non-negligible. Based rollups must pay for ZK-provers, app- and user-onboarding, R&D, etc, as well as L1 fees. We consider several continuous and once-off revenue models, and compare the pros and cons of each according to several key properties that based rollups are expected to have.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Rollups,Fragmentation,Tokenomics,based,Fragmentation,Rollups,Tokenomics",
      "keywords": "Based,Rollups",
      "duration": 1495,
      "language": "en",
      "sources_swarmHash": "b63d6fa56a15994cca2cdb01389e1a10844f54ba6e58a9f5e6b70b907db057af",
      "sources_youtubeId": "JFCfnhFL9Mc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673407ec3a168eb5356f00b8",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673407ec3a168eb5356f00b8.vtt",
      "transcript_text": " Hey everyone. There's a little box here just in case I need to sit down on top of the front row. Okay cool. So yeah my name is Connor. That's not my slide. Perfect. Okay. So yeah, I'm Connor. I'm from Nethermind. And today I'm going to talk about a revenue model for base rollups. Now, I wrote this title before I really sort of had an idea what I was going to talk about. And then I realized that revenue probably just refers to income. But base rollups are not just about when you're thinking about rollups in general, it's not just about income, it's about income and expenditure. And comparing the two, I'm going to talk about revenue, but base rollups are not just about, when you're thinking about rollups in general, it's not just about income, it's about income and expenditure, and comparing those two is how you decide if you want to be a part of the base rollup. And I guess not only are we going to talk about the base rollup as a sort of an entity, but also the entities, the real-life people that are participating in base rollups. So a quick primer on base rollups for anybody who isn't aware but they're lost is a rollup is said to be based or L1 sequenced when its sequencing is driven by the base L1. So basically, the L1 proposers are in charge of proposing the blocks for this rollup. And the rollup derives its state from the sequence of transactions that appear on L1. Now, a lot of rollups might actually, you could squint and say that most rollups are like that already, but I guess the key thing is that the L1 proposers aren't supposed to be sequencing the chain. So the goal of today's talk is introduce the players in base rollups. So we're going to look at each of the players and think about the income and expenditure that they have and should they be playing this game. Identify their income and expenditures. Discuss how likely they are, how these income and expenditures are going to change on the roadmap that's upcoming, the big change that we're expected to see, and then motivate you all to participate in base rollups and be thinking about these income and expenditure problems that exist. And then, yeah, I mentioned fat stacks for all, but really what that means is everybody has positive P&L when they're playing these games. Okay, so who do we have here? So we have the users. They're the most important, right? We're always trying to sort of make, although I was on a panel yesterday where we're talking about who should be gaining the revenue in a system and users were the bottom of the list. But users should be the ones that are gaining something from playing the game. So they're going to be the ones that are going to be driving the revenue for a base roll-up. Proposers and sequencers, I'm putting them in the same bracket. ZK-provers, so we have this proposer over here and this based blockchain that's accepting the blocks. We have ZK-provers, because I guess a stage two roll-up should have ZK-proofs. The DAO, so I guess when maybe some of you are here and you're thinking about the revenue model for base roll-up and you're thinking that it's the DAO that we're trying to create revenue for, but I think there's just too many players in the ecosystem to fully focus on just the DAO or whatever the treasury that exists on a base roll-up. We're going to be thinking about them along with everybody else. And then also we're going to have to think about these other chains and roll-up. We're going to be thinking about them along with everybody else. And then also, we're going to have to think about these other chains and roll-ups because they are actually a big driver of the revenue, the income and expenditure for a roll-up. So I have this, the shark from Finding Nemo. So depending on how you look at it, it could be a friend or it could be a foe, and there will be cases where they are both. Okay, so setting the table. So I guess maybe one of the reasons why I got accepted today was because basically when I submitted this talk, we were around June or July, and we were in a situation where Tyco was losing money by posting blocks to L1, which people were focusing on and people were getting really scared, like base rollups might not have a future, what's happening here. But there was a good reason for this. Tyco wanted to make sure that users were getting a reasonable UX, that they were getting updates every 12 seconds, because base rollups can only confirm blocks when they get posted to L1. Now again, there's this concept of pre-confirmations but they aren't live yet. At the time, Tycho was forcing L2 blocks onto the L1 while they were only half or less. So they weren't actually generating enough fees to compensate for the cost of posting to L1. But again, this was sort of the P&L that Grow the Pi was generating, right? And the P&L that they were looking at was L2 gas fees collected minus the L1 fees that were paid to post data and proofs to the L1. So that's a relatively simplified model of the revenue, the income minus expenditure for a base roll-up. There's actually a lot more components that we need to consider when we're thinking about is this base roll-up viable to run. So if you want to get a bit deeper, Nethermind Research did get a little bit deeper at one point, and I guess we're getting deeper by the day. We had this sort of maybe a little more complex model where we were looking beyond the on-chain costs and looking at the running costs of running a prover, for example. Running a prover is not cheap. Not only do you have to actually pay the electricity costs for running the prover, you actually have to buy the prover in the first place. So most people today aren't actually buying provers. They're outsourcing to AWS or any of these internet cloud providers. So there's additional costs that need to be considered there. There's also additional incomes. I'm going to get to those in a second. So, again, I guess we're in a situation where maybe, okay, someone says your income minus expenditure is bad. Someone else says it's not actually that bad. And really everyone is looking at this in sort of a quite imprecise model. And we're not actually able to consider all of the incomes and all of the expenditures. And I guess you can see that we're shooting from the hip a little bit. But these metrics do provide us with some indication of how things are changing, right? If they're getting bad, if they're getting really bad, or if this really is something unsustainable. And in that sort of early stages for Tyco, that was an unsustainable model, but it is getting better. And that's the key. So again, setting the table. This is the fourth slide that's setting the table. I was struggling for titles. So thanks to roll-up competition, base users can migrate easily between rollups that have similar network effects. Network effects are a key part of why users will stay on a rollup today, or base rollup today. But base rollups, the warm fuzzy feeling that base rollups give us is that they're actually very connected with L1 and other base rollups. This network effect is actually potentially strongest, like shared between all base rollups. So the individual rollups don't actually have much network effect. So base rollups, sorry users on base rollups actually have basically not much switching costs between base rollups. So in this setting the provider profits, so the providers I mentioned were the provers, the sequencers and the DAO. In this setting, the hypothesis that I'm claiming is that these profits of all these people must tend to zero. Because it's easy for another base rollup to come along, replicate all of the functionality that exists in that base rollup and basically just remove some of those overheads that the previous one was that the profits, sorry, the overheads, that base roll-up was trying to generate. But this zero-profit paradigm is far away. There's still a lot, because of all this haziness, because of this wild west that we're in, it's still possible that provers and sequencers can actually be making money. But this is going to be a long road, and we're going to get there incrementally. And again, depending on how you look at the road, like income is tending towards expenditure. You can see that as a bad thing, but you can also see it as a good thing, right? Especially in the case of Tyco. Tyco was when they initially launched where the expenditure was really greater than income. Getting it close to income and maybe even getting it past income is definitely a good thing. So how you look at these things is very important. Okay, so on this path to the endgame, this zero-profit endgame, there is going to be increases in income and there's going to be decreases in expenditure. The format is supposed to be decreases in expenditure. A little bit on a theme with Kevin's talk earlier on, but not like a HD video, so with music. So anyway, as we shed some of the technical debt or shed some of the inefficiencies of early stage based rollups, we're going to decrease the expenditure and we're also going to come up with new ways to increase the income. And yeah, basically that's the summary of the sentence at the bottom. Okay, so here's the list, right? So there's no real model as such, right? It's just a list. You have to basically sum up all of these peak components that you have in your base roll-up and check is the left-hand side greater than the right-hand side. And again, there's multiple players in the system. So when we're thinking of a base roll-up revenue, we have to look at it player by player. So the users have this warm, fuzzy UX or utility that's hard to quantify, but they pay very quantifiable fees. But we just need to make sure that there is still some utility that these users are getting. And if they stop paying fees, then we probably have the question, are we charging too much? Or are we actually generating any benefit to the users? Proposers. The proposers have relatively straightforward costs as well. They collect fees. They generate MEV or they execute MEV. They also potentially collect issuance. Issuance is a contentious debate in terms of who's paying and who's receiving. But L2s can, or base roll-ups can create a token and issue it to the proposers. And the costs are L1 posting and running costs. Provers, again, they can collect fees as well, issuance as well. But they have another cost, which is the setup, which is quite a little bit more significant than a proposer, because ZK-Provers are pretty significant pieces of machinery. The DAO actually does have a unique income source, which is investors. So when Base Rollup launches, you'll see that there might be a seed round or multiple seed rounds. And that revenue, that investment does pay for a lot of the upfront costs, which, again, you see the approvers and potentially the proposers. When you don't know what the fees are going to be, you do need some money in the bank to sort of get things rolling. And that's what sort of Tyco were doing at the start, right? And that makes sense. You need to get the users, create the utility, make sure that the UX isn't too bad, and then you start to generate the fees. And potentially the DAO can recuperate those losses for the investors. But the DAO also has significant... I also mentioned proposal fee. Proposal fee is sort of a more complex topic, but it is possible that the DAO can be collecting money from actually charging proposers on L1 for the right to be a proposer. But that's not going to be a focus today. The DAO is important, though, because the DAO does have to pay for things like R&D. We can't keep depending on the Ethereum Foundation for funding. We have to sort of go beyond that, especially when we're trying to create base roll-ups sort of parallel to the Ethereum ecosystem. So some R&D money does need to be generated and that comes out of the DAO. And there's also governance, running costs of the DAO. And then we have these old chains and roll-ups. So what do they actually mean? When these old roll-ups compose with the base roll-up, that could be seen as income, right? You're creating UX, you're creating improved utility for users. And that can allow you to charge more fees. But similarly, they also create competition. And with competition comes sort of tighter profit margins. So is income greater than expenditure? I've already sort of touched on why it should be. But in general, if it's not, you can pack your suitcase and you can go to some other ecosystem, some other roll-up. The proposers is an expectancy, yes, it should be, right? We can see that there is new technology coming along, and there's probably going to be more, where proposers may not actually be profitable in the short term because they may commit to offering pre-confirmations, for example. And if you offer a pre-confirmation too cheaply, it may be significantly more expensive to renege on that pre-confirmation than just posting it at a small loss on the L1. The prover in expectancy and long term, yes. But again, it may be a substantial cost to pay for proving hardware. So the prover must invest and see that there is a substantial chance of return. And the DAO, again, I mentioned this investment model, long-term is not sustainable. So they need to be investing in, I guess, sustainable infrastructure for these fee mechanisms, these proposer fees, these sequencer fees. And we're yet to see what that's going to look like. But it's definitely going to look like but it's definitely very interesting if the dev can actually recuperate those costs. Okay so the costs right now are relatively straightforward but our hope considering the sort of the type profit margins from Tyco is that these will change right so I quote this sort of ancient philosopher, or alter ego philosopher, who says, based income and expenditure is always in flux. You can't step into the same best based revenue model twice. What that means is that depending on the day, like as obviously today, when I go to the toilet, I hear people talking about what the markets are doing. Like, revenue is changing all the time, right? The fees are changing all the time, but also the technology is changing all the time. So when we think about the base revenue model, we need to have a dynamic framework for thinking about these things. And that's sort of going to be the focus next. So here are some of the big changes to base revenue that I expect to see in the near future. We're going to see cheaper L1 costs. We're going to see proving becoming more efficient. We're going to see prover and builder markets maturing. So a lot of L2s, base rops, will run their own prover. But there's whispers about prover markets and potentially also builder markets, right? So builders and L1 are generating revenue for the proposers, and that's probably going to happen as well on base rollups as well. So that's going to be an income source for proposers. And shared everything, okay? So I mentioned this composition, but it's definitely going to be a significant area for income and reduced expenditure. So with shared resources or shared blog space, shared proving, we're going to get bigger network effects, we're going to get better utilization of the resources that we're using, but we may also have higher costs, right? So how are we going to balance these things is going to be very interesting. So let's go through each one in a little bit of detail. I'm running out of time. Okay, so cheaper L1 costs, right? So EAP-484-4 was relatively straightforward. You just have this sort of temporary data that's being posted to L1 and it makes it cheaper for L2s to post data to L1. And then we're seeing proposals like PeerDAS. I don't really know what it is but I hear it's going to reduce cost significantly. But it uses some pretty significant sampling, or it's pretty high-tech sampling procedures. You can check out Vitalik's recent blog post for more information. Zika proofs becoming more efficient. This is a bet. I can't speak from personal experience with Zika proofs. I don't have any. But, again, we're constantly seeing cheaper proof generation and cheaper proof verification. We're also seeing increasingly bad naming terminology. But again, it looks like they're getting better. So the worse the names get, the better we're going to see these proofs becoming. And we're seeing also more and more investment, more and more teams deploying people to ZK. So I expect this to continue. A market that I saw this morning but again it's been taken down, no one's trading on this thing. So the prover and builder market maturing, I mentioned this already. So Uncle Ben mentions that with great competition comes efficient pricing and innovation. And again, that's just what we're seeing on L1, and we expect to see that on L2 as well. So proof of competition, user and sequencers have to pay less fees. And build a competition, we're going to get better UX through more efficient block building, and the sequencers are going to get higher fees. And then shared everything, right? So with shared sequencing comes more MVV, more users, and we're going to get higher fees. And then shared everything. So with shared sequencing comes more MVV, more users, and we're going to get higher fees. We're going to be able to charge higher fees. So this is again increasing the income of the providers in our system. Shared blobs, so right now Tyco is creating their own blobs. If you can share your blobs you can get approximately 100% blob utilization. So you're no longer paying fixed cost, but you're now getting 100% utilization. You're able to claw back some of the costs that you're paying. Share approving, we're going to get economies of scale and cheaper L1 verification. So again, these are all sort of things that you're trying to do as a provider to increase your bottom line. Also improve your bottom line. And composability. So less risk and complexity for proposers. You no longer have to use someone like a cross. You can just basically sign a transaction that interacts with a shared smart contract. And pre-comps. That's a big area of efficiency. It'll reduce the cost that we'd expect to see. And improved UX. That will definitely improve the revenue model for base rollups. But it's also going to increase expenses, right? So there's definitely a greater cost for sequencing two rollups together than sequencing both of them independently. Just because of the increased search base, the greater amount of MEV opportunities that exist. Shared proving, again, similar concept. As far as I understand, proving two states together is more expensive than proving two individually. But the big question will be, are some of the benefits greater than the increase in proving costs? We're only going to see that when we see what the results of this R&D are. But we definitely are optimistic that we can get these ZK proving costs low and the utility from shared sequencing high. And centralization, I guess that's sort of a tough one to quantify, but it's definitely a significant risk and risks are effects risks or costs in our revenue model So centralization poses a risk and we need to be conscious that all of these shared things will add the centralization risks So conclusion when you're thinking about the revenue model for your best roll-up You want to you want to basically get taught up all of your incomes take away all of your expenditures and ask yourself Why am I playing this if I'm losing, and how can I play more of this if I'm winning? Base heroes assemble. So again, do your own calculations, and innovate on increasing income and reducing expenses. So remember, heretical strike. You can't step into the same base revenue model twice, so we want to see less trickles and more waves crashing, all right? So thanks very much. And yeah, appreciate your time. All right, great talk, Connor. I'm just going to direct you to stand over here for the QA. Yeah, this one will be good. Alright, so we do have a few questions. Let's go from the most voted question. How can we reconciliate revenue models with the decentralization of ZKP roll-ups where multiple sequencers and validators will need to be compensated. Did the person that wrote this question, did they explain what they mean by reconciliate? Yeah. Sorry. Can we get a mic to that person? Okay. We can go to the next question while we're waiting. Maybe I can touch on my interpretation of the question. In the case of current roll-ups where we have a centralized sequencer and a centralized prover, it's still not clear if the incomes are greater than expenditures. Okay, but yeah, we have the mic now. Let's go. Essentially, how do we... As there's a risk of fees increasing with higher number of validators in L2ZK roll-ups. What are the risks of that making the situation more unsustainable and what are ways in which we can address that? Well, the good thing about base roll-ups specifically is that we don't have any validators to pay. It's just the L1 proposer. Now, in terms of the multiple approvers that may exist, we're basically going to be creating a market, right? And it's a bit like Bitcoin and some of the old arguments for Bitcoin where you're like, okay, well, I have my hash rate and I have the expected Bitcoin rewards. And if I'm losing money there, I can go to Bitcoin Cash or I can go to the alternative blockchain. The same applies to any of these approval markets. So if it becomes too expensive, users will stop paying for it and basically you're going to have to spread out your costs or just wait until things become cheap again. So there's definitely a risk in a fully decentralized system. Yes, we're going to be exposed to these things being too expensive. So the only thing we can do is try and make sure that we have fallbacks like maybe cheaper fallbacks in the case that those things we are very exposed to fluctuations but I think pre-confirmations and things like this that allow us to wait maybe 16 blocks or 32 blocks, L1 blocks, where we get to sort of pick and choose where we include things and maybe pick and choose which provers to pay, that can maybe help to reduce the risk to maybe spikes, let's say. But not sure if that answered your question, sorry. We do have around two minutes for another question. Which one would you like to take, Connor? I'll go for the shorter one first. If the base rollup has a sustainable and positive revenue, can it become a de facto design for the layer one? Well, I guess so, right? At some point, if there is one base rollup that is taking up all of the L1 block space, that is a theoretical possibility with base rollups, that all the computation is being done off-chain on the base rollup. So in that sense, it could be doing all of the execution for the L1, but I'm not sure if it can actually replace the L1. Because it is still being sequenced by the L1. All right. Do you want to answer another one? Okay. I'll go for the short one. What is the plan to increase the income, but with the scale the expenditure also increases? Yeah, that is a tricky one. So it's like, again, we have to take a bet, right? And we're looking at the fact that a lot of people like Flashbots, like all of the ZK teams that I mentioned, they're playing ahead with improved mechanism design or improved proving systems, improved mechanism design, regardless of, let's say, any specific endgame that may be likely. They're trying to make things cheaper. And if things do become incredibly expensive with scale, we can always scale back. There will definitely be a limit to the amount of things that we can merge, the amount of state that we can merge. But right now, we can see that single roll-ups can survive. And then it's just a case of how can we actually just scale up the merging of all this state and that's definitely going to be a really interesting question. But we can always scale back, that's the key. Alright, cool. So we're right on time. Thank you so much, Connor.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1RyOkM2nzZPnG9r_QZgUBb98ZHMt2VtBlxR0avT7eWHA",
      "resources_slides": "https://drive.google.com/file/d/1QCcwQBW0aLVnC2JVdxUdjx7Bpe55d-XC/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "art-exhibition-guided-tour-by-artists-and-writers-cohort",
      "sourceId": "9HS9CX",
      "title": "Art Exhibition Guided Tour by Artists and Writers Cohort.",
      "description": "Please join the guided tour for the art exhibition \"Trusting the Unseen: Elements of the Infinite Garden\" by the Devcon Scholars Artists and Writers cohort. Let's walk through diverse spaces at Devcon with the artists and writers, and discover the essence of Ether–a symbol of unseen connection. This exhibition invites attendees to witness the Ethereum’s ecosystem in a new light, honoring subtle connections between physical and digital worlds in the realm of the Infinite Garden.",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "Art",
      "keywords": "Writers,Exhibition",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T13:00:00.000Z",
      "slot_roomId": "artists-cohort-pyramid",
      "resources_presentation": "https://docs.google.com/presentation/d/1xaK3SW7SLn-t1fQVQV2EcoNXQjJ5XG6AO0udCWpcCng",
      "resources_slides": "",
      "slot_room": {
        "id": "artists-cohort-pyramid",
        "name": "Artists Cohort Pyramid",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "bridging-the-gap-how-to-effectively-talk-to-regulators",
      "sourceId": "VNHCTQ",
      "title": "Bridging the Gap: How to Effectively talk to Regulators?",
      "description": "Regulators are eager to learn more about different aspects of blockchain. Over the years we can see is a growing interest in a dialogue with the industry participants and specifically technical experts. Now more than ever it is crucial for the community to speak with an informed and empowered voice. The workshop we are proposing aims to share all the knowledge we have accumulated over the last 4 years working with (mostly EU) regulators to empower technical experts and founders.",
      "track": "Real World Ethereum",
      "type": "Workshop",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Best Practices,Auditing,Regulation,advocacy,Auditing,Best Practices,Core Protocol,Regulation",
      "keywords": "Advocacy",
      "duration": 6531,
      "language": "en",
      "sources_swarmHash": "b4b3eae9ce143541d3aad89ba1ba1dbcf3d62325019fd965614c7cc28e48f3db",
      "sources_youtubeId": "9PS67asKERQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733f28f3a168eb5353f3e90",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1pzlL4H4Mj25_flDl4GMvEE-meKuh9eh2PLHklBBwEdw",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "daos-and-borgs-blending-the-best-trust-minimization-techniques-of-the-onchain-and-offchain-worlds",
      "sourceId": "3E7R7G",
      "title": "DAOs and BORGs: blending the best trust-minimization techniques of the onchain and offchain worlds",
      "description": "In his talk, Gabriel will give an overview of the legal challenges faced by DAOs and how ‘making DAOs modular’ with specialized legal wrappers and smart contracts known as ‘cybernetic orgs” (BORGs) can solve these problems. The talk will tie the evolution of DAOs to the modular revolution in blockchain infrastructure and a detailed walk-through of how modified Gnosis SAFEs can be combined with legal entity bylaws to blend the best trust-minimization techniques of the onchain and offchain worlds.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,Governance,Decentralization,cybernetics,DAO,Decentralization,Governance",
      "keywords": "Cryptolaw,cybernetics",
      "duration": 1641,
      "language": "en",
      "sources_swarmHash": "c99eeecfe8caaabcd7d43ec47b485f6adbdc298c8022c488832256b05188d012",
      "sources_youtubeId": "2pYy07uUG4c",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1Pq-UfROf_3nVsy2VhJLpxOcTmyPsVPQsHMIH4SZRIfY",
      "resources_slides": "https://drive.google.com/file/d/1ofAkN0PJc0taGsGZCR4HRJ_e8ZfEA2A7/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "ecosystem-development-best-practices-and-why-we-need-to-start-with-builders-first",
      "sourceId": "EY3HL9",
      "title": "Ecosystem Development Best Practices, and why we need to start with builders first",
      "description": "Given the myriad of chains out there, it is increasingly crucial for L2s to solidify their ecosystem building playbook and constantly refine it to win over (and more importantly, retain) users and builders. As an ecosystem builder in SEA (Thailand) who has worked with over 10 ecosystems including other L1s, on local, regional and global initiatives, I am excited to share the ins and outs of ecosystem building from a neutral perspective.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,DevRel,Best Practices,management,stakeholder,Best Practices,DevRel,Layer 2s",
      "keywords": "Ecosystem Building,Ecosystem Design,Developer Experience,Stakeholder Management",
      "duration": 407,
      "language": "en",
      "sources_swarmHash": "3ca335e97a65bd21e260157bab87ec0fc8fb8c50e77214212c844d794eb17896",
      "sources_youtubeId": "xqs8trszoOY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T09:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12auC9dhscSkSUtYU1CqtRqHz64-ljDXc1f7otM8hLMw",
      "resources_slides": "https://drive.google.com/file/d/1nw2s9vBzS30DiwEVaX24JFLSD7IyozOn/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "ensuring-privacy-in-digital-identity-to-prevent-a-dystopian-crisis",
      "sourceId": "TZQYGY",
      "title": "Ensuring Privacy in Digital Identity to Prevent a Dystopian Crisis",
      "description": "This talk will explore introducing a method for privacy-preserving proof of user uniqueness in contexts like elections using DIDs, ZK, and VCs for verifying credentials without revealing unique identifiers while ensuring compatibility with multiple trust sources. This enables self-sovereign digital identity, allowing selective disclosure of verified credentials while protecting personal data, supporting privacy-preserving KYC, sybil resistance, compliant access to financial services, and more.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,Zero-Knowledge,Security,zk,proof,Identity,Security,Zero-Knowledge",
      "keywords": "Zk,Proof",
      "duration": 1426,
      "language": "en",
      "sources_swarmHash": "27893580368e8396780c9f103b3d94703c4ab1700db60ed5816b3334de7aff27",
      "sources_youtubeId": "6EJBsHydyVU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1tnxzF5on5Su2ji2vPSGG1B6RHmxzdqDiuloznxu_PIg",
      "resources_slides": "https://drive.google.com/file/d/1Koenif0k3CGI6ivvq0vU8SzVly_8gT4Z/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "ethereum-execution-layer-specifications-eels",
      "sourceId": "3GCD7S",
      "title": "Ethereum Execution Layer Specifications (EELS)",
      "description": "An introduction and walk-through of the executable specifications for the Ethereum Execution Layer. \r\nGithub (https://github.com/ethereum/execution-specs)\r\n\r\nEELS is an implementation of the EVM in Python that has been optimised for readability. A great tool for EIP authors looking to prototype new ideas on the EVM, it is easy to understand as well as update with new features.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Layer 1",
      "keywords": "Execution,Layer",
      "duration": 1253,
      "language": "en",
      "sources_swarmHash": "5152428075c4cdb0ae87fd4ba618e21a8b8d00dee0da4e8f53acff649df95802",
      "sources_youtubeId": "WEvCFg0Z1D4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1tBeUpTPFPiF-99JI_q0F1DV1g8Bx09ZHLkprfgVzn2c",
      "resources_slides": "https://drive.google.com/file/d/1Y4rDk-2mnBsfHal-yDu0-ywHbIKGTh6F/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "synthetic-melodies-a-digital-soundscape",
      "sourceId": "EZ3EVX",
      "title": "Synthetic Melodies: A Digital Soundscape",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience! Dive into the bleeps and bloops curated by RBRD, weaving together experimental, ambient and IDM. Let’s connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1kQVFXulZrmOXmwN9TZ75ma4CYWlC0Kv9WxWB6w0qyAg",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "wen-p2p-electronic-cash-system",
      "sourceId": "ZFX3ZF",
      "title": "Wen p2p Electronic Cash System?",
      "description": "16 years have passed since Bitcoin whitepaper came out. Bitcoin was created as cypherpunk cash replacement. Cash means easy payments. But bitcoin found its PMF as 'digital gold', not as 'digital cash'. What happened to cash? What needs to happen for mass adoption of crypto payments?\r\nWe will go through the history of failed attempts. We'll end up with a hopeful analysis of why it's different in 2024 (spoiler alert: stablecoin adoption, cheap L2s, AA).",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Conviction,Payment,Account Abstraction,stablecoin,Account Abstraction,Conviction,Payment",
      "keywords": "payments,cash,stablecoins",
      "duration": 1549,
      "language": "en",
      "sources_swarmHash": "63b3f30cc56be0d58d80b8295e68bf9bdc088a286e0e5c0e86738ee20d1e2e1c",
      "sources_youtubeId": "Kw3rxKFUEKc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673321cc3a168eb53554b6fe",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:00:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1JImpxFx5TF-6ESwxVVo3QOw9b3RrwbHwCF5idb0IZDY",
      "resources_slides": "https://drive.google.com/file/d/1dMvlJ2kGv4x2idmRzv87LISnMMOksadw/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "building-experience-insights-from-dapp-learning-how-to-create-a-thriving-ethereum-focused-community",
      "sourceId": "9FPGBW",
      "title": "Building Experience Insights from Dapp-Learning: How to Create a Thriving Ethereum-Focused Community",
      "description": "Building the Dapp-Learning community has been an exciting and insightful journey. It's more than just a collection of developers; it’s about fostering a space where like-minded individuals can grow, collaborate, and innovate in the Ethereum ecosystem. I will share the experience to  build a Ethereum-focused  developer community.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,DevRel,Public good",
      "keywords": "developer,community",
      "duration": 527,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "92NTfwByGVQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:10:00.000Z",
      "slot_end": "2024-11-12T09:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1V_J6YbuL2ThEQuDEoLf9zCN3vAnn9d3pOrEfEmHUw0E",
      "resources_slides": "https://drive.google.com/file/d/1szo5giWjIOl5NmKdsMwCIbWY2lOxPR5y/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "challenges-developing-and-maintaining-open-source-software-in-web3",
      "sourceId": "RT8WNR",
      "title": "Challenges Developing and Maintaining Open Source Software in Web3",
      "description": "Producing high-quality developer tools for the Web3 ecosystem is a challenging task that requires significant effort (and funding). Many of the best and most used tools started out as a lone hackers side-project, and then evolved into longer-standing projects by being absorbed into a larger companies efforts. In this talk, we'll share RV's open-source tool development story, and discuss what a better future could look like.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Open Source Software,Public good,community,Open Source Software,Public good,Tooling",
      "keywords": "Grants,lessons,community",
      "duration": 545,
      "language": "en",
      "sources_swarmHash": "25e6ed887207dbca41799d009fc12cdef9f1b8e73205686de952a3102690d3fe",
      "sources_youtubeId": "U93wI1oLKCA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:20:00.000Z",
      "slot_end": "2024-11-12T09:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1qjEbqOegs1ocDdXvx_djLjbOkH762J5kzikQ4VqSqdg",
      "resources_slides": "https://drive.google.com/file/d/1uDceJAf9QJwPami2jzrfbEOxGyRKTaL3/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "advancing-op-stack-to-zk-rollup-achieving-efficiency-and-security-with-zero-knowledge-proofs",
      "sourceId": "ZKKUNZ",
      "title": "Advancing OP Stack to ZK Rollup: Achieving Efficiency and Security with Zero Knowledge Proofs",
      "description": "OP-stack based rollups now retrieve L1-to-L2 deposit transactions and L2 transactions from Blobs. Current solutions face two issues: 1) increased operational costs due to batch submission overhead or 2) protocol complexity during challenges. We'll share our experience addressing these using ZK fault proof. Our new challenge system is cost-free for users and easily extendable to ZK rollups. The presentation includes our example of switching from zkEVM to zkVM and optimizing proof generation speed",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Optimistic rollups,Zk Rollups,ZKP,zkvm,Optimistic rollups,Zk Rollups,ZKP",
      "keywords": "OPStack,ZK Fault Proof,zkVM",
      "duration": 1378,
      "language": "en",
      "sources_swarmHash": "85b56b290f00d5e5f5c5a70cd6ea9df49ab5ac76aea6dc76efd6adfb2bb59ec6",
      "sources_youtubeId": "PCvewGIWtMA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673328393a168eb535690c58",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673328393a168eb535690c58.vtt",
      "transcript_text": " 자막 제공 및 자막 제공 및 광고를 포함하고 있습니다. Hi, how are you guys doing? So I'm TA, aka FakeDev. I'm a ZK engineer from Chroma. So today I would like to share our experience of advancing OP stack into ZK rollup. So we're trying to enhance efficiency and security with zero knowledge proofs. So for those of you who don't know about Chroma, I want to first celebrate our successes from last year, our mainnet launch. So we were one of the projects built on OP stack, so we're one of the first, I mean we're the very first optimistic roll-up with an active fault-proof system. So we utilize ZK proofs for it. And so ZK enables to solve some problems in traditional fault-proof systems. And first we approach with a circuit-based implementation of ZK provers, but it has a very, very big issue. So we're trying to transit from circuit-based approach to ZDK VM-based approach. And lastly, we're also building a ZK backend library to push the boundaries of proof generation speed. So why ZK fault proofs? So in traditional fault proof systems, it requires a lot of interactions, which leads to high bond amounts, which decreases the level of decentralization. So with ZK fault proofs, we can just narrow it down to a block level instead of instruction level, which requires much less interactions. So it not only reduces the operational cost itself, since there's less interaction, so we need less computation, but also it can significantly reduce the bond requirement. So it naturally leads to better decentralization and security of the network. So moreover, for like arbitrage fold, if we apply ZK technology into it, we would be able to reduce the bond requirement significantly that is usually used for blocking Sybil attacks. Also Optimism is putting a lot of resources into exploring ZK fault proofs. So we have an announcement that we have received retro PGF round 5 from Optimism to explore more on the ZK fault proof size. And this recognition, I think, shows how much important and possibilities are there into building a permissionless fault proof system with ZK. So we've looked at our achievements since last mainnet launch. And now I would like to share some of our lessons learned and challenges that we've experienced for building ZK file proof systems in our mainnet. So the very important fact that we learned is that the circuit-based approach is just not sustainable. So we currently have about 100k lines of code, which are for custom circuits. So we have to write it in a circuit language like Plunkish. And it's very, very hard to write and debug and verify. So what we're trying to do is check the integrity of EVM state transition function. But we need several specific custom circuits written in circuit languages to verify that the state transition function, given the input, is done correctly. And back in 2022, Vitalik even mentioned at the time there was no like production ready ZK EVM circuit codes, but they had a POC version of it in Ethereum's PSE. And back in the days, it was like 35K lines of code. But even Vitalik mentioned that it's never going to bug free for a long, long time. So it's very prone to errors, very prone to human errors. So to sum up, there are two challenges. Writing the circuit itself is tough because it's hard to audit and debug. And also, since Optimism and Ethereum keep bringing upgrades to their mainnet to support full compatibility, we have to keep up with the protocol upgrades they're making, which means that we have to modify circuits again and again 메인넷을 통해 전체 공정에 도움이 되는 경우, 프로토콜 업그레이드를 계속 만들어야 합니다. 즉, 업그레이드들이 계속 늘어나는 때마다 모든 공정을 계속 변화시켜야 합니다. 이런 경우, 더 많은 변화를 만들면 더 많은 error의 가능성이 생길 것입니다. 이는 메인넷의 안전을 부담할 것입니다. 그래서 더 좋은 방식이 필요합니다. 저희의 메인 넷의 확장에 영향을 미칠 것입니다. 그래서 우리가 더 좋은 방법을 필요로 하고 있습니다. 그래서 지금 저희가 ZKVM에 대한 방법을 고려하고 있습니다. ZKVM은 우리에게 전형성과 정의성, 즉, 이 방법의 의미는, 제가 말했던 루틴 적용 방법에서 루틴을 Plunkish 같은 루틴 언어로 적용해야 합니다. 이것은 매우 불가능합니다. we need to write circuits in a circuit language like Plunkish, which is very unfamiliar. And because of that, the auditors will also have a hard time auditing, and even us can't really verify well that our code is written well. But ZKVMs allow us flexibility. We can write any program that we want to verify the computation in Rust. And since it's written in Rust, it's very, very better to audit and debug. So now I would like to dive in more to ZKVM area. So how we are trying to move on from circuits to ZKVMs. So as I mentioned, ZKVMs can provide a general purpose environment for verifiable computation. So what it means is that we don't need to implement circuits anymore. We can just write Rust program and ZKVMs can basically execute the program and give a proof which proves that the computation was done correctly. So how it works is, first, each ZKVM vendor would have a compiler toolchain that can compile a Rust program into, let's say, if it's based on RISC-V, it would turn into a machine code based on RISC-V에 의해 기술을 만들 수 있습니다. 그리고 ZKVM 안에, 프로그램을 실행하는 기술이 있습니다. 이 기술은 실행 트레이스입니다. 실행 트레이스는 기본적으로, 기술의 규모에 따라서의 폴리노미오가 있습니다. 그전에는, 기술을 적용해서, 기술의 인풋 데이터를 폴리노미어에 적용하고, 그 다음에는 그에 대한 규칙을 얻을 수 있습니다. 그런데 지금 ZKVM 실크리티는 그것을 해결할 수 있습니다. 그래서 지금 우리는 그냥 Rust 코드를 적용할 수 있습니다. 더욱 더 flexibly. 그리고 ZKVM가 실크리티를 적용한 후에, execution trace, it would commit to the trace and then generate a proof. So I have a summary kind of picture that we can compare, like how it's different. So the colored green parts are the code that we used to maintain and we have to maintain. So back then, we would have to maintain like ZK EVM circuits written in Plunkish에 적혀 있는 ZK EVM circuit을 지정해야 했죠. EVM, ROP, NPT, 그리고 통제 circuit가 있었죠. 이는 EVM의 행동을 반복하는 것이죠. 그래서 이는 통제 체계와 통제 체계의 연계를 확인할 수 있었습니다. 예를 들어, 블록 헤더와 블록 바디와의 비교는 핸셋과 블록 캐시를 비교하는 것입니다. 이것은 우리가 사용할 때 사용하는 것입니다. 하지만 매우 어렵습니다. 그런데 지금, 오른쪽에는 ZKVM의 시선이 있습니다. ZKVM 시선은 그것을 대응할 것입니다. 대신 블록 실행 프로그램을 적용할 것입니다. circuits would handle that and instead we will just write a block execution program which for us as an Ethereum layer 2, we would derive the batch from L1 and then execute it on L2. So that would be the program we were trying to write. And some of you might ask like Vitalik mentioned in 2022, we launched our mainnet in 2023 and then then why didn't you guys first approach with ZKVMs at first? Why did you guys write circuits and do all that hard work? That's because back then, ZKVMs were not performant. So since the proving time is long, within the challenge period we have, there can be possible attack vector for delay attacks. So that's one of the reasons why we chose a circuit-based approach. But now, risk zero calls it continuation and SP1 calls it sharding. There was a breakthrough called sharding, which with the full execution trace, you can now divide it into a unit called shards. So it's usually represented in the number of cycles of RISC-V. So SP1, they kind of have an optimal number of like two to the power of 22 22차체의 수단을 사용할 수 있습니다. 수단을 나누어 분리할 수 있도록 수단을 사용할 수 있도록 수단을 사용할 수 있도록 그리고 나중에 한 개의 수단으로 수단을 공급합니다. 이제는 수단을 공급할 수 있도록 더욱 높은 수단을 사용할 수 있습니다. 그래서 우리가 노력하고 있는 것입니다. 그리고 거의 메인 네트에 배송되고 있습니다. We can parallelize proofs. So that's what we're trying to do. And we're almost there, shipping it to the mainnet. And now I want to share more of my thought that what we can explore more in the future to overcome barriers to become from an optimistic rollup to a ZK roll-up. So my concerns about ZKVM-based approach is since ZKVM vendors business model is usually running a prover network, so they get a delegated proof and then they generate a proof for it and then return the proof to the client. But there might be a situation where ZKVM prover network might fail. So usually ZKVM vendors, since they've built their ZKVM, they're very, very good at giving a good performance. So usually they will have a cluster of machines, and then they would split the workloads to each machine, and then orchestrate it, and then create a proof and return. But since most of the such proven network-related codes are closed-sourced, maybe to handle this kind of network reliability issue, we might have to, like us, Chroma, might have to implement a code to orchestrate multiple machines 크로마는 여러 기술을 모아야 할 것입니다. 모든 validator가, 이 내용이 맞는지 모르겠지만, 네트워크에 증명을 받을 수 있는 클러스터를 운영해야 합니다. 또 다른 문제는, 지금 단계 1 롤업이 더 많을 것입니다. Another concern might be, so now there's going to be more stage one roll-ups, but for stage two, we only want the security console to come in and resolve the problem if the failure is only on-chain provable. But currently, in my perspective, for the ZKey VMs, currently there's no way to make the network failure to be on-chain provable. So that's another concern. ZK VM의 경우, 네트워크 실패가 안체인으로 인정될 수 없다는 점이 제 생각입니다. ZK VM의 경우, ZK VM이 부가가 될 수 있을 수 있습니다. 그래서, 우리가 부가를 감소시킬 수 있는 멀티 프로브 시스템을 필요로 해서 그에 대해 버그에 대처할 수 있는 것을 생각합니다. 그래서 그 경우에는 네트워크의 확장력을 향상시킬 것이며, 하지만 상승을 원하지 않습니다. 그래서 우리가 더 오래 사용할 수 있는 시기가 없기 때문에요. 그래서 멀티 프로브 시스템의 3가지 종류가 있을 수도 있습니다. three types of multi-prover schemes instead of having a single prover. One, I think most of the networks like scroll and Tyco is trying to use T's. In my opinion, there's a trust issue that we have to trust the T implementation itself. There's also hydro centralization problems, so we might have to come up with a clever idea to maybe use three vendors of Ts and then have a multi-sig kind of thing. And another one would be kind of very, very labor intensive, but come up with another circuit-based CKVM. So instead of Plunkish and Halo 2, another circuit-based ZK-EVM. So instead of Plunkish and Halo 2, you might have to write another ZK-EVM circuit code with another language. I don't think no one will try that. And I think the best would be our approach to have another ZK-EVM-based ZK-EVM. I'll tell you why we think this is the best. And if we have a multi-prover system and if we have ZKVMs now, we could really, really easily ship ZK roll-ups than past one or two years. But I've did some benchmarks. 제가 ZKVM을 사용한 적은 몇 개가 있습니다. 하지만 아직도 비용이 부족합니다. 첫 번째는 비용 범위를 제공하는 것입니다. 이것은 매우 중요합니다. 제 의견으로는, 사용자에게 사용할 수 있는 것을 매우 어렵게 계산할 수 있습니다. 대부분의 레이어2, charge the users to use it since most of the layer 2s and most of the optimistic rollups now have very very cheap fees. So there are three types of costs that we need to pay to run a ZQ rollup. First, it's the biggest bottleneck which is the proving cost. So I've tested with a network with about three TPS, and you need about a million dollars for proving the blocks for a year. So I think it's not that feasible now, and also we would have to commit to the batches we've settled into layer one, so there will be settlement fees also. And to finalize the blocks, we would have to send the proof to the contract and verify on chain. So there's also going to be verification piece. But that's gonna be really really small because we will be able to aggregate proofs inside a ZKVM, which will be way cheaper. But there's to be trade-off between finality. And also we could explore more on future directions where I introduced the multi-prover system and also the CKVM vendors would have to consider more about prover decentralization where they don't keep the clusters running on only their network and getting proof requests, but instead have an incentive mechanism so that Prover network can be decentralized a bit more. We're also pushing, at the same time, pushing the boundaries of proof generation. We have our own very unique open source library called Tachyon, which is a particle that's known to be faster than light. 저희는 자키온이라는 독특한 오픈소스 라이브러리를 가지고 있습니다. 자키온은 빛보다 빠른 공간이라고 불리는 공간입니다. 자키온은 모듈러 CK backend 라이브러리입니다. GPU 확장 기능도 있습니다. Halo 2를 통해 CK VMM을 구축하고 있습니다. 먼저 Halo 2를 많이 정리를 했고, 저희는 1.5X, 6X 더 빠른 벤치마크를 가지고 있습니다. 그리고 저희는 지금 SP1 ZKVM을 사용하고 있습니다. 그래서 저희는 Plunky 3를 정리를 하고 있습니다. SP1에서 사용되고 있습니다. 마지막으로, CIRCUM은 매우 많은 공간에서 사용되고 있습니다. optimize Plancky3 as well, which is used by SP1. And last, since circum is very, very widely used in the community, we also optimize a lot in terms of rapid snark. So it's also faster. And we could wrap up into this one figure where if Chroma has a multi-CKKVM prover backing the network, we would also have Tachyon, which is a modular ZK backend library, which could support all kinds of ZKVMs. So we would only have to maintain the very front, like ZKVM main, which is written in Rust, and which mainly uses the library Kona, which is maintained from Optimism, which has block derivation logic and execution logic, so we could easily write a program for our mainnet within like 200 lines of code. And we would have an input, which would be the block we were trying to prove, and we would have a multi-prover, like one of the frontiers, RISC-0, SP1, O1VM, Alida, maybe. And then we have Atakion, which is backing with powerful GPU optimizations, which will accelerate the proof generation speed. Yep, that's it for today. And you can contact me with fakedev999 on Twitter, Telegram. Yep, that's it for today. And you can contact me with fakedev9999 on Twitter, Telegram. Please contact. We can share more about fault proof systems. I would like to hear your ideas. And that's it. Thank you. . You can just stay over here. All right. Great talk, TA. So thank you to the crowd for submitting all these amazing questions. We do have less than five minutes to go through some of them. Let's go through the most upvoted one. How is it different than OP Succinct? Yeah. Some of you guys might know and not know, but OP succinct and us, we have a very, very similar logic. Actually, we were working underwater for about two months. We shared it to the SP1 team about, oh, we have a Proma SP1 prover repository. You could take a look if the logic would be 있습니다. ZKVM-based Prover에 대한 정보가 정확한지 확인해 주시면 좋겠습니다. 그리고 그들은 동일한 정보를 사용하고 있습니다. 거의 비슷한 정보를 가지고 있습니다. 그리고 그들은 제가 말씀드린 동일한 정보를 분류하는 것에 대한 추가적인 기능을 가지고 있습니다. 그래서 정보는 비슷하지만, of what I mentioned of aggregating the proofs. So the logic is very similar, but basically we built the same thing. So I'm going to mark this as answered. Any stats on prover cost, specs, and time? So the prover cost for our fault proof system for about a block with a TPS of 3 to prove a faulty block of TPS 3, I think it costs about like, currently, so it depends on the pricing of the prover network you're trying to use, but if you're just using an on-demand plan, which would be the most expensive per cycle, if I remember correctly, it was about like 20 bucks for a block with about like TPS3. And the stats and the machine specs, it's not revealed at the moment for SPO and Prover Network, so I'm not sure about that. All right. Do you know if the Arb ecosystem have similar plans? Yes, I definitely know. And I also know the fact that they have a fast withdrawal feature now with zkproofs. I didn't have much time to like deep dive into it, but I definitely think like Arbitrum also is going through the right path, so maybe we could collab more and talk more about it. Alright, great. So we can go through, I guess, two more questions real quick. In your prior implementation by Plunkish Circuit, you mean Plunky 2 or 3 or something custom? Oh, it's actually Halo 2's Plunkish arithmetization. It might have confused you a little bit, but we're using a back end currently, Halo 2. All right. So one last question. What happens if the guest program panics? Can you still produce a proof? So if a guest program panics, then you can't produce a proof. So, I'm not sure the person who is asking the question is intentionally panicking the program or not. But like, yeah, if it's a bug that's causing panic, then proof is not produced. So this is one also another thing I mentioned in the slides before is we need a way to like overcome if the program panics but if it's a bug inside a CKVM or if it's a bug in a program then we would need a way to verify it on chain or have a way to prove it on chain so that security counsel could intervene or what. All right. Great. So our time is up. Thank you so much, TA.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:30:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1C4LP01Njg8d8_7focQ3IHctmO58TbdilXcn-G6_m3sM",
      "resources_slides": "https://drive.google.com/file/d/1EwXB1d2zhOExBSKgASQE3XAKPIyYPH0r/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "an-introduction-to-cryptography-new-and-old",
      "sourceId": "R3JC8U",
      "title": "An Introduction to Cryptography, new and old",
      "description": "A beginner level view at what cryptography is, from signing, encryption, and hashing, to ZK, MPC, and FHE. We will answer \"What do all these things mean?\", \"Why do they matter?\", \"What can you do with them?\", and \"How do they fit in the real world?\".",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zero-Knowledge,Cryptography,MPC,Homomorphic Encryption,education,Cryptography,Homomorphic Encryption,MPC,Zero-Knowledge",
      "keywords": "Education",
      "duration": 1465,
      "language": "en",
      "sources_swarmHash": "16a3771bca4b5ce1a6ac65bcc81327d002a7cb59b050322799fee53af537ac03",
      "sources_youtubeId": "E6u3uQGP9J4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:30:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1IY1pWsDydf5hoQbu9K7EcpHpB9Y8bnVDSIAu6RSXu0A",
      "resources_slides": "https://drive.google.com/file/d/12eI28_lYc9ZglgMbd104kQwafq_Myyz7/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "anatomy-of-a-web3-startup-chapter-1",
      "sourceId": "EQL7FK",
      "title": "Anatomy of a web3 startup (Chapter 1)",
      "description": "POAP started in 2018 and has navigated the turbulent waters of the space to become a well loved application that represents lots of the values of the Ethereum community.\r\n\r\nThis talk reviews the internal organs that compromise a web3 startup for aspiring founders to get a glimpse of what challenges they will find and how to deal with them.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "startup",
      "keywords": "startups",
      "duration": 1597,
      "language": "en",
      "sources_swarmHash": "3b4f2b957b31b6d42a2c99bf467b7419d0dd03e4bb80dbf8f511e5cae5a59a35",
      "sources_youtubeId": "Hp52dqbz3RU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:30:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1nL-c7JYqWnQddW0BtHR56cUv9hNYnZFOgm3Ce4DvdEQ",
      "resources_slides": "https://drive.google.com/file/d/1KoAHQ0pS1aOhjoMNaeAvdlG_YMY1zxxB/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "eth-a-roadmap-to-real-decentralization-in-a-world-of-centralized-power",
      "sourceId": "C3HTZP",
      "title": "ETH++: A roadmap to (real) decentralization in a world of centralized power",
      "description": "Unfortunately, trends in block building and MEV furnish rapid centralization pressures that erode the protocol guarantees we gather here to build for Ethereum. We must now define a roadmap to save proof-of-stake. This requires help from builders, transaction originators, protocol designers, and you. We will demistify the hype on how and if trusted hardware (TEEs) can help us decentralize. Let's focus on geographical diversity and permissionless designs, to bring the world together.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Censorship Resistance,Decentralization,MEV,Censorship Resistance,MEV,Protocol Design",
      "keywords": "TEE,hardware,decentralization",
      "duration": 1502,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "ivuV-H3UUtA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:30:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1bcWYCRlknrhBAHOizptWAGujiHHrSU_sAh9xh-oi1js",
      "resources_slides": "https://drive.google.com/file/d/11AmauLfDF5czSctk2D28zIEfUAv972CA/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "tales-from-interop",
      "sourceId": "UQPDPQ",
      "title": "Tales from interop",
      "description": "A deep dive into the interop process for Pectra and how it evolved over the year. Find out how 100 people can work on 3 forks at the same time and how we avoided the devops bottlenecks.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Security,Testing,devops,Core Protocol,Security,Testing",
      "keywords": "DevOps",
      "duration": 1433,
      "language": "en",
      "sources_swarmHash": "383122b77f86b227e151f74387c9f010ac758d64fca5abea34685147c14c417d",
      "sources_youtubeId": "NHsi-lyOEUA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:30:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1EI6PvXpSa-LCMg1S_f31vrLcip8y61g5BqDRGaUIJe0",
      "resources_slides": "https://drive.google.com/file/d/19rzwzD9EyqmEfORc3qmLOZME_VIxPIH3/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "the-next-generation-of-decentralized-governance",
      "sourceId": "WUSAHA",
      "title": "The Next Generation of Decentralized Governance",
      "description": "In this talk, tracheoptryx will share thoughts on what will define the next phase of decentralized governance and how that has informed the design of EigenGov, EigenLayer’s forthcoming governance system.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "see,doc",
      "duration": 1629,
      "language": "en",
      "sources_swarmHash": "75a12cae9fadbaeaba434231a49a634d15b4251288154859b4667cd19622b603",
      "sources_youtubeId": "VhkP2OIwIFY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:30:00.000Z",
      "slot_end": "2024-11-12T10:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/12GuPqjQk66_MOFYNzQAXdDgl9b2uXDcWEc4im_qwX7E",
      "resources_slides": "https://drive.google.com/file/d/10Th3sjYiiC0Yr7U4CXxNK7ALJ962nVxo/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "find-yourself-on-the-mat",
      "sourceId": "PYKTTA",
      "title": "Find Yourself on the Mat",
      "description": "By master Aoei \r\n- Self-tune\r\n- Find yourself along the journey with Oracle Cards\r\n - Gentle yoga flow & Stretching for Office Syndrome\r\n \r\nNov 12 16:45 - 17:30",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T09:45:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/1TFFR57Pxj41MY1aoKmiTItEaSPtPRaK4-BbRLFZTnQQ",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "dark-daos-and-private-coordination",
      "sourceId": "SX8B9E",
      "title": "Dark DAOs and Private Coordination",
      "description": "Dark DAOs allow for undetectable private coordination and are feasible to launch in Ethereum today. In this talk, I will introduce Dark DAOs, highlight applications that should be aware of their possibility, and point to the ways they can be harnessed as mechanisms for both prosocial and antisocial coordination. I will also discuss how the encumbrance of keys utilized by Dark DAOs can generalize. I will introduce Proofs of Complete Knowledge as an available countermeasure.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Privacy,dark,Coordination,DAO,Privacy",
      "keywords": "encumbrance,TEE,Dark DAO",
      "duration": 1515,
      "language": "en",
      "sources_swarmHash": "a97c78c1b59811eb08440fb9b149c05cd099f82a29ca38cecae87b77c00ab27e",
      "sources_youtubeId": "dC3pIbvEte0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67335b2b3a168eb53591f192",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1t0x6tAJgffLnu_2grB_zh1mp_RzOCOpsI9MX1oYUMlY",
      "resources_slides": "https://drive.google.com/file/d/1nSPoVGUa-X0K_WHRAeIRS-Ee5YovlOzb/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "decentralize-your-sequencer-a-guide-for-l2s",
      "sourceId": "BNBCVC",
      "title": "Decentralize your sequencer -- A guide for L2’s",
      "description": "This talk will act as a river guide exploring the design space for L2 sequencer decentralization. It will cover:\r\n\r\n1. Should L2’s care about decentralizing a sequencer?\r\n2. What does it mean for UX?\r\n3. Forced Inclusion ≠ Decentralised sequencing\r\n4. ELI5 the approaches being taken by L2's\r\n5. Based rollups to the rescue?\r\n6. What are for optimistic / zk and / privacy rollups\r\n7. L2 Consensus networks are not the solution\r\n8. Decentralisation is not just about sequencing rights",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zk Rollups,Sufficient decentralization,Decentralization,sequencer,Decentralization,Sufficient decentralization,Zk Rollups",
      "keywords": "Sequencer,Decentralisation",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "c9a23e3b6a9906ea44962ac58a6e09b68f3d360182917fcd270688efa206aee9",
      "sources_youtubeId": "UmvxGOLEMLc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1faHlm1Vau0v1_f53rf67KFbBYY4FT3pugB04UolFn_M",
      "resources_slides": "https://drive.google.com/file/d/1NlDWwn2TMsryPhQyAL1cVqaq01pZV7Z_/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "elliptic-curves-and-snarks-past-present-and-future",
      "sourceId": "Y3PMMA",
      "title": "Elliptic curves and SNARKs: past, present and future.",
      "description": "Elliptic curves are used in many proof systems. Some systems (e.g. Bulletproofs) use plain curves (e.g. ed25519). Some (e.g. Groth16, KZG-PLONK) use pairing-friendly curves (e.g. BLS12-381). Some recursive systems require pairing-friendly 2-cycle (e.g. MNT4/6) or 2-chains (e.g. BLS12-377/BW6-761). Some other recursive/folding systems require plain 2-cycle (e.g. Pasta). In this talk we will go through the difference between these curves and why there isn't a silver bullet curve for all scenarios.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Cryptography,SNARK,elliptic,curves,Cryptography,SNARK,ZKP",
      "keywords": "elliptic,curves",
      "duration": 1518,
      "language": "en",
      "sources_swarmHash": "d418d4f93106c8a1c844d7ddadd6ef00204c7d15d551d1e3a9732f82c007bf46",
      "sources_youtubeId": "Bey043R_52k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/15MaGmHzAvHj765BvqDHs0ZxiiGevi3H9hscAvkCAGTc",
      "resources_slides": "https://drive.google.com/file/d/1EAw9Aa4uOHBB6FS3nuABbJHpHCyd6S7R/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "how-web3-and-rwas-unlock-exponential-wealth-via-a-computable-economy",
      "sourceId": "GFAA97",
      "title": "How Web3 and RWAs Unlock Exponential Wealth via a Computable Economy.",
      "description": "Keynote based on Justin Banon And Prof. Jason Potts academic paper:  How Web3 enables the transition to a new computable economy and exponential growth in economic complexity, wealth, and prosperity by extending the reliability and programmability of on-chain transactions to the entire economy via RWA tokenization. Web3 is not just a new information technology, it is a new institutional technology on the scale of language, writing and code.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "RWA,Economics,web3,Economics,RWA",
      "keywords": "Web3",
      "duration": 1461,
      "language": "en",
      "sources_swarmHash": "7973ee90af2d2e28084a7ccfb9e884a2054adff1ea6d872dfcad14f5d7a07916",
      "sources_youtubeId": "Mf8KUbNrO58",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733353b3a168eb535cc568c",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1rY0yIyNGkdtc2aIioukR3vUzIU0ERrllvWthuyIH1UU",
      "resources_slides": "https://drive.google.com/file/d/1XPGaufR9IeaZorFFzBSiuM5zplJyZOVM/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "keynote-title-redacted",
      "sourceId": "8GH8TR",
      "title": "Keynote: [title redacted]",
      "description": "[description redacted]",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": true,
      "doNotRecord": false,
      "tags": "Consensus,Ethereum Roadmap,cryptoeconomy,Consensus,Core Protocol,Ethereum Roadmap",
      "keywords": "beacon chain,research,cryptoeconomics",
      "duration": 1582,
      "language": "en",
      "sources_swarmHash": "93e32548a38d598d71fdd21d2c49f3012ba3a510cc1214362a4ebbda8529763e",
      "sources_youtubeId": "lRqnFrqpq4k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67338ac53a168eb53526cd0d",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1hcsmjIHu5W9-usVg_e3DGrH4QnmLER-OPOZ_0ccXjKU",
      "resources_slides": "https://drive.google.com/file/d/1qTsFVcfzJoIbybFV-_c6WhA77ExxnGNH/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "opsec-for-the-dark-forest-or-how-to-avoid-getting-rekt",
      "sourceId": "TAEPPF",
      "title": "OpSec for the Dark Forest (or how to avoid getting rekt)",
      "description": "We will focus on the most important things you need to do to have a good OpSec to survive in the Crypto Dark Forest. I will cover: computer, mobile phone, email, telegram, social media, phone numbers, password  managers and 2FA strategy, security software & social engineering.\r\nThis is based on many years of experience and in the cases we see daily on SEAL 911.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Security,Hacks,2FA,dprk,2FA,Hacks,Privacy,Security",
      "keywords": "OpSec,Social Engineering,Malware,0days,DPRK",
      "duration": 542,
      "language": "en",
      "sources_swarmHash": "0fb90958816f38c563510ad9f68ada525a114c7dbdf95c1534f4a4675a6e902c",
      "sources_youtubeId": "nM2BBNlIRe4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1jLrqWU4lm17NODOESY5ysFcreo3AXNtlq_mO-78OMZY",
      "resources_slides": "https://drive.google.com/file/d/1DQLkWajUKovIrzj9DazShb8WX1o9TEXe/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-next-era-sequencing-and-its-real-impact-on-app-users",
      "sourceId": "9M78AK",
      "title": "The Next Era: Sequencing and Its Real Impact on App Users",
      "description": "This talk will discuss app sequencing products which were developed to enhance decentralization and security via distributed transaction ordering with independent sequencing (native Mainnet L2 sequencers i.e. Base, OP) and the impact to end users and applications. It will also discuss the tradeoffs of LVR, shared sequencing, and app-specific sequencing.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,User Experience,Transaction fees mechanisms,sequencer,Layer 2s,Transaction fees mechanisms,User Experience",
      "keywords": "Sequencing",
      "duration": 975,
      "language": "en",
      "sources_swarmHash": "707cb042ec5704ebd467c773dedb087d6fc5a3c474c0c41441a2ed12ac9ec02d",
      "sources_youtubeId": "-S2rlhSUHZY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:00:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1l63vZZz_0RN-aU0hwjhmdAat5Fq0OFy7UoMYiS3KJxc",
      "resources_slides": "https://drive.google.com/file/d/1EZcCr_PB3hKeryX58xnCfgzh5KsXRnOK/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "redefined-interactions-transforming-user-experience-with-intents",
      "sourceId": "Q3SF8Q",
      "title": "Redefined Interactions: Transforming User Experience with Intents",
      "description": "Intents are on their way to improving users' interactions with DeFi. This panel of experts from leading protocols will discuss the impact of Intents on user experience, focusing on streamlining processes, enhancing security, increasing decentralization, and making DeFi more accessible. Explore the future of user interactions in DeFi and the collaborative efforts driving these advancements.",
      "track": "Usability",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "User Experience,Intents,defi,Intents,User Experience",
      "keywords": "DeFi",
      "duration": 2896,
      "language": "en",
      "sources_swarmHash": "b62c90d88cd31739d845fd3c69549705e18eb151c919421eddbcaa08ad72ab94",
      "sources_youtubeId": "hId-FQUOpJ0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:10:00.000Z",
      "slot_end": "2024-11-12T11:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1pQP77cQCgded-4Om05CtsNholtmf6N8hdDeVEVTDvKU",
      "resources_slides": "https://drive.google.com/file/d/1Dhpszofn8VleUqzXfNDiIh0QAnU7aaMC/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "security-through-obscurity-using-microdots-to-store-secrets",
      "sourceId": "UHQDPU",
      "title": "Security through obscurity. Using microdots to store secrets.",
      "description": "Key custody remains a tricky problem to solve. Most of the focus around improving the security of key custody revolve around software based approaches like secret sharing. However, physical approaches are also possible. \r\n\r\nThis talk discusses on how to secure secrets using microdots and how microdots may be fabricated at home with legally accessible tools.\r\n\r\nMicrodots is a technique which allows one to shrink documents down. This allows one to embed secrets in documents in plain sight.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Lobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "Digital Sovereignty,Cryptography,Security,Hardware wallets,Custody,Cryptography,Custody,Digital Sovereignty,Hardware wallets,Security",
      "keywords": "None",
      "duration": 579,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "k9Dfg19JPEw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:10:00.000Z",
      "slot_end": "2024-11-12T10:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1zGqyVZiy__TgQYZes9fefN5S6uBUQLT9Yl6wbxjJ-2M",
      "resources_slides": "https://drive.google.com/file/d/1NvOTq-c11nvheTGJj72lD0LAQJCrQhZP/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "rethinking-user-risks-at-l2beat",
      "sourceId": "8YKV8H",
      "title": "Rethinking user risks at L2BEAT",
      "description": "We want to announce a new L2BEAT feature of viewing protocol risks that individuals are actually exposed to. When we researched risks in the past users didn't find the information relevant, because they weren't aware they were using a specific protocol. Bridges are one example where users forgot about escrow risk as soon as the funds were bridged. In this talk we'll show how rollup risks translate into risks associated with individual assets held by users.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Token bridging,Security,trusted,Layer 2s,Security,Token bridging",
      "keywords": "risk,trust",
      "duration": 523,
      "language": "en",
      "sources_swarmHash": "5426184a342e67741c5784af3fa3bad843721262e251fc34edd8c6527df7d9e9",
      "sources_youtubeId": "CM6e_wwieeo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:20:00.000Z",
      "slot_end": "2024-11-12T10:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1eDeIVW8yw0TTm6i7x1PFeXMtab7BfMey3gIO056ytDw",
      "resources_slides": "https://drive.google.com/file/d/1j1FSVwP2PC5EuMa4fndo7QchramS8SLe/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "a-playbook-of-secure-smart-contract-development",
      "sourceId": "HVX8WB",
      "title": "A Playbook of Secure Smart Contract Development",
      "description": "One-off audits can provide a good security baseline but fall short in continuous security assurance, especially for upgradeable and actively developed protocols. We'll cover how to set up the smart contract development processes to ensure the top level of security guarantees, including design review and property specification stages, as well as the integration of security tooling, including testing, fuzzing, and formal verification, into the CI pipeline and development lifecycle of a protocol.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Security,Best Practices,ci/cd,Best Practices,DevEx,Security",
      "keywords": "Development Processes,CI",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "82a6c0d37546dd4c2219242187c8d128050ef2534ab04d1ffb07bd5ecb7633aa",
      "sources_youtubeId": "88hS4MsUwR4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T10:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1w5ssCjwzeMzULOUFJzm_OVAD4K7Y8ivlYMu9NQds-Gw",
      "resources_slides": "https://drive.google.com/file/d/1XJH4bqZ4aunIU75rBUJ95WoUXxH-0qtr/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "are-l2s-extractive-to-ethereum",
      "sourceId": "ESERNB",
      "title": "Are L2s extractive to Ethereum?",
      "description": "An in depth study of on chain metrics, fee markets, contracts, and wallets to answer if L2s are extractive to Ethereum",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,User Research,Transaction fees mechanisms,debunking,Layer 2s,Transaction fees mechanisms,User Research",
      "keywords": "debunking",
      "duration": 1363,
      "language": "en",
      "sources_swarmHash": "b5b977302068601225e40569ffc8d69c685132a70133de755774793111af393f",
      "sources_youtubeId": "TP6roml9JTA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733fcc13a168eb53547f781",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6733fcc13a168eb53547f781.vtt",
      "transcript_text": " Cool. Thanks for coming. I know it's been a long day. Today I'll be talking about the topic of RL2s extractive to Ethereum? My talk, I'm going to cover three main areas. The first is on the topic of L2s keeping fees and not paying enough for blobs. This is quite a nuanced topic and I'm going to dive into it. The second, what I think is a more important question is, are L2s generating ETH-denominated activity in terms of ETH being used as a gas token, ETH being used as an asset, and has it generated new holders of ETH? And I think the third topic that I'm going to touch on is, what is the dynamic between L2s and Ethereum? So if you go on crypto Twitter, if you read forums, blog posts, et cetera, one of the most popular narratives is L2s are keeping the fees that they're generating and they're not paying enough for blobs. And you can kind of see this when you look at the margins for layer 2s post-EIP-4844, going from 20% to 30% margin on fees to upwards of 99.99%. But I think the nuance behind this is that blobs are still in the cold start period. And what that means is that it is a byproduct of the mechanism of 4844. And David Krapis predicted the cold start problem actually last year. And he ran a simulation from March 2023 using assumptions from the transaction volume on Arbitrum and Optimism at that time. And it would take roughly 250 kilobytes per block to get into price discovery for block pricing. That's roughly 1.8 gigabytes per day. And in some of the later charts, what I'll show you is that we're basically there on average per day, but not yet on a block-by-block basis. And actually, there have been some notable periods when blob pricing has gone into price discovery. This is an example from earlier this year, which shows the blob fee breakdown between blob space fees and execution fees during the layer zero airdrop. And you can see that for a period of roughly one to two hours, blob fees actually dominated compared to the execution fees. And actually, more recently, blobs have been getting really, really close to the target amount of blobs, past which the blob pricing would go into price discovery, and you would see this drastic increase in the cost of blobs faced by the L2s. But one of the interesting dynamics behind this is that layer twos are fundamentally businesses. And when you see a fee spike for blob pricing, we tend to see that layer twos actually start to slow down the pace that they post blobs in order to dodge these high spike moments. Another way to see this is through the distribution of blobs per block. So the gray color are blocks with no blobs. These blocks serve as a temporary buffer where, for instance, you might have a day where the total amount of data posted might imply that the blob fees should go into price discovery, but that mechanism is actually triggered on a block-by-block basis. But we're going from a world where half of the blocks had no blobs to a world where only 20 to 30% of the blocks have no blobs. And the same is true on a capacity basis. So in the summer of 2024, roughly 20 to 30% of the data capacity in terms of bytes was unused, and now we're approaching a period where only 10% to 20% is unused. And overall, Blob's inflow per day are quite healthy. They're actually well above the 1.8 gigabyte target, through which, if that were true block by block, Blob fee pricing would go into price discovery. So I think we're actually well on our way to that. So now that I covered some of the nuances behind the blob fee market, and that it's not necessarily a byproduct of the L2s not wanting to pay for blob storage. It's more that the way that the mechanism is designed is what is precipitating what we're seeing for the blob fees at the moment. So the you look at L2B and various block explorers, I'm sure everyone has seen a chart that looks something like this, where the L2 activity has grown tremendously over the past year, going from around 50 TPS up to 170, 180 TPS across the L2s. But L2's usage of ETH as a gas token has only grown modestly due to lower L1 settlement fees despite higher activity. That's partially contributing to things like lower burn, a lot of concern from the community. But again, this is more of a byproduct of the mechanism of 4844 and less about any nefarious intention. But I thought this chart was actually pretty interesting. So if you zoom out and you look at the collective fees in terms of ETH being used as a gas token, and you compare that to the gas fees used on mainnet, you can barely see the L2 line, which is great for users in that their transactions are quite cheap, but potentially some concern around the security of mainnet. On the flip side, the L2 usage of ETH as an asset has grown tremendously. So what we see here is a chart of the total value locked, stacked by type. The purple on top is the canonical bridged ETH. And that's just gone pretty much up only since L2's launched. So in summary, to cover the first half of the talk, in terms of the current state, there is not enough L2 blob demand to consistently get blob space into price discovery. That requires consistently block by block L2s consistently posting blobs in order to trigger the price discovery mechanism behind 4844. But I think we're almost there. Blob pricing today is currently in a bootstrapping period. It has shown periods of price discovery. This is known. It's been posted on the ETH research forums in 2023. There's nothing inherently wrong with the network or the way that the pricing is working. And it's actually approaching data inflow levels that would actually start to trigger and begin price discovery. And looking at activity on the L2s, we do see a lot of new usage of ETH as an asset in terms of being used in staking protocols and DeFi and et cetera, but very minor usage of ETH as a gas token relative to mainnet. So the second half of my talk, I'm going to talk about the dynamic between L2s and Ethereum. So, at the limit, L2s and mainnet appear symbiotic. Tim Robinson had a great post and he actually built this really great calculator where he shows that he's showing a scenario under which Ethereum, for Ethereum's blob market post-surge. So what this shows is a simulation for 10,000 TPS across L2s, which is pretty feasible and reasonable, I think. 16 megabytes of blobs per block, and that would imply a 6.5% yield on ETH given some reasonable user price sensitivity assumptions. So what that means is, is when the transaction fees go up, a smaller percentage of the L2 fees are very low, and we're in a period where we're probably most likely going to exit the bootstrapping period of block pricing, and we'll likely see a period where the L2 fees tick up. So, for example, in this scenario here, this uses the current specifications based on 4844. In this scenario, it assumes five roll-ups, 300 TPS per roll-up, the same user price sensitivity assumptions. And what you can see in this chart, and I think the interesting line to call out is the green line, which is the actual TPS. That reflects user sensitivity to higher transaction fees. And in my opinion, five roll-ups and 300 TPS, if you look at the number of projects kind of in the pipeline, that is pretty doable in terms of what we might expect to see over the next year. And what we can actually see in this chart is it implies that the Ethereum blob market could potentially serve as a constraining factor on the ability for these rollups to increase their throughput and increase activity. So in terms of the question, are L2s extractive? I think the question is actually very nuanced and is actually quite complicated. And I think the true answer is that we don't actually know. And I think there are three reasons for that. The first is the L1 settlement fees have never actually been high enough to test Ethereum's network effect. So what that means is we've never actually seen a scenario where L2s have been faced with increasing L1 settlement fees. So we're really uncertain about how the L2s might react in that type of scenario, whether they might look to an alternative DA solution, whether they might be forced to decrease their margins, whether they might potentially even become an L1 on their own. So I think that first scenario, we've never actually really seen that in practice. So we don't really want to impose that judgment upon the layer twos. So the second point is that there's a lot of value for L2s being L2s. And I think beyond just the label of what people might say, oh, like, I'm going to launch an L2 on top of Ethereum, what they actually get is quite a lot. So imagine having to write your own block explorer, your own virtual machine, your own smart contract programming language, recruiting developers to learn your smart contract developing language. And I think the reality is that the L2s are relatively new. And because they are relatively new, they benefit tremendously from being able to bootstrap their virtual machine tooling, developers, users, community assets from Ethereum, and even in terms of applications as well. And I think the third reason why we don't actually know whether L2s are extractive today is that most of the analysis, in terms of even Tim Robinson's dashboard, tends to assume that you have N equal-sized L2s, which one might consider the current state because we're kind of in this moment in time where there isn't a clear breakout L2 winner. And whether you look at TVL, whether you look at transactions, whether you look at number of wallets, whatever metric you look at, generally you tend to see fluctuating periods where you have three dominant L2s and they're all splitting market share at different points in time. And I think that type of analysis and thinking about the current state is probably not likely to hold into the future. So it doesn't really consider the case where you have one or two L2s that take 80 to 90% of activity and economic value, whereas the remainder have marginal economic activity. And I think this is actually a really important case and scenario to think through, because when you look at these financial networks, generally you do tend to see power law effects. And I think the scenario in which you have one or two very dominant L2s, the question about where does the market power and the leverage between the L2s and mainnet, at what point might that flip? And I think probably more work has to be done on that side. So in the final few minutes here, I want to talk about some of the hypothetical conditions where L2s could become more attractive. So the first is about if the L1 has limited transaction processing capability. And I think this is important because I think, first off, the economic situation of ETH, the asset, could become more risky if the amount of fees that are being generated on mainnet are not high enough to ensure that a large portion of the network ends up being staked. I think L2s also tend to benefit from highly developed financial system on the L1. And I think the second scenario, which I talked about a bit earlier, is the case where you have a power law effect for L2. So for example, a dominant L2 would be the majority blob purchaser. And with very clever batching, they could potentially pin blob pricing below price discovery. That's actually happening organically today. So if you look at how the L2s behave in periods where blob pricing does spike, we're already seeing behavior that suggests that they're willing to spread out how quickly they sync their state across time. And the consequence of that is that users who might want to look for a unilateral exit from the L2 might not be able to do that if the L2s are being more cautious and doing more optimization about how frequently they post. The next hypothetical scenario is a case where the incentive structures for L2 tokens or non-ETH gas tokens become more prominent. So if you look at the breakdown of activity on L2s and you look at the split between the L2s that are using ETH as a gas token and the L2s in quotation marks or L3s that aren't using ETH as a gas token, today that split is actually 50-50. So I think the notion that every single roll-up necessarily has to use ETH as a gas token, we're already seeing some scenarios in which that might not necessarily be the case. And I think that's related to one of these other points, which is the notion of a siloed L2 with their own settlement kingdoms for L3s or similar types of roll-ups. That's another scenario in which the L2s might push for an alternative gas token, which again would not benefit ETH as an asset. And I think the final point is potentially incentive structures for L2s in which they start to develop competing security models in terms of being able to use their native governance token for things other than just governance could potentially be a condition in which the L2s could also become more extractive. So that concludes my talk. I think it's still ultimately too early to tell in terms of whether the L2s are extractive or not. The reason for that is we've never actually seen a scenario in which the L2s have been put in a position to make a choice, whether they stay as an L2 or they use an Alt DA layer or they become an L1 or they do something else. But I think that time is actually coming very soon. So probably likely over the next one year, we'll start to see some of these choices being forced just by the nature of the blob fees going into price discovery. Thanks. All right. Thank you, Ren. Feel free to come to the center over here. Yeah, this one will be fine. So let's go over some questions. Let's start from the first one. Do you think the exact copy of EIP-1559 mechanism determining L1 fees is the right approach for determining blob fees? Yeah, I think this is a really tricky question. Yeah. I think this is a really tricky question. So I think where my mind jumps immediately is I look at the amount of execution that has been moved off from the L1 to the L2 and the amount of transactions on the L2 today greatly outweigh the amount of transactions ever in Ethereum's history in terms of like per second or per block. But in terms of ETH being used as a gas token for those transactions, it's actually quite minimal even compared to the earliest periods in Ethereum's history. So I think one of the things to consider is that that ratio might be slightly off. And I think the only way you really see that settle is blob pricing has to go into price discovery, which means there needs to be more L2s, and that will likely happen over the next year. Cool. And another question. Currently, the L2 mainnet seems symbiotic. But what if they started paying fees in their own tokens? They have a fair amount of incentive to keep assets logged in their system without interoperability. Yeah, I talked to this point in the last slide about hypothetical scenarios. So I think some of the highest forms of risk are exactly that. So, for example, L2s in their current state as having their tokens as a governance token, it's highly likely that's, it's probably more likely that changes in the future than stays the same. So I think that is a very real risk. Have you or others been able to isolate the decrease in L1 execution demand due to real transactions from the reduction in call data demand? Will L1 execution demand go to zero? So I don't think L1 execution demand will go to zero. So a chart that I didn't include just because it's a very basic chart, is that the number of transactions on Ethereum has actually not gone down. And the amount of gas, in terms of gas consumed, has not gone down. It's that the nature of the transactions have changed. So historically, the transactions on Ethereum had been transactions that had a lot more right conflict or right contention. And right contention leads to higher fees because people tend to be rushed in terms of those types of transactions. So for example, if people are trading various forms of assets, there's a high degree of urgency that those transactions land. But most of that activity, at the moment at least, has shifted to the L2s. Perfect. And how to prevent a single L2 from eating up all the space or spotlight, aka base, not from the tech perspective, but from the marketing public perspective? Yeah, so I don't necessarily want to call out a single L2, but I think the hypothetical scenario of an L2 becoming the dominant L2, where 80 to 90 percent of the economic activity flows through there, I think that's a very real risk. I don't believe there's been very many payment networks in the past where you haven't seen a monopoly or oligopoly type of structure just due to the nature of network effects in these types of financial systems. So I think that risk is very real. Perfect. And I think this is a great comment to close off the speech. Thank you so much.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1vwJxkmHD52YwdthsPLe4BX6pJ2iLQrw9Qqa8nA5c4cI",
      "resources_slides": "https://drive.google.com/file/d/1UoPixl8ypsnAk4CFZFYyPkrX-sKf1YY3/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "keynote-the-next-10-years-of-web3-in-africa",
      "sourceId": "GSNQLC",
      "title": "Keynote: The next 10 years of Web3 in Africa",
      "description": "When Africa reaches 2 billion people, what are the profound ways Web3 shapes its economy? Historically, millions of Africans repurposed and stitched together crypto tools for real-world utility. Now, a new generation of builders is developing tailored solutions. In the next 10 years, what can we expect to be built that redefines trust and finance in Africa? And what needs to be true for more than half of African economies to be powered by decentralized technologies?",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": true,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,Use Cases,macro/micro economics,adoption,africa,mass,Ethereum Roadmap,macro/micro economics,Use Cases",
      "keywords": "Africa,Mass adoption,",
      "duration": 1531,
      "language": "en",
      "sources_swarmHash": "2415d9c9f111fd9ab297194c0cc7b8de5a938accb641bfdb907c2ca01e0958d3",
      "sources_youtubeId": "DRAs6Yh7g4I",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733f64b3a168eb53542528d",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1IAQR41JAk7FPn24OGhprL4uyoP17OlBMG8dv6oyQ_n8",
      "resources_slides": "https://drive.google.com/file/d/1SIHPEYzOTtYz9BIoMkgVFbQFlxH_My1Q/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "optimism-retro-funding-so-far-so-good-so-what",
      "sourceId": "QCMZS8",
      "title": "Optimism Retro Funding: So Far, So Good, So What!?",
      "description": "So far, over 50M OP has been awarded to projects with no strings attached. So good, another 800M OP is planned for future rounds. So what ... is the impact? My talk will offer an objective, data-driven perspective on the \"so what\" of Optimism's Retro Funding. It will include analysis on how different cohorts of projects have performed longitudinally across a variety of growth and quality metrics, while controlling for different funding and market-related effects.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "RPGF,Collective Intelligence,Open Source Software,grants,Collective Intelligence,Open Source Software,RPGF",
      "keywords": "Data Science,Impact Measurement,Grants",
      "duration": 1542,
      "language": "en",
      "sources_swarmHash": "047944c236f3e1dd0245dbd16955b54f0bc3a72e7dfec5f04b2ab12b56574f74",
      "sources_youtubeId": "pz4vGh53qSo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673358ad3a168eb535865df1",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/13Pt_GSxCedQkGTiptcOxzfpSOiZRApdYLaDdfjTzw8A",
      "resources_slides": "https://drive.google.com/file/d/1OFjPmwCYJt0dPVSjmkeQ2tJW8Tf3d5PC/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "postcards-from-the-cutting-edge-of-gas-research-what-you-dont-know-can-hurt-you-and-your-users",
      "sourceId": "X8VZDJ",
      "title": "Postcards from the cutting edge of Gas research: what you don’t know can hurt you & your users",
      "description": "In July of 2024, we shared original research describing how the interaction between privately transmitted transactions and altruistic self-built blocks unexpectedly increase Base Fee volatility (see references below). We also warned that this effect would likely get more pronounced as private transaction share continues to grow. In this session we will revisit our original findings but with 4 months of additional data and deeper investigative research. Has gas price volatility increased as predi",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "eip-4844,Gas,Layer 1,UI/UX",
      "keywords": "1559,Blobs,4844",
      "duration": 434,
      "language": "en",
      "sources_swarmHash": "a727fa169242eec4b80126341a1150efb4a45bc5a1b4a6a288a8c0e8bf19c107",
      "sources_youtubeId": "NKGOZ154rPM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T10:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1AzgmOOm16-VrlFGtmsr5MOvsAabE-h1nClU9xydV9I4",
      "resources_slides": "https://drive.google.com/file/d/1uyzMjnZLyMuDfMc6HN5tWiAbrNz3s9eQ/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "the-history-and-philosophy-of-cypherpunk",
      "sourceId": "8JVYCQ",
      "title": "The History and Philosophy of Cypherpunk",
      "description": "Rather than bend to knee to Donald Trump, the goal of the cypherpunk movement is to abolish the state in order to maximize human freedom via privacy-enhancing decentralized technologie. After reviewing the history of this deviant group of programmers in the 1980s, what philosophical and technical lessons do the cypherpunks hold for Ethereum today? Censorship-resistant digital cash was only one the start, and the missing parts of their legacy: mixnets and anonymous credentials for identity.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Anonymity,Censorship Resistance,Digital Sovereignty,cypherpunk,mixnet,cryptoanarchy,Anonymity,Politics,Values",
      "keywords": "mixnets,cypherpunk,cryptoanarchist",
      "duration": 1555,
      "language": "en",
      "sources_swarmHash": "89ddf65d60e9d080ec70f2820e3674c757d151a0741047ec721bd81ba034a27e",
      "sources_youtubeId": "OZwG_Tx1hdA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673351433a168eb5355b5f3c",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1ovH3oyNrS_ZaZbKCeLkHxgPjrRCAzaWP7RVIf9TRkOo",
      "resources_slides": "https://drive.google.com/file/d/1PuUJE7912QacTfZSG5XsYskVHL4tIxrY/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "zkmpc-bring-public-auditability-into-mpc",
      "sourceId": "XNN3XR",
      "title": "ZKMPC: Bring public auditability into MPC",
      "description": "In multi-party computation (MPC), participants collaboratively compute without revealing private inputs. To secure MPC on a blockchain, preventing collusion is essential. We developed a \"publicly auditable\" version of SPDZ, a widely-used MPC protocol, that enables third-party verification through zero-knowledge proofs (ZKP) collaboratively generated by multiple parties. We will also demonstrate application examples, such as a Game Master-free werewolf game.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,MPC,collaboration,zk-snark,MPC,ZKP",
      "keywords": "Collaborative,zk-SNARKs",
      "duration": 1399,
      "language": "en",
      "sources_swarmHash": "84b05559d4df707a8f29bbb79e18bb1bdb1fff62ae2738288c7d4be463f3b188",
      "sources_youtubeId": "aWQ8zzi1EAQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:30:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/10GOrQfQ0ldlyvKU05TvdHfQd4G2zNNTzfEe2i2bfgMQ",
      "resources_slides": "https://drive.google.com/file/d/14MIJiVuYvN9GsgXtB9LkgWl_8Bwt1J5s/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "lighthouse-introduction-to-siren",
      "sourceId": "F3ZPRJ",
      "title": "Lighthouse: Introduction to Siren",
      "description": "Sigma Prime would like to introduce Lighthouse's official user interface called Siren. Siren was made to monitor performance, display key metrics and help make lighthouse validator management easy. Siren comes with built in metrics, logging, and other features users will find useful when updating their validator.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Home staking,UI/UX,Accessibility,ui,Accessibility,Home staking,UI/UX",
      "keywords": "lighthouse,UI",
      "duration": 388,
      "language": "en",
      "sources_swarmHash": "581e106f3b22dfacc1d56771706969f972348e514d3d6a94afc75aac47317df4",
      "sources_youtubeId": "RhlKmJqk0go",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:40:00.000Z",
      "slot_end": "2024-11-12T10:50:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1iWFucLqzajqGIcn5d4YFuRZ1zk1Y8VHURhoTiKQ1T-w",
      "resources_slides": "https://drive.google.com/file/d/1lwroo0zWbWBkMkkJx2L-r2Ug7eDQHyGN/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "the-10-most-common-vulnerabilities-found-in-audit-contests",
      "sourceId": "LYFXZN",
      "title": "The 10 Most Common Vulnerabilities Found in Audit Contests",
      "description": "This lightning talk offers a quick survival guide for DApp developers and security experts, highlighting the most common vulnerabilities found in audit contests. As these contests are often the final step before mainnet, the identified vulnerabilities have typically been overlooked by multiple developers and auditors. The session includes a link to a guide on fixing each vulnerability and a 2-minute Q&A to explore any of the 10 vulnerabilities in more detail and discuss why they are often missed",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Auditing,audit,contest,Auditing,Security",
      "keywords": "Vulnerabilities;,Audit,Contests",
      "duration": 595,
      "language": "en",
      "sources_swarmHash": "3103f2e82576803c887da36c890760dec4bb346076f23924fe2e0ecaf42099a0",
      "sources_youtubeId": "MT7mYhwgksI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:40:00.000Z",
      "slot_end": "2024-11-12T10:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1_iMeu-TIt6aOehgouo5xQOCb89l5Su5oE2WffTDcOII",
      "resources_slides": "https://drive.google.com/file/d/1y7ExLB5m_41dHQpU0shgISHC5VGpS21i/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "evolution-of-scams",
      "sourceId": "WZWPE9",
      "title": "Evolution of Scams",
      "description": "The goal of this talk will be to give a quick history of the evolution of scams and the new techniques employed to combat them. I was previously the co-founder of Wallet Guard, which has since been acquired by Consensys. I now am responsible for the research and development of the security engine employed by MetaMask to protect its users.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "metamask,Hacks,Security",
      "keywords": "Security,Drainers,MetaMask",
      "duration": 558,
      "language": "en",
      "sources_swarmHash": "10b2a71955b16582577039f8e25b02ba983b4c003975aa7d0a9f7e11ca72f537",
      "sources_youtubeId": "SgkEwSDkBnI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:50:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1fLuDyHluumURppoq7gyTD9d7Z-wKLdy5qsCg-Tytso0",
      "resources_slides": "https://drive.google.com/file/d/1Vz1SLhZKIwhKSL27b_6HrlSDrJTI1Vfu/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-state-of-web3-support-today-what-just-happened",
      "sourceId": "BZRKUD",
      "title": "The State of Web3 Support Today: What Just Happened?",
      "description": "One of the most common and painful experiences someone can have today is also one of the most fundamental concepts we tend to take for granted - transactions. Users who seek support for their issues lack the appropriate level of information to even understand what they were doing when it all went wrong. This talk will examine why core web3 experiences are still problematic and propose things to consider when building experiences for everyone that ranges from in app UX to community support tools.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "community,Accessibility,Tooling,User Experience",
      "keywords": "User Support,Community",
      "duration": 304,
      "language": "en",
      "sources_swarmHash": "fb6714e3f29aebfbf0c0287d93a797c37483f8f4909ffb6478031e93712229e4",
      "sources_youtubeId": "sur3bRJQw-U",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T10:50:00.000Z",
      "slot_end": "2024-11-12T11:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1jmtrpYtos5-qZy0sfliSMlhtQfUi9JSCAcTEP4C554k",
      "resources_slides": "https://drive.google.com/file/d/1uJ7lYFZ6AxX4mXijjN9idjzNk1LjnYNP/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "beyond-ligero-and-brakedown-building-a-fast-prover-based-on-list-polynomial-commitments",
      "sourceId": "L7YER9",
      "title": "Beyond Ligero and Brakedown: Building a Fast Prover Based on List-Polynomial Commitments",
      "description": "Linear codes underlie one of the main approaches in zero-knowledge proofs and arguments, including works like FRI, Ligero, Brakedown and Orion. In this talk, we describe how to extend one of the protocols from Ligero and Brakedown to the regime of batched polynomial commitments, at the cost of a single extra operation in the verifier. Similarly to Redshift, we opt for increased efficiency via the list decoding regime. We also present an optimisation for using the resulting commitment with PIOPs.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Rollups,Zero-Knowledge,Cryptography,Security,provable,Cryptography,Layer 2s,Rollups,Zero-Knowledge",
      "keywords": "Provable,Security",
      "duration": 1518,
      "language": "en",
      "sources_swarmHash": "35f25ffc9ab45f0268459c63c3352846e73f664bd476c7e32502f24f3e3bd539",
      "sources_youtubeId": "V7hmwJ-l0qY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1xTWn8OHn3Uo4DFB83e-yHN6sTgrIz_lfqTY2XGPrx98",
      "resources_slides": "https://drive.google.com/file/d/1BD6GcDhQxrmMGz4FbfAtS5ZoJf91IqUK/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "common-knowledge-machines",
      "sourceId": "XJPE8K",
      "title": "Common Knowledge Machines",
      "description": "Common knowledge is a precondition for collective action. Yet, increasing polarization in information ecosystems risks undermining common knowledge formation. This talk introduces Community Posts, a mechanism that leverages diversification and zero-knowledge proofs to help people identify divides, bridge them and find common ground, fostering greater common knowledge in social networks.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": true,
      "doNotRecord": false,
      "tags": "Censorship Resistance,Collective Intelligence,Consensus Mechanisms,Coordination,Identity",
      "keywords": "Stellar,Punk",
      "duration": 1057,
      "language": "en",
      "sources_swarmHash": "2fadd824928c32a979645300678ee71dea6d46f238ba5f1acf48e3791e8e8005",
      "sources_youtubeId": "YJz8BRromJQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67333e1d3a168eb535fd908b",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:10:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1a63MEhn5HWzDRARwTVIqwlXGcTuEurRAij2MM5-ACMI",
      "resources_slides": "https://drive.google.com/file/d/150sqO_98C8rtvEMVq5crZTtA6kSsccax/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "decentralizing-economic-opportunity-communities-using-crypto-and-decentralized-protocols-to-make-local-economic-impact-in-brazil-nigeria-and-kenya",
      "sourceId": "SRYTXY",
      "title": "Decentralizing economic opportunity: Communities using crypto & decentralized protocols to make local economic impact in Brazil, Nigeria & Kenya",
      "description": "In communities facing economic scarcity, decentralized currencies are seen as a transformative solution. But what is their real-world impact? This talk explores the stories of three communities using crypto to drive local economic opportunities. It examines what brought them to crypto, the pros and cons of adopting tokens and focuses on diverse use cases like UBI, credit, and community currencies. Features video, data, and impact metrics of the people on the ground in underserved economies.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Use Cases,Ethereum for Good,Local Impact,market,emerging,Ethereum for Good,Local Impact,Use Cases",
      "keywords": "ReFi,Emerging Markets",
      "duration": 1644,
      "language": "en",
      "sources_swarmHash": "d5621e2ea0a49b23844b7151c58ef13724516abf76cbd7ebc6f88e1234e217aa",
      "sources_youtubeId": "VduhOSI-CxQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6733f6d03a168eb535426732",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6733f6d03a168eb535426732.vtt",
      "transcript_text": " Hello everyone. Hi everyone. Stick around because that was really the perfect introduction for our talk. In a wider framing for how decentralized currencies are actually being put to work in different emerging economies. I'm really excited to talk to you about how we're using the free permissionless distribution of money to really impact people on the ground in last mile communities that are currently underserved by traditional financial systems. I am here as the executive director of a project called GoodDollar. GoodDollar is a universal basic income protocol whose mission is actually to enable anyone with access to a smartphone, with a cell phone, to be able to access decentralized currencies and actually onboard into crypto. We do this through distributing a digital universal basic income every day, and that's helped us onboard nearly a million people. I am thrilled to be joined here today with Jambi Jorge, who is the director of grassroots economics, which is, I would say, probably one of the leading projects, if not the leading project, that's actually innovating with decentralized currencies and really breaking the monopoly on money in terms of helping communities to create more credit, more commerce bottom-up through using decentralized tooling. And it's a great example of Ethereum in the real world. I will say that I've been building in this space for a long time, and it makes me very excited to see the talk that came before, if you will, that really shows that this progress is happening at scale. And Jambi is, I'd say, the zoom in, the double click in, as to the framing that we just saw. So some of you guys might have seen, crypto is useless. Some of you guys might have seen this tuk-tuk yesterday or seen some people walking around with a crypto is useless hat. And the idea here is actually a very clever campaign out of Celo, which is that crypto is useless unless you've had a real need to use it. And the meta-framing for all of this is that there are still 1.7 billion people around the world who are living in countries where inflation outpages wages. If the past few years have been any indication, that's going to continue to increase. There are 1.4 billion adults around the world that remain unbanked. That means people who have no access to any of the financial services that Joseph just mentioned in terms of savings, payments, credit, etc. Most of this 1.4 billion people have access to a very magic computer that they carry around in their phones, which is a smartphone. And that's what enables them to access the same Ethereum virtual machine that we all use. And so there's the 6.17 million crypto owners today represents a big gap to close in terms of where crypto still has its biggest potential to make a real impact on the financial lives of billions around the globe. We're here to talk about decentralized economic opportunities. So what are some of the principles that we're looking at? The first is permissionless access. So how are we actually removing gatekeepers? Crypto is the most obvious example of that. Anyone can sign up for a Web3 wallet. Anyone can send or receive currency. It is this permissionlessness that actually enables local empowerment to happen bottom up, where communities are able to grow in their own super cycle, if you will. And ultimately, this is designed to drive resiliency and adaptability for economies that, where it doesn't really make sense that the global macroeconomic shocks of, say, the U.S. presidential election have such a big effect on last mile farmers or communities all around the world. And so this is designed to actually increase resilience so that there's less of a single point of failure for individuals and communities. The reality is that still today, it's places where capital is needed the most, that capital is the most expensive. This comes as no surprise to anyone here from the real-world Ethereum track. But in Nigeria, just for context, for a $100 to $1,000 loan, you're looking at an annualized interest rate from anywhere from 30% to 50%, right? So if you're actually looking to start a small business, it means that money is more expensive for you than it is for many of us from the United States or from Europe. And the user experience has also kept crypto adoption from happening for a number of reasons, right? So up until very, very recently, it's been very hard to access Web3 in terms of the sign-up process for a wallet. Certainly, if you don't have access to a bank account, it's not that simple to go online and buy $100 of Bitcoin on Binance. All of this results in there being a very high cost of experimentation for individuals. And this is what creates this narrative around crypto being useless, right? Is that with all these challenges facing the space, it's hard to build products that seem like they're of real use. However, in 2024, we're facing a new reality where not all of our problems have been solved, but certainly the potential exists to make a massive change. We see that there are a number of simple mobile native solutions. There are over 100 million stablecoin holders around the world. This has probably been seen as one of the key killer use cases of crypto today. And this creates the preconditions for us to really begin to build for real communities and real needs. All of this is critical because everything happening on-chain, everything happening transparently, actually improves our odds for financial access because it enables individuals and communities to really build credit history in a transparent way. As we just discussed, it's actually lower middle income countries that are driving crypto adoption today. So India, Nigeria, Indonesia are the fastest growing crypto markets, and 15 of the 20 fastest growing markets come from low-income countries and Unsurprisingly, it's decentralized systems and currencies that are leading the way This is most likely because of leapfrog innovation, which was referenced before where when there's no pre-existing technology to overcome people just just jump directly to the next generation. And so grassroots economics, which has been building in this space for a long time, I consider one of the flagship leapfrog innovations in the space because they're innovating both on the payment rails and fintech level, but most importantly on the currency and the monetary level in terms of really breaking the monopoly on money and how communities can create their own value, create their own credit worthiness, and actually expand the amount of economic activity that they're doing within the community. This is where crypto has its biggest potential, and it's my true pleasure to introduce Jambi Jorge, who has been building this and implementing this on the ground for actually nearly 100 communities. And she's flown here today from Kenya to tell us about her work and some of the impact that it's had on their communities. Thank you, Anna. Yeah, my name is Jambin Jiroge. I have been working for Grassroots Economics Foundation for the last six years, and I'm really happy to be here. I am an advocate for community inclusion currencies for poverty eradication and sustainable development. So how do we heal a post-colonial legacy of torn communities and ecosystem? How do we enable communities to create scale and scale credit? How does this enable wealth creation to stay within the communities? This, commitment pooling. We've been working on commitment pooling, which is an economic protocol inspired by indigenous wisdom. So when I talk about indigenous wisdom, I mean rotation of labor association. I come from the Kikuyu community in Kenya, and we call it Mwethia. We have so many communities in Kenya with very different names for this indigenous practice. And it's not just in Kenya, it's even global. What you're looking at, the pictures here, the first one is a type of a rotation of labor, an educational kind of it. That is a service, women grinding maize for flour. That is an infrastructural kind of way and there's so many. So, this is the difference of what was and what is now. So, what was the shared economic commons? You know, we had synergetic commitments, harmonious flows, accountable systems, you know, ecosystem regeneration, democratic stewards, and like what we have now. You know, the fiat dominance, open markets, etc. Now, in communities right now, you know, the fiat dominance, open markets, etc. Now, in communities right now, they have these six assets. For them to understand they have the six assets and to understand at what level these assets are is very important, and that is what you're doing with communities now. And this image here is them realizing that they have it. Like, I have what you don't have, and you can benefit from one another. Creating that web is really visualizing what they can and what they have to offer one another. Now, I was talking about rotation of labor associations. So this is all about making commitments, fulfilling commitments, and accepting commitments. So just to explain rotation of labor in a bit, this is family one, family two, and family three. All the families have needs. They need a house built. They need a farm fixed. So what they do, they create commitments backed by, they create commitments, and then they say, okay, fine, I'm committing, I will come and help build a house for family one. I'm also committing that I will also help family two build a house. And eventually, they pull their commitments together, okay? So those are pulled commitments. So that means when I make commitments, I will accept commitments and I will also fulfill commitments. So when they all fulfill commitments, they are allowing accountability and exchange, and they have a clear and non-agreed-upon rules. So eventually when they do a cycle of helping one another, giving commitments, accepting commitments and fulfilling commitments, there is a range of physical assets grown, social assets grown, human, spiritual, political and natural assets, you know, they change. Now, Grassroots Economics Foundation has been helping formalize these commitments by adding technology and now it becomes easier for me to value what i'm offering in a community it's easier because they have a unique signature there is an expiration date you can you can decide to have a demo or not but a demurrage is very important to help with the circulation of the vouchers and then it also means that whoever accepts That voucher has the right to redeem it from the issuer. Okay? now Making commitments and fulfilling commitments and accepting commitment is paramount and this is what used to happen and it's now What you're advocating for but trust is very important. This is how trust was grown then. This is a nice illustration. We do a lot of games and practicals when you're training communities. And this is how they understand the current reality by understanding the six assets and at what level is which asset. And when they do this, they're able to come up with a vision and know how each person, each action step will be worked on according to the rotation of labor associations now this is production financing this is also a very important protocol that you've borrowed from our forefathers it only means that when the community members create vouchers, they put them into the pool. And when a financial wants to get some of the products that are backed by these vouchers, they will need to put in money into the pool and pull out of vouchers. I'm going to give a good example of a financial who pre-ordered coconuts from farmers in Kilifi. So what they did, they put money into the pool and took the vouchers backed by the coconuts. Coconuts take five to seven years to grow. So that means when... So the financier had the money before, prior, and the vouchers prior, and the community members got the money before. So that means they were able to sort out issues, fees, better farming methods, better tools. So we're just basically bringing a nice socioeconomic process that is about bringing together a nice blend of changing the traditional demand and supply models is about bringing together a nice blend of, you know, changing the traditional demand and supply models and, you know, tokenization. Now, this is what the communities in Kenya are doing. They're using the USSD, and there's also the web interface. So this is mostly, the web interface is mostly for those who do not have smartphones, phones, so they can create their own paper wallet, and they can use one phone. Now, we have so far seen over 65,000 households using vouchers in Kenya today. We have 4 million plus worth of transactions, USD worth and 5X average multiplier effect. So, pooled commitments enable us to scale into the future to our beautiful shared heritage. Thank you. Thank you. And so, this came up in the talk right before, but it's actually how do we use these systems rather to create new trust, but actually to unlock trust that actually already exists on the ground. And this is what I've admired, the work that grassroots and Jambi have been doing for so many years now is actually looking bottom up from the community infrastructure and developing tooling that actually enables those communities to unlock and assess value in new ways. Excitingly, this is one example of, I believe, thousands, if not hundreds of thousands that are happening around the world in terms of communities that are beginning to build circular economies using decentralized currencies that actually enable people to create their own value. This is a circular economy image from a community in Brazil that has hundreds of women that actually operate using the good dollar token as a community store to buy and sell services. We also have great examples of different entrepreneurs around the world who are using decentralized currencies bottom-up to create airtime shops, enabling people to use crypto to swap into mobile minutes, into airtime value, which actually is a basic need for anyone living in the contemporary world. And I think really critically the reason why this is relevant and relevant now is because now we start to compose the full stack that actually enables this to become practical and possible and it involves more than just technology, right? And so what we see is that building the stack bottom up from the technical layer, what makes it actually useful for real people, for the hundreds of people that Jambi and Grassroots have touched, is actually the social element, the community and education element that actually brings people along and customizes these technologies for their needs. So to bring it back, crypto is useful, but it's most useful in an ecosystem that has all of the different components of the stack that make it useful. And so this suit that I'm wearing is not just for fun. It is fun, though. It's to represent the idea of an ecosystem and the value of an ecosystem and that actually being the critical enabling linchpin to making the real world Ethereum useful. And I think everyone at DevCon who's made such an effort to come here to actually contribute to this ecosystem understands that. And the more localized ecosystems that we develop, the more we will see this come true. So yes, this is crypto today. Real world Ethereum is decentralizing economic opportunity. And I think we need to stay passionate and stay focused because this is still the biggest upgrade to money since it went digital. We see decentralized economies actually upgrading the money itself and it enables money to flow back to the parts of the economy that are otherwise deemed unprofitable or non-profitable and this is what grassroots and other projects are doing it actually enables the real wealth to come back into community because in you know many communities around the world the people are there the resources are there the desire to collaborate is there. But what's actually missing is money. And so if we can actually innovate on the money itself through using the right tooling, then we are able to support the decentralization of economic opportunity that we want to see happen. Thank you very much for listening. And thank you to the Ethereum Foundation for making this possible. All right. Awesome, guys. So again, Q&A session. We have the QR code on the screen. Scan it and send the question to the screen so we can see what you have burning question here. All right, guys. Let's see, I think people just wait a little bit. People are still scanning. I think we have about a few minutes while we're waiting for them to send the question. Anything you want to say, add on a little bit? There's something else I wanted to add. For the production financing page, when you put in money, community members can now withdraw that or ramp that via Valora, MiniPay, through Kotani, OneRamp, PhoneBank into Kenya Shillings. So it's really been amazing. And I would also want to thank Ethereum Foundation for making sure that I got here. Thanks. All right. You have a question? All right. Do you mind pass the mic? One second. Oh, here's the question. But let's hear from the ladies first. Okay. Okay, go ahead. Thanks for calling me lady. I'm not sure I identify that way, but sure. I'm sorry. It's okay. So, Jebdi, this is a question for you. What are the biggest barriers that are still faced? Because you've obviously had to overcome quite a few to get the technology in place to support these existing traditional systems. What are the barriers that still exist, if any, in utilizing technology in this way? Yeah, so one of the major barriers that we've had to, you know, we still live and experience right now is the network connectivity in some of the rural areas. So when they are trying to sort of exchange into the swap, yeah, so network connectivity in some areas really is a challenge and we normally sometimes have to force them to go to the nearest cyber cafe or the nearest school because we're assuming that is where they would get network. So movement challenges and I would say that is, like, one of the major, and really smartphones, phones in some rural areas. So we've had to, like, even give a group, like, one phone where they need to, like, log out, log in using their paper wallets. Sure. We have three questions on the screen. Anything you want to answer first? So, the first one, how do we quantify impact? So now, when I was talking about production financing, so what we do, we borrow this from the past, okay? So in communities, there were leaders in each community, like village elders. They would sit and decide the next rotation of labor activities. So now we don't have staff at grass, like salaried staff. So what we do, we work with stewards, and stewards are members of the community. So they are the ones that handhold the community members. So whenever the communities are meeting for rotation of labor, whatever it is that they do, is it building houses, is it building roads or tilling farms? So they send that as a report. So what we do, we put in sell a dollar into the pool, get their vouchers. So the vouchers in this case is backed by their commitments or their M&E services. So when the report is in, then we pay them back using the vouchers. So really the trades on the Saraf.network platform represent the reports that they've sent. So these trades in the eastern part of Kenya equals to 10 houses, 5 ETC. All right, that's awesome. Let's go to the next one. I think we have like three minutes to question, Maria. Yeah. No, no, you take your time. I'm going to address the first two in one answer. So how do you actually deal with onboarding? I'm going to speak now from we gave money away. From the perspective of good dollar, UBI or free money has actually proved to be an incredibly effective onboarding method in terms of bringing people into Web3. And I think what's important about that is that we see that that onboarding mechanism then translates into people exploring a deeper financial life on-chain. And so obviously a good UX, simple onboarding experience, but actually making assets free and accessible and encouraging people to experiment has been critical to onboarding people, retaining people, and encouraging their ability and desire to experiment. There was one other question, but I forgot. Oh, it's about creating trust between local... How can trust between local communities in emerging countries and the global crypto ecosystem can be unlocked? So I think, in my experience, a lot of this is actually happening on the local and the community level, right? So it's through local community leaders that create the relevant education and context for the individuals that come along and are the individuals that are also members of that community. And as builders, it's about having the relevant on and off ramps and the relevant tools that are specific to that particular country and country context. So I think that has actually proven to be on and off ramps which come up all the time, and the wallet experience are the really critical bridges in the actual user experience when it goes from someone using a very specific local app or currency such as grassroots versus the cash-out experience, a.k.a. fiat in my pocket, mobile minutes on my phone, Ethereum in my wallet. All right. Anything, three obvious questions you want to answer? We have like one more minute if you want to. Yeah. How do we deal with failures of trust within a group? Like, from our experience as grassroots economists, we work with already existing groups that have their own constitution, and they know how they deal with some of these challenges. So what you're doing is just adding technology to a practice that they already know, that has already been there. So most of the times, the groups follow what they normally do, even without, like, amongst the community. Is it going to the chief? Is it going to the village elder? How do we sit down and sort that out? So mostly it's within the groups. Want to do one more? If you have a short one, we have a 30-second. You can do it if you want to. Want to do one more? If you have a short one, we have 30 seconds. You can do it if you want to. How can these initiatives be protected from bots? Yeah, go ahead. Sure. How can initiatives be protected from bots? It's a great question. Certainly when you're giving free money away, it's something that you have to confront and deal with. Decentralized identity or decentralized secure unique identity is not a new problem, and there's multiple ways to skin the cat. I'd say actually what makes me the most excited now is like new solutions that are based off of existing community and trust that actually enable members of groups to verify one another and verify the unique identity of one another. And I think this is a space that many builders in Ethereum have been tackling for many years. And we're about to see a next generation of solutions that are actually much more usable for individuals to verify one another. And I think that's how bots will be dealt with moving forward.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1_dONrIsV4L0B5mPO_9XqzEKOZKIP8ACpAPTEAQfEWMQ",
      "resources_slides": "https://drive.google.com/file/d/1ou7qNUWv2FPi4pnln0ZfGstsznoTy0Na/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "defi-cant-move-forward-without-clear-signing-let-me-change-your-mind",
      "sourceId": "9KWRRJ",
      "title": "DeFi Can’t Move Forward Without Clear Signing: Let Me Change Your Mind",
      "description": "Blind signing has been the default way of signing transactions in DeFi, but let’s be honest: as an industry we are shooting ourselves and our users in the foot by continuing to throw caution to the wind. \r\n\r\nWe want to make it easy to implement clear signing for every dAapp, minimizing the work required for developers to make the ecosystem more approachable and secure.\r\n\r\nBlind signing is an existential threat to what we do, it’s time to change it, and we need your help.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Open Source Software,Security,UI/UX",
      "keywords": "Clear signing,transactions",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "72ff21af14505627696a7bd93c8c07e233c737eb6578ca391f879c0a1fe3b6e9",
      "sources_youtubeId": "qGSGzHMqsSQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1oJyQ2nbiJ3dVyeigOw76p6xClrq9LFrInO05tg2QbVg",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "exploring-proof-of-personhood-privacy-biometrics-and-why-it-needs-ethereum",
      "sourceId": "TVSAZU",
      "title": "Exploring Proof of Personhood: Privacy, Biometrics, and Why It Needs Ethereum.",
      "description": "In this session, Remco Bloemen will explore the urgent need for proof of personhood and privacy in a digital-first world. Using insights from Vitalik’s blogpost 07/23, Remco explains why Ethereum’s trustless infrastructure is key to achieving privacy-preserving identity solutions through technologies like zero-knowledge proofs (ZK) and multi-party computation (MPC). This talk is designed to educate developers on creating equitable digital identity solutions without compromising user privacy.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,Privacy,Zero-Knowledge",
      "keywords": "N/A",
      "duration": 1479,
      "language": "en",
      "sources_swarmHash": "cefd278367af0d091b677cea10e548bc18dedd7bdc45fcbc3702cd2f211fcf46",
      "sources_youtubeId": "q3rpu8aDRA8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1tSAo9i2l_HRD2OBB2F6SjjF1Z6zy8MLucrc8FO5rWQs",
      "resources_slides": "https://drive.google.com/file/d/1xFOoVR6Ynp_k7aaex4BPgCsc3uswcpvF/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "from-concept-to-reality-the-triumph-of-blockchain-in-vaccine-distribution",
      "sourceId": "ZBC9ZM",
      "title": "From Concept to Reality: The Triumph of Blockchain in Vaccine Distribution",
      "description": "Join us for an inspiring session that explores the transformative power of blockchain in vaccine supply chains. Learn how we achieved country-wide deployments in Bangladesh and Costa Rica, enhancing transparency, traceability, and efficiency. Discover the real-world challenges we overcame, the innovative solutions implemented, and the remarkable impact on public health logistics, setting new standards for supply chain management and ensuring the safe delivery of vaccines globally.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Sustainability,Ethereum for Good,Public good,real-world,deployment,Ethereum for Good,Public good,Sustainability",
      "keywords": "Real-World,Deployment",
      "duration": 974,
      "language": "en",
      "sources_swarmHash": "a1313e35846a12d8159baaf1f5419c4b8846b08f315ac4c872079e5de0c97384",
      "sources_youtubeId": "0dSz0CN6bI8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:20:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1yuhgDizD0e2BcBSAmT-nwGyHIS4gNNqFjMZbvO34IPc",
      "resources_slides": "https://drive.google.com/file/d/1quEEZTN-yLwlDw1CfvkOJjm1DFpHiZgV/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "the-shape-of-protocols-to-come",
      "sourceId": "TYGBPN",
      "title": "The Shape of Protocols to Come",
      "description": "Ethereum defies easy categorization—it blends aspects of money, nations, and more, yet doesn't fit neatly into any single category. To build better mental models for understanding Ethereum, we've spent the past two years stepping back and exploring the broader class it belongs to: Protocols. This talk explores the fundamental properties of protocols, strategies for navigating them, and how Ethereum can uniquely contribute to this emerging research field.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,Protocol Design,Use Cases",
      "keywords": "",
      "duration": 1402,
      "language": "en",
      "sources_swarmHash": "43b3f1b06406e849ea5082a4989e38e5f86d942069ea888dc8d826cab53670a5",
      "sources_youtubeId": "3-9Ep6qQS3A",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673354fa3a168eb5356c37d7",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:00:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/15QhPTXl4SBVPn-h9srUsdXijj_OIaZYVL1C32DxEyiw",
      "resources_slides": "https://drive.google.com/file/d/1SIw7P-wt7_TCL7h30vY0ybz16D7Jr8O9/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "the-chain-mail-gaze",
      "sourceId": "73SKE9",
      "title": "The Chain Mail Gaze",
      "description": "With their dreams of new ‘Network State’ empires, resource extraction, and colonial domination, today’s tech overlords are the descendants of Europe’s mediaeval Crusaders: well-financed, zealous fanatics remaking the world in the name of their greater good. Through a psycho-political reading of scarcity, chauvinism, and colonialism, The Chain Mail Gaze connects Crusader ideologues’ desire for blood, land, and booty, to emerging ‘frontiers’ mediated by contemporary technologies.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Network State,decolonial,Governance,Network State",
      "keywords": "decolonial",
      "duration": 449,
      "language": "en",
      "sources_swarmHash": "422f0d629401b9f6bd9d6e38fd02fe8770da00b13ef29d52d9871a214bff2fc1",
      "sources_youtubeId": "zhsmBcFnDsE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67382f841b0f83434da7d8bf",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67382f841b0f83434da7d8bf.vtt",
      "transcript_text": " Jane Mailgaze by Waseem Z. Alcindi. Please, give him a warm welcome. Hello everybody. It's slightly out of place to have such cheerful music introducing this talk, perhaps. So the title of it is called The Chainmail Gaze, and hopefully the reasons for that will become apparent as we go on. But I'm sure you've all heard of the mail gaze and the kind of externalities and, you know, unfavorable things that come with that but what about the chain mail gaze well hopefully we'll find out not got very long so I'll try to give you a survey of what's actually quite extensive anthropological and philosophical research project some pictures of big strong men and their swords looking into the distance perhaps thinking about lands to conquer. A short poetic refrain, the church and the network, zeal and time, death and money, all sides of the same coin. I want to talk to you about an interim project that led up to where we're going to, that's called Profit Motives, looking at the false divinity of capital, how it animates motivations and desires in the technology space that might not be immediately apparent. as there's been financial capital risk and speculation have orbited manipulated and harnessed it as narrative feedback machines simultaneously reading and rewriting realities markets exist as a distributed conversation between uh amongst speculators driven by profit motives and appetite for divination and prophecy today new strains of techno-colonialism are emerging which are the latest of a series of echoes throughout Western history. An ascendant cabal of technology elites are attempting to reshape the world in their favor whilst hiding in plain sight behind the technologies that have enriched them. Theirs is a Promethean zealotry without faith, affecting an aura of divine sanction for the purposes of elevating the ego, enriching the chosen ones, and creating empires of various stripes. But was it not? Always so. I want to read you a short story, fictional speculation, about where we might be going in the future. It's called Seething Like a State. Decentralization has a cost, the price of anarchy. The price is always due, but the rewards weren't cheap to reap. So solid CCRU. What most did not expect was that payment would become due at the grandest scales of governance. The West failure state, forged under the fire of peer pressure, was ambushed by upstart modes of power, opening new vistas of communication, commodification, and communion. The orientations that nation-states had used to enshrine their power only made it easier to undermine them. The bigger they were, the harder they fell. Brextopia, the European Union, NATO's cave, the United State machine, all returned to dust. The decline of the nation state in the roaring 20s became a canon of canonicity for an entire generation of sole traders. It wasn't even just the bit leavers. In those days, there were many networks, many messiahs, many ideologics, all with their own profit motives. So, many of you have doubtless heard of this concept of the network state that has emerged in the last few years. Exit fantasies, arguably fueled by the failure or the shortcomings, the shortfall of the full promise and the dream of Web3. I'm going to read you a short section from Balaji Sreenivasan's self-titled book in 2022. A network state is a social network with a moral innovation, a sense of national consciousness, a recognized founder, a capacity for collective action, an in-person level of civility, an integrated cryptocurrency, a consensual government limited by a social smart contract, an archipelago of crowdfunded physical territories, a virtual capital, and an on-chain census that proves a large enough population, income, and real estate footprint to attain a measure of diplomatic recognition. The magic words in here are recognized founder and diplomatic recognition, and territory, I would say. So these projects carry echoes, in my mind at least, of some of the earliest expeditions set out from the West in search of new territories to conquer and subjugate. So many of you have doubtless heard of things like Liberland, Sealand, Seasteading Institutes. This is the Zahidi-designed metaverse HQ of Liberland on screen here. There's been so many attempts to make crypto islands, Bitcoin cruise ships, and they've all failed. It's quite interesting. But what seems to be different now is the level of capital on hand as a result of market success of these technologies to the people that want to reshape the map of the world in their own, you know, to their own preferences, into their own cause, we know that leaders of nation states, we even have leaders of nation states that are cheerleading some of these technologies. Nayib Bukeri, the authoritarian strongman in El Salvador, would like to build a Bitcoin city financed by volcano bonds in his country. He's not done it yet. But meanwhile, he's removed the judiciary, removed term limits, and instituted one-party rule. And most of the Bitcoins are cheering this on. So I ask myself, are we still in this for the freedom? And now, made possible by Ethereum, DAOs, and all the rest of it, we have projects like Aleph, Urbit, and even Praxis. Praxis on the left, and Prospera on the right. So these projects are trying to build physical cities, private territories, network states, or at least what may develop into the Bellagio concept of the nation state. And it's at this point I want to introduce the chainmail gaze. Today's tech overlords are the descendants of europe's crusaders well-financed zealous fanatics wreaking destruction on the planetary other in the name of their greater good the vatican sponsored waves of levantine invasions that began in the late 11th century with the midwife of capitalism colonialism and technology as we know it today. With the network state organizational concept, a cadre of powerful ideologues blessed with tokenized wealth, a toying with the prospect of reshaping national frontiers, mirroring the desires of Frankish noblemen and their knightly orders in the Levant a thousand years ago. And that's all the time I have. Seven minutes for a thousand years of Western history. Thank you very much.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:10:00.000Z",
      "slot_end": "2024-11-12T11:20:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/17RnVgqUzy-db9C_X4-QKgghgKSZ40O-5PtTPVJladMk",
      "resources_slides": "https://drive.google.com/file/d/1iUoQ4ZdzG69XCZPrnta5uZd5c0RAkUuc/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "transaction-simulation-the-good-the-bad-and-the-ugly",
      "sourceId": "TE9JUF",
      "title": "Transaction simulation, the good, the bad & the ugly",
      "description": "Transaction simulation allows users to preview the outcomes of signing a transaction, enabling them to make informed decisions rather than fully trusting the dApp. However, several caveats and risks are associated with relying on simulated transaction outcomes. State changes, differing contract behavior between simulation and on-chain execution, and randomness can all affect the outcome. In this talk, I'll share my experiences and learnings from simulating user transactions over the past 2 years",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,User Experience,safety,Security,User Experience",
      "keywords": "simulation,wallet,safety",
      "duration": 458,
      "language": "en",
      "sources_swarmHash": "1367b463e69cb498817ffc03a9949daeade7c14957d466768d66c65a2b542e0f",
      "sources_youtubeId": "12uW2nhIxN4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:10:00.000Z",
      "slot_end": "2024-11-12T11:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Bl4qs4Zj65LUtt4i8uht8GdKLHGxRkYht0gt_Qcd_n4",
      "resources_slides": "https://drive.google.com/file/d/1Al6HgtbtUJg7ek83Ogsd_SmRkc0kZfyb/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "a-cat-and-mouse-game-how-to-frontrun-a-transaction-in-the-future",
      "sourceId": "MMQNCA",
      "title": "A cat-and-mouse game: how to frontrun a transaction in the future?",
      "description": "This talk will describe the attack-defense game in the MEV world. First it will briefly discuss MEV transactions and how it can protect projects from hackers. Then it will delve into attack-defense games between MEV bots. Finally it will discuss our latest observations and direction in this cat-and-mouse game.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Fuzzing,MEV,program,analysis,Fuzzing,MEV,Security",
      "keywords": "Program,analysis",
      "duration": 356,
      "language": "en",
      "sources_swarmHash": "a3b09fc1d984db767d8af93657fd54b64f8af7c9391ada66823ed99d63801ffb",
      "sources_youtubeId": "9bY4tq0rGMc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:20:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12NbVEiYl32qL-QEUHOpKoymfYvpnypnNwC2bLEg0kzI",
      "resources_slides": "https://drive.google.com/file/d/1J0cKsHtFb1PEL9UqySbC9gQWhHnzkVjn/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "mycofi-mycelial-design-patterns-for-web3-and-beyond",
      "sourceId": "8CDPFC",
      "title": "MycoFi: Mycelial Design Patterns for Web3 & Beyond",
      "description": "Exploring MycoFi guides readers on an underground exploration into the world wise web of mycelial networks, the most prolific producers of public goods on Earth. This talk examines how the evolutionary adaptability of fungi could help us imagine biomimetic alternatives to status-quo economic systems that demand infinite growth on a finite planet. If we aim to design regenerative economies, what better\r\nplace to start than with the thriving evolutionary patterns of nature?",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Collective Intelligence,Conviction,Consensus Mechanisms,Civil Resistance,Sustainability,Public good,Regenerative Applications,Design Thinking,Civil Resistance,Collective Intelligence,Consensus Mechanisms,Conviction,Design Thinking,Public good,Regenerative Applications,Sustainability",
      "keywords": "nope",
      "duration": 544,
      "language": "en",
      "sources_swarmHash": "f520b12bc12e6e339bfff3be9b1d59b5019047a45c6c94f2fc1557b7e458af07",
      "sources_youtubeId": "0A4jXL5eBaI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:20:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1vPpKjEWNW5rkIevCpxSX6qLuE5usbq91oz2FVQk6gWw",
      "resources_slides": "https://drive.google.com/file/d/1nKmQK56TC5kRva-1yCnmf5UqyHz8gjtS/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "the-trustless-trade-supply-chain",
      "sourceId": "RQZADG",
      "title": "The Trustless Trade Supply Chain",
      "description": "Trades are fundamental to defi. Without credibly neutral trade execution – we risk the same centralisation and rent extraction through privileged actors that we have in tradfi.\r\n\r\nToday, the trade supply chain in defi is mostly centralised: Intent auctions, builders, solvers and market makers are handful of off-chain actors with privileged access.\r\n\r\nHowever, a trustless, and decentralised trade supply chain is possible. This talk highlights the current and future technologies that make it possible.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "PBS,MEV,Trading,Intents,TEE,Intents,MEV,PBS,Trading",
      "keywords": "TEE",
      "duration": 460,
      "language": "en",
      "sources_swarmHash": "8eddb90eeded5ff214a45d5bdf580280a4d8a2356f2f3614fcd3ea3f15d1049a",
      "sources_youtubeId": "9EPCog8GiiQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-12T11:20:00.000Z",
      "slot_end": "2024-11-12T11:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1ZpnW0qJAIFrezIxxeweffstYIWJbW-4Aa1uhy79go6A",
      "resources_slides": "https://drive.google.com/file/d/1Fx5_1U9978oSlaXO94gXfVLqxzdK9eT0/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    }
  ],
  "Day 2": [
    {
      "id": "butterfly-effects-paradoxes-of-account-abstraction-in-defi",
      "sourceId": "7VUDWG",
      "title": "Butterfly Effects + Paradoxes of Account Abstraction in DeFi",
      "description": "In this session, we’ll dive into the transformative potential of account abstraction in shaping the future of DeFi, exploring both its cascading impacts and inherent paradoxes. By pushing the boundaries of accessibility, onboarding, and wallet capabilities—such as multi-sig, social recovery, custom modules, transaction batching, and advanced transaction logic (ERC-7579)—account abstraction opens up new possibilities for collaborative DeFi, cross-chain interoperability, and enhanced composability",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,User Experience,Account Abstraction,defi,Account Abstraction,Decentralization,User Experience",
      "keywords": "DeFi",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "56c6f3a38e0e38a31344382f0ec51fe0d336a1c9a9fbd66a0b2d27bc50715645",
      "sources_youtubeId": "uCcU6qUX8w0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T03:15:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1zpiZg4dw7Yoaj_Sy6FpGr9LUxUTz8UzDFGi_e9GpUrU",
      "resources_slides": "https://drive.google.com/file/d/1mVq_DaNVNcgWJ-3owqcQknjlQVhd2ej1/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "decentralized-outcome-funding-for-investigative-reporters",
      "sourceId": "SJE7VP",
      "title": "Decentralized Outcome Funding for Investigative Reporters",
      "description": "Drawing upon the idea of impact certificates, this talk demonstrates how blockspace can be leveraged to solve double selling of impact (create change once and sell to many funders) and donating on brand rather than outcomes created. The session will go through a demo built by VoiceDeck in collaboration with the EF to help traditional newsrooms port their private database of impact as Hypercerts on Optimism, so they can receive funding based on recorded impact arising from their past stories.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Effective Altruism,Ethereum for Good,Regenerative Applications,hypercerts,Effective Altruism,Ethereum for Good,Regenerative Applications",
      "keywords": "Hypercerts",
      "duration": 1332,
      "language": "en",
      "sources_swarmHash": "6d4b3ffe78d8dfada9947518f0917021e6b83bdf11276fe5bd670fffba45497b",
      "sources_youtubeId": "BpTDLkSQhVs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67341c7e9dbb7a90e15fe62f",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T03:15:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Wcw6Mzk0DP95udiY_4VYK0pAVZ2Ac5fQgZmO7yWbJSg",
      "resources_slides": "https://drive.google.com/file/d/1oFesUG7pL5nwDig2XojA00nIAvxDejEn/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "empower-the-ethereum-network-with-your-own-node",
      "sourceId": "RAXURS",
      "title": "Empower the Ethereum Network with your own node",
      "description": "Stereum is an easy to use MIT-licensed Open Source GUI open-source Node Setup & Management Software.\r\nAfter a couple of clicks you have your hardware set up for  \r\n1) Solo Staking (MEV)\r\n2) Distributed Validator Staking(Obol, SSV)\r\n3) running an Archive Node \r\n4) node operation of several protocols (SSV Network, CSM and Simple DVTM)\r\nWe want to make a workshop, where you can tryout a setup yourself and take time for your questions. dApps, testnet-mainnet switch and client-diversity supported!",
      "track": "Usability",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking,Best Practices,Accessibility,network,access,Accessibility,Best Practices,Staking",
      "keywords": "Ethereum Node,Tooling,Network Access",
      "duration": 4470,
      "language": "en",
      "sources_swarmHash": "f4e918da3ae9577243a5645f92b870e4d206670389a185a629e9ac6921540464",
      "sources_youtubeId": "cAsztMfLfF0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T04:15:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1pvjBcm_guIMvayHy6vzCMwdxhLF_FviCoXJx10mrzT8",
      "resources_slides": "https://drive.google.com/file/d/18_mx6yh7ovdLdB05kbiE_RlwQJHL08YI/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "finding-bugs-42-tips-from-4-security-researchers",
      "sourceId": "AZNENK",
      "title": "Finding Bugs: 42 Tips from 4 Security Researchers",
      "description": "Billions of dollars are at risk, and protocols spend millions on security through audits and bug bounties. Have you ever wondered how you can become a top security researcher securing these billions?\r\n\r\nIn this workshop, 4 recognized security researchers share their experiences on smart contract security with practical tools & techniques to find & report vulnerabilities. Security researchers, even aspirational ones, can take away some key advice to improve their smart contract security skills.",
      "track": "Security",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Auditing,Bug,Bounties,smart,contracts,Auditing,Bounties,Bug,Security",
      "keywords": "Education,Hacks,Smart Contract Security",
      "duration": 5654,
      "language": "en",
      "sources_swarmHash": "5115b9b314e63c202aea765f7fc8025db430ff8d7f370ddddc28e16273af4e24",
      "sources_youtubeId": "8d2UuzEBVdM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T04:15:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1HZSm9H-PuHEKe3mrj7Wl9hDODP3kP9iQMoLjxXQ81Iw",
      "resources_slides": "https://drive.google.com/file/d/1G8I9OkSEix5bqs0J6xrSDPkrVxLyCh0e/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "formal-verification-in-the-ethereum-protocol-current-status-and-future-directions",
      "sourceId": "KQCGWV",
      "title": "Formal Verification in the Ethereum Protocol: Current Status and Future Directions",
      "description": "Vitalik believes \"ethereum's biggest technical risk probably is bugs in code, and anything that could significantly change the game on that would be amazing\".  Formal verification is a key technology which many believe could significantly help.  However, it has yet to see wide adoption for a variety of reasons.  This panel will bring together formal verification experts working in blockchain to discuss the challenges faced in increasing the use of formal verification within the community.",
      "track": "Security",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Formal Verification,Testing,proving,theorem,Formal Verification,Security,Testing",
      "keywords": "model checking,theorem proving",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "d68ae03f4422ca57d79f914604ad876858505c2d4fddb19bfaaebd80f763b91f",
      "sources_youtubeId": "J71rLx8dBXY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T03:45:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1v3H83g6kUyGEXtHlMSYEBQw6ksu6---QQw0rs5zcxM8",
      "resources_slides": "https://drive.google.com/file/d/1O2JNnJWPh93Pn2jNl4yQp_CJGtb-DFHD/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "prize-worthy-an-ethereum-python-hackathon-guide",
      "sourceId": "73J9ZG",
      "title": "Prize-Worthy: An Ethereum Python Hackathon Guide",
      "description": "An interactive and beginner-friendly Ethereum Python Speedrun tailored for hackathons, hosted by the EF Python team. Quickly get up to speed with fundamental building blocks, then stack them into a live application. By the end of this workshop, you'll have a clear idea of how to get your own projects off the ground.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,DevEx,Open Source Software,solidity,DevEx,Open Source Software,Tooling",
      "keywords": "Vyper,Solidity",
      "duration": 4162,
      "language": "en",
      "sources_swarmHash": "0ff20ddcd42b89b3971de379d28a01b1621d77ab795fc1962c7fc0a4441104e6",
      "sources_youtubeId": "tetTX0ozcCg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673427ae9dbb7a90e19e4c2f",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T04:15:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1BdovxuMXRzh3v5kgPx7kmJtQ78cQ3TRzKpVqoR27GwE",
      "resources_slides": "https://drive.google.com/file/d/1b9gNV69oc0xVh6cerP43zpGHOqzD7c2U/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "programmable-cryptography-and-the-future-of-the-internet",
      "sourceId": "JVGEDS",
      "title": "Programmable Cryptography and the future of the Internet",
      "description": "You rarely hear of issues at the networking layer of the Internet: networking companies are running utilities business: they are fungible and can be swapped if distrusted.\r\nMost of the value captured on the Internet -- and also most abuse -- happen at the Compute and Data layer of the Web. Ethereum gave us a glimpse of a fundamentally different architecture for Compute and Data than Client/Server architecture.We think the Internet is 1/3 complete, and that programmable cryptography can finish it.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "None",
      "duration": 1525,
      "language": "en",
      "sources_swarmHash": "0fb50c195df8ce92474b5cefa3ba1a750793c1efe6f7bc27f06d16f5a2040a3c",
      "sources_youtubeId": "onclocmZeR0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67341d7a9dbb7a90e166b21c",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T03:15:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1yuek7FVsP0Ov8ZWMCbVJX0zA_KsFKhhx7JBnbKcs_qY",
      "resources_slides": "https://drive.google.com/file/d/1rSDOMq1chOochPW5XY3kb62IMl4OwqYI/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "rethinking-ethereums-account-model",
      "sourceId": "GEEQXS",
      "title": "Rethinking Ethereum’s account model",
      "description": "Account centric models are inherently faster.\r\n\r\nEthereum operates on a global account based model. This means a global lock occurs any time someone needs to touch a piece of global state, such as an ERC20.\r\n\r\nAn account centric model, instead, creates a new deterministic address or state for each account. This means calls into transfers on ERC20s and dexes can be made in parallel, accelerating speed drastically. It also is more secure.\r\n\r\nIt’s a forgotten mechanism to scale ETH.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Layer 1,Ethereum Roadmap,model,account,Core Protocol,Ethereum Roadmap,Layer 1",
      "keywords": "Account,Models",
      "duration": 537,
      "language": "en",
      "sources_swarmHash": "de96bb25225be90b669409315ef858bc5a173e1424895908951a4f1344789bca",
      "sources_youtubeId": "7B-ji-XrMio",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T02:55:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1S8CtqAgd4RfP7bFHLKoa51ch_PX1Vkr5bs1-02-C3XE",
      "resources_slides": "https://drive.google.com/file/d/1YBTf8ZTmOxhDG9BHwU5HBKAo3GDtjNzF/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "zero-to-dapp",
      "sourceId": "LUW7G9",
      "title": "Zero To Dapp",
      "description": "Learning Web3 programming. There are so many different tools and protocols to learn. Zero to Dapp is a workshop series that builds upon collaboration between different projects to guide the students from zero to their first Dapp. In this workshop, we review our learning from previous editions to encourage others give their own Zero to Dapp. Then we'll give a shortened version - usually, this workshop takes between a half day up to two full days. But we are fast learners at DevCon, aren’t we? ;)",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Layer 2s,Tooling,DevRel,Live Coding,onboarding,DevRel,Layer 1,Layer 2s,Live Coding,Tooling",
      "keywords": "Onboarding",
      "duration": 4607,
      "language": "en",
      "sources_swarmHash": "bde6952b4b8c11c866e8ce4a9d794f08480304cabca0b1c268e7489f5bca9451",
      "sources_youtubeId": "lRo-TBLTgzs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734297d9dbb7a90e1ae69fe",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:45:00.000Z",
      "slot_end": "2024-11-13T04:15:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1obE94TKOOHTvht_bjpYs85KpbFc9Qw-AagmzvQTXrYk",
      "resources_slides": "https://drive.google.com/file/d/1kclBwzP6Bh24AR18Vqmq1Fy3fQEhgDjB/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "encrypted-mempools-a-path-to-ethereum-l1",
      "sourceId": "SGDDEX",
      "title": "Encrypted Mempools: a path to Ethereum L1",
      "description": "This talk will explore the future of encrypted mempools, paving the way to enshrinement on Ethereum L1. Starting from current designs such as Shutter and SUAVE, security assumptions and out-of-protocol infrastructure can be stripped away with cryptography including homomorphic encryption, VDFs, and delay encryption. These approaches would trustlessly bring front running protection and censorship resistance to the protocol.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "encryption,mempool,Censorship Resistance,Core Protocol,Cryptography",
      "keywords": "Encrypted,Mempool",
      "duration": 565,
      "language": "en",
      "sources_swarmHash": "13fb566c3794a741fd8dff3d5d83fa04159a09104d886258558e6781631adaaa",
      "sources_youtubeId": "mUoWwRoHrvk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T02:55:00.000Z",
      "slot_end": "2024-11-13T03:05:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1lvMpzBomZ6dNVchh_7lRcXyFGQ2an1s7f3t0tDgzR2E",
      "resources_slides": "https://drive.google.com/file/d/1ADS4Li8Pfe0iFnTDavlUsqhKDb8wIwHi/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "buidlguidl-intro-to-ethereum-development-build-and-deploy-your-first-dapp",
      "sourceId": "Y8RETB",
      "title": "BuidlGuidl - Intro to Ethereum Development, Build & Deploy your first dApp",
      "description": "**Morning Workshop (10-11:45 am)**\r\n\r\nJump into Ethereum development with a hands-on introduction to Scaffold-ETH 2 and its powerful extension system! \r\n\r\nWe'll show you how to set up your development environment and how to use state-of-the-art tools to deploy live apps that people can use, while introducing you to the essentials of blockchain development.\r\n\r\nNo prior experience needed—perfect for those eager to start building in Web3. Let’s get your first Ethereum dApp live!",
      "track": "[CLS] Learn How To Build On Ethereum & Capture the Flag Game, by BuidlGuidl",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Live Coding,Open Source Software,Public good",
      "keywords": "",
      "duration": 5272,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "lNLskT2LV6Y",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67342bba9dbb7a90e1c0df38",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:00:00.000Z",
      "slot_end": "2024-11-13T04:45:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1KDpey24PdDXO0vBbx6eyVUR3g478eggW52p-Z2GsI8E",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "universal-eccs-use-cases-for-the-p256-precompile-in-decentralized-internet-infrastructure",
      "sourceId": "NX7U8B",
      "title": "Universal ECCs: Use Cases for the P256 Precompile in Decentralized Internet Infrastructure",
      "description": "## Summary\r\n\r\nThe session will highlight the history of adoption of P256 in Elliptic Curve Cryptography (ECC), its current applications in web security, authentication, and encryption, and explore future possibilities for its integration into Ethereum and ENS to enhance decentralized internet infrastructure.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "ens,Accessibility,Public good,Use cases of cryptography",
      "keywords": "ENS",
      "duration": 522,
      "language": "en",
      "sources_swarmHash": "d137af18f4692a1194d1e3d606910f72833ec4282b51cac0a9b1a317238c2ef2",
      "sources_youtubeId": "e_QBTQGMxPs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:05:00.000Z",
      "slot_end": "2024-11-13T03:15:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1-xDtu6rJ4NegQFgMrkNcVtzLJVJkvrYD_L3OYcBdFQo",
      "resources_slides": "https://drive.google.com/file/d/1V3xGcwnGPg1NGM8TREc4U1La2p8m_j2V/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "epf-day-introduction",
      "sourceId": "PE8JHU",
      "title": "EPF Day Introduction",
      "description": "Josh and Mario introduce the fifth cohort's EPF Day.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "EPF,Ethereum Roadmap,Layer 1",
      "keywords": "",
      "duration": 567,
      "language": "en",
      "sources_swarmHash": "07c8d90edd36f96c7e993e961857bd845f20407627a717428d0862e3d0594b15",
      "sources_youtubeId": "_2fNWYoBrT8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673887c51b0f83434dcfb118",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673887c51b0f83434dcfb118.vtt",
      "transcript_text": " All right. Welcome to EPF Day. Thanks for being here. My name is Josh. This is Mario. We coordinate the Ethereum protocol fellowship with the Ethereum foundation. And this is EPF day where the fellows from the fifth cohort will be giving you lots and lots of demonstrations about what they have been working on over the past five months. Though quickly, I want to talk a little bit about what the Ethereum Protocol Fellowship is. This is a program that's been running for about three years now. It's been going through five different cohorts, starting with what was called the core developer apprentice program started by Piper Merriam and then Mario and I have been running this iteration of the program for the past three cohorts. The protocol fellowship is a program that is meant to steward the protocol through the bringing in of more and more developers. So according to the latest protocol guild statistics, there are about 180 developers that are working on the core protocol, all the things from specification to working on EL clients, CL clients, doing testing, and also doing lots of the research that is necessary for future upgrades and forks so it's a lot of stuff that needs to get done and not that many people to do it as Tim Baco likes to say and I think others I'm sure you've seen this roadmap before that each one of these tiny little boxes could use an entire team to focus on just that box. So we are here to try to fill in all of those boxes. The EPF, the Protocol Fellowship, has sort of grown a lot over the past couple of years and we've been adding new and new features to it to enhance the experience and to make it easier for people who are interested in doing this work to do so. So it started with the EPF. Again, this is the fifth cohort. At the beginning of this year we started the Ethereum protocol study group and that was a three month, two month long program that offers some more basic introduction and overview to the different pieces of the protocol so those people that are interested and maybe have some developer experience but don't really have much knowledge of Ethereum itself can get introduced to it and learn about the different pieces and parts and understand what it is that they might want to focus on. In addition to that, we've created a resource, epf.wiki, which is the intention is for it to be the go-to place for people who are interested in learning about the protocol to do so answering questions and and helping them to understand more about it it is a growing and collaborative resource so it is in it's still it's sort of fledgling form and will you welcome any further contributions to it if you are learning about the protocol or already are very knowledgeable about it to add your two cents to it. And finally, like Piper likes to say, the door to core development is hilariously wide open. You just have to step through. The EPF open. You just have to step through. The EPF is a way for you to step through with somebody holding your hand a little bit. I'll go ahead and pass it to Mario. Yeah, thank you so much. And let me tell you a bit about the current cohort. So as Josh mentioned, we started this cohort. I feel like this cohort started even before it started because we did a study group first. And it was a very special experience. We started with that this time for the first time, and there have been a bunch of people who came to learn about the protocol basically from zero and then joined the cohort. And in the last six months or during this year, we started the study group, like, February, March. So, like, during this year we started the study group like February, March. So like during this year people who didn't have any idea about the core development became like almost full-time contributors in some cases. So yeah, it's been a wild ride. We had around 45 follows. Just some context to this number because we started, when we started the cohort, it's permissionless, anybody can join. And in these first calls, we had like over 50 people, maybe like 60 people who were interested in the fellowship, but then at least 45 of them submitted multiple updates throughout multiple weeks during like two months or something. So 45 of them at least made it to the second month. But to the very end, till today, we have 30 fellows who made it to the EPF day there are a few of them who couldn't make it of course because of the travel restrictions and so on but we had Anyway, okay, put it further, okay. Five months of the, ah, the echo. Five months of the actual fellowship, which was the first sub-learning part, first month to figure out what projects these people want to even work on. And for many, it was easier thanks to the study group. We had two calls every week. So all together we had over 40 calls together over the past five months. We met each other every week for the office hours, for the stand-up. So a ton of calls. But also, we met in person. First time we met in person in Brussels, we had a chance to meet with maybe half of the fellows, but almost everyone is here today at DEF CON. So it's really an honor to actually meet you finally all after seeing each other at these calls. Yeah, so 30 people made it to the end with 30 projects. It's not one project per person. There were teams of people Working on a bigger scope project together, which is also Amazing to see that people learn to collaborate together. And we will have presentation for 20 projects today. 30 were proposed. Yeah. And altogether we had like over 500 comments, over 500 updates in the repository. So when you open the development updates in the Core 5 repository with all of the tracking for every fellow, it's over 500 of them. So I guess I read all of them, but it's not really possible to do it by yourself. But there's been a lot of work done. And it's hard to summarize what individual fellow did. I know that Rahul summarized it, so I kind of borrowed his data here just to give you an idea what is the output of one fellow who dedicated 1,000 hours to the fellowship. And how many coffees? 228 cups of coffee, yeah. That's interesting metrics that I would like to also use for measuring DPF. But, yeah, thank you so much for the numbers, Rahul. So this is 26 updates just from one person. So you see, like, this is what the single person is able to dedicate to the fellowship over five months. And so with this, today, again, it's an honor to host the project presentations to see your recap of what you've been working on in past months. And there is 20 of them. There is a lot of them. So we have a ton of topics to go over. I'm not going to even mention each of them because you will see them. This is the order, and we are starting with EVM memory repricing. So maybe let's get slowly ready to that. And, yeah, we will have first half of the presentations in the morning with a pause at 1 p.m., 1.15, because we have quarter past 1. We will have 45 minutes for lunch, and then we meet here again at 2 p.m. for the rest of the presentations and for the panel. Two, we'll have a panel discussion as well. So that's the schedule for today. Please enjoy the EPF day. And yeah, I hope you get inspired by the fellows, by their work. I hope you are able to meet some interesting people here.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:15:00.000Z",
      "slot_end": "2024-11-13T03:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1UgPaeQAkzm7-SuT2jdxRMLHcVJEzy3CxxHN_BL0ftCg",
      "resources_slides": "https://drive.google.com/file/d/1Z9q2gfwmP7f9R9oGZnirNgq_xScRnNAu/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "a-mobile-based-light-client-solution",
      "sourceId": "7GAKDX",
      "title": "A mobile-based light-client solution",
      "description": "After PoS Merge, there was a bit of a lag in Light-client related work. In response, we developed a mobile-based Light-client to help more devices natively join the Ethernet network.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Light Clients,Tooling,DePIN,Mobile,client,mobile-based,light,Core Protocol,DePIN,Light Clients,Mobile,Tooling",
      "keywords": "Mobile-based,light,client",
      "duration": 547,
      "language": "en",
      "sources_swarmHash": "6df21ff37b96e555695667a0ee466203aa7e57bd2ae9e1087cdec81a86adcf76",
      "sources_youtubeId": "JQOfe-NUvKI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T03:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1IlPTrpB8SK9bkKystbWnIBPtxOnZNnAAkKebg5A6o9U",
      "resources_slides": "https://drive.google.com/file/d/1DDyRwxk4wDscM9cBtOC9vnFsYW3UpaX5/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "embodiment-practice",
      "sourceId": "LNF8NE",
      "title": "Embodiment Practice",
      "description": "By master Zoe\r\n- Gentle guided stretches to connect with the body and open different energy channels\r\n- A blend of embodiment, asana, meditation, breathwork, tapping, and somatics to weave together mind, body, and soul\r\n\r\nNov 13 10:30 - 11:15",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:15:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/16hER2e4hzqPjZrObAFmLsPIfyEkBHspMF-2HfxORQAg",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "eve-frontier-challenges-lessons-and-future-of-building-an-autonomous-world-on-ethereum",
      "sourceId": "QLK8UE",
      "title": "EVE Frontier - challenges, lessons and future of building an autonomous world on Ethereum",
      "description": "CCP Games—the creators of the legendary space-based MMO EVE Online, home to millions of space merchants, pirates, and explorers—is building a new world, and it is going to live onchain and run on the EVM.\r\n\r\nHear from the CCP team as they discuss challenges, learnings, and open questions of building massive virtual worlds onchain—what to put onchain first? What game mechanics are best suited onchain? What are the unlocks?—as well as what EVE Frontier might bring to the Ethereum ecosystem.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,Autonomous World,eve,online,Autonomous World,Gaming",
      "keywords": "MUD,EVE Frontier,EVE Online",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "fd9b8519e91577c8456926c1087173ad4da44d3aa5bfdad5333a9ed609273e33",
      "sources_youtubeId": "B7DCH9FwlWw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1mqLIgd8le45XgG2FPsR3vi1IafiikIiEzC9TaHmFCvk",
      "resources_slides": "https://drive.google.com/file/d/1Xa8HcAom0MyYJsXPFDd7ou0BSgTerpO5/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "everything-you-need-to-know-about-state-expiry",
      "sourceId": "MZXQKJ",
      "title": "Everything you need to know about state expiry",
      "description": "State growth is a ticking time bomb for Ethereum, yet concrete solutions remain elusive. While statelessness offers promise, it doesn't address the root cause. Enter state expiry – a compelling answer to our growing state problem. In this talk, I'll dive into the analysis of Ethereum's state growth problem down to the key-value pair level, the evolution of state expiry proposals, and the latest research on Ethereum's state expiry solutions.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Protocol Design,Verkle trees,state,expiry,Core Protocol,Protocol Design,Verkle trees",
      "keywords": "Statelessness,State expiry",
      "duration": 1345,
      "language": "en",
      "sources_swarmHash": "a17917322f9b68b641f7a7bb0aff74f02310d39e0fe79821d91feb668a19936e",
      "sources_youtubeId": "6j-7ZY2ITw8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673427cc9dbb7a90e19eae0e",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/18L4p0t-mR02cVw6JDvMHqUal5ARSQzWsubskb_x8FzA",
      "resources_slides": "https://drive.google.com/file/d/12ZTllKMycuMXs7Kb8AXKCPsJSZPJArno/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "evm-memory-repricing-and-gentest",
      "sourceId": "MTWH38",
      "title": "EVM Memory Repricing & Gentest",
      "description": "Memory is a critical resource that enables complex computations within the Ethereum Virtual Machine (EVM). The cost of using memory, designed to prevent its abuse, has not been revised since the inception of Ethereum. However, efficiency gains from hardware advancements and client code optimizations warrants periodic repricing of this cost. We explore possible ways to make memory more accessible.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": true,
      "tags": "EVM-equivalent",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T03:45:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1e6KETyrkOalajDAo2_dDl3Cg0oUXzpb7Ehl8aawzChY",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "fraud-proofs-war",
      "sourceId": "UTTXWB",
      "title": "Fraud proofs war",
      "description": "Fraud proof systems were originally envisioned to be able to protect a rollup with just a single honest challenger assumption. As it turns out, the matter is much more complex because of exhaustion attacks, a form of sybil attack where the attacker tries to win by economically outlasting the defenders. The talk discusses the tradeoffs in the proposed solutions to this form of attack by analyzing Arbitrum, Cartesi and Optimism fraud proof systems.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Optimistic rollups,Challenge period,Mechanism design,fraud,proof,Challenge period,Mechanism design,Optimistic rollups",
      "keywords": "Fraud,proofs",
      "duration": 1471,
      "language": "en",
      "sources_swarmHash": "6e327022853abdfce60bfc6ae70d8d83839fc2348138d39e128a8834bba9b846",
      "sources_youtubeId": "k0UooaY7VQ0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734395c9dbb7a90e17a7cc5",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6734395c9dbb7a90e17a7cc5.vtt",
      "transcript_text": " Hey everyone, I'm Luca, I do research at AltaBeat and I'm here to talk about fraud proofs. So there are many different types of fraud proofs. There are off-chain fraud proofs, on-chain fraud proofs, single round, multi-round. This talk is mainly about on-chain multi-round proof systems which is what the top optimistic roll-ups use today like Arbitrum, Optimism or BASE. And the thing is that these two proof systems so the one developed by Arbitrum, Optimism, or BASE. But the thing is that these two proof systems, so the one developed by Arbitrum and the one developed by Optimism, are very different. There are very different trade-offs, and my goal here is to give you an intuition on why they're different, and the security trade-offs that they make. So first of all, what optimistic rollups it's very simple intuitively you need to communicate to L1 the L2 state so there is an actor could be yourself that publishes the state routes of the L2 on L1 and there is a timer if this timer expires the state route is optimistically considered correct so that it can be used for withdrawals. This is the happy case. You don't need to prove anything about the correctness of the state. In the not so happy case, you can have this actor that publishes an invalid state root and now what do you do? You need to prove that this state route is incorrect. And as I said, there are a lot of different designs and this is what this talk is about. So the original vision about optimistic roll-ups was the following. That any single honest challenger can protect an optimistic roll-up with a fixed challenge period. So, you know, So optimism and arbitrum have a seven days challenge period. If there is anyone monitoring the chain and they see that some state route is incorrect, they should be able to challenge it. So this was the original vision. The problem is that this is not possible at all. And I'm going to explain you why. So the main problem on why this is not possible are Sybil attacks. So let's say that you're an attacker. What you can do in an optimistic roll-up is propose millions of invalid state routes at the same time. So you, as the single honest challenger, how do you defend yourself? How do you defend the chain? You know, imagine you have millions, again, and you need to prove all of them wrong to protect the fans that are in the bridge. There are two ways to do this. The first one is what I call full concurrency, which means you have millions of embodied state routes, you need to challenge all of them at the same time. Which means that for each state route, there is a challenge period which could be seven days. All these challenge periods overlaps. So the advantage of this approach is that since again all the challenge period overlap, after seven days some of them will be confirmed. So your settlement delay is seven days. The problem is that challenging state routes has a cost, has a gas cost because challenges are interactive, you need to interact with the one multiple times and what an attacker can do in this case is trying to outspend you. So the attacker again publishes a lot of invalid state routes, you need to spend funds to attack all of them, it could be that at a certain point, you don't have funds anymore, and one of the invalid state routes will be unchallenged, and it will be confirmed, stealing everything. So I have an example here with some data. Let's say that the challenge period is seven days. They overlap for all these invalid state routes. The cost of challenging on chain, like the gas cost is one ETH. This is symmetrical for the defender and the attacker because challenges are interactive. There are 1,000 if in the bridge. The attacker has 800 if. The honest challenger has only 500. At a certain point, the attacker is going to outspend the defender and the attacker is going to win. So right now, it's not true that any single honest challenger can protect an optimistic roll-up with a fixed period because not anyone has the same amount of funds as the attacker to protect the chain. So let's say I'm a simple user that only has $1,000 on a chain, and the bounty of the bridge is millions of dollars, an attacker can spend millions to attack it, I cannot defend it, clearly. So this is not true, the correct statement is that challengers with more funds than the attackers can protect an optimistic roll-up with a fixed challenge period, which is a much stronger claim. The second option is partial concurrency. Which is, instead of challenging all the Invali state routes at the same time, I challenge them one at a time. So I create this kind of queue, and I play in one challenge. When I finish one challenge, I go to the next one. And this is partial concurrency, because you can still have some kind of concurrency in the sense that if you have multiple defenders, they can play against different attackers in parallel, but like in the worst case, if you're alone, you need to play against all of them one after the other. So let's go through an example. Again, let's say the challenge period is seven days per challenge and there are multiple challenges. The cost is again one ETH. The funds in the bridge are 1,000 ETH. The attacker funds are 800. And the honest challenger now has very little funds. Having little funds here is not a problem because you can play like you only need one ETH for the first challenge. Then when you win, because you're guaranteed to win, if you're honest, you will get the bonds of the adversary. And with that bond, you can play the next one. So, like, you need very little funds. But the problem is that you have an attacker that can spend 800 ETH to play one challenge. You're going to have a delay of 800 weeks, because one challenge takes one week, and you have, you play them sequentially, so you will have 800 weeks of delay, which is more than 15 years. So this is what is called a delay attack. On proof systems that use partial concurrency. So here again, the claim that any single honest challenger can play an optimistic roll-up with a fixed challenge period is not true. The correct statement is that any single honest challenger, in the sense that you need little funds, so it's very decentralized, it can be you, can play an optimistic roll-up with non-fixed challenge period because they use partial concurrency. So there is this trade-off. When you need to design a for-proof system, you need to decide whether you want to use full concurrency or partial concurrency, which means you either are affected by resource exhaustion attacks, which means that the attacker can try to suspend you, or delay attacks. There is this trade-off. So now let's try to optimize these two cases, because it's not just as bad as I told you, but there is something that you can do to improve the situation. In full concurrency, what you can do is to add bonds. When you add bonds, you add a cost to create challenges. So as I said, the cost of one challenge can be one ETH, but if creating a challenge costs 10 ETH, it means that the attacker needs to spend 10 ETH to induce me as the honest defender one ETH of cost, right? So right now, there is asymmetry, like the C builds have to stake a lot of bonds, I only need to stake one, for each bond they induce me gas cost on playing the challenge, the bond is higher than the cost of one challenge. So let's go through an example, again you have a challenge period of seven days. You have the bond that are ten ETH. The cost of one challenge is one ETH. The funds in the bridge are 1,000. The attacker funds are 800 as before. Honest challenger funds are 500 as before. But now the defenders win because with 800 ETH, the attacker can only generate 80 CBELs, which means that I will only spend 80 ETH. I have more than that, so in this case I win, even though I have less funds than the attacker. The problem here is that this is very complex to do. You need something that is called execution history commitments, which means that you cannot allow... The problem is you need to play the base action when you need to go through a challenge, and you should be able to allow everyone to play within the same claim, because otherwise the problem is... So the interactive game works as follows. I propose a state route, the game is interactive, the next player is going to ask me for a midpoint, the problem is I can lie. I can propose the correct state truth at the beginning and then propose an invalid bisection. And then I can lose on purpose in this way. So the problem is I can only have this asymmetry if people can join together and state truths cannot lose on purpose. And the way that you do this, the way you prevent invalid bisections is by having a commitment over all the steps, the state after all the steps, so that when you bisect, you can provide a miracle proof that the bisection is correct. But again, this is super complex. You are not supposed to understand this, because as I said, this is super, super complex. So improving on full concurrency is very complex. You need to add levels because now when you have execution history commitments over all the steps, you need to merkleize the state for all the steps. These steps are like on the order of 2 to the 70 steps. This is unfeasible, so you cannot merkleize all of them. You need to have bigger chunks. And then when you reach, when you bisect and you reach one chunk, you need to repeat the protocol recursively. And then when you do this, you create another type of attack across levels. And then you might say, I don't want to use levels at all, so let me plug ZK proofs. Again, you're not supposed to understand this. You're just supposed to understand that this gets very complicated when you want to improve on full concurrency. So in practice, Arbitrum, the new proof system that they will deploy soon, and Optimism on mainnet today use full concurrency. So they have fixed challenge periods of seven days. Arbitrum uses this very complicated set-up called execution history commitments. Optimism doesn't. And for this reason, Arbitrum can have what is called the resource ratio of 15%, which means if an attacker, let's say, has 1,000 ETH, me as a defender, to protect the chain, I only need 15% of the funds that the attacker is willing to spend to protect the chain, as the example we made before with bonds. While optimism doesn't have this, they cannot have this asymmetry, this advantage, and indeed if you are a defender and you want to protect the chain, you need 109% of the funds that the attacker has. So like more than 100%. Like significantly more, I would say. So again, imagine you have a bridge with 1 million ETH, an attacker can spend a little less than that, and you need more than that to protect the chain. So it's a lot of funds. An attacker can spend a little less than that, and you need more than that to protect the chain. So it's a lot of fun. The problem is you need very big bond sizes because your bond size needs to be a multiple of the gas cost to play a challenge. And in Arbitrum, this bond size is 3600 ETH, which is insane. So you affect decentralization by a lot. And you have this trade-off between resource ratio and issue on bond size, because, as I said, right now, when you increase the bond size, you can have better resource ratio, like a lower resource ratio, which is better, which improves safety, but affects decentralization. Let's try to optimize partial concurrency. So, instead of having all the challenges one after the other, what you can do is to create a tournament, like a bracket tournament. This is something that was developed by Cartesi, which is another team working on for proofs. So what you can do is you create this bracket tournament, the matches are in parallel, and if you have a lot of C builds, since they are in this tournament, they will eliminate each other. Because they are in this tournament, they will eliminate each other because they are paired together. So this is a strategy to eliminate symbols together. And then since the brackets are then sequential, the honest challenger will only play a logarithmic number of challenges compared to the number of decibels. So if you go to an example, right now the delay is not 800 weeks, but it's a logarithm of that which is two months and one week. And in practice, the current arbitrum classic protocol, the one that is live on Meta that will be replaced, uses partial concurrency. Cartesi will use partial concurrency as well. Cartesi uses it in tournamentsrency as well. Cartesi uses the tournament. Arbitrum doesn't. Initial bond size, let's say, 3 ETH. Cartesi is much better than Arbitrum because, again, they use these... There are a lot of caveats here, but to simplify, a tournament is a very nice optimization to use when you want to use partial concurrency. And the bond size that you need are super small as well. So now you also have this trade-off between initial bond size and delay attacks because if you increase the bond size in partial concurrency, you can create less C bills and when you have less C bills, you have less, like a lower number of challenge periods. So like a lower number of matches, which means also the settlement is faster. This is the summary, more or less, which means that you can see on the vertical axis that if you go full concurrency, you're faster, your settlement gets faster, but you sacrifice safety and decentralization. If you go full concurrency, you're faster, your settlement gets faster, but you sacrifice safety and decentralization. If you go partial concurrency, it's very decentralized, very safe, because you need very low bonds, and your resource ratio is like, it's not even a ratio, it's constant. Then you can play with bond sizes for partial concurrency to be faster but less decentralized, or you can play with bonds on full concurrency to be more safe, to have a better resource ratio, sacrifice decentralization, or you can also play in principle with a challenge period to be like if you have a shorter challenge period, your settlement will be faster, but you sacrifice safety significantly. So I have some further points. I don't have time to discuss all of them, but one point is, is there any way to reduce this challenge period? Because right now it's seven days for all of them. Can we go below that? The answer is no. I don't think we can do this. Because, first of all, why seven days? The reason is to protect from a strong censorship, attack on Ethereum. So there are two types of censorship that you can have. You can have weak censorship or strong censorship. Weak censorship means that you have a certain percentage of builders censoring transactions. You know, like as a context, censoring is a concern because if the honest challenger is censored, the invalid state is going to be confirmed, right? So if there is a percentage of builders that is censoring, This is not really a problem because you still have a high percentage of probability, a high probability of getting included within a short amount of time. The main problem is that if there is a majority of attesters, like 51% of attester censoring, all the transactions from the honest defenders will be excluded from the chain. So now we will need to coordinate a hard fork to hard fork away the censoring validators. And we agreed a while ago that to do this, to coordinate a hard fork, we need around seven days, so if your protocol uses a challenge period that is less than seven days, you cannot protect from strong censorship attacks. Do optimistic roll-ups make sense if we have ZK? I think they do, in the sense that you cannot be cheaper than an optimistic roll-up. In the happy case, you don't have costs, apart from the bare execution of your state transition function. You don't have the proof generation cost. Your throughput is the same as the machine. And, well, in ZK, you can parallelize proof generation and so on, but there are also centralization concerns. So, like, you want to exit from a roll-up, not everyone has the hardware or the, like, ZK ASICs to prove a lot of blocks by themselves if they are censored. So optimistic roll-ups have these nicer properties. But should more project be ZK? I think so. I think the answer is yes, in the sense that if, like, most of the L2s today, we have L2s, we have scalability. The problem that L2s created is fragmentation. Most of the L2s want to fix fragmentation. Can you have interoperability with a seven-day challenge period? I don't think so. So every project that aims at being interoperable with a seven days challenge period? I don't think so. So every project that aims at being interoperable with all the others, I think they should be ZK. For all the other use cases, if you're building a game, you don't need interoperability. I think the optimistic approach is great. So that's it. Thank you. Wow, committing fraud is really complicated. I thought you just signed a different name on the check. So we got some questions from the audience. And the first one up there, I saw this one going up, it's really interesting. In all optimistic L2s, how many times has there been a fraud actually reported and acted upon? We actually have a few examples of fraud proofs happening on-chain. But those fraud proofs are not the result of people being malicious. Most of the time it was just that there were some validators, some nodes that didn't upgrade their software. So they saw an invalid state root because they were running on a previous version. This has happened in Chroma, I think. It has happened in Kinto recently, which is an Orbit stack, like an Orbit from Fork. So another example that is a fun one is on FuelV1, we L2Bit tried to steal $8 from FuelV1. And, you know, FuelV1 is this app-specific roll-up that no one uses, basically. There's no activity. So, like, we wanted to see whether there was anyone trying to defend the chain, even if it was not used. The bond size was 0.5 if we lost. So like all the TVL in Fuel V1, it's like our funds. And because there was someone watching, indeed. Yeah, that Fuel V1 story is going to be the stuff of legend. Next question. Can't the challenger just challenge the first wrong state root, hence invalidating all state roots that are proposed after by the proposer? Well, it depends on the protocol. Most of the protocols, like Optimism today, all the state routes are independent. So invalidating one state route doesn't invalidate all of the others, but also you need to allow multiple proposals to be sent for the same state. Because it's not just that state routes are sequential, it could be that for a single point in time, for a single block number, you need to have multiple proposals because again you're going to have a lot of symbols, so you need to challenge all of them. You could have a fallback mechanism such that if multiple state roots are confirmed, you halt or something. But challenging one state root is not enough. That's the TLDR. Cool. Next question. You kind of alluded to this in the slide. How do you see solutions like optimistic ZK hybrid solutions? Yeah, so this is an interesting one. As I said, if you need to use these execution history commitments, which most of the optimistic proof system use, you have these problems with levels and recursion, and you might prefer not to do that because, as I said, it's very complex. Cartesi decided to go hybrid in the sense that they have an optimistic protocol but at a certain point instead of proving just one single step in their RISC-V machine because they use RISC-V underneath they prove a big chunk of steps. So I think it makes sense to improve the gas cost. Well you can have off-chain proving cost so that's debatable, but another approach is single round. Instead of going to the root of multi-round proof system, you can do what Tyco does, which is you propose, you challenge, and when you challenge, you need to provide a Zika proof either with risk zero or SP1. So I think in the future, if an optimistic roll-up wants to use a fraud-proof system, it's very likely that they will also need ZK somehow in their stock. Cool. Next question. Can part of the answers be facilitating an honest offender having snowballing community support? The other honest observers can validate the challenge and support? Yes. So to allow honest defenders to play together as one, you need ad hoc mechanisms. It's not super obvious. There are some proof systems like the current one in Arbitrum that doesn't allow honest defenders to play together because there are some issues. But the idea is, yes, the honest defender in an optimistic roll-up is going to win, is going to profit. So that's the main reason why some of these projects say, look, it's not a problem if we need a lot of funds to protect the chain because it's profitable. A lot of people will join and defend the chain together. So yes, like the plan is to have multiple months. All right, everyone give Luke.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1ft-eFG4MqCEgA32GW7jQmKsNVc9dmE6ItmC7m8A1nFs",
      "resources_slides": "https://drive.google.com/file/d/1k16u0NpLME83FFa16mP1s3rZ7qZIXLM0/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "rethinking-usability-in-a-world-of-data-ownership",
      "sourceId": "RKNJED",
      "title": "Rethinking usability in a world of data ownership",
      "description": "What makes something usable in a world where the internet is built on open source cryptography? This talk explores how we might consider choice a key factor in the usability of applications where we are owners of our data which we can port, wield, and disclose at our discretion with other data owners. I will illustrate how we are testing our hypothesis that cryptography can surface meaningful connections through demo applications that embrace choice as a key usability factor.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "",
      "featured": false,
      "doNotRecord": false,
      "tags": "data,ownership,Best Practices,Design Thinking,MPC",
      "keywords": "applications,social graphs,data ownership",
      "duration": 1390,
      "language": "en",
      "sources_swarmHash": "3a0ef287d28a9cee43ea3e7397998aa6c6cd47df2b2d5f0010ba0e9da51827f6",
      "sources_youtubeId": "elXvPDai80c",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673424cc9dbb7a90e18ec653",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1J2Pvcrn11ngEmYIecAN4U40wGXlrktRwNsT9I3TM-YM",
      "resources_slides": "https://drive.google.com/file/d/1a4Y4-WWJwbZE_tuu1kGtw_Z6FpEQyA_G/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "voices-of-tech-and-open-source-movement-across-asia",
      "sourceId": "QCPSDK",
      "title": "Voices of Tech & Open Source Movement Across Asia",
      "description": "This panel discussion features individuals from the open source communities, developer and user groups across Asia. These figures span different decades and have witnessed various phases of the tech movement, including the rise of open source, in their respective countries. Some have been pioneers since the early days, while others have emerged as key players through recent college engagements and grassroots initiatives.",
      "track": "Cypherpunk & Privacy",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "FOSS,regional,insights",
      "keywords": "FOSS,Regional,Insights",
      "duration": 3310,
      "language": "en",
      "sources_swarmHash": "61c49c5cdb4bc3d649ccd86731882f2c81639c8d942834f908404b9e8bbc21d7",
      "sources_youtubeId": "TsI7-ejb_Ig",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343e109dbb7a90e1d6c5ab",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67343f599dbb7a90e1f03f5b.vtt",
      "transcript_text": " Hello everyone, my name is Rim. I'm a software engineer at BoxScout, which is an open source Box Explorer. And today I want to invite you to try to verify a couple of contracts along with me. Let's start with answering the question, what the contract verification is. When you deploy the contract onto the chain, it is represented as a number of bytes, which Ethereum virtual machine can understand and execute. And there are no solidity of IP sources stored inside the blockchain. So when indexers index the contract data, all we can see are those two bytecode values, which are contract creation code and contract and time of deployed codes. But most of the people are not good in understanding what the in understanding the raw sequence of bytes and what we usually want to see what is represented in the picture below so we ask our developers to send those sources to us and we recompile them and check that those actually correspond to the on-chain code values. And this is the contract verification. And today I want to verify a couple of contracts with you. So let's start with the simple one. Represented by those two code values here. What do we need from the user to verify the contract? First of all, we need the source files themselves, of course. And let's assume that this tricky storage contract is our potential candidate. It is tricky just because it adds some magic number before storing it inside the storage. That's it. Also, we need the compiler version, which the contract has been compiled with, and the compilation settings. With all that information, our next first step is just to combine all of that into the standard JSON input format, which has all information just in one file. We submit this JSON to the compiler, and what returns us back is the standard JSON output, which is quite big usually, but what is important here for us is that it returns two bytecode values, compiled creation and compiled runtime code values. So what we have to do here is to just take those two bytecodes and compare them. Do they match? Yep, they match, so that's it. Actually, is it always that easy to verify the context, though? Let's look at a little bit more complex example here, which is where we used as external library for making the addition operation, and external libraries as the contact codes, which are deployed once at some address, and when our contacts can link their addresses inside themselves, and reuse their functions by delegate call opcode. So we'll do the same transformations as before, and we'll get two bytecodes as well. But do they match? Well, we can see that there is a strange, not even a hex part inside the compiled creation code, which does not correspond to the on-chain value. So why it happens? Actually, this is the place where the library at least should be put at, but as we haven't provided it to the compiler during the compilation, it just doesn't know what to put inside and places some placeholder instead. And our question is how to verify such contracts. Luckily for us, there is a special section inside the standard JSON output, which is named link references, and which for each unlinked library contains some information how to, where this library address should be placed inside, especially the first byte where it should be placed at and its length which is always 20 bytes. So what we need to do is just to take the specified offset value, when take the next 20 bytes from the on chain code and substitute it inside the compiled code. So do those two byte code match now? Yes, we do. Luckily for us. So here we are just to verify the second contract for today. In general, such replacements, we name them code transformations, and those are some actions which may be applied to the compiled code before or after, during the deployment process, and which changes its bytecode a little bit, but which remains the functionality the same. And there are currently five of such transformations we know about and support. And we've talked about the libraries, but there are four more we don't have time to talk about today. But if you are interested, you might just follow the QR link and see some more information about. So, also, I think the last slide, my presentation title was Verifier Alliance, the first part of it. And I haven't talked about that a lot. But if you are interested in that part as well, you are welcome to the panel which will take place today at 5.30 p.m. where Boxcouts, SourceFind, RoadScan, the members of this Verify Alliance initiative will describe you this a little bit more and talk about verification as well. Thank you. I think that's it. Thank you, Reem. We have questions for Reem? Oh, okay. This is Mike, too. This is pretty awesome, actually. Why is it so difficult to have decentralized contract verification? We use services like BoxScout, Etherscan, but why after all these years is the experience still so bad in general? Well, I think it happens a lot because you have to store this contract somewhere first of all, and the resource file which tries to decentralize the storage process itself and but actually what is more important here were a lot of different formats and all Like different explorers use their own formats to store this data inside source file uses its own data and one of the Vsverify Alliance initiatives idea was to develop the schema in which all contracts should be sorted, and with that actually we are going to have just one database of all verified contracts shared between different verification providers and I hope that will help to increase the decentralization of this data so we're going to share some market dumps for that opens access to the database maybe and hopefully that will work. All right. Shoot. In the verification part for contracts that use library, looks like we are using the reference by code from the deploy by code. Is that safe? Yes, that is safe, because after the compilation we've seen that with 20 bytes, the library address was assumed to be put inside those 20 bytes by the contract code itself, and this address can be anything actually actually so we just take the actual value so we assume that the on-chain code should also contain the library address at this place and take it as our address. So it's actually safe just because this offset was in the standard JSON output section. Alright, thank you so much for this session. Please help me appreciate our amazing speaker, Ren.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1ADQtojPz5zGpvoa8L2aH0vcyddEYsowQH6-jcNkUIMU",
      "resources_slides": "https://drive.google.com/file/d/1x7Q40vFhsOaM-PITqgtBU7cZReJH6-sK/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "zk-email-fast-proofs-and-production-ready-account-recovery",
      "sourceId": "WNQBQH",
      "title": "ZK Email: Fast Proofs and Production-Ready Account Recovery",
      "description": "We discuss progress that ZK Email has made in making new proofs really easily, as well as interesting new on-chain directions for email-triggered transactions. We'll go over proof registries, email-based multisig signers, and email guardians for account recovery in production.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,ZKP,Use cases of cryptography,client-side,2FA,Account Abstraction,Cryptography,Identity,Privacy,Recovery,Security,Use cases of cryptography,Zero-Knowledge,ZKP",
      "keywords": "ZK,Email",
      "duration": 1518,
      "language": "en",
      "sources_swarmHash": "a94b9dc27784f47de11b6a11d62b5643a1cf29f711ee569154584f599c98f857",
      "sources_youtubeId": "YvzdNMpynZM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:30:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1G6_OH46sVVpOgDR1P1ZWqOpTtRzjcESBO1p9aHuVisY",
      "resources_slides": "https://drive.google.com/file/d/1xo7q6y2u3wR9Z7JcCzgdY094rNnKaFh5/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "from-nanoseconds-to-decades-the-timescales-of-ethereum",
      "sourceId": "CGTBC7",
      "title": "From Nanoseconds to Decades: The Timescales of Ethereum",
      "description": "Ethereum is an intricate machine with numerous gears meshing into each other. Some are tiny and spin at lightning speed, others barely move. In this short talk, we will embark on a brief journey through the various processes within Ethereum, examining how long they take -- from executing a single OP code to accepting an EIP.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,data,fun,Core,Protocol",
      "keywords": "Fun,Data",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "43f017afeffc251b70f96edf23f30b02cd8c99c3153c57fb863a1ed583064116",
      "sources_youtubeId": "Oylkbu1lmHw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:40:00.000Z",
      "slot_end": "2024-11-13T03:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ry_A-NlHMHVJmRMfoIquVsBqvO4xh-ZsvcBax7Ji6fk",
      "resources_slides": "https://drive.google.com/file/d/1Ke8oNo2T75sYW_ukY22IWDQ-T1Kv32HA/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "json-rpc-enhancement-in-geth",
      "sourceId": "7KZLFF",
      "title": "JSON-RPC Enhancement in Geth",
      "description": "Introducing trace_* namespace and eth_getTransactionBySenderAndNonce into ethereum execution clients(geth,reth) to enhance the transaction and trace querying capabilities.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Frameworks,User Experience",
      "keywords": "execution client,json-rpc",
      "duration": 801,
      "language": "en",
      "sources_swarmHash": "4e61ae38126f26ad651edd1931f371700863c255c80b6960001052dbc4aa16af",
      "sources_youtubeId": "ifYFJRoW4m8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734298f9dbb7a90e1af087a",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:45:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1seSZfQPsg8riFizMYXy6BWgpFjxQVJYELPyjZazrxIc",
      "resources_slides": "https://drive.google.com/file/d/1jXEZlPn40SQ2oW7F9lxWaQCmKnG2heiB/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "long-term-decentralized-storage-for-blobs",
      "sourceId": "RCVFHX",
      "title": "Long-term Decentralized Storage for Blobs",
      "description": "This talk will present a possible scheme to store blobs and other historical data for the long-term in a decentralized fashion. The technology relies on erasure codes and SNARKs. This talk is related to EIP-4444.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Blobs,Sustainability,storage,Blobs,Core Protocol,Sustainability",
      "keywords": "Storage",
      "duration": 324,
      "language": "en",
      "sources_swarmHash": "146236064ab2f260f965665a61e0db66f86769d420f93b351c716f64d8f6e2bf",
      "sources_youtubeId": "Bizi9n0t6pY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T03:50:00.000Z",
      "slot_end": "2024-11-13T04:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/19uBY8dZebCAmZtuh27GvgwcgDo7WY_BpHnT84sKBL6M",
      "resources_slides": "https://drive.google.com/file/d/1UGpCD6G-oXiK0AkIq_0k_39KCY7CNyG5/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "can-we-formally-verify-implementations-of-cryptographic-libraries-like-the-c-kzg-library",
      "sourceId": "HQP3RJ",
      "title": "Can we formally verify implementations of cryptographic libraries like the c-kzg library?",
      "description": "In this talk, we present our work on formally verifying the implementation of a cryptographic library key to the security of the Ethereum Data Availability layer: the c-kzg library. We will explore what we have been able to prove so far and what is ahead of us.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Cryptography,Formal Verification,saw,Cryptography,Formal Verification,Layer 1",
      "keywords": "Cryptol,SAW",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "18bdd6c02044f4d2a659c7ff87da1b5b56eeec9a92243f02be56200ba0d9d312",
      "sources_youtubeId": "5oqUAf7vy28",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1hG56xfpzqIZ6kxBtsd4z9tj3Nz-JQcrTh2y22jsemLo",
      "resources_slides": "https://drive.google.com/file/d/14Zu73Eq_kkRzOOJfJzFvnufist36ZupV/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "decentralizing-access-to-ethereum-utilizing-ethereums-portal-networks",
      "sourceId": "NWSNWX",
      "title": "Decentralizing access to Ethereum utilizing Ethereum's Portal Networks",
      "description": "Accessing Ethereum in a decentralized way has a high barrier to entry for reasons of cost (hardware), knowledge, or time. These problems cause users to rely on centralized providers.\r\n\r\nA few examples on how Ethereum's Portal Networks will tackle these centralizing forces\r\n- EIP 4444's + Portal History will allow nodes to maintain current day RPC, well saving 800GB of storage.\r\n- Portal State will allow wallets to use a decentralized backend instead of a centralized backend like Infura.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Decentralization,Light Clients",
      "keywords": "EIP 4444s,Portal Network,Decentralization",
      "duration": 1374,
      "language": "en",
      "sources_swarmHash": "f14d2e446ae47d87640a810710fe55bf479671fca84990d0b042594273747c36",
      "sources_youtubeId": "hG42ycpHyZQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67342c889dbb7a90e1c174fe",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1B7KXH5uVHB04jWwnsYtQMYYbRlXaYPjx6HTM5n2vYhk",
      "resources_slides": "https://drive.google.com/file/d/13CziBz3ONvnzss5-5qWPwTiItbR43bvZ/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "epf-nethermindil-evm",
      "sourceId": "QJNNDL",
      "title": "EPF - Nethermind/IL-EVM",
      "description": "This talk will discuss my EPF work on Nethermind's IL-EVM project, which included developing tools to analyze EVM execution patterns, writing  (optimised) opcode and top pattern implementations, and conducting and writing tests.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core,Protocol",
      "keywords": "EVM,Optimization",
      "duration": 699,
      "language": "en",
      "sources_swarmHash": "ee30c44852dd78c6f9f71de6a552443c01b1cd6fa673737ad972191ff03e328d",
      "sources_youtubeId": "9UhpqUzsEJE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673428639dbb7a90e1a39ee7",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/13ze8Pr4OxtxoFIIxGDV0SAGLb_aIJtknEOqxz5Ct5lA",
      "resources_slides": "https://drive.google.com/file/d/1d3_vY9k2PtKtEZyiEcGee-e-k9HwAidr/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "keynote-making-sense-of-stablecoins",
      "sourceId": "TDHR79",
      "title": "Keynote: Making Sense of Stablecoins",
      "description": "Everyone is talking about stablecoins now! In this talk I'll share what I learned about Tether on Tron in addition to stablecoins more broadly. Why are so many USDT transactions on Tron? Why did Bridge get acquired for $1.1B? What do L2s have to do with stablecoins? Are stablecoins a threat to Ethereum or an accelerant?",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": true,
      "doNotRecord": false,
      "tags": "Ethereum for Good,Payment,RWA",
      "keywords": "Stablecoins,Layer 2,RWA",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "4b5b1851c7f19fc2d903b32c4df01cd2e7e01bb4c81cddf2419d59e845b353e1",
      "sources_youtubeId": "_TDh8kCiwXw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1246DZFHYl7mJ0u_o2WRFQUGA1oxze-pQVaEpjC7wjPI",
      "resources_slides": "https://drive.google.com/file/d/1B8eIA8flhTpnkq-MXcWHxaLMgHS5IoWX/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "lessons-from-integrating-logup-gkr-in-the-miden-vm",
      "sourceId": "LL799L",
      "title": "Lessons from integrating LogUp-GKR in the Miden VM",
      "description": "In this talk we will describe how to modify the STARK protocol to prove multiset checks using the GKR protocol. We will take a deep dive of the approach we’ve taken to implement it in the Miden VM, covering the benefits and challenges we've experienced.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zero-Knowledge,Cryptography,gkr,Cryptography,Zero-Knowledge",
      "keywords": "LogUp,GKR",
      "duration": 1392,
      "language": "en",
      "sources_swarmHash": "7afb12e3dc341b4fc4f68df53f30360755a4e5033980052d6975716f890afc24",
      "sources_youtubeId": "f4Zwn6ItiNs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67342ddd9dbb7a90e1c34f54",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Eh_tW-ueqILgRF3_daF57cyNlIe38F86K1969SSn5sg",
      "resources_slides": "https://drive.google.com/file/d/1_Rvu9pBGs12QgqZWNYaxDt9ZteMQZh5n/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "singer-sing-writer-hour-with-adegbengaoggunbdeje",
      "sourceId": "R9KTR7",
      "title": "Singer sing writer hour with adegbengaoggunbdeje",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/188EWHuoqMHZmI_lZQs8v-nCOf8dWQUXTZ39BGcW23wE",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "the-dave-fraud-proof-algorithm-triumphing-over-sybils-with-a-laptop-and-a-small-collateral",
      "sourceId": "C7ZFH3",
      "title": "The Dave fraud-proof algorithm — triumphing over Sybils with a laptop and a small collateral",
      "description": "Current fraud-proof algorithms are susceptible to Sybil attacks, impacting security, decentralization, and (settlement) liveness. This presentation introduces _Dave_, a novel algorithm that offers an unprecedented combination of these three properties. We demonstrate that there's no realistic Sybil attack capable of exhausting defenders' resources or causing significant delays, even with minimal bond requirements.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Optimistic rollups,fraud,proof,Optimistic,rollups",
      "keywords": "Interactive,fraud,proofs",
      "duration": 1393,
      "language": "en",
      "sources_swarmHash": "f6b19521b73dd026fbdfe1a938aa2a58f1b3e9332c026eba6e6fdd0cc69e350a",
      "sources_youtubeId": "dI_3neyXVl0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1GhOQePXCr0xuShvpJcgSNAMhIC_wT2B34JYiogZJB7s",
      "resources_slides": "https://drive.google.com/file/d/1ulLAN8HrVwwBhQFbRhpx3KGT14CNJ0nZ/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "the-wallet-and-ux-stack-to-build-web3-applications-for-the-masses",
      "sourceId": "LCNEGW",
      "title": "The Wallet and UX Stack to Build Web3 Applications for the Masses",
      "description": "In this talk I will give an overview of how wallet infrastructure and the relationship between wallets and dapps have evolved over the past 5 years. And give a layer-by-layer breakdown of the modern wallet stack from signers to smart account modules, how each component contributes to a UX unlock on Ethereum/L2s, and how application developers can use them today. We will also touch on pertinent ongoing EIPs such as 7702 (deploy code for EOAs), and 7715 (permissions).",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,User Experience,Account Abstraction,permissions,Account Abstraction,Developer Infrastructure,User Experience",
      "keywords": "Wallets,Signers,Permissions",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "55d5707ff2452cbecbf3c9de175bda79205405d45aa4a8242a116659c8ea8838",
      "sources_youtubeId": "5vkjN9LKsiw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1EwxJbkAW9PZZpjRozkPVAnLaQpoQZm7uf1kolnUFM_0",
      "resources_slides": "https://drive.google.com/file/d/13VNvonmWR8H1GLFUkHCdztpesOJNWrVP/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "things-you-didnt-know-about-contract-deployment",
      "sourceId": "GJM9UC",
      "title": "Things you didn't know about contract deployment",
      "description": "In this session we will explore some of the lesser-known facts around contract deployment. To make the presentation accessible to all technical levels, the talk will start by recapping the three ways to start contract deployment (deployment tx, CREATE, CREATE2). Following this, we will delve deeper into the topic and highlight some interesting facts around contract deployment, including what happens when an address already has code, ETH, or state entries at deployment.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "deployment",
      "keywords": "Deployment",
      "duration": 455,
      "language": "en",
      "sources_swarmHash": "e5fd22d186e8fffb80536b2b8384bfe34be27dc9ada6f8ec30c26118b31bbf63",
      "sources_youtubeId": "BGT-VwLIbs0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:00:00.000Z",
      "slot_end": "2024-11-13T04:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1j7qMdITP1J2AjDNnsbYHtP1ZqxF408IJ_kLSInVI0qU",
      "resources_slides": "https://drive.google.com/file/d/19sJrvm-9JeKa0t_QTvSd7HUsfXsG0jBL/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "evm-charts-2024-whats-hot-whats-not",
      "sourceId": "R3UPGT",
      "title": "EVM Charts 2024: What's hot? What's not?",
      "description": "Thanks to the openness and transparency of blockchain we can study how developers actually use it. In this session we will compare the usage of EVM on mainnet from the last Devcon to this Devcon. Including questions like:\r\n* Which opcodes have become more/less popular?\r\n* Which precompiles have become more/less popular?\r\n* Has average memory consumption increased/decreased?\r\n* How actively are new features being used?\r\n* Are transactions getting more complicated?",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Architecture,Gas,EVM,trend,usage,Architecture,Core Protocol,Gas",
      "keywords": "Opcodes,Precompiles,EVM Metrics,Protocol Optimization,Statistics,evm usage trends",
      "duration": 445,
      "language": "en",
      "sources_swarmHash": "557c6cca7b7b2b59d76ce07897cfbc711c0cf196474cb55b4bf76b0106349118",
      "sources_youtubeId": "m1tdQfaKt7Q",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:10:00.000Z",
      "slot_end": "2024-11-13T04:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1jchtIsIrvcgl2q1AJ62ke7MdCNqf6zK1fAUfSJtbTac",
      "resources_slides": "https://drive.google.com/file/d/1LCV95XfblA3Uh2QXTVa9UmKy4zTpown4/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "proving-liquidity-of-an-amm",
      "sourceId": "AD3X38",
      "title": "Proving liquidity of an AMM",
      "description": "Liquidity providers in an AMM expect that they can always withdraw their tokens, even in case of a bank run.  Taking the concrete implementation of Uniswap v4, we formally proved that the funds owned by the contract always cover the provided liquidity.  This talk describes the methodology for proving this critical property, which can be applied to other protocols holding the liquidity for their users.",
      "track": "Security",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Formal Verification,Reentrancy,invariants,Formal Verification,Reentrancy",
      "keywords": "Invariants",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "4d4e75970d8eed994781f33c0552f03df8afc1086d5c55d330347bbaea91d763",
      "sources_youtubeId": "CnrZyDeGwKI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:10:00.000Z",
      "slot_end": "2024-11-13T04:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1QlA6rBFr3f12d9BFrh9CBVqTCO60FFqlit1W076MzQ8",
      "resources_slides": "https://drive.google.com/file/d/1yVT_piGcfSE6Y-oFggao2522m6sJ4N6H/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "samba-a-besu-portal-client",
      "sourceId": "FTC8PQ",
      "title": "Samba, a Besu Portal Client",
      "description": "A presentation about my experience participating in the EPF. Talking primarily about the project I worked on for the cohort and various obstacles that I faced along the way. I additionally aim to go into detail about where I see Samba going in the future and my role in that development.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Portal,network",
      "keywords": "EPF",
      "duration": 705,
      "language": "en",
      "sources_swarmHash": "e02934d00bab1d926fea5b2862138be2ad9176107861f2ad27b01d9f661df389",
      "sources_youtubeId": "11sZxJ4QuLk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67342b249dbb7a90e1bd5b22",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:15:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1V8MPOsuS_Y8NmrHqykkqj248dMqY5xfRkNholeY8m_Q",
      "resources_slides": "https://drive.google.com/file/d/1nU7h34dhmciALEpyYLrBBGUM1i9UWX5Z/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "clear-a-formal-verification-framework-for-smart-contracts-in-lean",
      "sourceId": "M3QKTW",
      "title": "Clear: a Formal Verification framework for smart contracts in Lean",
      "description": "Join us for an in-depth workshop on the Clear framework, a cutting-edge tool designed for the formal verification of smart contracts by extracting Yul code into Lean. This workshop will explore Clear’s remarkable expressivity, enabling any pen-and-paper proof of correctness to be mechanized in Lean. Participants will learn about Clear's compositionality and abstraction, allowing scalable verification of complex smart-contracts, and its automation capabilities to streamline proof generation.",
      "track": "Security",
      "type": "Workshop",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Frameworks,Security,Formal Verification,yul,lean,itp,Formal Verification,Frameworks,Security",
      "keywords": "Yul,Lean,ITP",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "da9e9a072c922d7a4acd547d980a47a675dd1fe56ac995d591a577d698f9fd83",
      "sources_youtubeId": "3i6dZgUm7JU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:20:00.000Z",
      "slot_end": "2024-11-13T05:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1J21ogENKuo-BCOrzHLQIUHr3yr7F7Ki9YBkOmnf6sTU",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "copying-memory-in-evm-how-hard-can-that-be",
      "sourceId": "JKFBN3",
      "title": "Copying Memory in EVM, how hard can that be?",
      "description": "Memory copy operations in EVM are a useful feature, but there are different ways to do. How do they differ? Which is the best?\r\nThe options are:\r\nMLOAD+MSTORE loop\r\nIdentity Precompile\r\nThe new MCOPY opcode\r\nBased on concrete examples we will explain how these options differ. We will use different examples as the amount of bytes copied makes a difference. For all these options we will present gas consumption and code size.\r\nThis way we can compare the different options to copy memory and crown the ult",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Gas,Developer Infrastructure,compilers,optimised,Core Protocol,Developer Infrastructure,Gas",
      "keywords": "Optimisations,Compilers",
      "duration": 495,
      "language": "en",
      "sources_swarmHash": "cfc406c95e513fab06ed3c115ed870f47b7edc58852cfbb26268ceb00350b80d",
      "sources_youtubeId": "en-Oeie6ZEs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:20:00.000Z",
      "slot_end": "2024-11-13T04:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1zHvG3U1k7Ixpod7JNDZSJechFPRUfpGmYtaC0t0ufJA",
      "resources_slides": "https://drive.google.com/file/d/103zrCHIutXZO8_mtK-68QpFlfxTOrWVv/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "developing-and-using-a-modular-folding-schemes-library",
      "sourceId": "PPFPQY",
      "title": "Developing and using a modular folding schemes library",
      "description": "We will present Sonobe, a modular folding-schemes library. It currently features implementations of Nova, CycleFold, Hypernova and ProtoGalaxy schemes and is compatible with a wide range of R1CS arithmetization libraries. we will briefly discuss what folding schemes are and how they fit into IVC-style proof systems. Next, we will explain how Sonobe was built and what features it supports. Finally, we will cover what has been built with Sonobe and how developers can start using it today.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Libraries,Zero-Knowledge,Cryptography,nova,Cryptography,Libraries,Zero-Knowledge",
      "keywords": "Folding schemes,IVC,Nova",
      "duration": 1574,
      "language": "en",
      "sources_swarmHash": "2e331d376e1fc71c8dba0d0ecc80ce2ae55d376fbce4fa2edba77aa3c2fca3ae",
      "sources_youtubeId": "biK_NKwdBk4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673437c69dbb7a90e12bca3e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1IOfjp_pKz83JTceKqk5Rve7U1YRQJSc4MA5OPmnj6oE",
      "resources_slides": "https://drive.google.com/file/d/14WvRoVTFSbrWP5sjs0vDsuhAPKoroIv4/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "hunt-the-bug-save-the-chain-uncovering-bugs-in-eip-implementations",
      "sourceId": "UQ8MWW",
      "title": "Hunt the Bug, Save the Chain: Uncovering Bugs in EIP Implementations",
      "description": "In this workshop you can find a bug in an EIP implementation on a test network!\r\n\r\nThe Ethereum Foundation Testing Team oversees cross-client execution specification testing, which is critical to avoid consensus issues at the smart-contract execution level.\r\n\r\nYou'll implement tests for a new EIP from scratch using the ethereum/execution-spec-tests framework and execute them on a local test network with a faulty client. Anyone attending has the chance to find the issue and break the network!",
      "track": "Core Protocol",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "Core Protocol,Security,Testing,python,pytest,specs,Core Protocol,Security,Testing",
      "keywords": "Python,Pytest,Specs",
      "duration": 6666,
      "language": "en",
      "sources_swarmHash": "d6bd3d078ed4fb9ae1b8c781897264b4738782f469f0d370dea340349c2b9587",
      "sources_youtubeId": "K0pQ7bRuJOk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T06:30:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/117F-s4Jnf3r7cRIQqAwsYqwIGULHx4JTcdJjW64wZag",
      "resources_slides": "https://drive.google.com/file/d/10lb4qgbLyZnPGyWgUhOLCvXsBWXLs8wl/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "keynote-the-real-state-of-l2s",
      "sourceId": "HCXUU8",
      "title": "Keynote: The REAL state of L2s",
      "description": "The evolution of Layer 2 solutions has been pivotal in scaling blockchain technologies. This talk, led by L2BEAT founder Bartek Kiepuszewski, delves into the current landscape, recent advancements, and future potential of L2 ecosystems. It will try to address some myths and current challenges of the space. Some important changes to L2BEAT risk framework will also be announced.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": true,
      "doNotRecord": false,
      "tags": "Architecture,Layer 2s,Best Practices,myths,reality,Architecture,Best Practices,Layer 2s",
      "keywords": "L2Risks,Myths&Reality",
      "duration": 1530,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "ik2JxmHDmyw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1NxPv65UP8MJMX2f8NWmiAL-GETRNifiDtkZS5evBvV0",
      "resources_slides": "https://drive.google.com/file/d/1DcThD-G-5SlT4Tx2WDp3IENxGIGFf-LL/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "programmable-cryptography-and-smart-contract",
      "sourceId": "VJEDLX",
      "title": "Programmable Cryptography and Smart Contract",
      "description": "Overview\r\nIn some use cases, developers may want to execute smart contracts based on the results of FHE or MPC execution. This session will introduce several design patterns for such use cases and show how Programmable Cryptography can be applied to dApps.\r\n\r\nIn detail\r\nThe results of FHE executions are encrypted and need to be designed to be processed by smart contracts. In addition, the MPC+ZK-based method can solve the private state problem relatively easily using the conventional SNARK verifier.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Cryptography,MPC,programmable,DevEx,MPC",
      "keywords": "Programable,Cryptography",
      "duration": 355,
      "language": "en",
      "sources_swarmHash": "17c804065df8bd60c3c0133f7d5ffdea4c30fe373f0ed0d016d0c9351279fb84",
      "sources_youtubeId": "j9YnU1-MeiU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T04:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1dUK2fPW4Yka7X0nBzFRlJXDKOPHcZn0iLzNpS3rUVcI",
      "resources_slides": "https://drive.google.com/file/d/1RO17LT7cZMlx8nuuMcUGCoRviKeGEkG7/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "redefining-daos-state-of-daos-in-asia",
      "sourceId": "PUMYRH",
      "title": "Redefining DAOs: State of DAOs in Asia",
      "description": "We are a team from Metagov and DAOstar, advancing the global DAO movement through standards like ERC-4824 and exploring diverse DAO narratives worldwide. We've commissioned multiple reports on the “State of DAOs” in Asia, covering Japan, South Korea, Taiwan, Singapore, Greater China, and SEA. Our panel will discuss these findings, focusing on DAO narratives, regulations, opportunities, and differences between Eastern and Western DAOs, aiming to bridge the gap in the global DAO discourse.",
      "track": "Coordination",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Governance,asia,Coordination,DAO,Governance",
      "keywords": "Standards,Asia",
      "duration": 3320,
      "language": "en",
      "sources_swarmHash": "b35d788dd5f9fe24db1d7b79ac09e50c80bc87072fc3500fd92bf56c3d47ded0",
      "sources_youtubeId": "8zy3C3pYh48",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343a1e9dbb7a90e18a29e8",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1ieI7X9rFpOPzhR32w8gT6d_tE2y-xDKaSS2cr_K6lgE",
      "resources_slides": "https://drive.google.com/file/d/1Hk2UfE5WN2V92GOwzmJjNvuY4AgA96eX/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "supernodes-on-a-shoestring-democratizing-ethereum-with-low-power-hardware",
      "sourceId": "W3DKPQ",
      "title": "Supernodes on a Shoestring: Democratizing Ethereum with Low-Power Hardware",
      "description": "Learn to run a full Ethereum supernode (L1 & L2) on affordable hardware (ARM devices) This live demo will guide you through selecting the hardware, installing EoA image who automatically install and configure all the software. Become a part of the decentralized Ethereum on a easy and power efficient way.",
      "track": "Core Protocol",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Decentralization Improvements,Layer 2s,Decentralization,hardware,low-power,Decentralization,Decentralization Improvements,Layer 1,Layer 2s",
      "keywords": "Node Operation,Low-Power Hardware",
      "duration": 4662,
      "language": "en",
      "sources_swarmHash": "46fe2d92049008021f297bb1ee93996f8d721813789639773fef05ea0c778d9a",
      "sources_youtubeId": "k2lYtOi1KJY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T06:00:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1iW-qq2w5XkPf2rNpSWzKfErwV_ysrpVcA97rrOKKEyQ",
      "resources_slides": "https://drive.google.com/file/d/11pPsDl0AmrhXn7n5c04QAvQrVP55d54A/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "the-three-transitions-cross-chain-smart-wallets-with-privacy",
      "sourceId": "JESAHN",
      "title": "The Three Transitions: Cross-Chain Smart Wallets with Privacy",
      "description": "Last year, Vitalik outlined [\"The Three Transitions\"](https://vitalik.eth.limo/general/2023/06/09/three_transitions.html) ahead for the Ethereum stack: moving to L2s, smart wallets, and private transactions. The Base team has built [Keyspace](https://docs.key.space/), a cross-chain keystore that helps all wallets makes these transitions. Come learn about how Keyspace works and how Keyspace helps smart wallets sync signers and send private transactions in a multichain world.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zk Rollups,Cross-L2,Account Abstraction,wallet,Account Abstraction,Cross-L2,Zk Rollups",
      "keywords": "Wallets",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "2dbb90931765bca295153542c6c3dc5980889b7e4a1c0368fba93fac384be971",
      "sources_youtubeId": "DibVD2gCyp8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/12qgh9Oa6U7CvGBkNUiXG-L-E0qYKLqahhOhkZATUF_Q",
      "resources_slides": "https://drive.google.com/file/d/1gxh22rMqmwQSB9UgDPjQN2pJKV2WBr-B/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "verkle-integration-in-reth",
      "sourceId": "T8LKTM",
      "title": "Verkle integration in reth",
      "description": "This talk concerns the presentation of EPF Project: Verkle integration in reth.\r\nThe project comprised of replacing the current state-commitment structure in reth with verkle tries and other modifications for statelessness, including implementing EIPs such as EIP-4762: Statelessness gas cost changes (to REVM), EIP-6800: Ethereum State using a unified verkle trie, EIP-7709: Read BLOCKHASH from storage and update cost, and passing the associated execution-spec-test vectors designed for these EIPs.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "EPF,Core Protocol,Cryptography,Verkle trees",
      "keywords": "Stateless clients,Verge",
      "duration": 871,
      "language": "en",
      "sources_swarmHash": "4c3add1def5321ff44e6a128ca79299c6b8f9c3c4c274d2d79412d0bcf266853",
      "sources_youtubeId": "7tOXl-C21CQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67342f409dbb7a90e1c9090f",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T04:45:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Uq2DzZBnDwPSfrV2xqfm-mlie2DOZZKEwi0Kk44YlQI",
      "resources_slides": "https://drive.google.com/file/d/1f2InLp14m4mOw2XdTyOMZmMgHq9fVR-6/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "wallet-ux-panel",
      "sourceId": "9HACGK",
      "title": "Wallet UX Panel",
      "description": "Wallets are here to provide great user experience with robust security. \r\nBringing the top wallet providers (Fireblocks, Safe, Metamask, Coinbase and WalletConnect/Reown) to talk about how Ethereum user UX evolved and how we can make it much better.",
      "track": "Usability",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Custody,Account Abstraction,standards,Account Abstraction,Coordination,Custody",
      "keywords": "Wallets,User Experience,Standards",
      "duration": 3396,
      "language": "en",
      "sources_swarmHash": "2f420c5d40f8af815eaa182ef386e07100536e8611142260c83d0014d1c20481",
      "sources_youtubeId": "UhQWxhId2Nk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673446f19dbb7a90e1697a9c",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1qtrl6r-TYlWqtL69dNckKj8GBF_OtG2FNSwchnfA6ew",
      "resources_slides": "https://drive.google.com/file/d/1A0h36Tj9OdHEsLBKh-SHtYSzoDfnzMcF/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "what-is-the-status-of-epbs-and-its-future-iterations",
      "sourceId": "3MUYVQ",
      "title": "What is the status of ePBS and its future iterations",
      "description": "We will go over the implementation and research status of ePBS (EIP-7732) and the future iterations and mechanisms it enables.We will describe in detail the main benefits to the protocol that are not directly related to any PBS system. We will showcase the tradeoffs that are present on each design decision and how the separation of validation between the consensus and execution layer in fact frees research with less technical debt and more independent mechanisms for future upgrades.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "PBS,fork,choice,PBS",
      "keywords": "PBS,consensus,fork-choice",
      "duration": 1483,
      "language": "en",
      "sources_swarmHash": "f5e5ad50e09c6e119cd1571e3ae0c3a54ebaf8460e392a4fd0abc96593c84a31",
      "sources_youtubeId": "w-VwYHq1FA4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673438b39dbb7a90e14a7f53",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:30:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1hihFfnTMBS1Mmp0aS3oHwzA-PX43SVRFqlRfNkbtOwU",
      "resources_slides": "https://drive.google.com/file/d/1Un8pdIcw2kPvOil4YyRjg4yXkkFBWG3_/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "verifier-alliance-inside-of-the-contract-verification-pipeline",
      "sourceId": "Q3EDF8",
      "title": "Verifier Alliance: inside of the contract verification pipeline",
      "description": "The talk will guide you through a smart-contract verification process step by step while introducing some technical details and challenges verification services have to handle. Will describe what we have learned building \"Verifier Alliance\" - a new collective that unites different verification providers to have an open and shared database of smart contracts (verifieralliance.org).",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,verification,contracts,DevEx",
      "keywords": "Contract,Verification",
      "duration": 519,
      "language": "en",
      "sources_swarmHash": "69a3730194c47cec0c3005a9eed1c4f8dd9c959160dc3ba772e0007bc7847a61",
      "sources_youtubeId": "2U4Wad2ebwI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:40:00.000Z",
      "slot_end": "2024-11-13T04:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1WNKyHeXOwkXmvaf0GIGfAtO5R7MQYyUbdRwxgk23ZzQ",
      "resources_slides": "https://drive.google.com/file/d/1LcM0tpfszmP_-tK-zsx9X767Zip80dBX/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "c-bindings-for-constantine-verkle-ipa",
      "sourceId": "TDQFPX",
      "title": "C bindings for Constantine verkle-ipa",
      "description": "The session is part of final presentation of Ethereum Protocol Fellowship. In this talk I would be talking about the verkle-ipa provided by Constantine and exporting those to be made available for integration by clients for the verkle integration of ethereum roadmap.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,Ethereum Roadmap,Verkle trees",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "5ee0568ebdea72398644ac8e95091444abbabfbcd722932eb41e1d3f5fba5178",
      "sources_youtubeId": "LpL-KJqxUwo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:45:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/10DqlfeLWBhwxvz53O8852FsvukCw9hOvBC9rvKdKNM8",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "firefly-build-your-own-hardware-wallet",
      "sourceId": "LMZKZS",
      "title": "Firefly - Build your own hardware wallet",
      "description": "Build your own Firefly hardware wallet and write your first custom firmware in a short interactive session. All parts provided, just bring a laptop and USB-C cable.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Hacks,Hardware wallets,arduino,DevEx,Hacks,Hardware wallets",
      "keywords": "DIY,Arduino",
      "duration": 564,
      "language": "en",
      "sources_swarmHash": "22e79a1778a0d016c579c6d3bff0ed86601dc90b1a2f896324503482136f2c30",
      "sources_youtubeId": "NWdMDKMZdpQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T04:50:00.000Z",
      "slot_end": "2024-11-13T05:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12mlEi-XhwS1335VqCql4XOq2MN1ZU6WJeQLvyAc-QHU",
      "resources_slides": "https://drive.google.com/file/d/1-ljAhJeqAvbFjYFghgSnECvbfwng3v6d/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "deca-12x",
      "sourceId": "WYESYA",
      "title": "Deca 12x",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:00:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1oXwhbXtA9_IkZiM1hj6x3YDfhq54XQGQeoHUbTHVK4I",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "efficient-non-native-snark-recursion-using-bivariate-polynomial-testing",
      "sourceId": "E8KYKE",
      "title": "Efficient non-native SNARK recursion using bivariate polynomial testing",
      "description": "Efficient SNARK recursion requires switching between pairing friendly elliptic curves. In most optimal approaches these curves would construct a cycle, but there are no such known cycles. Instead, we use non-native arithmetic to brute force the pairing computation at the cycle cut-off.\r\nWe describe an approach for combining direct field extension with polynomial-based non-native arithmetic. This reduces pairing computation to bivariate polynomial identity testing using Schwartz-Zippel lemma.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Cryptography,SNARK,zk,based,pairing,Cryptography,SNARK,ZKP",
      "keywords": "Pairing,based,ZK",
      "duration": 1510,
      "language": "en",
      "sources_swarmHash": "a458b8ba83746ad8de60ca9f7be70aed5513ad7a20085623774277b867f33c48",
      "sources_youtubeId": "ylmiiUnwhrE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343a9c9dbb7a90e193e070",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:00:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1uBrjsIa4svOJ9BePcS4YgEcFXFjVxeeeS9RBVSKBwzw",
      "resources_slides": "https://drive.google.com/file/d/1X08XqyRL51PhoFe6KKsGUJocEz9lmnrG/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "l2-interoperability-via-collaborative-snarks",
      "sourceId": "JPGEPU",
      "title": "L2 Interoperability via Collaborative SNARKs",
      "description": "Can contracts across rollups interact synchronously while maintaining horizontal scalability? The L2 interoperability problem can be viewed through the lens of collaborative SNARKs, where a coordinator splits a witness over N provers who collectively generate a proof, and the work each prover does should decrease linearly in N (horizonal scaling). This talk presents a solution for the special case of L2 interoperability and motivates new design constraints for SNARKs.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Fragmentation,Zk Rollups,Cryptography,interoperability,Cryptography,Fragmentation,Zk Rollups",
      "keywords": "Interoperability",
      "duration": 1530,
      "language": "en",
      "sources_swarmHash": "145060245426c6488d4e7abb9c124c221025d0620cf3cefc8df683e61eaacd97",
      "sources_youtubeId": "Et58V-0FYsE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343ad99dbb7a90e19c6d6f",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:00:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1ZVK2vJYrK2rxz9r6LEc4JhYeEu_tVk_8Q4kLDWRxY9k",
      "resources_slides": "https://drive.google.com/file/d/1HjfrYnUx3LXhiYt6NWvEo7mGKUGbDAnd/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "lighthouse-transition-journey-from-warp-to-axum",
      "sourceId": "ZF79GZ",
      "title": "Lighthouse transition journey : from warp to axum",
      "description": "This talk will explore how to approach a significant refactor of the HTTP framework for lighthouse. \r\n\r\nIt will cover:\r\n- Measuring the performance of endpoints between Warp and Axum\r\n- A concrete plan for implementing the necessary changes",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Light Clients",
      "keywords": "Performance,Developer,experience",
      "duration": 537,
      "language": "en",
      "sources_swarmHash": "0ba36082c0dc4df92f95f85927da7080d528c36d541326d01a2f2c36606c619e",
      "sources_youtubeId": "PX-LJ7iqHig",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343c729dbb7a90e1adea32",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:00:00.000Z",
      "slot_end": "2024-11-13T05:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1xTcTdXk_Eq4KKe0Dg4IQWit5KLqwivJSom49AbKiMFM",
      "resources_slides": "https://drive.google.com/file/d/1KxsX6opQOw677jx_ThW2hpE_hCsvxSOl/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "zkproving-the-history-of-ethereum-in-real-time",
      "sourceId": "TVNJ99",
      "title": "zkProving the history of Ethereum in real time.",
      "description": "I'll explain the current work that we are doing in the Polygon zk teams to improve the performance of the provers and the quality of the tooling.\r\nI'll will explain how we can parallelise the generation of the proof and how we can integrate with different hardware and software so that it should allow to build a zk proof of a block in real time. \r\nI'll explain also how this proofs can be recursively linked to build a zkProof that can proof the whole Ethereum history from the genesis.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZK-EVMs,ZKP,Zero-Knowledge,lightclient,type1,starks,Zero-Knowledge,ZK-EVMs,ZKP",
      "keywords": "Lightclient,type1,STARK",
      "duration": 1604,
      "language": "en",
      "sources_swarmHash": "848d3f552c5ce88efe748988407546a906a410c1d533f47a363d8c0dcf4463fe",
      "sources_youtubeId": "boSCLHs30tk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673441219dbb7a90e10a4706",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:00:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1p0VlUcR1aOi--jA4hFb8aBF8mAWBuf-2vwun38CXBtI",
      "resources_slides": "https://drive.google.com/file/d/1mdKsMi32bTXwb1XsJEfcfEWL-A0omDoP/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "tracing-integration-in-lighthouse",
      "sourceId": "RVZX3C",
      "title": "Tracing Integration in Lighthouse",
      "description": "During Ethereum Protocol Fellowship, I've worked on integrating `Tracing`(an async-friendly logging framework) into Lighthouse(CL client) .\r\nThis presentation will provide a brief overview of the work that I’ve done.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Frameworks",
      "keywords": "",
      "duration": 841,
      "language": "en",
      "sources_swarmHash": "175d7bc039ec1952a66d831fd8c59ca61cd356e1f223f38794226fa65f26d38e",
      "sources_youtubeId": "MuOh05pJID0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343e3a9dbb7a90e1dcb5b6",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:15:00.000Z",
      "slot_end": "2024-11-13T05:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1RQXvuQDzdyRtC3YArjUnvZw9pKG8y3WwlKPipk1FNJE",
      "resources_slides": "https://drive.google.com/file/d/1DQwrgn53uSbJQhmgt1Wohxnf2iWAkP3m/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "circle-stark-gpu-acceleration-an-analysis-of-performance-and-implementation",
      "sourceId": "MUG9QP",
      "title": "Circle STARK GPU Acceleration: An Analysis of Performance and Implementation",
      "description": "The session will cover the overview of GPU acceleration for ZK-SNARK/STARK proof systems, focusing on the GPU implementation of Circle STARK. It will address challenges and solutions in implementation, delving into parallelizing computations in provers. We'll compare CPU and GPU performance. Finally, we'll explore future directions for hardware acceleration (ie. FPGA).",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Zero-Knowledge,STARK,gpu,STARK,Zero-Knowledge,ZKP",
      "keywords": "Circle STARK,Stwo,GPU",
      "duration": 1341,
      "language": "en",
      "sources_swarmHash": "aca6bb10e220e0006d3bfb5794638f15a49f13f070b281e3aa6f13a85f48d15b",
      "sources_youtubeId": "7EOjrYnrE8g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673440649dbb7a90e1ffe27c",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:30:00.000Z",
      "slot_end": "2024-11-13T06:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1P0tuKkaODBsM7KxkSGtcEnPO4Ol_WaMBik4Mne_Fr0Y",
      "resources_slides": "https://drive.google.com/file/d/1WZd4UpPYJ-I4MOlEJNXuEvtytQSsubSs/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "erc-3668-on-linea-built-in-trust-minimized-l2-to-l1-data-retrieval",
      "sourceId": "FARJAG",
      "title": "ERC-3668 on Linea: built-in, trust-minimized L2 to L1 data retrieval",
      "description": "ERC-3668 (aka. CCIP-read) enable L1 contracts to access Linea state. No special library need to be integrated, everything is built into the protocol and secured by Linea's zero-knowledge proofs. During this presentation, we will go into the details of how this works, the benefits and use cases you can start building today.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Zero-Knowledge",
      "keywords": "Cross-chain",
      "duration": 945,
      "language": "en",
      "sources_swarmHash": "c2f71910dba4040f55e11efe7077f7d2fe0251f2e870654772778d56bdc18b30",
      "sources_youtubeId": "7Ov7JcxHE-s",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734478f9dbb7a90e1759be2",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:30:00.000Z",
      "slot_end": "2024-11-13T06:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1caoeThC6_UrDRFE2PQIcbIczFePi9G_E72Ud7YuJejc",
      "resources_slides": "https://drive.google.com/file/d/13gD9b7yT2mqWHnNDJlt_p5QNVTKsX-mP/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "light-client-support-in-prysm",
      "sourceId": "9PC3EY",
      "title": "Light Client Support in Prysm",
      "description": "Showcasing the addition of Light Client server support to the Prysm consensus client.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "EPF,Consensus,Light Clients",
      "keywords": "Prysm",
      "duration": 724,
      "language": "en",
      "sources_swarmHash": "87a24b2a455f641db3325c1e58fa4d6f7aa331c6e585835bb8a48f1ac01635a0",
      "sources_youtubeId": "ZAU0kJJut54",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67343f059dbb7a90e1ea95b6",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:30:00.000Z",
      "slot_end": "2024-11-13T05:45:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1o1_9VdMiq5Uf_dyQTPf5R3mVbxhL4d0QI33ZiZNm28Q",
      "resources_slides": "https://drive.google.com/file/d/1BTsXmJDB0RxfhgdxdqKtZ0LtK0A_tOuo/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "mud-how-we-built-an-evm-application-framework-from-the-ground-up",
      "sourceId": "883QBY",
      "title": "MUD - How we built an EVM application framework from the ground up",
      "description": "We wanted to accomplish one simple task: put a game—with all its data and logic—on a blockchain. What followed were countless technical challenges, years of efforts, and learnings that are applicable to anyone building complex onchain apps.\r\n\r\nHow should data be structured? How can complex world state stay up-to-date on the client? How do we allow multiple teams to build on one single world, without it all breaking apart? Join us as we share the pitfalls and learnings.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Frameworks,Gaming,Autonomous World,onchain,Autonomous World,DevEx,Frameworks",
      "keywords": "Onchain,Games",
      "duration": 1167,
      "language": "en",
      "sources_swarmHash": "513b226e691f7a5f2159693e0c0d7ba73d8e00f6db1ff632d1fb557fd67fcb9f",
      "sources_youtubeId": "w02aI1S7gcc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734401e9dbb7a90e1fc1c7a",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:30:00.000Z",
      "slot_end": "2024-11-13T06:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/13IffrHXnDmcykkm_fptRD_pUCl4g2eRLtXlWD6o8UUE",
      "resources_slides": "https://drive.google.com/file/d/1XCfl0PQIMidppS580U-TfI-GCZxVKHtJ/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "libp2p-implementation-in-c-and-prysm",
      "sourceId": "F7UVJP",
      "title": "Libp2p implementation in C# and Prysm",
      "description": "Joint talk will discuss a project which split to work on the C# implementation of Nethermind's libp2p, where we implemented the TLS protocol, upgraded the Noise protocol, and added the Perf protocol. Although the first version of the C# implementation isn’t released yet, it is integrated with Gnosis Chain’s Shutter node. Second part of the talk focuses on new goland libp2p library for Prysm CL client",
      "track": "[CLS] EPF Day",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Network State",
      "keywords": "Libp2p,Networking,P2P",
      "duration": 1015,
      "language": "en",
      "sources_swarmHash": "688bcf8ed0f257f588315fb5d07e0d1d44de2a4780efe358e803d9c20c0c708e",
      "sources_youtubeId": "zAlxmEONKGE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673443a29dbb7a90e136e2e7",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:45:00.000Z",
      "slot_end": "2024-11-13T06:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/114N7ZFoxay5HlB8T-LpY8iCOTKX70658i91W43eXDpc",
      "resources_slides": "https://drive.google.com/file/d/1MuNTL7JM66f3ktnqh8vGUCq2b3NiGW3m/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "a-toolbox-for-monitoring-the-health-of-the-ethereum-p2p-network",
      "sourceId": "E3GEEN",
      "title": "A toolbox for monitoring the health of the Ethereum P2P Network",
      "description": "Monitoring the P2P layer of Web 3.0 networks is extremely critical for the healthy operation of the system, but has been overlooked for quite a while. ProbeLab has developed a suite of open source tools to monitor closely the journey of blocks and messages in the Ethereum network, as well as the operational details of Ethereum nodes. The target is to assess the health of the network as a whole.In this workshop we’ll walk through and demo the details to enable others to benefit from our tooling.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Tooling,Network State,gossipsub,Network State,Protocol Design,Tooling",
      "keywords": "P2P,DHT,Gossipsub",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "28de20a4a6013f71cac622046aa24c6d4d7b3e298993fbdeddfac4fd84ca0ca7",
      "sources_youtubeId": "utTJs7G11WY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T05:50:00.000Z",
      "slot_end": "2024-11-13T06:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1l0VK-WM6V4AzPauAgjULwr3ct22celTYmuy97iN9sPw",
      "resources_slides": "https://drive.google.com/file/d/1QgFNMwJs-VdRx5UQSg_8J9nkMdF7fsze/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "building-a-future-proof-l2",
      "sourceId": "KAADMF",
      "title": "Building a future-proof L2",
      "description": "I will present some of the considerations, mechanisms, technical and algorithmic breakthroughs that are required to build a future-proof L2, with Post-quantum cryptography (PQC) in mind, to enable mass adoption of blockchain technology. E.g.: Full L2 that runs atop multiple L1s, next-generation proving, innovative use cases, and more.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Zk Rollups,Use Cases,starknet,next-gen,zkproofs,Layer 2s,Use Cases,Zk Rollups",
      "keywords": "Bitcoin,next-generation zkProofs",
      "duration": 1528,
      "language": "en",
      "sources_swarmHash": "e7726d6bfe472ba409a1dc036b957042b90e4efd19ee458e57bd638dd8dfbc1e",
      "sources_youtubeId": "bO4BI-OOfHg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67344eb19dbb7a90e1a960e0",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:00:00.000Z",
      "slot_end": "2024-11-13T06:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/14QG3jNVI1Dkw_-jw6BLW28LVEfOTx51EPqm3Dradf3o",
      "resources_slides": "https://drive.google.com/file/d/1Bahj1d2OatLylbD4s0WeQ1xrBmlTSuQe/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "eip-7732-enshrined-proposer-builder-separation",
      "sourceId": "TKBF9R",
      "title": "[EIP-7732] enshrined Proposer Builder Separation",
      "description": "ePBS implementation in Prysm and Nimbus, fundamentally aimed at solving about solving trust issues. We're gonna discuss the block-auction, slot-auction and the approach proposed by Francesco during the cohort. Some technical challenges and problems that we came across like separating EL and CL block, PTC committee etc.",
      "track": "[CLS] EPF Day",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Censorship Resistance,Consensus,Core Protocol,PBS",
      "keywords": "ePBS,EIP-7732",
      "duration": 751,
      "language": "en",
      "sources_swarmHash": "e326ff4a5c85f7cfdcbb4ccebbd229632df88de258c3b4daa59aac0bad48ad30",
      "sources_youtubeId": "r-ku7h6bC8M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734486c9dbb7a90e1866fa0",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:00:00.000Z",
      "slot_end": "2024-11-13T06:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1XP6W6A3-lCz0aeamZyGShkdG9rB-Lpip1Ceasz22olM",
      "resources_slides": "https://drive.google.com/file/d/1xx07mqLAHk-KjUlkJzkvaImI4kX2luiq/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "end-to-end-internet-games",
      "sourceId": "EZ9T33",
      "title": "End-to-end internet games",
      "description": "For the past 1.5 years, I've been building fully onchain games–games where the entire state is onchain for some reason (have launched 7!). \r\n\r\nThere is lots of cryptographic data floating around the internet. New primitives are allowing all this data to be interoperable with each other... and even verifiable on-chain. \r\n\r\nI'll discuss some of this tech (tls notary, app attest, zkml, etc.) and discuss what new wild games we can build with them.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,Mechanism design,Mobile",
      "keywords": "ZK,Programmable cryptography,onchain games",
      "duration": 1486,
      "language": "en",
      "sources_swarmHash": "46a058b4803524f956646260cd45e173c58e0e34f6aa603b9af7f661f00a18ff",
      "sources_youtubeId": "4Cblt2gOIas",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673447d19dbb7a90e17a1d59",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:00:00.000Z",
      "slot_end": "2024-11-13T06:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1SKERFupONxE6JOQvDC21CI1lz62VYcj5ZdZOGlXcWOg",
      "resources_slides": "https://drive.google.com/file/d/1GepqHXoeuAw6stm_ph4M35l4Vu5Wtr9h/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "leveraging-high-performance-computing-for-efficient-stark-provers",
      "sourceId": "ZGXYDF",
      "title": "Leveraging High-Performance Computing for Efficient STARK Provers",
      "description": "Zero-Knowledge Proof (ZKP) protocols' applicability hinges on the prover's ability to efficiently generate proofs. This talk explores the computational aspects affecting ZKP performance, specifically focusing on STARK provers. We will analyze performance across high-performance and standard computing architectures and interpret results by examining key workload characteristics. From this understanding, we can project ZKP capabilities in future scenarios.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZK-EVMs,ZKP,STARK,optimization,STARK,ZK-EVMs,ZKP",
      "keywords": "computing performance,optimization,",
      "duration": 1650,
      "language": "en",
      "sources_swarmHash": "466210110f16c732a0fb6c39294ffcee236d6138e2103181a425734ea235f63c",
      "sources_youtubeId": "-4Sz8etUrig",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673448629dbb7a90e1853564",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673448629dbb7a90e1853564.vtt",
      "transcript_text": " Bona nit a tothom, gràcies per ser-hi. Soc el Ricard Borré, com va dir, gràcies per la presentació. Jo treballo a Polygon. Bàsicament treballo en desenvolupar i optimitzar la tecnologia de GKE. Aquesta presentació es tracta de l'estabilització de la computació de alta performance per a proveïdors d'estalvis eficients. La presentació es divideix en dues parts. Primer, parlarem del present en termes de producte, que és la ZKEVM. I explicaré una mica les lesions aportades en el desenvolupament de la ZKEVM. I després, lessons learned on the development of the ZK-EVM. And then I will talk about the future, which is what we are working now, which is the ZK-VM, so a general proposed proving system based on STARKs as well. Okay, let's go for the ZK-EVM. The ZK-EVM was released two years ago, and its proof is based on a Stark method and we have a Stark and then we have aggregation to have a recursion process to aggregate proofs and then with a certain frequency we will send the proofs to L1. The only thing that we need to understand about a Stark for this talk is that it basically has two stages, or two phases. In the first phase, we will compute the trace, and then in the second phase, we'll take the Frey polynomial, which is kind of a summary of all the trace in a single polynomial, and we will prove the proximity to a low-degree polynomial. en una única polinomia i provem la proximitat a una polinomia de baix degrés. La fase de comit es fa en diferents estages. Això vol dir que la trànsit es fira en diferents estages i això és perquè necessitem randomitat per a alguns dels estages. Necessitem uns números randoms per avaluar algunes de les polinomies de la trànsit i aquesta randomitat es rebut i aquest ràndom és obtenit des dels estats previus. Les raons de la polinomia avaluades en els estats previus són adjuntades al transcripte i després tenim ràndom per al següent estat que hem d'avaluar. En la aritmetització de la ZKEVM tenim 1.300, més o menys, polinomials. I després, el ràndom de la polinomia és el que hem de fer. to evaluate. Okay, in our arithmetization of the ZK-EBM we have 1300 more or less polynomials and then the number of rows of the trace which is something that we can fix, it's a parameter of the prover let's say, it's 2 to the 25, so it's about 32 million. This is quite a lot and it ends up posing a requirement of 800 gigabytes of memory. I will talk more about this. So, why we use such a big number of rows on our trace? Basically, because you have a monolithic approach on the remethization. This means that we have a static capacity, so the capacity of our trace is determined by the number of rows. And so we fix a big number because, basically, we want to be able to handle any transaction that comes to the network. That's why for the most heavy-loaded transactions, the ones that will need more steps, we need to have enough space to handle them. This approach has advantages, but also some disadvantages. A clear one is that when we have to close a batch because we don't have more room for new transactions, because some of the state machines of the trace are filled, then there will be some spare cells on the trace perquè alguns dels machines de la trànsit són llocs, llavors hi haurà uns cels en la trànsit que hem de proveir però no estan llocs amb valors reals venent dels inputs. Així que tenim alguns efectes de padding, però també tenim uns avantatges, per exemple, que tenim un lloc constant que podem optimitzar molt bé, podem al·locar buffers on on advance etc so it also has some advantages related to performance now let's talk about timings a proof of this huge trace for the zkabm takes about six minutes and the cost with the i el cost amb la instància GCP que suporta l'AVX 512 és només 0,46 dòlars. Normalitzat pel nombre de transaccions, transaccions d'Ethereum que podem fitxar en un batx, és d'uns 0,2 mil·lidòlars. Aquest serà l'abast per transacció. És un abast molt baix, que és negligible en termes de la gran esquema de coses, en termes de cost que adquireix la transacció. A més, OKEx ha fet un treball molt interessant, que ha portat el proveïdor a GPUs, i això ha reduït la latència a tres minuts. I fins i tot reduint el cost. És una situació perfecta on reduïm la latència al cost utilitzant GPUs. Aquests són els números donats per ells i al final acabem tenint 0,1 mili dòlars per adreçat al cost de la transacció. Així que és molt bo. La velocitat obtenida des de la CPU a la GPU, si considerem la prova de l'endemà a l'endemà, és 2X. Aquí hi ha una de les limitacions de la prova. Com que tenim un trastament tan gran, a l'endemà, transferem molta data entre el host i el dispositiu durant l'execució dels kernels. I serem limitats pel PCI Express. A més, si mirem els costats reals, el que és important, són molt menys. No és una cosa que és molt rellevant en el context actual. Si mirem una mica més enllà i mirem la distribució dels kernels quan fem una prova, és una situació perfecta o bona perquè tenim el 95% de la temps de la prova és només concentrat en quatre kernels, especialment en dos, que són l'entitat i la mercantilització, que prenen el 70% del temps. I després tenim expressions que són una combinació línia de polinomials, i després l'executor, que és la primera fase de la avaluació de la tracció. Així que és la part de la avaluació de la tracció que depèn dels inputs. Aquesta és l'execució inicial dels inputs. Bàsicament, hem de focussar-nos en aquests quatre kernels, bàsicament en dos, i això és el que vam fer. Molt de feina en això, en el passat, abans de publicar el ZK-EBM. The first thing you have to consider when you want to optimize a code is to understand where are the boundaries for this code. Basically, you have to do a roofline analysis. Basically, the roofline analysis shows you where is the limit depending on the arithmetic intensity of your kernel. If your kernel does a lot of operations per each field element that you upload to the cache of the CPU, it means that it will be bounded by the CPU, by the computing part of the process, and if you have a lower emitting intensity, you are becoming bounded by the memory transfers from the main memory to the cache of the CPU. In our case, we are basically bounded by the memory transfers from the from the main memory to the cache of the CPU. In our case, we are basically bounded by the memory transfers for all the kernels except the hash. And this makes sense because the hash does a lot of arithmetic operations per each field element involved in the hashing. Okay, so we focused a lot on optimizing the optimizing the entity. We did a lot of basically the focus here was to minimize the L3 misses. Focos a l'optimització de l'entitat. Focs en la minimització de les emisses de l'L3. Al final, el resultat és que la performance que obtenim és de fer l'entitat en 100 polinomials de 2.23, és a 1.9 segons. Aquesta és més o menys la referència. I després, la mercantilització, és més o menys la referència. I després, la mercatització, és clar, és sobre, en la CPU, és una altra cosa, en la CPU és sobre vectorització, per tant, prenent avantatge dels registres vectorals per fer, en cada cop, 4 o 8 operacions al mateix temps. I anem a una performance amb Goldilocks 1, no Goldilocks 2, de 1 milió de hashes per segon. Així que aquest és el resum del que vam fer més o menys dos anys abans de llançar el ZKVM. A un punt vam arribar a la conclusió que mantenir i millorar el proveïdor no tindria cap impacte irrelevant en la performance que vam fer a l'aplicació. Aquesta és una bona situació, podem focusar-nos en altres coses, i això és el que fem ara. El projecte en què estem involucrats és el CISC, el CISC-AVM, que és un CISC-AVM generalment proposat. which is a general proposed ZK-EBM. Basically, as others that are in the space, we want to prove any program that is written in a high-level language like Rust, CPP, or others. The motivation of doing this project, it's very meaningful for us because we have acquired a lot of experience, a lot of tooling, a lot of knowledge on running the ZK-EBM, és molt significatiu per a nosaltres perquè hem adquirit molta experiència, molta informació, molt de coneixement en fer el ZK-EBM, que ha estat en Mainnet dos anys i hem patit molt amb les performances. Així podem prendre avantatge de totes aquestes coses que hem après per posar-les en servei d'aquest nou projecte, que és més general i pot ser més impactant en general per a qualsevol usuari. És obert, tu pots xerrar la GitHub, no és tot complet per fer una prova, però és gairebé allà, potser en un o dues setmanes serem capaços de provar qualsevol codi que es trobi escrit a REST. Ara, la major diferència amb aquest nou to prove any code that comes written in Rust. Okay, now, the main difference with this new proving system, so the proving system that we are doing for Zisk, it's different in many ways, but the main difference comes from the remitization, because now we don't have any more static capacity for the trace. We will have a dynamic capacity. This means that we will shape the trace depending on the inputs. And it will be divided into different instances. All this will be explained in the next talk, so I cannot skip the responsibility of explaining this well, but of course I encourage you to stay to understand all the implications of this. On the proving time, basically you will have to prove all the instances that you have generated on the arithmetization. We call them sub-proof. So we have an array of sub-proofs to be proven. And then, of course, we will have an aggregation phase on which we will aggregate all the proofs resulting from for all the instances that will result on a single proof. Okay? en què agregarem totes les proves que resulten en una sola prova. Pensem en les implicacions computacionals de tot això. De fet, en l'eficiència de la CPU, no es pot obtenir res d'aquest. Es pot obtenir una hora pitjor. Per què? Bàsicament, el cost d one proof is proportional to the area, and this stays true even when you go to small instances. So you have proportional cost. You have divided the main trace into different sub-traces, but the sum of the areas is more or less the same. Of course, we have eliminated the padding effects. This is helpful, and it will reduce a little bit the overall area, la suma d'àrees és més o menys la mateixa. És clar que hem eliminat els efectes de padding, que això és útil, i això reduirà una mica l'àrea overall, però no és una gran diferència. I tenim altres costos de degrada. A l'endemà, estem fent la prova una mica més costosa. Aleshores, amb aquesta nova granularitat, hi ha un munt d'oportunitats. However, with this new granularity, there are a bunch of opportunities. And the most important one is that we can take advantage of accelerators without having this limitation of being to move data back and forth from the host to the GPU, with this PCI Express limitation. So this is one very important improvement here. We will be able to exploit GPUs much more efficiently. We have other ideas, like vectorizing the execution of Starks. So we can kind of run eight Starks at the same time with trying to do every operation in a vector register containing elements of different Starks. These are some kind of ideas that we are considering. But the most appealing idea for us is going to distributed computing. Basically, the idea of distributed computing però la més apropiada idea per a nosaltres és la de distribució de computació. Bàsicament, la idea de distribució de computació és que vols fer-ne la teva prova sense fer-ne una sola instància al cloud, sinó diverses instàncies, i amb això intentes escalar més fort la teva aplicació. Vols produir una latència, que és un dels focussos de aquest projecte. Parlem de la distribuïda proveïda. Primer de tot, som molt feliços, tenim una avantatge molt important en aquest projecte, és que tenim accés a un supercomputador, que es diu Selenius, és part de la organització de la CCEURO, una organització en els Neus. És part de l'EUROHPC, però és una divisió en els Neus, diguem-ne. Ens van donar recursos per fer les nostres proves. Vam fer un projecte de recerca on volíem trobar els límits de la lluita per la Stark. Vam veure on ens podíem anar, com anar en la reducció de la latència. Aquest és el nostre objectiu. Aquí tenim tres línies. Tenim centenars de milions de CPUs que podem llançar juntes en una prova. No anirem a aquest nivell, però la hardware és allà. I també tenim centenars de GPUs que podem llançar per fer una prova. A més a més, volem targetar la performance de màxima edat, però tot el que fem pot ser fer-ho a la cloud també. Volem reproduir a la escala menys, volem reproduir els resultats a la cloud. I això és molt important perquè la producció no es farà en un supercomputador, serà fet en un sistema de la cloud, com els que utilitzem avui. Parlem de la primera part, de fer la traça. La computació de la winters és la part de fer la traça que ve des dels inputs. Com ho farem en un ambient distribut? La manera més naïfa de fer-ho seria... Bé, tinc diversos processos. Parlo de processos i darrere. Un procés. Dos processos no comparteixen memòria, doncs són processos distribuïts a través de la xarxa i després els processos espanten darrere. Tu pots assignar un procés, el rol màster, i dir que computaràs el trac, i després distribuiràs the result into equal parts, and then send to the others, okay? This is a functional approach, but it's not scalable at all. So we are not parallelizing anything. We have a lot of memory requirements in the master process, and we are also adding a high communication overhead. So it's not a serious approach. Now, let's try to get rid of the communication costs. Let's do the computation of the witness, redundant in every process. Okay, that's fine. We have to get rid of the communication costs, but we are not accelerating because it's redundant, and we have high memory requirements in all the processes perquè és redundant, i tenim requeriments de memòria en tots els processos, perquè tots els requeriments han de guardar tots els trets. No encara hi són. La segona estratègia que hem desenvolupat és minimitzar els trets que generem. Hem dividit la generació de trets en dues estades. En la primera estada, només generem el que anomenem el tre minimum, divided the trace generation in kind of two stages. In the first stage, we only generate what we call the minimum trace, which is the information that you require to then, in a second stage, restart the evaluation of the trace at any point. So in a second stage, we can, in parallel, recompute the trace and divide this computation. So we have this first stage of computing the minimum trace, which is very fast. And then we can, from the information generated in this process, we can distribute the trace between processes. I mean, each process will have had run this same... This first step of generating the minimum trace has been run by all the processes. aquest primer pas de generació de la trama mínima ha estat fet amb tots els processos, tots els processos saben quin és el lloc actual que s'ha de distribuir, i tots els processos prenen una part i computeixen aquesta part, però només aquesta part. Això escala molt millor. Ok, mirem els resultats. El test que estic utilitzant aquí i en els següents slides Let's look at the results. The test that I am using here and in the next slides, it's a program that executes 10,000 SHA-256 hash operations. The decision that we are generating is not complete, because as I said, this project is under development. At the actual moment, at that point, we're generating instances for the main and the binaries and some multiplicity tables. Aquest projecte està en desenvolupament. En el moment actual, a aquest punt, estem generant instances per a la majoria i les binàries i algunes tables de multiplicitat, però és prou per entendre els resultats i el potencial d'això. Ara, si considerem com a referència l'estona d'emulació, que és executar el programa sense generar cap trac, encara que no sigui el tracció mínima, el temps és 0,6 segons per aquest programa. Això vol dir que estem fent 80 MHz, és la nostra velocitat. L'abans de generar les traces ranya amb un instant, amb un nodi de computació, és al 100%, doncs estem doblement aquest temps, a dalt del 37% nodi de computació, és d'uns 100%, doncs estem doblement aquest temps, a 37% en instances de computació. Així que anem molt endavant. La escalaritat no és gaire bona perquè estem ja molt fàcils en l'execució d'aquestes... de la traça, perquè ho estem dividint en threads en un nodi de computació. Però, a la fi, because we are dividing it into threads on a single node. But at the end, really this overhead is very acceptable, okay? Only 37%. Okay, I will go a little bit fast because I am running out of time. Then let's talk about the distributed, how we manage the distribution of the sub-proofs. So we have to solve the proof for all these instances that have been generated. Here we have a synchronization point because all these instances share the transcript. This means that this object to get the randomness for the next stage is shared between all the instances. And this poses some synchronization between the execution of all these instances. What we do is a communication stage where we share the roots that we have to put to that transcript and then we get the same randomness in each of the proofs. Let's see the results. Scalability of this phase, generating the sub-proofs, it's 100% efficiency. We go from 70 seconds down to 3.3 seconds. It's a little bit suspicious that we have even superlinality. So if you see the efficiency, which is the line, it goes over 100% at some points. So this is something that is not, let's say, which is the line, it goes over 100% at some points. So this is something that is not, let's say, a little bit suspicious. The problem here is that the distributed prover even runs better in a single node. Because you can fix better the granularity of the threads that you are using for the subproves. Then if we compare the base point is the distributed proidor en un únic nodi computador, obtenim una cosa més significativa. Així obtenim una eficiència paral·lela de 80%. Això és bo. Anem a la distribució. La recursió. Tindrem requeriments de comunicació quan generarem el 3, recursion, we'll have some communication requirements when we generate the tree, because the subproofs that we are aggregating can be hosted in different processes. We have to transfer to the process that will generate the aggregation. And if we look at the parallel efficiency, really, it's not so appealing, right? It's 44%. Okay. But here, the problem that we have is that when we are going down to the tree, at some point we have less aggregations to do than processes. So there are some resources that are being not used at all. So if we just release these processes to do other things, the real parallel efficiency related to the hardware that we are consuming is 88%. So it's, again, a very acceptable parallel performance. Now, if we look at the complete overall situation for all the proof, including the win-loss computation, the sub-proofs and aggregation, the parallel efficiency is 78%. We go from 125 seconds down to 12.2 secondsons. La recursió es pren la major part del temps ara. És el 65% de la prova. Així hem de focusar per optimitzar la performance i menjar més la latència. I la conclusió més important és que, com que tenim una bona eficiència paral·lela, és el 70%, podem augmentar la latència per 10X. Hem augmentat la latència de la prova it's 70%, we can increase the latency by 10x, right? We have increased the latency of the proof by 10x, while the costs have only increased 27%. So this is a very good situation to have because basically it means that you can really decrease the latency without additional costs. So that's why we... And this can be reproduced in the cloud because we are not we don't rely on a very high performance network because the transferring of data that we are doing is really minimal. We are transferring proofs or we are transferring routes which is minimal information. So just I am closing with this. So we have, we can decrease, reduce the latency at a very low Estic acabant amb això. Podem reduir la latència a un increment de cost molt baix. I, com a millors millors, ens centrem en la recursió, en alguns algoritmes, com la estira i la multifrida. És clar que aquí no hi ha la GPU. Amb la GPU podem extreure-la encara més, perquè la nostra performance, la nostra arquitectura, pot suport accelerate more, because our performance, our architecture can support GPUs very well, because the instances are smaller. We can even reduce more the performance with the GPUs. And also, run on the cloud infrastructure is our next step. OK, sorry for extending a little bit. And I can take any question. Thank you, Ricard. Yeah, a round of applause. So we'll have a few questions. Reminder that great for everyone who submitted. The QR code is always in the presentation. And you can also vote them. So we're going to start, if it's all right for you, with the first one. Could you give an intuition of how do you generate a minimum trace that can then be used to restart the trace at any point? What's the data structure like? Yeah, the minimum trace only has, it's kind of evaluating the columns that you require to then be able to extend the trace. And this basically is two columns for the main machine, which is the ones that load data from the memory. It's kind of a A and B register. So it's only two columns. From that, the rest of columns that compose the main trace can be generated from this information at a given checkpoint. So that's basically it. So it's all waiting just a subset of two columns. Great. So the next one voted, can we say SyscVM basically trades more aggregation costs with better concurrency architecture? So basically, yeah. Yeah, that is one way to see it. Although the aggregation costs now are huge, and we have to pay for this aggregation, of course, to have this granularity that we are interested in, but we want to reduce as much as possible it by improving the algorithms. For instance, Flunky2 has a very fast aggregation. We can do something similar. We have the approach that reduces the number of queries that you have to verify on each aggregation step. So we have a package of things that we can do to improve this part. Also, we have to really think which is the granularity that is more suited for our hardware. So maybe we can generate less instances if we can really exploit the hardware well. And then the recursion will have less levels Okay, so a lot of investigation. It's it's coming on this part because as you saw it's the one that It's more relevant at this point. Okay, great great great The next one is can Sisk run programs written in lower level circuit languages will they be faster than rust will the gap ever disappear? the top one. Well, the purpose of this is not this. The purpose of this is basically generate the inputs with a high level language. Basically Rust, C++, or any other language. So this is mainly the design point of this. Of course, you may be able to generate some arithmetization and then use the prover as a component that can be self-contained, and you may be able to use it. But the main purpose is just you generate your code in a high-level language. You don't care about anything. you will get the proof of the execution of this program from the inputs which would be the inputs of the program, the outputs of the program, and the ELF of the compilation of the program with RISC-V or any other architectures that we are also considering like Wasm or LLVM. Great, great, great. And I think we got chance with one more quick. Just trying to be futuristic, can you give insights of what could probably be done in order to improve the CKE VM or VM prover in one or two orders of magnitude? For me, it's clear that we have to go, the distribution vector is a path, or a vector of improvement that we have to take, because if you rely on more specialized and complicated hardware on a single, but relying on a single instance, on a single node, you won't accelerate. But we're going to take advantage of the most sophisticated node available, the most sophisticated hardware available and compose it and take it two, three, four, whatever number can fit with a good parallel efficiency. Because we want to control all the time the costs of the proof. If you have parallel efficiency, it means the cost doesn't grow. So that's the trade-off that we really have to take into account. Amazing. Please give a big welcome to AlphaPlus.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:00:00.000Z",
      "slot_end": "2024-11-13T06:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1J3KMOMYAXjSesFqZthBz2neGQcOt3Ui_KyKgToVj0Z0",
      "resources_slides": "https://drive.google.com/file/d/1YdxtiNk4sgXtJUUMEEdBzY-r1BjMQ8vn/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "live-music-open-jam-or-something-in-between",
      "sourceId": "FVHR9Y",
      "title": "Live Music, Open Jam, Or Something In Between",
      "description": "This will be an open, emergent, co-created format where we're inviting everyone to make music together.",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:00:00.000Z",
      "slot_end": "2024-11-13T07:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1CYvsKADAZ5-gmioFl_lFIeEXHIyeSBbn2GOtenPyFq4",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "building-for-the-next-billion-product-validation-tactics",
      "sourceId": "BUEYDW",
      "title": "Building for the next billion: product validation tactics",
      "description": "The term “next billion users” gets bandied about in our industry. But change in the way we approach product development has to happen to meet these users across the chasm. \r\n\r\nLearn of some specific techniques for assessing product-market-fit and positioning, and how to design the ultimate test of your solution without wasting thousands of hours of development time.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "10x,Product-market fit,User Experience,experiments,10x,Product-market fit,User Experience",
      "keywords": "User research,Mass adoption,Crossing the chasm,experiments",
      "duration": 472,
      "language": "en",
      "sources_swarmHash": "0ca16abb7cbeb4f382d594bd8d286e06a8e158abd2f4d595f51b43383de3fce7",
      "sources_youtubeId": "3QR6yZ076s0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:10:00.000Z",
      "slot_end": "2024-11-13T06:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1IXXA9ruxzg_Nt5FLQ1mscpwiYUMY8eIgfJ5XX04lyi8",
      "resources_slides": "https://drive.google.com/file/d/1xtNVNWsvQ0sU5LPA3skEVMI9ta-bp0Ui/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "buidlguidl-build-your-first-five-apps-on-ethereum",
      "sourceId": "3BCKLQ",
      "title": "BuidlGuidl - Build Your first five apps on Ethereum",
      "description": "**Afternoon Workshop (1:15 - 2:15 pm)**\r\n\r\nAccelerate your Ethereum skills in this rapid-fire workshop! You’ll build five dApp prototypes on Ethereum using Scaffold-ETH 2 and its powerful extension starter kits\r\n\r\nWe’ll cover key concepts in smart contracts, front-end dApp development, and how Scaffold-ETH 2 streamlines the process with ready to use state-of-the-art tools. \r\n\r\nThis session is ideal for beginners looking to gain hands-on experience quickly.",
      "track": "[CLS] Learn How To Build On Ethereum & Capture the Flag Game, by BuidlGuidl",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Live Coding,Open Source Software,Public good",
      "keywords": "",
      "duration": 4305,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "hSQm26fSz68",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673456529dbb7a90e105884c",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673456529dbb7a90e105884c.vtt",
      "transcript_text": " You are a builder, go to speedrun ethereum to learn how to build on ethereum. This will help you with your first five apps. We're going to speedrun the speedrun today. I've written a bunch of code. This will probably be a little less code writing and more just like going through the tutorials and talking about some of the aha moments you have along the way. Just to like zoom all the way out, we are the build guild. We are focused on developer education and tooling. Our main tool of choice is scaffold ETH and we use that to build a bunch of things. But that is also the heart of Speedrun Ethereum, which is our curriculum. And we're going around the world teaching people how to build on Ethereum, and anybody can just go to Speedrun Ethereum and get started today. And we're going to kind of just speedrun the speedrun today. I'm going to go through the challenges and just kind of talk through the kind of the aha moments you'll have. But hopefully you can go home and speed run Ethereum on your own time and really kind of go through this at your own pace. But I'm going to go through it quickly just to kind of cover like, oh, here's the stuff to look out for, basically. Let's do just real quick, if you guys could give me a show of hands, how many of you have deployed a contract on main net before? Oh, yeah, we have five or six main net deploys. How many of you have deployed on an L2? Actually less, I think, right? Less people? Okay, we're going to change that today for sure. We have to get you ready for that. I deployed one to base earlier and someone attacked it and stole my money. It was pretty fun. How many of you are software developers? Okay. Those. Look at those. I would say that was almost all the hands. I just got goosebumps. I'm excited. This is awesome. I'm going to do the speed run. Probably 20, 30 minutes, I'm going to speed run the speed run, and then I'm going to hand it off to Etta and she's going to do I think like an NFT build, right? Cool. Awesome. Okay.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:15:00.000Z",
      "slot_end": "2024-11-13T07:30:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Y7SoCe3qErMg50qHQ7I69ZuGqYs3OYa620D1qj0qI8A",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "launching-projects-out-of-the-global-majority",
      "sourceId": "7VZ8WH",
      "title": "Launching Projects out of the Global Majority",
      "description": "Launching projects has been an almost entirely US driven exercise, with a handful of expectations out of Europe and Asia - and basically 0 examples out of LATAM or Africa. This talk aims to shed light on why this is a reality and how we as an ecosystem can support more experimentation and launches out of the global majority. Talking through cryptoeconomics, investors, narrative and positioning of previous high impact project launches.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Business",
      "featured": false,
      "doNotRecord": true,
      "tags": "DAO,Sufficient decentralization,Best Practices,macro/micro economics,global,Best Practices,DAO,macro/micro economics,Sufficient decentralization",
      "keywords": "Global",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:20:00.000Z",
      "slot_end": "2024-11-13T06:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1BZ-1nzUuvITdZkK8Kxj9N_dkxHmkmlG75RJ7u4tbtAc",
      "resources_slides": "https://drive.google.com/file/d/1M8U1t40smifJRjqZk0BVPIuD-sy4IRg-/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "beyond-the-surface-the-hidden-benefits-of-client-diversity",
      "sourceId": "ERVNRG",
      "title": "Beyond the Surface: The Hidden Benefits of Client Diversity",
      "description": "When people discuss client diversity, they often focus on technical benefits like resilience to DoS attacks or preventing catastrophic errors like finalizing an invalid chain. But there's so much more to it! Client diversity not only spreads the responsibility of maintaining the blockchain across multiple teams but also brings fresh ideas and perspectives into the mix. In this talk, I aim to explore all the hidden benefits of embracing client diversity.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Staking,Home staking,diversity,client,Core Protocol,Home staking,Staking",
      "keywords": "client,diversity",
      "duration": 1405,
      "language": "en",
      "sources_swarmHash": "dc446cb5399350518a86b8c030d50ad9f3e1bb55fb912558696eff8b3d45c985",
      "sources_youtubeId": "MY8LFVqBOyA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67344e6d9dbb7a90e1a3d964",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:30:00.000Z",
      "slot_end": "2024-11-13T07:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1YCHEohvVe_Js1rQhdgpyK_NLyxE88glwaYOMJmSs5aQ",
      "resources_slides": "https://drive.google.com/file/d/1Dq1wJ6i383jlHxnKH7CRIecj68kKlW7P/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "how-to-onboard-22-million-users-overnight-using-non-conventional-cryptography",
      "sourceId": "SDPVVF",
      "title": "How to onboard 22 million users overnight using non-conventional cryptography",
      "description": "Since 2004, the Mexican tax administration started to issue digital identity certificates that linked government IDs to sovereign private keys. These has facilitated the electronic invoicing system that is designed around a public key infrastructure maintained by the central bank.\r\n\r\nThis infrastructure has provided with private keys to over 22 million people. We're onboarding all of those using Account Abstraction in a friendly-manner.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,Cryptography,Account Abstraction,pki,Account Abstraction,Cryptography,Identity",
      "keywords": "ERC-4337,RSA,PKI",
      "duration": 495,
      "language": "en",
      "sources_swarmHash": "27f0ff51b8cf2bd6235c4f7c336e2d44d2515212562c231633e54fcb571a19f5",
      "sources_youtubeId": "DKJYpdXsOwQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:30:00.000Z",
      "slot_end": "2024-11-13T06:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/131bdLWEGmE-yZLMUwmeE98y-D2mP5uniqwKdaak6J1c",
      "resources_slides": "https://drive.google.com/file/d/1Arr2Kme_G-7vSGBTo2fcaraKJ-x-zs4k/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "realizing-the-rollup-centric-roadmap-with-rollup-boost",
      "sourceId": "YRTHKH",
      "title": "Realizing the Rollup Centric Roadmap with Rollup-Boost",
      "description": "L2s are the future, but they're also the past. At this point it's clear that your phone is most likely an L6. Let's examine the feedback loops between L1, L2, and beyond and form community standards around multiprovers, distributed block building, inclusion guarantees and more that feed back into L1.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Protocol Design,Scalability,Appchains,Decentralization,User Experience,MEV,pre-confirmations,Appchains,Architecture,Decentralization,MEV,Protocol Design,Scalability,User Experience",
      "keywords": "Preconfirmations",
      "duration": 1514,
      "language": "en",
      "sources_swarmHash": "1c7523d53da2a837574c9984cb490286cc347f860d0cd7fa94d06b7d23bd592a",
      "sources_youtubeId": "IYZiYFzIzKc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673980981b0f83434d1be72e",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673980981b0f83434d1be72e.vtt",
      "transcript_text": " Tanya Cushman Reviewer Reviewer Hello, hello. Awesome, wow. Cool to see so many people turn out to discuss the Rollup-centric roadmap. It's a pretty full house, so I'm going to have to ask everyone to really just squish in. I think there's a lot of people standing in the back for anyone watching the live stream. Cool. Well, let's get into it. I'm Dan, also DMARS online, and I'll be talking about how we can turbocharge the rollup centric roadmap with Rollup Boost. I'll be using a lot of triple R alliteration. If you see any of those, talk to me afterwards and I'll give you a special prize. Cool. So this will be structured in three parts. I'll go over a little reflection on the rollup centric roadmap. I'll then talk about what a layer two technology tree looks like and then I'll discuss talk about what a Layer 2 technology tree looks like, and then I'll discuss how we can use Rollup Boost to accelerate innovation on the Layer 2 technology tree. Cool. So let's get into the first section. So, yeah, we're four years into the Rollup-centric roadmap. If you're unfamiliar with what that is, the Rollup-centric roadmap was Ethereum's strategic shift towards scaling the network by embracing rollups as a primary solution. If you're unfamiliar with rollups as well, they are a way which we can process transactions off-chain and submit compressed proofs to Ethereum mainnet, which allows us to increase throughput of these off-chain components without necessarily sacrificing the decentralization of the underlying layer. In this vision, Ethereum becomes a more robust and secure settlement layer while most activity happens on these layer two roll-ups and we've already seen that a lot of activity has shifted to a lot of layer two rollups. Yeah, and there's a lot of benefits of this, and we'll be going into a lot of details. But one of the biggest benefits in my opinion, is that we can innovate on a separate layer than the layer one, which doesn't need to go through the entire core dev process or have tons and tons of research and formal analysis before we ever actually start to create a proof of concept or ship something. Because a lot of the times when you are trying to build innovative features, you typically don't discover... There's a lot of unknowns, unknowns you don't discover until you start building. And we've seen many such cases of that with MEV on Ethereum. Yeah, cool. So let's go into it. So we're four years in. How are we doing? Well, we had roughly three goals. One, increase the transaction per second of Ethereum. Two, reduce transaction costs. And three, outsource innovation to the layer two ecosystem. So let's go through each of those. First, how are we doing on TPS? Well, if you look at L2B, we have scaled Ethereum 26x in transactions per second, which is no small feat. That's more than double per year since we've announced the rollup centric roadmap. So that's pretty awesome. We can check that off. Oh, and on the way to scaling transactions per second, we also discovered maybe that's not the best metric, as there's a lot of activity on chains, which aren't just, which can't just be basically categorized via a transaction so a lot of chains simply just have a transfer and that's like very simple it only touches the part of the chain associated with your balance and someone else's balance but when you look at more complex activity it could be touching hundreds or thousands of accounts doing very complicated computation and so we also started to look at GPS as a metric, which is gas per second. And this is a bit small, but if you can see on conduits, rollup.wtf, we have increased the gas per second by almost 55x compared to the Ethereum layer one. So even more great progress. Cool. So we've checked that off. What about transaction costs? They should be lower just because we're also scaling. And yeah, the slide basically speaks for itself. Most of the top roll-ups are less than a cent sometimes to transact on. Cool. And so the last one, which is much more complicated and will be the segue into the rest of this talk, is how are we doing on outsourcing of innovation to the Layer 2 ecosystem? And I think this is really important because with the roll-up-centric roadmap, we essentially said, hey, everyone focusing on Layer 1, you keep doing that, but we are going to push the boundaries and the next version of Ethereum will be built on layer twos. And so it's really important that that actually succeeds. And we as a community lended all of our credibility to these layer twos. And so, you know, we should be checking in on how they're doing. So it's a little bit complicated to look at how innovation is transferring between these layers, but there's roughly three ways we can look at it. It's innovations ported from the layer one to the layer two, innovations from the layer two back to the layer one, and then innovation between the layer twos. And so we'll go through each of them. So yeah, what's been ported from layer one to layer two? Well, basically everything. Most of the popular blockchain clients, layer two blockchain clients, with the exception of some, actually use a minimized, a version of the Ethereum one blockchain clients that has like a very minimum diff to it. And so this means anytime there's an upgrade to the layer one client, layer twos can easily pull it in. You can see this with chains like Optimism where after EIP 1559 went live, they had it very soon after and all other upgrades. But this is not true for all chains. Layer two to layer one. Basically nothing. If you know of any improvements to the layer 1 that have been made, because as a result of research from layer 2s, please let me know. But there really haven't been any EIPs driven through the layer 2 process that have gone into the layer 1. There is one large notable exception that ZK proving and zk technology has like massive massively been accelerated by the fact that layer twos have poured tons of research in usually through the support of a token but again none of that has actually gone live on the layer one yet and then layer two to layer two there's almost nothing we are now starting to see some collaboration. I think I just saw the first cross L2 EIP slash roll-up improvement proposal like yesterday, which is pretty cool, but it was only two roll-ups. So we still have a lot of room to go. Yeah. And I think that just brings me to the sort of the main point of this is we have a lot of flourishing gardens on layer 2 ecosystems. But because of coordination failure and the lack of incentives for these layer 2s to collaborate with each other, we've essentially had coordination failure where we haven't been transferring innovation between each other. And this is my chat GPT rendition of the scenario. We have a lot of awesome spaces to do cool things, but none of them are really helping each other out. Yeah, so that's like a pretty serious concern because this is the future of the ecosystem and the future how can this be the future of the ecosystem if none of these participants are talking to each other? So I think we can start debugging a few things. One is peripheral tooling lock in. And so a lot of layer twos have complained that they can't do anything very customizable or unique because they need to satisfy the existing wallet API standards, the existing indexers. If you spin up a new type of block, now integrating with BlockScout or Etherscan is suddenly much harder. All of the tooling, like the SDKs that front-end developers use to integrate with these chains also become much harder. And on top of that, you know, it's one thing for a layer to modify the tooling to make it work with, like, say, a popular SDK. But then now the maintainers of that SDK need to also maintain this modified version of the SDK. And so the incentives also aren't great there. And what I think is potentially another reason for this is what I call an innovation death loop because of tokens and tribalisms. I think it roughly goes like people launch a token, tokens tie the incentives of groups and individuals together. These shared incentives create the appearance of in-groups and out groups and out groups which you can see on Twitter when we have a roll up teams fighting against each other about very small details and then going separate ways and not creating standards to address these differences. That then leads to all of our tooling being for a specific L2. Yeah, and I think because of this tribal rivalry, innovations are gatekept. And even the extreme of this is it manifests itself in restrictive licenses. Some of the most popular L2s today, you're not even allowed to fork or you are allowed to use them, but you need to give a certain percentage of sequencer fees back to basically the creators of the chain. And I get that incentives are very hard in open source ecosystems, but I think we should hold ourselves to a higher standard because we gave our credibility to these layer twos. Yeah. So I think for the rest of this talk, I'll identify four key areas to help improve Ethereum's innovation on layer twos. And I hope that next year someone or in two years, someone will give a talk addressing one of these areas as I think they're critical. Cool. So let's go into a layer two tech tree. If you're unfamiliar with what a tech tree is, it's, you know, you can see them in strategy games, a tech tree or a research tree is a hierarchical visual representation of the possible sequences or upgrades a player can unlock. You can think of this in civilization games where if you want to reach the iron age, you need to be able to mine iron, you need to be able to melt it, you need to be able to do that at scale. And there's another, so there's an institute called the foresight institute which solely is focused on basically creating these tech trees and then allocating funding to areas of the tech tree that are underdeveloped. And here's an example of one which is very fun of a longevity tech tree where one of the top things is body replacement and in order to to do that, we need head transplants, we need brain tissue replacement, we need organ replacement. And so we set out to do this for Ethereum layer 2 to try and help us get to the golden age of layer 2s. So this is the tech tree. I'm going to go through every single leaf. No, just kidding. There's way too much. If you want to follow along, there is a URL and a QR code you can scan to reach it. If this is very interesting to people, we may end up turning this into a website that you could easily modify and add to. Cool. So let's get into it. So this is the high-level tech tree. There's four subtrees in here. There's funds are safe. This deals with the safety properties of the roll-up in that, you know, basically if the roll-up were to go away, if it was to die, if the software was to go down, we want to make sure that the funds inherit the liveness of the layer one. There's web to scalability. And so the idea here is that we want to approximate the holy grail of everything on one computer. But obviously, we can't put everything on one computer. Don't let anyone let you think that. And yeah, it should be as easy as you going to a cloud provider and hitting the auto scale button. And now you have compute for all of your use cases as long as you keep putting money in or as long as there's someone funding this software, then it should be able to grow and have abundant compute space. A big requirement of that is seamless interoperability. That means you don't need to think about the fact that you're on one chain and your assets are on one chain, but you want to do compute on another. And then another one is also like plentiful compute environment. So we don't want to just have the EVM. There's, you know, by developers, the largest ecosystem in the world is JavaScript. There's like hundreds of millions of JavaScript developers. There's like only a couple million Solidity developers. So if we really want wanna reach the next billion, we need to expand. Cool, so let's get into it. I'm gonna go into the left side of this funds is safe subtree first. And then I'm gonna focus on the censorship resistance portion of liveness. And so under that, under liveness, there is censorship resistance and ledger progression. Ledger progression is just the idea that the blocks keep coming. They keep going. The system doesn't go down. And censorship resistance is really about getting either inclusion guarantees or execution guarantees. And so some stacks actually already have a very basic version of inclusion guarantees where you can force include your transaction on the L2 from the L1. The only problem with that is it's actually at a greater than the L2 or even L1 block time granularity. So you only get the guarantee that your transaction will be included in the, like, you know, like in the epoch of L2 blocks, which could be, you know, tens, hundreds of blocks. Yeah, and so there's a lot of areas here. As well, a very popular topic is multiple concurrent proposers on Ethereum L1 right now, as well as pre-conformations, encrypted mempools. These are all things we need to improve these inclusion and execution guarantees, but we actually have no one working on them on L2s. And specifically, no one is creating shared standards across L2s to enable these. So I think, yeah, I think there's a heroic opportunity here where you can just skip the politics. Just go build multiple concurrent proposers on a layer two. Yeah, you can also experiment with fossil other censorship resistance research proposals, as well as encrypted mempools. Cool, the next part of the tree, the right side of this funds is safe, is under validity. And this one is also very exciting. Yeah, cool. So under validity, we developed a multi-prover. We developed this term, multi-prover. It's been a known term. But the idea is no rollup will likely ever trust a single proving system. And so the idea here is rollups want a proving system so that they can have faster finality. Right now most rollups are optimistic rollups, meaning they get pushed to the L1, and it takes some amount of time, which is very annoying to users, for someone to say, like, there's no fraud here. This state transition was good. You didn't have your funds stolen. But validity proofs allow us to be confident in that state transition. And so the thing is, a lot of people have developed validity proofs, but none of these teams are talking to each other. And on top of that, it's very unlikely that we will ever trust just a single proving system because of how new this crypto and technology is. Some of these proving systems are 80K lines of circuit code, and good luck auditing that. So I think probably for the next two to four years, we will not trust a single proving system, so we need multiple. And under that family, there are ZK proofs, and then also very recently, there's been a lot of teams developing TE proofs. One specifically is we've been collaborating with Automata to develop TE validity proofs, and the performance is looking pretty good, actually. We've been able to prove in TDX in 0.2 milliseconds specific blocks, and so we think this should be an additional layer of proving. But yeah, I think the main thing here is there's a lot of teams, and none of them are talking to each other. So we need a standardized interface to align proof system development teams under one multi-prover. And also we need this to enable prover markets so that anyone is able to provide these proofs for these layer twos. Cool. We'll get into the next part of this tech tree, which is Web2 scalability. As I said, the idea here is you want auto-sc scaling on these servers. You don't want to think about it. I don't want to, you know, get woken up by DevOps at 2 a.m. and like, oh, we need more capacity. And I need to go like manually deploy script to launch another chain. And, you know, all of the configurations associated with that. But in order to get there in this world where we could have multiple chains for a single application, we need seamless interoperability and this is just a massive mess. There are so many things here. This tree is broken up into out of cluster and in cluster. The idea here is that stacks like Optimism are creating a cluster, meaning they have multiple roll-ups that have some type of shared settlement and shared infrastructure and governance that allow them to make assumptions which make it easier for them to communicate with each other. And this doesn't mean you can't communicate with the outside world, but what it does mean is if I'm developing an application and I have a neighbor that I need to interact with frequently, I can put the walls down a little bit and transact with them more easily. And then if I need to call out to some more advanced or some other specialized use case, then I can just go through the regular Internet. Yeah. So that's out of cluster and in cluster. I think honestly the biggest thing here is literally just getting the layer to use to talk to each other. This doesn't even need to require, like you don't need to develop anything for this opportunity or spec anything. Just go talk to all of them, figure out what they want, write that down somewhere. That would already be a huge improvement. And then lastly I'll go through plentiful compute environments. And so this one's pretty fun I think because recently the Ethereum Foundation has launched a Manhattan project to formally verify a RISC-V VM. And so I think that's something you could easily do on like top of the OP stack or any other stack is start experimenting with different VM types. On top of that we could be experimenting with other privacy-enhanced computation as well, such as like FHE VMs, ZK VMs. There are some experimenting, but there's no open standards. They're all either partially closed source or in a very obfuscated code base that you need to be like an engineer on the team to understand. Cool. And so that's opportunity number four, experimenting with virtual machines on open source and open license. Don't be fooled by open source being the only requirement. Some are open source, not all of them are open license. And two popular ones here are like RETH and RollupGeth. Cool, so those are the four areas of improvement I think we can make today is experiment with inclusion and execution guarantees, build a multi-prover interface, help coordinate interop standards and build open source and open license code bases with different VMs. And so I'll get into the last section on rollup boost. So this is something this is a product we've been developing at Flashbots. It's open source, open license. And so I'll start with the basics. So in Ethereum, there is something called the engine API. It's how the consensus layer and the execution layer talk to each other. In order to be like maximally Ethereum compatible, you actually also ideally are using this engine API in your layer two architecture. And so, yeah, the ideal scenario is that we also use this in the CL and EL, and there are some stacks that do. And so we at Flashbot developed the software to just sit in between the CL and the EL. It uses the additional API that these two clients use to talk to each other, and we're able to send these requests between each other. We're able to proxy them to other components in the ecosystem. The example we've built today is a block builder, so you can add customizations to your chain, like revert protection or faster pre-confirmations, all through this block builder so you can add customizations to your chain like revert protection or faster pre-conformations all through this block builder. But in the future, we also see this component as being critical in outsourcing proof production to like prover markets and even multiple different types of prover markets. So yeah, I think the really cool thing about this is you don't need to fork the OP stack or the other stacks you're dealing with. If they use the engine API, this works out of the box and you can start adding customizations to the chain. And it's permissionless to innovate. And I think if we're able to innovate without these, like, you know, if I go to the repo and I don't need to sign a contract to advance the code base, and the code base is also maximally compatible with the layer one code bases, then this is like how we recreate this innovation loop. Yeah, also on top of this, Rollup Boost is powering the latest Unichain launch. And then on top of that, WorldCoin also recently announced that they're using it to enable a new type of block building algorithm that prioritizes humans. So there's already a ton of innovation happening in the space on Rollup Boost. And I think it's going to be one of the new innovation hubs for Ethereum Layer 2. So if you see me later, chat with me how we could build CR committees, multi-chain block builders, and multi-provers on Rollup Boost today. Thanks. All right. Thank you so much, Dan, for a very thorough presentation. We do have a few minutes to answer maybe a couple of questions. Oh, yeah. So let's start with, has it ever happened that innovation at Rollup or Layer 2 was later on adopted on Layer 1? I don't know of any scenarios this has happened. happened at innovation at roll-up or layer two was later on adopted on layer one? I don't know of any scenarios this has happened. So yeah, I think that's the big problem right now. Great. All right. Next is, what do you think about the fractional liquidity from L1 to L2? Would that be the main issue? Interesting. Fractional liquidity. Yeah, I guess it's like, yeah, the idea of fragmentation. So if my liquidity is on one roll-up, it doesn't work on the other roll-up. I think if we solve the seamless interoperability subtree, then you won't even notice this. All right. We do have still a few more minutes. We'd like to answer, does Arbitrum need a boost? Does Arbitrum need a boost? I think they're boosting time is what I've heard, which is a joke. I think they have an algorithm called time boost. I don't totally know what this question means. Okay. Let's see. What is next? Is it this? What about new innovations on L2s? Not transferred. Yeah, there are some cool innovations on Layer 2s. I think, as I mentioned, like ZK, ZK tech is like by far the coolest. Yeah, there are some people deploying like Solana, like the SVM and their architecture as a layer too, which I also think is cool. I think we should keep pushing the window on different blockchain clients and architectures on layer two. All right, last few seconds. What do you think of Solana? What do I think of Solana? Solana's pretty cool. I think Solana is what it looks like if you try to optimize a blockchain client for performance to the max. But I don't think it's what a blockchain client that optimizes for innovation looks like. Interesting take. And one last is, what is Rollup Boost? Okay, what is Rollup Boost? It is a sidecar you can use on Layer 2 blockchain clients to enable features without having to fork the underlying blockchain stack. Amazing. Thank you so much, Dan. And please give him a round of applause.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:30:00.000Z",
      "slot_end": "2024-11-13T07:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1B_rCk0bkXtF-tfbBfcDeRBqZxjx4AKThyOjuNnKCVhw",
      "resources_slides": "https://drive.google.com/file/d/1i_7sRiZ93vorEEhDkZSPbBS4rflcWKoz/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "unified-ethereum-vs-l2-ecosystem-competition-can-we-have-both",
      "sourceId": "HZCDFP",
      "title": "“Unified Ethereum” vs “L2 Ecosystem Competition”: Can we have both?",
      "description": "This panel will dig into the delicate balance of Ethereum's rollup-centric future. We'll talk about the \"frenemy\" dynamic between competing L2 ecosystems, and how this can lead to a fragmented user experience. We'll strategize on ways to maintain diversity while making interoperability easy for users—including a discussion on the pros/cons of supporting standards like ERC-7683. Can we get the best of both worlds: the innovation and diversity of many L2s, with the UX of a unified Ethereum?",
      "track": "Layer 2",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,UI/UX,Intents,ethereum,unified,Cross-L2,Intents,UI/UX",
      "keywords": "ERC-7683,Interoperability,Unified-Ethereum",
      "duration": 3385,
      "language": "en",
      "sources_swarmHash": "8edb1f118a91d0cb8965596fd17130e941f3aea7bcada1981305da431687f90d",
      "sources_youtubeId": "4Tds-Bik7zM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67345b159dbb7a90e13ea261",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:30:00.000Z",
      "slot_end": "2024-11-13T07:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1sjVmE9pcutiBwFVJbYVV2KdRqnNTg_wv6ZwyrExBY2Y",
      "resources_slides": "https://drive.google.com/file/d/1wh1013mz6bb1M6-JnXCuwvyHrz2g7v_w/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "vadcops-leveraging-starks-for-tailored-proof-generation",
      "sourceId": "BEJPG8",
      "title": "VADCOPs: Leveraging STARKs for Tailored Proof Generation",
      "description": "VADCOP is a proving method using STARKs to achieve cost-efficiency by focusing on active parts of the execution trace rather than the entire trace. Traditional modular designs, which divide machines into components and use relational arguments, face inefficiencies due to the padding of unused cells with dummy values. VADCOPs optimize performance by allowing maximum modularity and avoiding unused components, making proof generation precise and efficient without unnecessary redundancy.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "vadcops",
      "keywords": "STARKs,VADCOPs,",
      "duration": 1347,
      "language": "en",
      "sources_swarmHash": "9c2a0481d79b3081115348cb2d026e8ce0cd54d6d9a79ef0a589e2d6c845643e",
      "sources_youtubeId": "cQWKTyyoeto",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67344f579dbb7a90e1b0dad9",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673453859dbb7a90e1de05dc.vtt",
      "transcript_text": " essentially distributing a verifiable binary version of the chain from one machine to another, essentially. And what this diagram is showing is you've got a set of pages and they're all hash verified. Now, what are the implications of all that process? Well, basically what you end up with is very quick sync performance effectively so the big difference between Eragon to an error Eragon 3 is the amount of time it takes to sync something and this is You see this particularly on large chains so Basically my role in the Eragon team. I mainly work on Polygon rather than Ethereum. So, basically, my role in the Aragon team, I mainly work on Polygon rather than Ethereum. So, basically, my role in the Aragon team, I mainly work on Polygon rather than Ethereum. So basically my role in the Aragon team, I mainly work on Polygon rather than Ethereum. So basically my role in the Aragon team, I mainly work on Polygon rather than Ethereum. So basically my role in the Aragon team, I mainly work on Polygon rather than Ethereum. So basically my role in the Aragon team, I mainly work on Polygon rather than Ethereum,",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:30:00.000Z",
      "slot_end": "2024-11-13T07:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1vlLbALGk1-PoxsWpK3hZ1d85x7eK1bnX8dA5Jjf4Yj0",
      "resources_slides": "https://drive.google.com/file/d/1k4WOtyYcQjruZ-dITnCUItGJ6Dn33vN2/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "why-erc-7683-is-broken-and-how-to-fix-it",
      "sourceId": "YT3SSN",
      "title": "Why ERC 7683 is broken and how to fix it",
      "description": "While I appreciate the authors spending time on this problem statement and thinking about standardising flows, ERC 7683 is deeply flawed it still forces offchain agents to understand the order they are trying to fulfill and it doesnt give users any guarantees of execution or understanding of whats happening under the hood, I think its because its standardising things on the \"intent\" layer where instead we need to standardise more downstream so information like security can be better presented",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Appchains,Cross-L2,Token bridging,Accessibility,erc-7683,intent,Accessibility,Appchains,Cross-L2,Token bridging",
      "keywords": "chain-abstraction,intents",
      "duration": 1275,
      "language": "en",
      "sources_swarmHash": "5fc0bc4aab0210e4bbcfda418f6debbf3e708920939b5efb8bc1b105399cf8f4",
      "sources_youtubeId": "TOkUi0asAd0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67344f0d9dbb7a90e1accefa",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67344f0d9dbb7a90e1accefa.vtt",
      "transcript_text": " Thank you so much for sticking through. I know everyone's hungry for lunch, so am I. Hopefully, this will be a solid one, after which you think you will need a break. Super glad that you are still up and alive. Sweet guys, I think to get into it, I sort of wanted to talk about this upcoming thing called Intents, which is supposed to change how we interact with chains going forward. And a bunch of projects are starting to look at it in a pretty deep level, including people at Uniswap, Cowswap, and a bunch of other protocols. So I just wanted to chat about an upcoming proposal there and talk through some of the design choices that have been made and why those might not be right and how could we potentially fix it. This is not a diss track. I'm using the EIP or ERC as sort of like a channel to talk about this stuff. Not really talking about this specific implementation. I work at Socket. I'm a web host. Also go by VC. You can find me after the talk. But if you want to check out the exact proposal I'm talking about here, there's a quick QR code. I think this is by Mark Tura from Uniswap. And this is what Uniswap is going to be using for the cross-chain swap stuff, generally speaking. I think we always begin this topic with what really are intents, right? Unfortunately, that is something I don't want to get into for this particular doc, right? It always sort of goes into something you don't want to get into. But I think broadly speaking, you want to do something on-chain, you specify it, someone else takes care of it. That's the rough mental model you can have in mind. And this is different than transactions, because in transactions you sort of like send the transaction on-chain saying exactly what you want to do, but here you just talk about the end goal you want to achieve, and someone else takes care of the execution. That's roughly how to think about it. I'm going to talk about a particular design pattern here. Goals of this particular ERC, 7683, are about how do we make it such that fillers aren't fragmented. This probably doesn't make sense right away, so let me give you guys a quick walkthrough about what really is happening here. So you see a nice farmer emoji there? That is our normal blockchain user today who is sort of like signing something that looks like a struct where he's sort of like talking about what chains he wants to go between, what's the input between, what's the input asset, what's the output asset, things like that. And he signs this stuff and sends it on chain to a particular settlement contract on some source network where a bunch of these robots, these bots sort of look up to this source chain contract for what are pending intents that need to be filled. Then they sort of like fill it on the destination. And then there's a oracle that sort of lets the settlement contract on source chain know that hey, the request that you had has been fulfilled by this particular bot on destination. So really simple. User sends funds on source. The bots fill it on the destination. And then the oracle sort of conveys the message that things are done, really. I think filler fragmentation starts here. So now we have two structs, which basically means now we have two intent protocols. The only difference between the first one and the second one is that the second one has an extra parameter called call data. Let's assume everything else is the same. This is the only difference, right? Due to which now we have these two separate contracts, and earlier while we had had eight fillers solving for this particular intent protocol, as soon as we have two intent protocols, these are now split in two. As more and more intent protocols go live, fillers get fragmented because they need to do more work. They need to do more integrations and stuff like that, super annoying stuff. This is harmful for everyone. Not exactly harmful, but really undesirable, specifically because there aren't too many fillers anyway in the space. There are probably 15 to 20 entities that are operating in this particular role. And if you start fragmenting those, we don't know where we'll end up. So we want competition over here. So we want as many fillers as possible. role and if you start fragmenting those, we don't know where we'll end up. So we want competition over here. So we want as many fillers as possible. So fragmentation is something we want to avoid here. Fragmentation is bad for all three of these parties. It's bad for the user because if people aren't competing, you're going to get the worst possible price. If you don't have an intent standard, then you don't actually understand what you are",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:30:00.000Z",
      "slot_end": "2024-11-13T07:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1MNzcD3lH260PkgaznRJQQW41lkxoYMoKXT73MHMNPfg",
      "resources_slides": "https://drive.google.com/file/d/1hSjtyhdmrJ0ifk7fMlYIdCBcGzLX5F2e/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "eea-and-the-institutional-infinity-garden",
      "sourceId": "JQBXXD",
      "title": "EEA and the Institutional Infinity Garden",
      "description": "This talk would be to give an overview on the latest from the Enterprise Ethereum Alliance, how the year has progressed in enterprise and how EEA seeks to support and guide institutions to participate in Ethereum's Infinity Garden.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Vision,Use Cases,institutional,Coordination,Use Cases,Vision",
      "keywords": "Business,Enterprise,Instituional",
      "duration": 602,
      "language": "en",
      "sources_swarmHash": "627a8020ea8fffe7e60da9ea41e68e2239bf60b5384058c21bd9da1f40eec92e",
      "sources_youtubeId": "dYgucH3a7sI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:40:00.000Z",
      "slot_end": "2024-11-13T06:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1f1uiHRqQIfhY0F3DmSPJ-wpu3lCdP00YCID-a2-UblQ",
      "resources_slides": "https://drive.google.com/file/d/1C3UMP1JMq728oimD3Zqw0gSUYzmwFURN/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "what-defi-founders-can-learn-from-web2",
      "sourceId": "QB8CGR",
      "title": "What DeFi Founders Can Learn From Web2",
      "description": "Most DeFi founders come from crypto native backgrounds, but there is much to learn from the operational mechanics and metrics of web2 companies. \r\n\r\nThis talk will be a brief tutorial about web2 business mechanics, specifically SaaS. Concepts like unit economics, CAC, LTV, ARPU and the science of building and growing scalable companies.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "Metrics,Unit economics,Growth",
      "duration": 551,
      "language": "en",
      "sources_swarmHash": "2a6da17012439090d0f3e3a01b43f095606ddc22b273ee597298003c1d4338d5",
      "sources_youtubeId": "BYAUg-nibMs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T06:50:00.000Z",
      "slot_end": "2024-11-13T07:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Gix77PnI2mYDQXanQIb49GstVRHx_-5qwgYKGNsIxzs",
      "resources_slides": "https://drive.google.com/file/d/1Hs1_6UZ4ttrG-1KbtyHy-LCTw--sx29L/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "amerameen",
      "sourceId": "3V9XLK",
      "title": "Amerameen",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:00:00.000Z",
      "slot_end": "2024-11-13T08:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/19hVh-i336QQTm56Yaa0OcRApQQX8RzhtksYA5s12z9M",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "deep-dive-how-to-use-erc-3668-to-trustlessly-read-l2-data-from-l1",
      "sourceId": "ZAKUY3",
      "title": "Deep dive: how to use ERC- 3668 to trustlessly read L2 data from L1",
      "description": "In this workshop, the ENS, Unruggable and Linea team will demonstrate how one can use ERC-3668 (aka. CCIP-read) to read L2 state trustlessly from L1, with concrete examples. Let us show you how it works!",
      "track": "Layer 2",
      "type": "Workshop",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 3425,
      "language": "en",
      "sources_swarmHash": "8485f4455db9042e3922945f07943632c1c1308f9e9210832cc846611241f919",
      "sources_youtubeId": "L1pwtkKSTvs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673483049dbb7a90e1e6f0c7",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:00:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1W0jwOLdutdtpuJNo6WvxKfcV8v0h4mUvf0CLm68DfjQ",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "erigon-3-a-new-paradigm-for-ethereum-clients",
      "sourceId": "CWZK8G",
      "title": "Erigon 3 a New Paradigm for Ethereum Clients",
      "description": "Erigon 3 represents a step change for Ethereum clients:\r\n\r\n* Modular client combining EL & CL\r\n* Transaction Centric\r\n* Deterministic storage model built to optimize EVM based chains\r\n* Performs on commodity drives\r\n* Sync model uses verifiable data replication and minimal re-execution\r\n* Acts as block consumer and producer, RPC, or indexer\r\n* Splits chain dissemination from chain distribution\r\n\r\nThis talk outlines the key features of Erigon 3 and explains how it will change Ethereum client  landscape.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Data Availability,Scalability,modular,Architecture,Data Availability,Scalability",
      "keywords": "efficiency,client,modular",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "e50220090933449af7ced2212fd9fade4e5054c7a674ee3f7ee568402c27bf34",
      "sources_youtubeId": "sMPe1Ae99aA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:00:00.000Z",
      "slot_end": "2024-11-13T07:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1AXdOVnj0u1_i9ZgFD0ao2vPGkVGyZU_aLbplEgtr5iY",
      "resources_slides": "https://drive.google.com/file/d/1opEm2S90hqk37kx_NonDgDc7o5Szc6Ew/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "grandine-on-windows",
      "sourceId": "SUTU99",
      "title": "Grandine on Windows",
      "description": "In this talk, the speaker will discuss the problems encountered in porting Grandine, an Ethereum consensus client, to Windows systems and the solutions from the perspectives of language, engineering, and cross-platform. The speaker found that these problems are common to the current Ethereum infrastructure based on the Rust language. Finally, the speaker will summarize and look forward to the development of Ethereum clients based on the Rust language, especially from the point of cross-platform.",
      "track": "[CLS] EPF Day",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Best Practices,Core Protocol,Languages",
      "keywords": "Rust,Client,Engineering",
      "duration": 759,
      "language": "en",
      "sources_swarmHash": "3c373e0cffaa2ece62499d889302699c6414cc3b3651b30fc5031e2b0dd91bff",
      "sources_youtubeId": "8GcL9zQdrrQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734602e9dbb7a90e1626bf0",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:00:00.000Z",
      "slot_end": "2024-11-13T07:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1W4lSdrWzgMoJHrCdD1XG6PWUZq7B7QBp09iTEJj9lH0",
      "resources_slides": "https://drive.google.com/file/d/1W2PWD0BKpvQqWTqMjMzb5gBqDM3R7hAM/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "indexing-ethereum-when-and-how-to-build-an-indexer",
      "sourceId": "BGGFDD",
      "title": "Indexing Ethereum: When and How to Build an Indexer",
      "description": "Open source Ethereum Indexers are great for quickly getting your project off the ground. However, there are limits to these tools and in some cases building your own Indexer is the right thing to do. This talk will explore why you might want to build your own and outline a technical approach for building simple, reliable Indexers.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Developer Infrastructure,Best Practices,infrastructure,Architecture,Best Practices,Developer Infrastructure",
      "keywords": "database,indexing,infrastructure",
      "duration": 1567,
      "language": "en",
      "sources_swarmHash": "0a2be92ccf1e09bf590a829255287bef39cd443f27bd890025f50ed271c35286",
      "sources_youtubeId": "WgBab6kamtg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67345e239dbb7a90e1548599",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:00:00.000Z",
      "slot_end": "2024-11-13T07:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1UA3bcjbOHIUGe57PEX-2bhr64qsal8zYSkn0UedXY0E",
      "resources_slides": "https://drive.google.com/file/d/1JJdphZ73AA4Ug1x25QL7NT7047cO_Pvf/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "the-wellbeing-protocol-scaling-localism",
      "sourceId": "HC3QGN",
      "title": "The Wellbeing Protocol - Scaling Localism",
      "description": "Imagine a world where:\r\n - hyper-local marginalised communities could create impact DAOs as easily as creating FB groups\r\n - we could create a UI that abstracted the complexity of quadratic / conviction / delegated voting to create a continuous resource allocation alternative to governance\r\n - funders could stream money into millions of these treasuries\r\n\r\nFind out how this New Zealand government funded project, now running trials in three countries, is creating a network of  grassroots changemakers.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,Governance,Quadratic Voting,Collective Intelligence,Conviction,Ethereum for Good,Public good,Climate,ReFi,Regenerative Applications,User Experience,zealand,Climate,Collective Intelligence,Conviction,DAO,Ethereum for Good,Governance,Public good,Quadratic Voting,ReFi,Regenerative Applications,User Experience",
      "keywords": "conviction,zealand",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "262eb3ab3f23b0ae9b773e2fc4e01025258988511f1fd7cd530249bccbf39dba",
      "sources_youtubeId": "doOQJRqTzSs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:00:00.000Z",
      "slot_end": "2024-11-13T07:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1RsF9WALoUv0Wv3Pc036sfCbuKskiOHZzZRM1r385Iew",
      "resources_slides": "https://drive.google.com/file/d/1hx9ym73B6CqpDzuifBxUrRLBa90J68EW/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "securing-grandines-performance",
      "sourceId": "GGWXYQ",
      "title": "Securing Grandine's Performance",
      "description": "Our project focuses on improving Grandine’s performance and stability through targeted benchmarking and profiling. By conducting a comparative analysis with Lighthouse, we aim to identify architectural optimizations, especially those related to parallelization. Establishing baseline metrics is key to this approach, as it allows us to focus on refining critical areas within Grandine for optimal, efficient performance, thereby supporting the robustness of the Ethereum network.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Consensus Mechanisms,Core Protocol,Cryptography,Security",
      "keywords": "",
      "duration": 813,
      "language": "en",
      "sources_swarmHash": "3c895500f7d570e1fc742742e5d135753d9fe1464ebae8ef9cf72bb0ac6582c2",
      "sources_youtubeId": "78N_LYGSZ3Q",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673459489dbb7a90e1204ed1",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:15:00.000Z",
      "slot_end": "2024-11-13T07:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1prZ931qBVTXdBa8oGWfuFhX5yIKVdrAsZ9rAg99ejog",
      "resources_slides": "https://drive.google.com/file/d/1ewIaM4llzZdfdKOAYb_BEGlRfJl63Uhl/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "passkeys-the-good-the-bad-the-ugly",
      "sourceId": "XFLPAR",
      "title": "Passkeys : the good, the bad, the ugly",
      "description": "Passkeys are the new hype for easy onboarding, but it's a quite old protocol that has been hijacked for crypto purposes. We'll dig through the standard history, the potentially misleading security expectations, and see how to reverse engineer an implementation to validate its soundness",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Account Abstraction,TEE,Account Abstraction,Security",
      "keywords": "TEE",
      "duration": 1528,
      "language": "en",
      "sources_swarmHash": "ba22dd73d7a6bf4d9643ab45b06a1aeb4b9628b46d99373ca721a38b46437d5e",
      "sources_youtubeId": "TEjNSr8jjUI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673982fe1b0f83434d5ab3b3",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673982fe1b0f83434d5ab3b3.vtt",
      "transcript_text": " Okay, so Makupai everybody. Today we'll be talking about passkeys, we'll be talking about security and memes as well. So you'll see it will be entertaining, I hope. i'll be going fast because i have a lot of slides so don't sleep so first you might be wondering what you are doing in this room why are we talking about identity and access management protocol phylo means fast identity online so why are we here it looks very complex server blocks client blocks a lot of stuff we don't do with Ethereum. But if we are DGENs, like we all are, we don't really care about all the parts on the server. We care about the fact that with passkeys, we can generate keys on the web browser. And this is very interesting for us because we like keys, of course. So next slide. Yes. So if I want to go through a Presentation as a FIDO protocol in one slide, I would say that FIDO is an authentication protocol You have a registration phase where you will create a key You will bind it to a web origin and then you have an authentication phase where you will get a challenge You will sign it and so you can verify that you are in the right place We have recent gas optimizations that allow us to run the FIDO protocol on-chain, which is why it's interesting, and there are a lot of abstractions going in FIDO, so a lot of different implementations of this protocol going on. Why is this interesting? Again, we got a God tier UX on mobile with account abstraction with passkeys. If you have not tried it already, I will suggest trying the Coinbase Smart Wallet, which is very impressive. But you get something that is very close to Web2 experience. You just use your biometrics, and you have a wallet. This wallet is self-custodial, so you can pretty much do whatever you want. And, well, it's super easy to create an account on mobile with Passkeys. On desktop, it's a bit different, but it's standard as well. So it's good because you have a common interface. You have a QR, you can redirect to your mobile. If you have an Android phone, you can just avoid scanning the QR every time. You can scan it once, and then it will be recognized automatically, but it's proprietary. So it's not a great interface, but at least it's standard on all the implementations, so you have a way to connect your desktop to your mobile, and it's not too confusing, let's say. Regarding a web developer, the experience is pretty standardized, pretty simple as well, because this was acquired by WebO10, by the W3C, sorry, under the name WebO10. So you pass common parameters when you register, like the user information, the key types, the challenge. So two APIs, super simple, have been going through this very, very quickly because we are not yet at the interesting part. The interesting part is going to start now because you are all wondering where the key is stored. We say we have keys, but well, where is it stored? This is the real question. And unfortunately to answer this we have to go through a very very large bit of history so I will be just doing this. The FIDO story starts in 2013 with two protocols the universal second factor and universal authentication factor. Universal second factor is the best known. And both protocols are mostly stateless. Stateless meaning that all credentials are not saved in the device. Credentials are generated in the device, saved on the server. So the device can be used to generate an infinite number of credentials. It was reported launched initially by google and ubico 2014 this is a real commercial launch with two devices uh ubikey and one device on which i worked on if you look at it you might recognize the early ledger design very similar 2018 502 is launched 502 introduces something new it's basically a fusion between U2F and UAF and introduces a concept of resident or discoverable credential. Those credentials are stored into the device so this completely changes the state of the protocol because now it goes stateful. 2019, WebO10 is launched by W3C, so it's basically FIDO but acquired by W3C, and at the same time, on Android, you have the Strongbox API which appears, and it is the equivalent of the secure enclave on iOS devices, so a very secure place to store keys, and Android got FIDO to certify. I think you are getting a trend there. We get iOS support in 2020, and we see that FIDO starts to be supported in devices which are very secure. So it all looks very good at the moment because we have dedicated devices with secure hardware to support FIDO. We have it in phones with strong security. And in 2022, things start changing because we get a shared announcement from Google, Apple, and Microsoft saying, this protocol looks nice, we are going to support it a lot more. And usually when you get this kind of announcement, it's the beginning of assimilation. And this assimilation has a name, Passkeys. So, well, first thing we can say, in 2023, it starts with the introduction of syncable credentials. So credentials that can be synced to different devices, either with a proprietary scheme or a password manager. So here's the illustration. It's a big temple because we are in Bangkok. Temples are called Wat. So this is Wat Pho. I didn't visit it yet, temples are called Wat. So this is Wat 4. I didn't visit it yet, but yeah, Wat. And in 2024, the FIDO alliance finally performs a general rebranding around passkeys. So now a passkey is just describing a FIDO credential. And we'll start a specification to make the synchronization a bit less proprietary. So we might ask ourselves, what is a syncable credential? Because if we search for this in the documentation, we won't find any reference to it. That's because the proper name for it is multi-device credential. So that's one thing. And if we want to define it, we can say it's a discoverable or resident credential in the context of a smartphone. But the only way to know if a credential is really syncable is to look at the answers that you get when you register it. You will get a flag which is called backup eligibility. If it's set to one, then the credential can be synced. So it's something on top of the spec, not super easy to get. The real problem starts now because we have several security misconceptions in FIDO. The first one is that, well, FIDO protects against phishing, not malware. So we can't really expect that FIDO is going to be good to protect against malware, but at the same time, it is implemented on secure devices. So we have strong... we think that the key is going to be strongly protected because this is the way that FIDO was defined. Then we can ask what is the consequence of introducing thinkable credentials to this. And to get even worse, when you look at the implementation on FIDO by crypto people, we abuse it routinely because FIDO is only designed for authentication. When you use it to sign transaction, we can't really verify what you are signing. And if we lose a key, the impact is going much more important for crypto than it is for authentication because you can always revoke an account and you can't revoke a transaction on the blockchain. Then FIDO is designed to be bound by the web origin. If we want to build a common web wallet, we are going to break this property by design. So we have to hack around it, but it's a hack around the specification. And just to say that we have been abusing this protocol for a long time, I'm not especially proud of it, but just mentioning it, I used U2F to communicate between Ledger and Metamask, sorry, and my Ether wallet in 2016 because there was no way to communicate between a browser and a USB device. So it was used as a tunnel at that time. Short break, maybe why secure hardware is important. Secure hardware is important because it will protect the key against malware. Any hardware does that but secure hardware is supposed to do it better. Then it will protect the key against physical attacks. Physical attacks are the last line of defense. When an attacker has access to a device you always think that your key is lost. Secure hardware is supposed to protect you against this. But even more importantly, secure hardware will protect you against passive attacks. Passive attacks meaning trying to obtain the key by listening to what the ship is doing, by listening to electromagnetic radiation, power differences, that kind of thing. And the only way to really protect against this is by using dedicated hardware because even if you use the best open source library like LeapSecP 266K1 here, it needs to be customized to your chip to avoid leaking information. So if you are not working with secure hardware you're going to have problems. That's basically, you are just one speculative leak away from losing the key, for example. So what is FIDO security model for non-seekable credentials? They cannot be extracted by malware. This is very important. The authentication is always done at the enclave level. The enclave is basically in charge. It's holding the key. It's holding the key, it's doing the authentication. If you want to do something with the enclave, you have to authenticate. This cannot be bypassed. So the malware cannot, a very strong malware that managed to modify the kernel of the device can fool you into signing something, but it would have to do it every time and it can't do anything else. Now let's think about some hypotheses for the synchronization. The first one will be the good hypothesis. In that case, we imagine that there is a hardware security module sitting at Google and Apple, and this hardware security module is doing a synchronization protocol between two enclaves. In that case, the security model doesn't change, the credential is never exposed, everything is good. At least it doesn't change. Now the bad synchronization hypothesis. In that case, the key is still owned by the enclave, but you have a way to put it into the application processor to start the synchronization mechanism. In that case, a malware could be able to extract the key, but after prompting the enclave to start the synchronization mechanism. In that case, a malware could be able to extract the key, but after prompting the enclave to start the synchronization protocol. And finally, the ugly synchronization hypothesis where the enclave might not even be used anymore. The key is in the application processor. Everything is in the clear. And then a malware could be able to extract the key, and the malware doesn't need to be as sophisticated as in the previous cases. And unfortunately, the only way to know what the implementation is, is to reverse it. So we are going to do this. First, on iOS. So iOS, to do this, we need to act as a malware. iOS is a bit difficult to jailbreak as you know so I made a reference to a recent jailbreak to show you how complex this is but since we can synchronize we can jailbreak an older iOS device and see what happens. So for that we are going to use the checkmate bug which is quite powerful and allows us to jailbreak a lot of older iPhones. I use PayRain for that, but you might want to use another exploit. It's a bit hard to run, but you will manage to run it if you want. Then we can dump the keychain. We have some information about the keychain at Apple, but here the first thing that we notice is that there is no security property that says that the credential needs to be authenticated every time. It's basically authenticated when the device is unlocked and then it's not going to be locked again. So we see a first problem here. Then digging into it, dumping the kitchen itself, we get more information. We got a first attempt that was done on an older version of iOS, a description of what the kitchen looks like. We basically, we have items in the kitchen which are defined by a metadata and the secret itself. The metadata and secret are wrapped by a key which is handled by the platform. So you need to have the device in order to decrypt this. And you have an additional indirection level for metadata. So you have an extra key which will be decrypt this and you have an additional indirection level for metadata so you have an extra key which will be decrypted by the platform as well but the scheme is always metadata in secret protected by a key basically so there is a small difference between two item version and basically knowing this you can fix what is on the internet today and you can make it dump the recent items because protocol buffers is very easy to describe, I mean it's self describing so you can modify this. When you look at the decrypted item you'll see that the value, the length of the value is 65 bytes plus 32 starting with 04. If you have played with key, you think that, okay, this might just be the public key and the private key concatenated. You can verify it by dumping the key, verifying that the public key associated to the private key is the right one, and it is the right one. So this means we have a way to decrypt. A malware has a way to decrypt the key. It can steal it, and it's fully handled by the by the application processor on ios on android we can do the same thing uh it's much easier to jailbreak an android phone because you can just unlock the bootloader and solve magics and your magis mat magisk and you're done and to look at the application we'll use a framework called frida which will let us introspect the application and inject code to understand what the application we'll use a framework called Frida which will let us introspect the application and inject code to understand what the program is doing. First question, it's very clear on iOS where the kitchen is, on Android not that much. So we want to know where we are and for that we are going to look at the logs. We see that we see a lot of logs referring GMS for Google Mobile Services. Looks like a good place to start, so we will instrument it. To instrument it, we'll just use the signature API in Java, and we will ask for the class that is being used, because that way we can dig further, and we can try to know exactly what is happening. So we do that with GMS. We see finally the name of the class. And we can find some information about that class on the internet. We see that it's a wrapper to another class. So we still don't know if the credential is handled by the secure enclave on the device. But at least we can dig further. So we start instrumenting again. And this time, we will dump the key. So we will assume that if we have the right class, we can cast it, and we can ask it to dump the key. And it works. So we can dump the key, we can verify it's the right private key, it matches the public key, which means that on Android and iOS, we have verified that the key is enabled by the application processor. As a bonus on Android, so it's yet another big what here, we get the key before the user authentication, which means that there is a catch mechanism that is loading the key, and user authentication is just there basically to make you think that it's secure, but it's not really secure. Finally, looking at an external password manager, here we have absolutely no expectation, so it's good. I did the example with Bitwarden. You can see when you dump it that the credential is listed as a public key. Bad news, it's not a public key. Of course, it's a private key, so we can just dump it, look at it, look at the private key, verify that it matches the public key. Exactly the same thing. So if you expected some security by saving your passkey to a password manager, you have none, which is completely the expected result. So to summarize, this is where we are today on smartphones. We have to choose between secure credentials or back-upable credentials, which might not be a good thing because you want both. So the passkeys are handled by the application processor. They are easy to extract by a malware when the device is unlocked, and there are physical attacks applicable, which is probably the worst part because something might be able to dump those keys at a later stage. So user-present enforcement is not really linked to usage of the key. And non-thinkable pass keys, things don't change there. They are still very secure. So we can wonder, how did we get there? One way to say that is that, well, we have conflicting rules between the vendors. So Google says that you can inject a key into the enclave, and Apple says that this is absolutely forbidden. You cannot inject a key into the enclave. So this might be the reason why the enclave were not modified and not used in that case. And all agree that the key cannot be exported from the enclave, so that's another good reason why it's not implemented that way. So FIDO says, FIDO introduced some enterprise, some difference between enterprise and customer pass keys. So this can be another sign that things are not going very, very well. If we wonder what will happen in the future, it's not going to change much because here we have a draft regarding the synchronization protocol, which does not describe how the key is used. If we want to add a trust encore to the key, so if we want to link a device-bound passkey to the key, there were a few extensions to do that, and they were finally dropped. And in the end, well, if we want a better UX for passkeys, this is also going to be dropped. So we had three possible improvement protocols that were dropped. So we can't think that this is not going to change much. If you are disappointed by Platform Authenticator, you can always rely on external passkey implementations. So I listed a few open source ones. The good news is that you can use your favorite hardware wallet if you want. The application is open source and you get a backupable passkey. So it can be another option. What happens on-chain? Well, I have one minute, so maybe I will go over there very fast. Cartridge initiated the move on StarkNet. So passkeys are now very popular because we have been running several optimizations in order to be able to verify passkeys on-chain very efficiently. So, PaaS keys are now very popular because we have been running several optimizations in order to be able to verify PaaS keys and chain very efficiently. I have a few kernels, I have listed a few kernels that you can use to check to use with PaaS keys. So, 0dev, Safecore or Coinbase Smart Wallet. Here's the main difference. I will just look at how they are using PaaS key. You make no difference with a common credential on 0dev. On Safecore, PaaSKey are supported but discouraged. If you look at the specifications, Safecore will tell you it's better to associate the PaaSKey with a regular credential. And if you want to run it on Coinbase Smart Wallet, they support PaaS pass keys, they also support regular signatures, and they will prompt you and tell you, yeah, you might want to use a recovery key. So the answer to the maybe burning question, should we drop pass keys? In my opinion, no, because they still offer the best way to onboard people. But we have to think about the threat model and we have to code accordingly. And one thing we can do since we have smart contracts, we can associate less privilege to pass keys that are not, that are syncable. Because since we know that syncable pass keys are way less secure than pass keys that are handled by the device, we can use them with quotas, we can use them, for example, for less amount of assets. And then we have something that is acceptable. But the most important thing is to know the threat and act accordingly. We are finished. So I will just let you with one last meme for the road. This is the difference between 2014 and 2024. We get rid of password by storing key in password managers. Might not be the best idea. I will let you decide. You can reach me on Twitter and there will be code on GitHub to describe all this so that you can run it on your own device and you can know if at some point implementation gets better and, well, the less ugly solution for the synchronization is being picked. Thank you. All right thank you so much Nicholas that was very entertaining as well to see some memes on the slides I really enjoy that. We do have a couple of questions for you I hope you don't mind answering. So the first question is is Bitwarden not safe generally or just for managing passkeys? So Bitwarden is safe for a password manager but you don't expect the same security to handle password and to handle keys. So that's why I think it's important to store keys in hardware because if you consider that a key is a password you lose a lot of security. So Bitwarden is reasonably safe for a password manager but of course you can extract anything you want from it because this is the way they work. Alright. There's one that's not a question, but thank you for making that big whack pun. It was brilliant. Thank you. Thank you. Next is, do you think intentional security lowering of the standard could be a dual... I don't like... I see the reference, so thanks for making it. No, I think it is... Sorry. I got the direction wrong, sorry. So, no, no, I don't think it is. I don't think it was... I don't think it was pushed by the government. I think it was really a choice to make the UX easier and to make it, make it easier, again, to not lose private keys. So definitely ease of use was the driver for that, in my opinion. Alright. And next question is, what do you think of spending limits for mission management? What should I use as well? I think spending limits and permission management are definitely a good way to deal with that. And what should I use as a wallet? So I will just speak about wallet frameworks. So not necessarily wallets, but for wallet frameworks, any framework that will support this is good. So there are a lot of them. I named three of them, and I think the three of them are good to manage passkeys and to manage additional permissions on top of them. All right, so there's four people who voted this. Well, that was depressing. What to do? What to do? Not panic. So that's the most important part. Keep using passkeys. Just think about using them well, using them with knowing this and use them knowing that they are pretty easy to extract by malware if they are synchronized. So just, yeah, it's not the end of the world. We just have to be more careful. All right. The next one is hardware UB keys would be relatively secure. Yeah, they are absolutely secure, but using your smartphone to store a key that will not be syncable is also very secure. But then you can't back up it, so it's always a choice. All right. And next question is, what are options for a non-syncable passkey? Exactly the same. You have to pass it as being non-discoverable when you create the key, but you can use a smartphone, and in that case, you are on an old style, I would say, file credential. All right. And let me just check. Okay. So next top voted question is, do you think pass keys are easy enough? Oh, it keeps moving. Do you think pass keys are easy enough to use for normal users? Even as an advanced user, I have sometimes lost access to pass key and had to jump through hoops to recover Yes, I think they will be I think they are in regular cases it might be a bit rough on the edges at the moment because the protocol is still very new Passkey synchronization is only one years old so I think it will get better and ultimately all the big firms want to push it. So it will definitely get better. All right. We have a few more seconds to answer this last question. If you turn off syncing passkeys on iOS, are they safe again? They are still on your device. So the problem here is that if you created the passkey as syncable and you turn off passkey synchronization, you get a passkey that is stored in the application processor and which is not synchronizable. So basically, you get the worst of both worlds. Sorry. Actually, we have a few more seconds. Maybe you want to answer also what's the future of secure chip? How do you feel JavaCard in general? I hope there will be secure chips that are more secure, more, sorry, more open in the future, and FlashBots is doing a lot of research in that, so it's good. JavaCard is outdated. That's one of the reasons why I decided to start Ledger, because I wanted to have native code running on a smart card, so that's my general take. JavaCard is good,",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:20:00.000Z",
      "slot_end": "2024-11-13T07:50:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1qSDCPwnZ7bDT8RyjyUEMjDpMOU2yF_Nq0xmCkw7SprQ",
      "resources_slides": "https://drive.google.com/file/d/1YvALPBQ5IfyYEdjUFFfjAfRZ6mj1ELdH/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "security-of-fiat-shamir-transformation",
      "sourceId": "VMNCS8",
      "title": "Security of Fiat-Shamir transformation",
      "description": "Fiat-Shamir transformation underlies virtually every SNARK used in the Ethereum ecosystem as it makes interactive proofs non-interactive. In this talk, we discuss the security issues if the transformation is used incorrectly (e.g., parallel repetition of a ZKP defined over a small field; such protocols became very popular thanks to their efficiency), provide examples, show the security loss that the transformation brings, and the concrete security of ZKP. Finally, we discuss best practices for k",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Fiat-Shamir heuristic,STARK,Security,iop,Fiat-Shamir heuristic,Security,STARK",
      "keywords": "small fields,IOP",
      "duration": 1593,
      "language": "en",
      "sources_swarmHash": "48a411b091a8c6046acad9b1ee6abd821654d0b9005b8e2e4fffe3fe33eac9c6",
      "sources_youtubeId": "VIMnaOUvw08",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:20:00.000Z",
      "slot_end": "2024-11-13T07:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1qlPnS97cEpEKuQEuS06efm97LnehdTDo-7FRoyWVIHY",
      "resources_slides": "https://drive.google.com/file/d/1hVGyOb5GLWW87XGWxh0ptWmu-zAZOWJS/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "bringing-web2-users-onchain-picnics-case-study",
      "sourceId": "E7RSG7",
      "title": "Bringing web2 users onchain: Picnic’s case study",
      "description": "Account abstraction had mixed success so far. Some specific purpose apps have been getting great usage, but there is still little economic value being transacted using account abstraction and very low repeat usage.\r\n\r\nBy building Picnic, we discovered that account abstraction is currently much better suited to bring web2 users onchain than to cater for crypto natives. We'll share our learnings from the trenches and offer builders a fresh perspective on how to accelerate adoption.",
      "track": "Usability",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Use Cases,User Experience,Account Abstraction,adoption,mass,Account Abstraction,Use Cases,User Experience",
      "keywords": "Onboarding,Mass Adoption",
      "duration": 4074,
      "language": "en",
      "sources_swarmHash": "6c7a78a70d240767eb04e0e57d5e8c3f7bac563d03d06bef103a2698e8d2d1f8",
      "sources_youtubeId": "aKbsZkxeJNs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673481849dbb7a90e1d0191d",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673481849dbb7a90e1d0191d.vtt",
      "transcript_text": " Hello everyone, great to be here. My first DevCon talk ever. So super excited you joined me. I'll be talking about a counter-protection. I have been building a DeFi consumer app in Brazil. It's called Picnic. We have been in the front lines trying to get Web2 users to be on-chain and execute transactions on-chain. And the idea here is to share what we learned. So who are we? Like, building consumer DeFi app, very focused in Brazil right now. We are seeing, like, really interesting feedback from users. So, like, when I go to conferences in Brazil and I meet people there, it's really nice to see that people really like the product Connect and see the vision and see the benefit in using day-to-day. And there's two metrics I really like about that. There's something called a Sean Ellis score. That is basically a form you send your users and you ask how disappointed you would be if you could not use the product anymore. 60% of users say they would be very disappointed, which is a compelling thing that people really care about what we're doing, and they're seeing really good repeat usage. So once a user swaps, there's a 57% chance the user will swap next month. So this is really good. We'll put that a bit into context, but it's a real-world use case that we have been seeing repeat usage. And on the DeFi front with account abstraction, we are doing really good volumes. So we're doing close to half the swap volumes using ERC-427 across the main EVM chains. And why did I decide to apply this talk and come here to bring what we have learned? I think we talk a lot about mass adoption, and though everyone needs to be on-chain, but I think that this is very vague and abstract. There is, I mean, at least on the talks I have been participating, there is not a whole lot of depth on how you actually do that like from the point we are at right now how do we evolve this thing to really get people to mass adopt it and use it day to day and as we are building something with like a few thousand users and we have been shipping, iterating and learning I thought it would be valuable to come here and share what we have been shipping, iterating, and learning. I thought it would be valuable to come here and share what we have been seeing. For who this talks, I think it would be valuable if everyone that's interested in mass adoption and getting the nuance on how to get there. It will be mostly focused in strategy and product is talking about what decisions we took, why we took them. So builders, people on ecosystems, VCs, I think everyone can see some value here. It should be pretty open, so if you have any questions at any point in time, just raise your hands. We can make Q&As and talk a bit and explore topics. I think they're very, I'll give a brief overview and we can deep dive on any specific topic we want to. I don't have, I have 50 slides or so, but I don't think it will be enough to cover all the time. So yeah, we have time for a Q&A. I think it will be very valuable. And to give you a sense on what we're covering, I'll start giving you an overview of the account abstraction landscape, kind of what's working, what's not. Then we'll talk a bit which kinds of users see the most value from this piece of technology and how should we approach the tool set we have, what's working best. Then we're going to picnic, trying to give a sense of product decisions, what worked, what didn't, and small tweaks we made that make a difference. We're going through some of our user research and then going after, like, what are problems we see today? How can we solve that? How is the roadmap for the future? Like, I think we learned some cool stuff, but there's a lot of work ahead. So on the account abstraction landscape, if I would summarize account abstraction in one slide, you basically get a bunch of different problems like authentication, gas, like teaching users that you need a specific token for a specific chain to pay for gas. I think that there's no way this thing will be mass adopted just because of that one thing. Seed phrase is another thing that doesn't make sense to have to teach someone about a seed phrase to be able to onboard it, right? People should just be able to use it and the thing should work. Same as chains and transactions, a proof swap, anything that gets a bit more complex is just too much work if you're talking with the average person. So account abstraction, it solves most of these issues or at least give you more options on things you can build. And the result of that, it should be usable by Web2 users. So account abstraction, I think that a fundamental promise is like you can break the bubble and you can make those products usable by a much wider audience. And it's growing. Like if you see the charts, most dashboards, you see things like this. So this is number of deployed smart accounts across the main EVM chains. Trend looks good. Not exponential, but still healthy. And it's kind of obvious it should grow. Like, you're making it easier to use. So if something is easier, people just use it more. But if you do some more detailed analysis and you look at individual projects, the trend is not usually that just make it easier and things will go. This kind of pattern that you see like spikes and things dying down, this is very, very common across this space. Not sure who has seen that, but this is retention rates for the whole account abstraction space. So like this is like 2% retention for the first week. The best one we have here in all ones is like 12%, which is reasonable. But most are four or three. It's really, really low retention. So for every 25 users you get, only one returns next week to do something. So there is this problem with repeat usage. And the main thing here is like, why this thing happening, right? Like why people are not coming back to do transactions next week? Why the value is not being sustained? And just to get a sense, who here is using account abstraction wallets on like a monthly basis? Okay, so five people. And who here is using wallets on a monthly basis? Yeah. And this is kind of my point. I think I don't use... I use Picnic, but it's a different use case. I don't even think about Picnic as a wallet. But account abstraction wallets are not a very good solution for Web3 native users. I think they are worse than YoA wallets for Web3 natives. The first one is you pay more gas. There is gas overhead to be able to use account abstraction wallets. Second thing is limited use cases. So if you want to deploy in a new chain, if the wallet provider you're using doesn't support that chain, like if you want to do that manually, maybe you can't. Maybe you can, but you need to be a developer. So it's this weird thing that you can't do whatever you used to do. You don't have that much freedom. There are steps that won't work with that. If you deploy it on zk-sync, you have a different address. So there's all those weird quirks that, yeah, it is worse in terms of use cases. And you also need to learn it. So like I spend a lot of time around smart contract wallets. I know how it works. I trust it. But if you talk even with the typical Web3 native user, there's a learning curve. Here is probably the most sophisticated piece of that cohort, so people get it, how it works. But a lot of people is, why does it have a different address? Where is the seed phrase of this Mark-Ontrick wallet? And all those questions that come up that you need to educate, and that is a learning curve. And in the end, you pay more gas, you have less stuff to do, but you already learned how to operate a EOA wallet. So why are you going through all the hassle? You don't really need a smart contract wallet. You can do it without. So that's the thing that's like trying to put account abstraction as is in the hands of WebTree natives is not the path. I think 7702 changes a lot of that, but for now it hasn't worked. I don't know of any examples. And the examples I know that are working well are in a different direction. And the question is, okay, so Web3 natives not really working. So for who this technology adds value? And the thing we have been seeing is, like, people that cannot do it otherwise. So if you have someone that wants to do something on-chain, be it because you want a product that's non-custodial or want to buy a token that's not available anywhere, this person doesn't really know how to use an EOA, then you can deliver a lot of value because it's not a matter of being easier than the alternative. It's being a matter of, can I do it or can I not do it? And the person doesn't have the option of the EOA. And getting specific use cases, I think it's an important one as well. Polymarket I would put in that bucket, WorldCoin in that bucket. That's people that just like more... If you abstract the UX and you make a very simple flow very easy, that seems to work much better than trying to do everything. And the thing that you're seeing, like this is the kind of chart I showed, like you get spikes, you get web users, you get a lot of hype, but it's very hard to sustain it. The one on the right is Picnic. That's weekly transactions for the past few months. You don't see necessarily the big spikes, but retention is really good. So people come, they use the product, they like it, and they keep using it. And the idea here now is to give you a sense on what are the product decisions that we made and why the thing is working. But now, like any questions so far? How is that going? Just okay? I'll keep pushing and then we can do Q&A later or because they already went to like 20 slides and yeah let's see how that goes so the things like the idea here is go to the things we I think we did right and what were the key learnings we had in each of the steps so first thing I think one of the ways I describe Picnic is that it's a centralized exchange-like experience or a FinTech-like experience, but everything running on-chain. And I think focusing on that very core use case, it has been... It makes the product much easier to build. It makes the product much easier to build it makes the product much easier to understand for users the simple use case you come to buy and sell tokens basically and it makes it easier to use it's just a single flow and one thing I think we spent a lot of time is just like okay account abstraction is really nice. I think it's a fundamental building block if we want to deliver like DeFi for a wider audience for Web 2 users. But what else beyond that do we need to build? So what other building blocks are key and critical? And I'll give you an overview of the whole product. This we see here is basically like the main pieces of the flow. I'll get a bit more in detail. But basically, you can sign in with Google. You can sign in via email. If you want to connect with an external wallet, you can as well. Once you sign in, one of the critical things we spent a lot of time thinking about is how to integrate on-chain with off-chain. This thing here, this is IBAN. IBAN, que é basicamente um banco de bancos europeu. Então, a segunda linha é um número de banco de bancos. Se você transferir dinheiro para aquele IBAN, isso é vivo na Europa. Nós estamos principalmente no Brasil, mas também estamos presente na Europa, e eu acho que esse é um exemplo melhor, porque é um banco de bancos de sua própria cadeia, essa é a abstracção que eu estou fazendo. Se eu transferir euros to that IBAN, euro stable coins are minted in the wallet as soon as the money arrives. And if you transfer 100 euros, you receive like 100 euros in your wallet and you can spend from there. And this is like the swapful. So let's say I transfer 25 euros to that IBAN. Now I have 25 euros in my wallet. I can buy ARB with that. So that's kind of the flow is very similar to what you would get in a fintech, very similar to what you get in a centralized exchange. But that's all happening on-chain with pretty, pretty abstracted UX, and I'll go by every one of the components of that flow. So it's basically authentication on and off-ramp, gas, chains, transactions, and a few other tricks. On the authentication part, you basically have Google email and external wallet. A very peculiar thing we learned, like we have been iterating with this sort of product for almost two years. We didn't start with connect with the external wallet. That's something we added much later. And it's really interesting, at least on our perception, that just giving the ability for the user to connect with an external wallet, connect with MetaMask or anything else, most users are not using that. That is fairly complicated and not really the point of what we're doing, but it helped increase trust a lot. Maybe not necessarily with the end user, but for example, content creators and YouTubers and people that really helped us grow, they trusted this a lot more when we added the option to connect with MetaMask. Because now it's much easier to understand that things are actually happening on chain. And this is actually non-custodial. So that was a small detail in the user flow that makes a lot of difference in how you market it, and how you access users, and how you can distribute it. Some other thing that I think it's mentioned. Was adding the external wallet layer, was adding the external wallet integration part of your effort to onboard Web3 natives? Because you mentioned somewhere in the beginning of your talk that the user is, he's not a Web3 native when it comes to A. It's somebody who is outside, right? So what made you feel this was important? People ask for it. Like, a few users ask for the external wallet option. So that's why we added it. Like, people are asking quite a few, let's implement that. But it's like 15% of our users use external wallets right now. So it is significant. But in my perception, the biggest gain we got from the external wallet was the trust. That people see you can connect with an external wallet, so they understand quicker it is non-custodial. Yeah. But... If you see Web2 users then switch to this external wallet thing, is there an existing cohort of users who... If we're seeing Web2 users migrate to external wallets after they start using with email, we have not been monitoring that. I don't think it's happening that much. Still, people use email logins still a lot. So we're much more in the path to improving email login and getting a better security. Could you get a microphone there? Can I get some help with the mics? Titus. But yeah, the external wallet, I think, is mostly good for trust. So I'm curious, did you build your own custom auth or did you use something off the shelf? We're using Magic Link. Magic Link. Yes. Yeah, but that's just for the email, right? Or is it also Google authentication? We have custom auth for both. For both. Okay. Gotcha. Gotcha. And the external wallet is a separate thing? Yeah, it's separate. The external wallet, it's directly, I think it's Connecticut connected with Wagme. Oh, Wagme wallet, yeah. All right, thank you. Okay. Yeah, and one interesting learning we had on this auth piece is that getting something that is more centralized but is more reliable is much better than more decentralized and less reliable. If you're having trouble with 0.3% of login cases, which we have with a different provider, that's a really bad thing. This thing cannot break. Never. It's very important to be super, super robust. So now, which I think is probably one of the most interesting pieces, is the integration with Fiat, the off-chain, on-chain integration. We were doing DeFi native stuff before, and we worked with a French protocol with Jarvis. They did a lot of exotic stable coins, so they did Brazilian RAL stable coin, Japanese Yen stable coin. And spending time there with the founder made me realize that stable coins, like local stable coins, are the best possible solution for on and off ramp. That's for mainly two reasons. First is you need it off account. So people in Europe are already used to thinking in euros. So all your balances are in euros. Like Brazil is the same. Everything is in reais. And in many, are in euros, like Brazil is the same, everything is in reals, and in many, many countries it's like that. So if you're funding an account, you're already thinking your currency, better to keep that way, right? It's just easier, less cognitive overload, things just work. And one really cool thing that I don't see a lot of people talking about. Stablecoins make money just sitting on the money. So they get the treasury yield. Like in Brazil, they get paid 10% a year just to hold reais. In Europe, it's like 3% or 4% a year, something on that range. So they have a revenue stream just by holding your euros and give you stablecoins. So they can incentivize on off ramps, so they can subsidize on and off ramps, which is what we have been seeing both in Europe and Brazil. We have free and very fast and very good on and off ramps, and that seems sustainable. We're not subsidizing it. That's the business model for the stable coins so it is both from the UX perspective as well as from the economics perspective the long term way to onboard users and it's free and instant, it's a really really good UX like you can go from bank, money or bank account in Brazil to money or bank account in Europe through Picnic in five minutes. And you can do the inverse as well. If you have a fast bank in Europe, that's usually the bottleneck. One cool thing we did, this was more recent, but across the app, we're not calling stablecoins by their name. So the Euro stablecoin we use is URI from Unirium, but we just call it Euros. For Reis, it's BRLA. We just called it Reis. We had quite a bit of feedback, especially on the less sophisticated side of users. It's like, okay, okay, I deposit and I got this weird thing called BRLA. What is that? So those small details, they matter a lot, especially when you go to less sophisticated users. So I think spending time there and improving small details matter a lot. One other thing that I think is worth mentioning is KYC and UX to be able to on-ramp. We had a different on-ramp provider before the ones we have right now. And it was quite bad KYC process. It was confusing, not very well polished. I think it's worth spending a lot of time in getting KYC works really good. Conversions stay a night. I think that one of the things that made us able to grow and see some repeat usage was getting the flows easy enough for people to onboard and start using and then be able to keep using. Gas abstraction, this is something I think very cool that we did. You can pay gas in whatever token the user has in its wallet. So here is dollars, it's USDT. But even if you have some random meme coin that is listed on Picnic, and our process for listing is basically if it's in CoinGecko, it's in Picnic, we just scrape that and list the tokens. We accept it. So if you have basically among like 5,000 tokens, you can use that to pay for gas. So it is just fees. You don't really need to teach the user that, oh, you need to get a separate token to pay for this transaction. The thing just works. Like you have Pepe or whatever, you pay gas and Pepe and it's all good. Any questions here? So you have Strack in exchange for them then? We actually hold the Pepe and sell it later. And we settle with the Paymaster. Yeah, and connect to the previous slide as well. Did you find that you needed any kind of regulatory approval to realize a product like this? Because it's, like, quite a lot of money handling at, like, 20,000-foot overview. I'm just curious how the regulators saw it. Yeah, we see ourselves as a tech provider, mostly. All the things we do, you could do without us for the most part. Yeah. And so it's still not very clear what we need and what we don't. At some point, I think it will probably get regulated. Now I think there is a lot of grace on. It's still 100% non-custodial. So there's still an opportunity for us, like, being non-custodial and not being regulated. If we were to hold users' money, perhaps we should. Yeah, the regulation applies to the stableholder, to the stablecoin issuer. Yeah, exactly, yeah. Yeah, and is that a barrier to adopt USDC, for example? Is that a barrier to adopt USDC, for example? Is that a barrier to adopt USDC as well? Because I saw you on Euro and Real, but... We could adopt USDC, but the thing is that Circle does not provide a dedicated ramping service like Monerium for end users. They have, as far as I know, they only have like ramping for institutional institutions. So if you want to do like a retail solution, we would need to find another provider that could do the KYC. Yeah, like a Moonpay or something like that on Rampy. Yeah, and they would have a fee on it. It would not be probably one-to-one. Makes sense. Thank you. So, like, the relationship with the stablecoin is directly with the user. So the user transfers money to the stablecoin provider, and the stablecoin provider mints directly in the user wallet. It's not like, yeah. The event that we see over there, who's it from? Like you generate one for each user? The I-band we see here, it's one for each wallet. And that relationship is directly from the user with the stablecoin provider. The stablecoin provider has an e-money license in Europe. And then they provide the I-band to the user with the stablecoin provider. The stablecoin provider has an e-money license in Europe, and then they provide the IPAN to the user. Yeah, sure. It gets created as soon as the KYC is approved. Sometimes it's, like, within a minute to three. Sometimes it takes a bit more. I was curious to know if the KYC was made with some sub or do you use any other third-party provider? KYC is always handled by the partners. So the Brazilian Real stablecoin has its KYC flow. The Euro stablecoin has its KYC flow, the Euro stablecoin has its KYC flow. And one of the problems we're getting is like how to make those people talk together because it's quite bad. UX to make your user KYC for every product, especially with RWAs and things like that, that can be really painful. So yeah, those solutions of like reusing KYC, sharing KYC, that's really valuable. Just want to add one thing there. Because there are so many things that we need to do to improve as a company, as Ethereum and crypto as a whole. And this KYC sharing scheme is one of them. How are we going to manage identity? And if we think, like, this is just one use case of stable coin on off ramping, but then you can plug in, for example, debit cards. And if the debit card provider is different from their own ramping solution, will you need to do another KYC for the debit card? It doesn't make sense. So, we need to find a solution where like KYC shared, it would still decentralize and everything. It's a big problem. Yeah. Just a UX thing that we found has been really irritating is for stablecoins because they always, they're not, they don't't exactly peg do the users get confused as to why they even see that second amount and like what that means and how do you explain that to them? It is indeed a problem. We solved it somehow by the stable coins that have direct on and off ramping we we just packed it to the value. Because they are packed, in fact. Because if you have 100 euroes and you want to withdraw 100 euroes, you're going to get 100 euros. So they are essentially packed. They might not value the same in dollar terms when you swap them into a DEX, for example. But that's another issue. That's another problem that you mix with slippage, we've mixed with liquidity, but we just packed it into the UI and it works. Cool. Thank you. Okay. Nice. Happy to see some questions. Thank you. nice happy to see some questions thank you so it's a two-part question the first part being that can you go yeah okay that's like yeah so now that you see here it's a dollar in USDT on the OP network and the resulting currency is OP on the OP network. Now if I want to change and take it on a different network, so does it act like a swap or a bridge or both? Male Speaker 2 in audience 2nd question That's Male Speaker 2nd question Both, like look there, yeah, it's the same. Okay. So that's one part of the question. And the second part of the question is that can I switch from one currency like the euro to maybe a dollar or to maybe an Indian rupee or whatever is available at that moment? Can I switch between two fiats?",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/13A9C80P0zlw8I1eLl0T6fZOyBk0YNwjGY4iHKV2trZs",
      "resources_slides": "https://drive.google.com/file/d/1qgQun5VXrikpxicltaIEq4RBCuqWZ2lt/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "buidlguidl-ctf-capture-the-flag-game",
      "sourceId": "DNNSNX",
      "title": "BuidlGuidl CTF - Capture the Flag game",
      "description": "**Capture the Flag for Ethereum Developers (2:30 - 5:30 pm)**\r\n\r\nShow off your Solidity skills in BuidlGuidl's CTF! \r\n\r\nTackle 10+ challenges across all skill levels (beginner to advanced), and race to grab the most flags for a chance to win ETH, swag, and fully synced ETH nodes!\r\n\r\nOpen to all skill levels — join the fun! (solo or teaming up with frens)",
      "track": "[CLS] Learn How To Build On Ethereum & Capture the Flag Game, by BuidlGuidl",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Live Coding,Open Source Software,Public good",
      "keywords": "Capture,the,Flag",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "3f98b4b5a33951c995d1bd8ab5ebe10dec689e24b1a89d174020876ced3d25e3",
      "sources_youtubeId": "hHVGYhRsEO0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1xr4Ix-QggZF86XTqtv9VesZjbkocLKoff6BZ8MaCOVM",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "keynote-the-universal-cryptographic-adapter",
      "sourceId": "R9X9ZG",
      "title": "Keynote: The Universal Cryptographic Adapter",
      "description": "The \"secret\" third affordance of Zero-Knowledge proof after 1) Privacy and 2) Succinctness is Interoperability. ZK enables us to continuously refactor data, aggregate it from different sources, and transforming it without loosing its integrity.\r\nStarting with the Zupass project, and now with the broader adoption of the POD and GPC format, 0xPARC has been exploring using ZK for data sovereignty and creating more interoperable data ecosystem. We will cover our learnings and progress in this talk.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "Not financial,Permissionless,ZKP",
      "keywords": "None",
      "duration": 1186,
      "language": "en",
      "sources_swarmHash": "0fed334b4f3cf12a05223fe8bfbc21fcefcf7147b55c4d9a16be9d98a8f7200c",
      "sources_youtubeId": "Qob-AsX0mxY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67345eef9dbb7a90e15752b1",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T08:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1DIuykDDTe3d5hT9NzR3bnBAg1TQAoLS7n9JoGbIFyAg",
      "resources_slides": "https://drive.google.com/file/d/1xh1CIkzROhhehPBlIRdudtMvCmaaz7SP/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "keynote-unifying-ethereum-through-intents-and-erc-7683",
      "sourceId": "WHYZCD",
      "title": "Keynote: Unifying Ethereum Through Intents and ERC-7683",
      "description": "Ethereum has scaled with a diverse ecosystem of L2s—but this created a new challenge: how can this fragmented landscape of potentially millions of rollups feel like a **unified Ethereum**? In this talk, I’ll discuss how intent-based architectures—and new standards like ERC-7683—can help unify Ethereum while maintaining the benefits of Ethereum’s rollup centric architecture.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": true,
      "doNotRecord": false,
      "tags": "Cross-L2,UI/UX,Intents,interoperability,erc-7683,Cross-L2,Intents,UI/UX",
      "keywords": "ERC-7683,Interoperability",
      "duration": 1543,
      "language": "en",
      "sources_swarmHash": "2191e709cbfc256b9d47ddae48814a876c2d304b09cbc7c61b9fe25875504fda",
      "sources_youtubeId": "jjBxfIsTrLE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67345cf19dbb7a90e14fb5ef",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T08:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1HFUKCdHq2CnGEM-2BvyaHipzeUf2aeP32TKRHPxKnWY",
      "resources_slides": "https://drive.google.com/file/d/1GcTGXXeQJC-xSRme_K5zxzlB7te95zzO/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "peerdas-in-grandine",
      "sourceId": "YLLNEW",
      "title": "PeerDAS in Grandine",
      "description": "EPF project presentation on improving PeerDAS implementation in Grandine",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,DAS,Data Availability,EIP4844",
      "keywords": "",
      "duration": 714,
      "language": "en",
      "sources_swarmHash": "b9c24999fe0efb631c31d2e6218f40dd82f654c4e6d6511549ece5006b3b141d",
      "sources_youtubeId": "Z0mXYd2UAkM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67345a769dbb7a90e13568c9",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T07:45:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Iiq2VFXcakCQ4LfaHpWejg013im1G0mu9_E24tzaarE",
      "resources_slides": "https://drive.google.com/file/d/18WsNmmLQ0SQjS394K8Y-1Ux2RiQVR4Qi/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "protec-and-attac-programmatic-execution-layer-consensus-tests",
      "sourceId": "GZBP8A",
      "title": "Protec and Attac: Programmatic Execution Layer Consensus Tests",
      "description": "We'll give an overview of Ethereum Execution Spec Tests (EEST), the new Python framework used since Shanghai to generate test vectors for Ethereum Virtual Machine (EVM) implementations. By generating tests programmatically this modular framework allows test cases to be readily parametrized and dynamically executed against clients on live networks. It tightly integrates with the Ethereum Execution Layer Specification (EELS) and could potentially be used across the L2 EVM ecosystem.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,EVM-equivalent,Testing,pytest,Core Protocol,EVM-equivalent,Testing",
      "keywords": "Python,pytest",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "5a5ad86c8d093f50adfe6bd4116a642bdc62e1c5b5634d61f35d653a32f5e250",
      "sources_youtubeId": "4OF-TJ9nB4I",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T08:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1H_C3_bcxmpSTe9V9Z7CXA4jdQBIVdf6U0HYmPOFadS0",
      "resources_slides": "https://drive.google.com/file/d/10jgTJmeBRAW0McIiqqpsKrZ8DAAd9dBP/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "rlnv2-enhanced-spam-protection-for-all-peer-to-peer-networks",
      "sourceId": "ZFJXFP",
      "title": "RLNv2: enhanced spam protection for all peer-to-peer networks",
      "description": "RLN is a protocol designed to prevent DoS attacks in a privacy-preserving manner. It uses zero-knowledge proof to limit the number of actions a user can take. In a p2p network, it can be used to limit messages sent over a period of time by one sender. RLN’s latest upgrade limits to N (instead of 1) messages per epoch. Also, the Merkle tree is now built on-chain, greatly improving the UX.\r\n\r\nCome learn how to use an implementation of RLNv2 to DoS protect a peer-to-peer network.",
      "track": "Cypherpunk & Privacy",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Censorship Resistance,Decentralization,Zero-Knowledge,network,peer-to-peer,Censorship Resistance,Decentralization,Privacy,Zero-Knowledge",
      "keywords": "Anonymity,peer-to-peer networks",
      "duration": 3144,
      "language": "en",
      "sources_swarmHash": "6ea0528ba8f1725dea3e57b64456bbc3b2119584f9f8c6c02f8558bd98ae88e5",
      "sources_youtubeId": "EH6zUu6AzlQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1ab7Dm_NLmbdVl-rQdbpavpCT-nXILHwBPKMRvciyvFQ",
      "resources_slides": "https://drive.google.com/file/d/17x2lMNSNLoeu0Q0z9SNHtLI-zBk_DdIB/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "top-hacks-since-devcon-vi-what-did-we-learn",
      "sourceId": "FCWCBG",
      "title": "Top Hacks since Devcon VI: what did we learn?",
      "description": "Discover the most daring blockchain hacks of '22-'24 and how to defend against them. Join Mudit Gupta, CISO of Polygon, and Matthias Egli from ChainSecurity for an analysis of tactics and vulnerabilities, and gain valuable insights to stay ahead of the game. And stay tuned for a prominent anon surprise guest!",
      "track": "Security",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Hacks,Use Cases,war,room,Hacks,Security,Use Cases",
      "keywords": "Learnings,War Rooms",
      "duration": 4878,
      "language": "en",
      "sources_swarmHash": "ff29aee71f1c81c6d57f6b49d5c3bcca90c840e14321a24a57645153e3b4b044",
      "sources_youtubeId": "MQjw2ffttzw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673473899dbb7a90e121fd13",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ic4xQqu3tPIGtBkRi-td-CDrhLlNwW9GBWn1_dYegTE",
      "resources_slides": "https://drive.google.com/file/d/1kzSPIKpTuasqzSKDl8GLJyGzv8YrglUN/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "peerdas-metrics-specifications",
      "sourceId": "UYPWVK",
      "title": "PeerDAS metrics specifications",
      "description": "The PeerDAS Metrics Specifications help make testing more efficient and straightforward by creating standard metrics for Consensus clients. With a unified Grafana dashboard, teams can monitor performance in real-time, compare client data side by side, and quickly spot issues. This approach makes troubleshooting faster, supports research, and encourages teamwork, helping strengthen the Ethereum ecosystem and improve scalability.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Testing,Tooling",
      "keywords": "DevOps",
      "duration": 706,
      "language": "en",
      "sources_swarmHash": "710e662c1b81b646ab2603eb50ff3b4385f2f967194e6fd32bf5901d839045b0",
      "sources_youtubeId": "BRzI_IyU5SU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347b4e9dbb7a90e15bbdc6",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:45:00.000Z",
      "slot_end": "2024-11-13T08:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1K_w0rS7tGijHA1ThVt6Mzpg7shFMcaOpglVD01dIMPQ",
      "resources_slides": "https://drive.google.com/file/d/1XflmzTS8Jr4PnTuD2OfUcZc9FmJgpUuV/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "data-driven-tokenomics-analyzing-incentives-in-action",
      "sourceId": "LPDCMK",
      "title": "Data-Driven Tokenomics: Analyzing Incentives in Action",
      "description": "This session explores using empirical data to analyse the health of tokenomics, monitoring how incentives impact user behaviours. This is important as we need to start shifting the conversation from pure simulations to data analytics and update token incentives in the same vein.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "macro/micro economics,Tokenomics,User Research",
      "keywords": "data analytics,growth analytics,user behaviour",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "fd5d936b101d87ae0481fc107d0991452d257551ca224e89a95c302747151d84",
      "sources_youtubeId": "J7rIcyjJc3o",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:50:00.000Z",
      "slot_end": "2024-11-13T08:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1a7L3Uq6GwbOc7abUc_joXIpX0LM57pFSIKpUmrjx4rU",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "how-to-destroy-a-network-offboarding-the-mainstream",
      "sourceId": "XNCFRL",
      "title": "How To Destroy A Network: Offboarding The Mainstream",
      "description": "Crafting Ethereum into a setting (both technically and reputationally) where The Institutions feel comfortable participating in it at scale has been the life work of hundreds of people over the last nine years. And yet, for our success, many feel that the victory has come at a cost too heavy to bear: our losing focus as to why we built the global computer in the first place. If you feel the same way, join me for a brief exploration of what would need to happen for us to cut the cord.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Network State,Privacy,Anonymity,Digital Sovereignty,value,Anonymity,Digital Sovereignty,Network State,Privacy",
      "keywords": "Values",
      "duration": 1441,
      "language": "en",
      "sources_swarmHash": "5562c301da48c32f85eb28983b8b07f898e24606054da60482c5cd878fcf3584",
      "sources_youtubeId": "axoRHSVU9KU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673469329dbb7a90e18ab8ef",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:50:00.000Z",
      "slot_end": "2024-11-13T08:20:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1mVbPl6HPZouYDklCGe84dKqjwtSkE7VTKOYNdWU6URc",
      "resources_slides": "https://drive.google.com/file/d/1suVsjiBN2otsA7oL_GggfpGKLOGBIHsJ/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "zkpassport-private-unforgeable-identity",
      "sourceId": "K3GWST",
      "title": "ZKpassport: Private Unforgeable Identity",
      "description": "This talk presents ZKpassport, an identity verification solution integrating zero-knowledge proofs with ePassports to achieve privacy-preserving and unforgeable government-attested digital identities. We will delve into the technical architecture, implementation challenges, and practical applications. Attendees will gain insights into the development process, benefits, and potential uses of this technology in enhancing digital identity privacy and security.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Identity,Zero-Knowledge,noir,Identity,Privacy,Zero-Knowledge",
      "keywords": "ZK,NFC,Noir,PLONK",
      "duration": 1189,
      "language": "en",
      "sources_swarmHash": "8ff112bba1682d13788042e5ab586ab285935e607ec2031734b6ed5acbad29ea",
      "sources_youtubeId": "W6C-duDEiOU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T07:50:00.000Z",
      "slot_end": "2024-11-13T08:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1oOW6cu6Z74Nvx5lSpva4kFP8hggWPnZdL6MvVt9Hc9U",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "are-blobs-good-for-ethereum",
      "sourceId": "K7ZGQ3",
      "title": "Are blobs good for Ethereum?",
      "description": "Join Anthony Sassano as he moderates a debate between some of the sharpest thinkers in the Ethereum ecosystem when it comes to all things data availability, blobs, fee revenue, and ETH value accrual.",
      "track": "Cryptoeconomics",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": true,
      "doNotRecord": true,
      "tags": "Data Availability,Transaction fees mechanisms",
      "keywords": "ETH value accrual,ETH as money",
      "duration": 3292,
      "language": "en",
      "sources_swarmHash": "b2e22058de3d7ab81e710169be4bd1beb3c00abf2cff55f6ff66114aa92a94eb",
      "sources_youtubeId": "ZqCI252vMYI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346b5f9dbb7a90e1b17110",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1SAdKiPawHNzsIk6mSX5zU6XFK1nPnDm393UMkF2hq-w",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "circle-for-all",
      "sourceId": "VPNPYY",
      "title": "Circle for all",
      "description": "By master Aoei\r\n- Self-Tune\r\n- Circle movement bonding activities\r\n- Sharing and Reflecting\r\n\r\nNov 13 15:00 - 15:45",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:45:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/1hrC4BF-BEAqbZu7xLGF7HSCIT40WN7UKvfkGD14lwho",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "i-read-every-single-1990s-cypherpunk-email-heres-what-you-should-know",
      "sourceId": "V8FHZL",
      "title": "I read every single 1990s Cypherpunk email. Here's what you should know.",
      "description": "What would Hal Finney, Tim May, David Chaum, and other cypherpunks think about the current state of Ethereum, cryptography, privacy, and trusted hardware? I read every single 1990s cypherpunk email (thousands) to learn more the original movement. I gathered the most interesting and relevant cypherpunk emails, and put them together to make this best-of-the-best cypherpunk presentation.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Permissionless,Free Speech,Censorship Resistance,cypherpunk,Censorship Resistance,Free Speech,Permissionless",
      "keywords": "Cypherpunk",
      "duration": 1437,
      "language": "en",
      "sources_swarmHash": "fae41435b03f33e46404a9520b791f54ab7e3b8d4c4c283b8ed9535edcef438d",
      "sources_youtubeId": "4DtB96PlAtQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346f1e9dbb7a90e1e41835",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1GfxZnDdh1oYJ0Cmi0EqvJ6n5WY4Rvok97rq_GW9HmJA",
      "resources_slides": "https://drive.google.com/file/d/1GeYZ6UhWREC-3HLhVaL7MfPH7Xul4fQP/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "native-implementation-of-ephemery-testnet-on-besu-and-teku-client-pairs",
      "sourceId": "EAABPS",
      "title": "Native Implementation of Ephemery Testnet on Besu & Teku Client Pairs",
      "description": "This presentation covers the work done to enable native support for Ephemery on Teku and Besu clients. Ephemery is a short-lived, resettable testnet designed to automatically revert to its genesis state after a defined period, making it ideal for intensive, short-term testing without concerns over issues like insufficient testnet funds, inactive validators, or state bloat that long-running testnets often face.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Developer Infrastructure,User Experience",
      "keywords": "Client Development,Teku,Besu,Ephemery",
      "duration": 847,
      "language": "en",
      "sources_swarmHash": "4be7f94081d144df637bc9d6a640778b80c300ec6a7e9d2689a3fc17055bfd24",
      "sources_youtubeId": "Rpg9irPn3jI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347bee9dbb7a90e163940f",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/18GeJQc_Z-ecQcsBsDbtRfawOuvBvptEDPby_7BDdqUU",
      "resources_slides": "https://drive.google.com/file/d/1cgYKFUdZHK8ss3hDYnFg-G6oL5j3F0Fn/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "scaling-autonomous-worlds-building-the-foundations-and-sewers-for-millions-of-inhabitants",
      "sourceId": "QPAXL7",
      "title": "Scaling autonomous worlds - building the foundations… and sewers for millions of inhabitants",
      "description": "One tends to think of Ethereum scaling in financial terms—how many transactions per second? What’s the TVL? How much liquidity?\r\n\r\nBut in a possible future where Ethereum applications extend beyond finance, into areas like autonomous worlds, games, and social, what does scaling look like and what challenges await?\r\n\r\nJoin us as we explore challenges, solutions, and open questions in this space—how do we bring latency down despite seconds-long block time? Could we shard an app across multiple chains?",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Cross-L2,Autonomous World,cross-chain,Autonomous World,Cross-L2,Layer 2s",
      "keywords": "Cross-chain",
      "duration": 1310,
      "language": "en",
      "sources_swarmHash": "a23391ef57a705e96c256c35383615ce8ca6f48030865d8df316b6c794d67ba3",
      "sources_youtubeId": "uJ2nhPDvjLw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346ed49dbb7a90e1e1b1a2",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67346ed49dbb7a90e1e1b1a2.vtt",
      "transcript_text": " All right. My name is Tidat. I'm a software engineer at Lattice. And yeah, I'm about to talk about bringing you on our journey, basically building the best infrastructure for autonomous worlds and mod apps. basically building the best infrastructure for autonomous worlds and Mudd apps. Okay, so I will start by talking a little bit, bringing a little bit of context for those who don't know what Mudd is and what are autonomous worlds. I will then kind of describe some design patterns that we learned about while we were essentially solving problems for Mudd apps.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/11DTfplHre4QguicqcET5ubMdfycNHdyjo8Imn5A0lWc",
      "resources_slides": "https://drive.google.com/file/d/1Ku6W_lj1fSlmK07byI0Goh-r68o-uoLN/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "start-contributing-to-economic-protocol-development",
      "sourceId": "CEZPBS",
      "title": "Start contributing to economic protocol development",
      "description": "Protocol development needs more economists, yet many potential contributors do not know which problems are important to Ethereum protocol development. This talk bridges the gap for those interested in blockchain research who want to work on impactful problems. The talk will overview different economic research areas at the protocol level. Examples include an economic perspective on consensus systems, transaction fee mechanism design, and economic sides of current EIPs.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Economics,introduction,Core Protocol,Economics",
      "keywords": "Introduction",
      "duration": 423,
      "language": "en",
      "sources_swarmHash": "6341c1973358b364e4a0b42f702c81d66a466e72787bc14c72216143e19bfb17",
      "sources_youtubeId": "CaauVb5jcH8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1oT8-qF_kFLzRfy9StlucF5G7CCSCbwTrU3VGnmV4M-M",
      "resources_slides": "https://drive.google.com/file/d/1rREcVDVR1m5EXuLcfwseRIHwcLqWJrWB/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-supreme-ruler-of-the-world",
      "sourceId": "TLWWCW",
      "title": "The Supreme Ruler of the World",
      "description": "VK rules the world. ZK rules the world, too, like a straightedge wielded with eyes closed. Rulers rule in simple ways: by lining things up and by checking they're all in line. Bring your high school math to learn straightedges called SumCheck and SumCalc and begin to appreciate ZK in simple geometric terms. No moon math. We'll visit lines, cubes and polynomials, to see how they can be used to deduce and to generate, to check and to delegate.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Scalability,Validiums,Zero-Knowledge,sumcheck,Scalability,Validiums,Zero-Knowledge",
      "keywords": "sumcalc,sumcheck",
      "duration": 1477,
      "language": "en",
      "sources_swarmHash": "1022835086d9f6a0b9aaa7c7256f587089b8b04eaddf0f24409aae60c8908355",
      "sources_youtubeId": "xJNvd6hbCdE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346c7c9dbb7a90e1c29eff",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67346c7c9dbb7a90e1c29eff.vtt",
      "transcript_text": " Thank you so much for that nice introduction. I did actually go all out to try to make this the best title ever, or if not, it's at least a tribute. What I want to talk about today, just to give a level set, is Zero Knowledge, of course course this applied cryptography track it's intended and directed at people who have a little bit of knowledge about cryptography a little bit of knowledge about zero knowledge but are basically mostly developers interested in finding out why bother doing this and what is actually going on behind the scenes so without further ado I'm going to talk a little bit not too much metaphysical or mystical, but more about how do you get calculations done by powerful entities, and how do you trust that those results are actually true when they hand something to you. This is really the realm of zero knowledge. It's actually a bit broader, zero knowledge and verifiable knowledge. There's a lot of different properties that are important. They're really important to scaling things up on the blockchain, any kind of blockchain in general, and I'm sure many of you will have seen a lot of buzz around these topics as well. So what's the buzz about? How does this actually work? What is going on behind the scenes at some reasonable level? In general, I'll do a quick background. So zero-knowledge proofs or interactive proofs are ways for a computationally limited verifier, a client, to actually ask for the truth or falsity, ask for a claim to be made, and for somebody with a lot of power to give them a proof that that claim is true. So, for example, some of these problems might be complicated, like, hey, I know a Sudoku solution. And, you know, a 9x9 is not a big deal, but an nxn is actually an NP-complete problem to solve. So these are actually intensely complex problems that people work on and try to demonstrate the truth of. More practically, you might be looking at consensus algorithms and saying, hey, look, I can tell you that I checked 400,000 signatures, and they all checked out. You don't need to bother. So trust me. Well, in this setting, we don't really want to just trust people. We want to actually verify what's going on. So that's the setting. The scalability argument here is that one person is going to do the work, and everybody else can just sort of check after that one person did the work. Okay, so for example, what did the consensus say? I checked a bunch of signatures. You know, did they all pan out? What's the new state on maybe on my own chain or on somebody else's chain? That's a lot of work to do. If you try to do that work inside of a contract, it's going to be very expensive. If somebody can do that work for you outside of a contract and just prove it to you in a contract, that helps you scale. Because it's a much cheaper thing to check than it is to calculate. And so in general, these things help us scale with things like roll-ups with other chains, with exchanges, and so forth. Anytime you have a big calculation that you want to land just the result of on another chain, we'll use these kind of results. So, all right. There are two kind of very closely connected problems here. In order to motivate it, I mean, really, we're going to talk about proving a calculation here, which is the middle one. We'll look at a couple tools. One is called sum check. It's actually kind of a very old play on words. When you're sending a message from A to B, you do a check sum to make sure it wasn't actually munged in the middle. Sum check is a way of getting a calculation from A to B and making sure the calculation wasn't messed up, that the result that you get is really true. Related to this is something I'm calling sum calc, which is how do you delegate a problem to somebody else and get them to calculate it for you. You're a contract on chain. You ask somebody off chain to do the calculation. So very related math behind it. You know, and then we'll go into a little bit of the math and see what helps us scale these different algorithms. Okay. So what's really happening here, what is the supreme rule? I'll get to it more in a second. It is basically a trick that helps us look at very, very large calculations, essentially exponential-sized calculations, and use a moderate amount of checking to see whether they're true or false. You can scale this down a little bit. You can say maybe I have a human scale and step calculation, and I have a very, very tiny verifier that only uses logarithmically many steps. I won't get too far into the weeds about the complexity of the theory, but really if you wanna think about it concretely, you got a million step calculation",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1IP5PshRsU2LlH33ndPmkTGZJki3mzS-uZ3M-Yc5vD6o",
      "resources_slides": "https://drive.google.com/file/d/1jzTFK1TLrydMBGUwNpxJBQyWtxPkcsTv/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "using-reth-execution-extensions-for-next-generation-indexing",
      "sourceId": "YUFRTQ",
      "title": "Using Reth Execution Extensions for next generation indexing",
      "description": "Recently, Reth and Geth released the ExEx and live tracer features, respectively, which share similar functionalities. Both provide real-time, detailed access to chain and state events. As ExEx developers begin to persist this data and explore ways to make it accessible to users, new questions arise: how can we best serve this data to users, and what might the indexers of the future look like?",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Developer Infrastructure,Tooling,plugin,Developer Infrastructure,Layer 1,Tooling",
      "keywords": "client,plugin,indexer",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "e434c586141c1e57eb8d7a9a5d407ed187d97b43e104b724b1d5ada20df26dff",
      "sources_youtubeId": "GhEhzE9SFqY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1grvRBeTUC4cPjxwSFQPy6d3VmlJ6P3Y2_R99fgeourE",
      "resources_slides": "https://drive.google.com/file/d/1PliCCH-hA4rZOGxo9U4esC6V8VI6TOIQ/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "web3-poetry-jam",
      "sourceId": "V79DXK",
      "title": "Web3 Poetry Jam",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:00:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1XSH7eVjgLTTnQVBK8jxg0l8v8m1RayeW3qpih1FAFHY",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "exploring-auction-mechanisms-in-protocol-design",
      "sourceId": "WAKEL9",
      "title": "Exploring Auction Mechanisms in Protocol Design",
      "description": "Auction mechanisms are fascinating, and so are protocol designs. When you put both together, things get really interesting. In this talk, we'll dive into various auction mechanisms and see how they shape protocol design choices. We'll cover key aspects like the timing game, MEV burn, and participant trusts. Then we will look at case studies: Ethereum, Optimism, and Arbitrum. For each case, we'll conclude how protocol impacts auction or vice versa.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Economics,MEV,auction,Core Protocol,Economics,MEV",
      "keywords": "Auction",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "312aa12503f300c3636ada438111a3dccdf8d3fac350ae974219de7171eb5568",
      "sources_youtubeId": "rd7f7697yoY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:10:00.000Z",
      "slot_end": "2024-11-13T08:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1SW7qjLygGhslLlaFTINPgoKm5AVWrY8ItfsnIvPnUq4",
      "resources_slides": "https://drive.google.com/file/d/1nM63U0oMNehJSp9rBYhZSRDjjLiWBJE4/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "shadow-network-simulations",
      "sourceId": "H7HCJJ",
      "title": "Shadow Network Simulations",
      "description": "In my EPF project, I implemented Ethshadow, a configuration generator for simulating Ethereum networks using Shadow, and used it to research improvements to the current state of PeerDAS and to estimate the effects of IDONTWANT on node bandwidth. In this presentation, I will present my findings and make a case for testing using Ethshadow.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Layer 1,Testing",
      "keywords": "",
      "duration": 936,
      "language": "en",
      "sources_swarmHash": "02ccab671e4eb30a5b598c337aa17ffcc9a69a77816b79dcc9bd7f7834af3df6",
      "sources_youtubeId": "uVvbuK0dpeQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347cd49dbb7a90e16882fa",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:15:00.000Z",
      "slot_end": "2024-11-13T08:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/13dCJ8eFHfsvUgtv1Dz5mrPCKUF6Y5dXPwWu0wN0ixkY",
      "resources_slides": "https://drive.google.com/file/d/1_R2ZrugYt8vE__LbiUbCccitwR7Dk9zp/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "digital-pheromones-mpc-for-human-connection-and-coordination",
      "sourceId": "LMCG3V",
      "title": "Digital pheromones: MPC for human connection & coordination",
      "description": "Recent MPC research from Cursive and PSE enables a new concept called \"digital pheromones\": the ability to produce lightweight, privacy-preserving signals that people can use to coordinate safely and efficiently.\r\n\r\nThe primary result we will cover is Trinity, a new 2PC scheme with nearly ideal UX/DevX, built on the trio of PLONK, Garbled Circuits, and KZG Witness Encryption. We will do a live demo with attendees and explore what a future filled with digital pheromones will enable!",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "MPC,Privacy,Use cases of cryptography",
      "keywords": "",
      "duration": 1517,
      "language": "en",
      "sources_swarmHash": "1907282ffb309a0d8b977413439d8bf99c1a3372f0b59ab6607b011a0491ebce",
      "sources_youtubeId": "TY2ZWmR_UqM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:20:00.000Z",
      "slot_end": "2024-11-13T08:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1VlzRulp0j62UZdPbUEc2y_6-IxSsimLBL_t3kn0xprA",
      "resources_slides": "https://drive.google.com/file/d/1mxbzO7KuAwRpyJnV6QwNnfPLzsODzRRo/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "grapheneos-a-brief-introduction-to-private-and-secure-android",
      "sourceId": "QK3ZTL",
      "title": "GrapheneOS: a brief introduction to private and secure Android",
      "description": "Smartphones have become an essential part of our lives. The operating systems on smartphones act like a boundary layer between personal data and a plethora of untrusted code, but how easy is it to penetrate this boundary? We present GrapheneOS - the privacy and security-focused operating system developed as a non-profit open-source project. We will focus on some state-of-the-art GrapheneOS features such as low-level USB-C control, hardened memory allocator, and Sandboxed Google Play.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": true,
      "tags": "Privacy,Security,Mobile,android,Mobile,Privacy,Security",
      "keywords": "Android",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:20:00.000Z",
      "slot_end": "2024-11-13T08:50:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/105h0erRlmvaHWuoC8pgHHPTJmdK7CiGkTOcyb1Vs4Nw",
      "resources_slides": "https://drive.google.com/file/d/1TmNUJgwWoxHx2V1WXJIN5AsiCx7gNCHI/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "arbitrumdao-a-year-review-with-lessons-learnt-interesting-research-questions-and-future-direction",
      "sourceId": "ERUXWJ",
      "title": "ArbitrumDAO: A Year Review with Lessons Learnt, Interesting Research Questions & Future Direction",
      "description": "The ArbitrumDAO was born in March 2023.  To date, the DAO has voted on 280+ temperature checks (snapshot), 40+ on-chain proposals (tally) & allocated >400m ARB. We take this time to reflect on a few questions:\r\n\r\n- Why should an L2 care about governance with on-chain voting?\r\n- How is the DAO's organisational structure evolving over time?\r\n- What does a DAO like to vote on? \r\n- Can too much decentralization lead to oscillation? \r\n\r\nFinally, a list of interesting research questions for others to tackle.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,Governance,Decentralization,organisation,DAO,Decentralization,Governance",
      "keywords": "Organisations",
      "duration": 1627,
      "language": "en",
      "sources_swarmHash": "7c8f90a74abeecd314b6470f83bb4b75105cd78c5b2749fe25b1e20cc0f63c88",
      "sources_youtubeId": "DNHso6qvJ3M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346f9d9dbb7a90e1e914b8",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1feUrTp0VPOlKMoCAdlFDOUvfhWnTwVbESRqA1FvQsWQ",
      "resources_slides": "https://drive.google.com/file/d/1QrWYlNlliTMEwPswI6FqVLjyR0xk1N52/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "hardening-the-commons",
      "sourceId": "BMTVJK",
      "title": "Hardening the Commons",
      "description": "A hands-on workshop for those interested in strengthening the capture resistance and general survivability of commons under their stewardship. This session will be a sequence of guided small group discussions that will flesh out the levels of a <b>capability maturity model</b> for how a commons resource, whether it is a blockchain or a city, can be gradually \"hardened\" by developing and maturing capabilities at material, philosophical, skill, social, and mission levels.",
      "track": "Coordination",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "adoption,Censorship Resistance,Coordination,Solarpunk",
      "keywords": "Impact,Commons,Adoption",
      "duration": 10846,
      "language": "en",
      "sources_swarmHash": "1e514371335c0a4647504273d5c37330cd160b33866c5d01bbef14c88c145b82",
      "sources_youtubeId": "8pIq-LaP9x0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673492da9dbb7a90e1a406e2",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673492da9dbb7a90e1a406e2.vtt",
      "transcript_text": " Hi everybody, thanks for coming. Can we actually all please come into the center of the room since it's a smaller group? Can all the people who have been part of the Summer Protocols program raise their hands or just stand up? Stand up. All right. Do not be in one cluster. Please distribute yourself. I do not want to see two SOP people in the same breakout group. So SOP people, please spread yourselves out. All right. People trickling in. As you can see from my hat, it's my birthday today, so make this workshop a success. It's not just my birthday. It's a big one. It's 50. So how this workshop goes is going to be the predictor of the rest of my life. Please make it go well. Okay. All right. Welcome. This is the Hardening the Commons workshop. If you were at Tim's talk yesterday, you heard him raise the team. It's been part of, like, many of the talks here, the Cypherpunk history session. So a whole bunch of talks had these teams going. So this is a workshop, and it'll have some talking, but it's mainly a working workshop. And this is a research workshop. We are not here to teach you anything. You are here to help us do some research. So we have what we think is a good question. It's one of the most important questions. How to harden the commons is in our opinion one of the most important research questions and this has been validated by like two years of the Summer of Protocols program. And we're hoping that in the next three hours you guys will help us come up with some really good answers, right? And this will actually, oh sorry. Can you hold this? good answers, right? And this will actually... Oh, sorry. Okay. So this will actually help us shape the agenda for the 2025 program. So this is not just going to be like, you know, you do some exercises on paper and it then vanishes into the void and nothing else happens. So please, we will be collecting the workshop output. We will be analyzing it and we'll be trying to make use of it to shape the 2025 program. If you don't know what Summer Protocols is, go to summerprotocols.com or talk to the various program alumni who are scattered around the room. All right, let's see. Okay, oops. So the goal of the workshop is to try and put together everything we know and have been talking about for years in the Ethereum community. Capture and censorship resistance, decentralized governance, permissionless innovation. How do you put this all together in a big picture model and what we're calling a capability maturity model? Raise your hand if you've heard the term capability maturity model. One, two, three, four, five. maturity model, raise your hand if you've heard the term capability maturity model. One, two, three, four, five. So if this had been one of my traditional, you know, big corporation consulting gigs and I'd asked the question, 100% of the middle managers would have raised their hand because this is like standard LinkedIn boilerplate business management stuff. It can feel very like awkward and bureaucratic, but it's a very useful way to put lots of random thoughts together in a model. So this is a capability maturity model workshop. I'll show you a few examples to see what that means. And if we do a good job, it can serve as a roadmap for a lot of people trying to adapt the technologies we've been all working on. So harden the common. So two terms there. Harden is in two senses. One,. So harden the commons, so two terms there. Harden is in two senses. One as in harden technology against threats, like radiation hardening electronics for space missions. That's one sense of harden. The other sense of harden is the ability to make strong commitments about the future, and this is something Josh Stark will talk about in his provocation in like an hour or so. And the commons, of course, you've all heard the term commons, and we have a lot of examples in our mind. So, you know, Ethereum L1 is a commons. Open source projects are commons. Forests and lakes and rivers are commons. So we want to like think broadly about commons. And Trent is here. He'll be actually leading a breakout session on how to think about commons. And Trent is here. He'll be actually leading a breakout session on how to think about commons. But here are some examples to keep in mind. And I've put this icon on some of the slides, because for the workshopping pieces, it'll be helpful to have these lists handy. So take pictures of the slides where I have these sort of like prompting things available. OK? So these are some examples of commons. Don't worry too much about the definitions. Systems for producing or stewarding shared resources is a good enough working definition for our work here, okay? All right, how many of you have heard of Eleanor Ostrom? Okay, this is impressive, like more than half the room. Nobel Prize winner, did some amazing work on how commons are built and stewarded. These are her famous principles for stewarding commons. There's a little bit of a cult or religion aspect around Ustrom, so don't take this as gospel, but take it as a thought starter. They're good ideas, but feel free to contradict and challenge them, okay? So again, another picture-taking slide. We do have a shared G drive where hopefully you'll dump your final CMM model. So you'll be working on paper. You'll be, like, making revisions of the model that you're working on. But the final one, take a picture and dump it in this folder. If you have trouble, just hand your paper model to Timber. Timber, are you here? Raise your hand. Can you stand up and show your face to everybody? So Timber is going to be running logistics in the room. Okay, so that's the housekeeping done. So the agenda is, I'll give you a quick introduction to what CMMs are. Then we'll do a set of five alternating provocations and breakouts. So short, like seven-minute lightning talk type thing, followed by 17 minutes where you do one pass through the CMM, then another talk and so on. So we'll do that. Halfway through, we'll take a little bit of a break, and then we'll come back, do some sharebacks, broad room comments, and then wrap up. So standard workshop structure, but hopefully the content is what will make this. Okay? Okay. So short TLDR version of hardening the comments problem, right? How many of you are seeing this cartoon for the first time? Raise your hands. Okay. So most of you have seen this cartoon, right? So you don't want to be too idealistic. We are not writing science fiction. We are not writing weird time machine stories. We are being realistic. So any speculation we do has to be kind of like plausible and realistic, which means, yeah, don't get into like weird fantasy scenario about cryptos. Keep the, you know, $5 wrench condition in mind. That's what it means to harden a commons against attacks. You should be able to defend against $5 wrench attacks. Okay. What's a CMM? It's a well-known model that management consultants like me and, you know, career bureaucrats use to just think about how organizations acquire and learn capabilities by rising through several levels over several years. So we need to adapt it a little bit to apply to commons and open kind of systems, but I think it works. And yes, it can be LinkedIn middle manager bullshit if it's done poorly, but I think this room can do it well, okay? Because remember, it's my birthday. You have to do it well as a gift to me. Okay, so here's like a classic. If you Google CMM, this is the kind of diagram you'll find. It's a pyramid with five levels. You start out with an initial condition of skills where maybe there's random skills. People have like crappy, disorganized ways of doing things, like all of us using Web3 wallets. I would say we are in this initial disorganized level. Managed is when there's a little bit more structure is coming in. Then defined, people have textbook definitions of what things are. Then people are beginning to measure things, so things are getting quantitative. Then finally, people are starting to optimize things, like this is the mature stage. So this is a generic CMM template. Here's another way to visualize one. You can think of it as like a ramp increasing in time. So this one happens to be about analytics. So this type of diagram was popular about 10 years ago when people were talking about analytics. So as you can see, the maturation is when your capabilities are very primitive, you can only do hindsight analytics. But as you progress through the level, you can do foresight. You can do insight, right? So this is evolving high-level capability based on the micro tool-level tactical capabilities. So you learn to use the tools better and better, and you go from being reactive in hindsight to predictive. Some organizations actually acquired capabilities this way. Other people talked a lot about analytics, but didn't actually get there. Here's another one. These are all things I pulled from Google Images, so they're real images that people presumably used in some organizations. This is an example of security capabilities, and this one is interesting because it visualizes it as growing capabilities on three vectors, technology, process, and people, so the red, blue, and green, and each of them evolves through multiple levels, right? So another good example. You've probably heard a lot about this one. If you've been following like the self-driving cars discourse, people talk about five levels of autonomy. What does it mean? Well, a little bit bureaucratic, a little bit engineering. It's a mix of both. It's technocratic. We'll have Sam doing a provocation later on challenging the ideology of technocracy. But there's a technocratic model of what self-driving should look like. Bureaucrats plus engineers putting it together. This one, actually, I just made half an hour ago. I was in a workshop that I was running called Web of Roots. And it was all about the problems of crypto adoption. And the breakout session I was in was about how there's a lack of tools in the ecosystem. And I made up this capability maturity model. So I have like five levels, AI is like a cloud around that. So you can get creative with this stuff. This one, Tim, do you want to quickly speak to this? Vitalik's roadmap is kind of like CMM. So Tim, come around and speak into the mic. Oh, we lost my slides. Okay. Yeah, so in Ethereum, like Venkat was saying, we don't have a lot of LinkedIn CMMs, and I think this is the closest we've come to as an ecosystem to mapping things out. And it's sort of like an implied CMM here, where if you think of the way Vitalik summarizes the Ethereum roadmap, 2020, it was just like, here's a bunch of stuff we have to do in the next 10 years and how they fit together. And not on this slide is the 2019 version, which is here's a bunch of stuff scattered over the internet that we have to do in the next 10 years. And it's sort of like a refinement in thinking over the years where a couple years after that, we were able to actually break this down in specific tracks, so like the merge, verge, surge, all those things. There's still sort of just like these rough pointers that we have that are quite low context. And then just before, DevCon and Vitalik actually put out a blog post for each of these tracks, sort of like going very deep into details around like, OK, what's the actual thing that needs to be done? Where is it at? What are the blockers and whatnot? And so you can see that we have this sort of evolving capability, or at least Vitalik has, and then guiding the ecosystem to articulate or understand what this entire set of things we have to do is and how they all relate to each other. Thanks, Jim. All right, so now you have a bunch of examples of CMMs to refer to. All these are real world ones, so don't worry too much about what an abstract definition of a CMM is. Keep these examples in mind. So how do you make a CMM? So one, pick a specific important commons. Two, pick a specific target social group that's trying to mature its capabilities, right? So if the commons you pick is like a lake and a community around it that wants to keep the lake ecosystem healthy, the lake is the commons and the people who are trying to keep it healthy is the social group. Try and define four to seven levels, don't go over seven, keep it actually close to four or five if you want because it gets weird. The traditional ones are initial, manage, defined, quantified, optimized, but feel free to get totally creative. Name each level and characterize it in terms of people, technologies, capabilities. Then this one is very important, number five. Try and define a test for each level. So if your CMM is about preserving a forest and you have a five-level hierarchy of how to preserve a forest and say, oh, Brazil is at level three, but Indonesia is at level two. What the hell does that mean? Define a test for level two and level three about how rainforest commons work, okay? Pick a visualization. So I gave you a few examples, pyramids, ramps, stacked blocks and stuff. So pick a good visualization. If you don't like the ones you have, make one up. But, you know, this is not an art workshop. This is like bureaucrat workshop, so you don't have to get super artistic with it. Think of this whole CMM exercise as, this is a collective learning curve from ad hoc, sort of like, kind of illiterate levels to very mature and refined capabilities. And it can go from very scattered skills, like everybody here uses Web3 wallets, I presume, but there's no coordination. It's very fragmented, but if it matures, presumably there'll be a wide conversation about which wallets are better. Maybe there already are, but there's not textbook versions of the discussion. So collective learning curve from ad hoc to mature, from scattered to integrated. And usually there's an element of codification and documentation too in building a CMM. And both the organizational structure and the people in it are learning as you go along, right? Forming procedural memories so that even if people join and leave, the organization or system still remembers, right? So if a whole bunch of people create a CMM for something and then half of them get new jobs and go and a new half comes in, the system should not break completely. The knowledge continuity should be there. So special challenges for us as kind of an open ecosystem that's decentralized and all these weird things. So CMMs are typically for top-down traditional organization where the CEO can say, hey, I'm appointing a bunch of people to impose this training program and all of you are going to go from level zero to level five. This is not that. So how do you do it in an open ecosystem context? CMMs rely on codification and documentation. In open systems, that typically happens in a much more ad hoc way. Like maybe some volunteers start documenting stuff. EIPs are a good example where it's somehow gelled into a very formal codification, but not in a corporate way. CMMs are usually technocratic, but can we actually not make this about LinkedIn middle managers and work with direct technological agency? And finally, CMMs often are about acquisition and installation of behaviors in big corporations because it's all theater. The CEO wants to have like a big program that they can brag about. Once that happens and then the big parties are done, people forget and skills are not actually maintained. So in an open ecosystem, actually the reverse emphasis is better. Like it's better to acquire and install a very limited capability that then endures for 10 years than to pretend that everybody's level five black belt and then one year later, nobody knows how to log on to the system. So you really should think in terms of maintenance of small skills. Okay, we are going to head into the first breakout session. So this is my provocation. Okay, so what is the bounding box for your chosen commons? We are going to head into the first breakout session. So this is my provocation. Okay. So what is the bounding box for your chosen commons, right? So what is the physical medium of the commons? So if you're talking forests, it's like trees and grasslands and the animals and the ecosystem. If it's software, it's the Git repository where all your software lives. So think of what the actual physical material medium is. Think about the neighbors of it, right? If your code is on GitHub like Ethereum's code is, one of your neighbors, whether you liked it or not, is Microsoft because Microsoft owns GitHub and so forth, right? So think about your neighbors. What is the nature of the boundaries and the threat and exposure you have across those boundaries? So here's an example of what I mean by a bounding box. So treat this as like straw man bullshit. I don't know if this is really any good, but you can think of like, you know, censorship resistance, Ethereum development processes. These are the boundaries for it. The fact that the code lives on GitHub, that's a common, not a threat, but a thing you have to think about, right? Everything Ethereum does is based on compute and compute relies on chips, and chips come from TSMC, and that's vulnerable to conflict, right? Quantum resistant cryptography, another threat. So all these are like the bounding conditions for what makes it possible for Ethereum to be healthy. And just to have a very different example, if you have a public lake, you have to think about things like, are there invasive species? What are the environmental regulations that are like constraining what you do? A really good example of this came up in one of our SOP research projects last year, where the people in Brooklyn trying to preserve the waterways, they discovered that the local regulation said that if the water is being used for like human activities, it has to meet higher quality standards. So an activist group started canoeing in the sewage contaminated canals of Brooklyn, and because of that, they forced by regulation, the regulators to actually start cleaning up the canals. So this is a very interesting example of stewarding the commons. So lake. Okay, so breakout number one, so this is the first one. So first form groups of three to five people, so just where you are, look at your two nearest neighbors, So this is the first one. So first form groups of three to five people. So just where you are, look at your two nearest neighbors, bring your chairs a little closer together. And SOP people, I do not want to see two SOP people in the same group, so distribute yourselves accordingly. And yeah, together choose a commons to work on, choose a target population, and maybe draw a bounding box if it's useful, then draw version one of this, you know, capability maturity model for your comments. So set of named levels, people, skills, and so forth and test for the abilities. So it will take 17 minutes to do this starting now and I think Timber is going to go around distributing paper if he hasn't already. So form your groups of three, please. Three to four. Thank you. All right. Thank you. . Yes. Thank you. Yes. Open the phone.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1gO904DKuSqj1sNQuLtbP57gbG3NphApmqMl4sI6azOs",
      "resources_slides": "https://drive.google.com/file/d/1QQFuE14oWAmsJ7yypiW4y7ouxF6nJNwI/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "human-stories-of-real-world-ethereum-next-billion-fellows-ef",
      "sourceId": "7SXGVX",
      "title": "Human stories of real world Ethereum - Next Billion Fellows (EF)",
      "description": "Next Billion Fellows work on projects that give a glimpse of what Ethereum means to everyday people. Through their lens, we can see what human coordination might look like someday. Come discuss the realworld, tangible impact of Ethereum on Fellows’ communities and explore the challenges they face along the way.",
      "track": "Real World Ethereum",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Free Speech,Not financial,Public good,Quadratic Voting,Use Cases",
      "keywords": "real,world,usecases",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "breakout-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1cnh924lOiBxB_1BdOH0enegLlg7UzzZ8tJU5R7Qt-wI",
      "resources_slides": "https://drive.google.com/file/d/1yboVz8dzE4QSNeurAq-wVHr0poDOuraH/view",
      "slot_room": {
        "id": "breakout-2",
        "name": "Breakout 2",
        "description": "",
        "info": "2",
        "capacity": 100,
        "youtubeStreamUrl_1": "",
        "youtubeStreamUrl_2": "",
        "youtubeStreamUrl_3": "",
        "youtubeStreamUrl_4": "",
        "translationUrl": "https://stm.live/Breakout-2"
      }
    },
    {
      "id": "ktv-attestation-winners",
      "sourceId": "MP9UQV",
      "title": "KTV Attestation Winners",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1r7I3zIxHezXZS2fH5PPvuIsIk0QSyDWrsnAqeEl-U6o",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "reth-10-how-did-we-get-here-and-what-is-next",
      "sourceId": "UTDCDM",
      "title": "Reth 1.0: How did we get here and what is next?",
      "description": "Reth is an Ethereum Execution Layer in development since 2022, focused on contributor-friendliness, modularity and performance. \r\n\r\nIn 2024, after rigorous testing and security review, Reth had its first 1.0 prod-ready release. \r\n\r\nIn this talk, we review the process of shipping a state of the art & novel Ethereum node, and lay out Reth's plans for the next years.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Developer Infrastructure,Tooling,rust,Core Protocol,Developer Infrastructure,Tooling",
      "keywords": "rust",
      "duration": 1535,
      "language": "en",
      "sources_swarmHash": "1eb58b04417a1a528d7cc630bdf462cbe06aebd7de0276370fa8c28db3227a32",
      "sources_youtubeId": "10xaWE28WCM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346d4b9dbb7a90e1ceb852",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1UdyIubnyXa-jfQkQkNDBDIP68YwdvTL9o61nG4a3fFU",
      "resources_slides": "https://drive.google.com/file/d/1d3v0ieahxoCTV3twrrXpAaqDPnX_LTMM/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "sybil-proof-mechanisms",
      "sourceId": "7QENZH",
      "title": "Sybil-Proof Mechanisms",
      "description": "I discuss a fundamental impossibility result on proposer selection mechanisms: If different actors can generate different value from block proposal (or sequencing) rights, the only sybil-proof and incentive compatible way of assigning proposal rights is through an (arguably centralizing) auction. In other words, any proposer selection mechanism can at most satisfy two out of three fundamental requirements: incentive compatibility, sybil-resistance and decentralization.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "PBS,Mechanism design,Game Theory,MEV,apps,Game Theory,Mechanism design,MEV,PBS",
      "keywords": "APS",
      "duration": 534,
      "language": "en",
      "sources_swarmHash": "128d8549b9f6be7505306f18e50a994b8b3afbd30a997bae8bf236e23af9240c",
      "sources_youtubeId": "ifMRDZvV2kU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T08:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1zjLtbzOM-9p0FmUus6R7GhQq9rHDQj5paePedPnL_rA",
      "resources_slides": "https://drive.google.com/file/d/1zV0pmox5cTThXa2BFFIP_cPKpPJx323Z/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-rated-list",
      "sourceId": "QNYDCR",
      "title": "The Rated List",
      "description": "The Rated List construction aims to minimise the number of requests required to complete sampling in Data Availability Sampling (DAS) for Ethereum. This optimisation becomes especially critical in the context of Full DAS, as data production per slot is anticipated to far exceed the current Deneb-Cancun (Dencun) specifications. The Rated List attempts to improve rate of successful sampling against unfavourable network conditions there by reducing the bandwidth consumption of the overall network.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAS,Data Availability",
      "keywords": "",
      "duration": 1052,
      "language": "en",
      "sources_swarmHash": "e480e5733ce7ffe3ddd9be001a4ffd129ed57c434bd01f5a7f62211dff3e67ab",
      "sources_youtubeId": "edyn9Hi2zsY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734828d9dbb7a90e1dd808a",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T08:45:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1tvKSVVMilC4YJnTAe-LSaWUsQBBm9OaP3zYQYmWuVJ4",
      "resources_slides": "https://drive.google.com/file/d/1-3xV6cSq7k-Oh3oiJeP7Y6Lns7tpY2Ey/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "this-cursed-machine-onchain-game-post-mortem",
      "sourceId": "UBFQ9V",
      "title": "THIS CURSED MACHINE: Onchain Game Post-Mortem",
      "description": "“Live in the pod, fulfil orders, get bugs.”\r\n\r\nTHIS CURSED MACHINE is a fully onchain sci-fi body horror fulfilment center simulator by Moving Castles, a game studio for the tactical research and development of autonomous worlds.\r\n\r\nWe will speak about learnings of launching an autonomous world onchain (Redstone) and how we embraced the emergent chaos by making the bot attacks, exploits and player corporations part of the narrative of the world itself.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Best Practices,Gaming,Autonomous World,worldbuilding,Autonomous World,Best Practices,Gaming",
      "keywords": "Worldbuilding",
      "duration": 1202,
      "language": "en",
      "sources_swarmHash": "b4c4551ccb33d3795de7e41d4cb7cd97e5953ba31de85f1be90de8c82c4ba2cf",
      "sources_youtubeId": "vNIQeYXxmVw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673470199dbb7a90e1ed9779",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673470199dbb7a90e1ed9779.vtt",
      "transcript_text": " Hello, hello everyone. Nice to be here. I'm Arb, also known as Arthur Rowing Bear. I will be talking about this Christmas machine. I work at a company called Moving Castles. We're a game studio for the research and development of autonomous worlds. I have a quick definition of autonomous worlds. If single player worlds emerge out of personal computing and multiplayer worlds emerge out of client server computing, then the definition of our organization is that autonomous worlds are the new games and worlds that are made possible by P2P computing. I also want to say that I'm fighting a fever, and if I'm not making sense, you can blame the bugs fighting against my immune system right now. Okay, so we started building this autonomous world called this cursed machine a while ago. It looked like this in the beginning. And I will show you the first minutes of gameplay now. Can you make it darker? Yeah, I skipped over this part because it's boring. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Bugs. Okay, I'm not going to show all of this. You're getting a kind of feel of what's going on. You're a stump trapped in a pod, and you need to produce different materials. It's a fully on-chain sci-fi body horror fulfillment center simulator, which is less important than you live in the pod, you fulfill your orders, and you get bugs. At some point you unlock a multiplayer market, you compete with other stumps to solve orders, and you can build new things on top extend the world. So, why? Why did we do all of this? But first I think there's a message from our sponsors. In the heart of industry, our machine's so grand. Transforms complex matter across the land Caffeine, MSG and plasma powder in hand Our cursed machine makes everything planned From waste to worth in our expert command Okay. Dear TCM, how did you do this and why? So the first thing we did was... The game is quite simple. It's a token in, push it through a circuit, token out game. We created a vast material map. And so if you take bugs, for example, token out game. We created a vast material map. And so if you take bugs, for example, you input bugs, you input them into the rat cage, you put the rats from the rat cage to the burner, you get dead rats. And there's a lot of them. Okay, composability, tokens. Second thing is narrative composability, which is even more important I would say. So what's great about building within a cursed corporate environment is that a Excel sheet is lore consistent. This has advantages over other worlds. It makes it really easy for people to contribute to it and it kind of blurs the barrier between user-generated content and what we do. It also makes our job really easy. The next is LARPing. So everything we did was in character, which means that... Oh, yeah, diaper is in order, just so you're not confused. We did everything in character, so the Telegram chats, the Twitter, everything was done in character as we are also employees at the same company. We're just the middle managers, and we are blaming other, you know, the corporate overlords, et cetera. The third thing that was important was to embrace, fourth thing, to embrace failure. Every issue is a narrative opportunity. Very simple one is we're building an on-chain game, which means we're dealing with this vast broken machine. So the game is about being trapped in a vast dysfunctional broken machine. But also this applies to more things, as you will see. being trapped in a vast dysfunctional broken machine. But also this applies to more things as you will see. So what happened? A lot of things happened. We launched and players started building different things. This is a kind of overview of the different organizations, the different stump initiatives that emerged. Someone found out that the orders for stumps to create different materials, the smart contracts were open, so they created a site called Stumpfmakler where you can buy materials from the stumps. Different companies started emerging that sold and cornered parts of the market. We had a first issue. Evil hackers started stealing the bugs we had wanted to give to the good stumps who were joining the game. Very bad evil hackers. So we gave a grant to two new insurance companies, which were player-led, which gave predatory loans to new stumps so they could now play the game. Of course, this would bite them in the ass later. There's a second company. We also, at some point, the medical records of the stumps got leaked. Oh yeah, Frolic built an answering machine. I don't know if the sound works. Thank you for calling the SEUMP unemployment hotline. Your call may be recorded for quality and training purposes. Please hold while we connect you with books. Order fulfillment center. So this was built by a player. And it reads the smart contracts and reads you the current orders. You can actually call it now. There are six jobs available. Job order number eight, 6461. We need 79. Okay, then we not going to do all of this. Okay, then we had another hack. Someone stole all the bugs. So we went out with an official statement. We started telling the stumps that they need to get ready for the war against the hackers. And we didn't. We beat the hackers. Actually, it was me DMing the hacker and asking him to return all the funds which he did also we had a corporate partnership with TSMC and this is a Can you turn up the music? I got a scat on that back on them corns She cut it for me, she want me in that corns She ice and mail it with no stop at corns Every time I suck it, it was all learned I ain't take sides, I ain't rock with no terms You can get hit with that fire, it's a burn Slow out the pulse, but I feel like a worm Bitch, I be deep, you can cut me in thirds I got a ease in me and I opened that part Yeah, bitch, I ran over the curb Bitch, I'ma pull out your toes and I turn them You knew what I said, bitch, you know that you heard me 24-7, we get just seven and eleven, double, till my face ain't been blurred My money's hard, my money fast My money thick, my money sturdy My tooth is gon' pull if I'm long-range, hit up thirty times, yeah, you callin' them crud Okay. So that worked, kind of. We are really happy with the outcome. We saw a lot of things emerge and we think that happened because of the different methods we used and I think that there is a lot of unexplored potential for leaning into issues, especially in the game space. As people come from the protocol space, they're kind of clinging to this idea of security and I think it doesn't matter. If you're actually creating a thing which is supposed to be fun, the most fun about crypto is all the hacks, all the drama, all the on-chain shenanigans and why not embrace that and make it into the narrative of the game itself? And I think that that is really how you get a decentralized narrative where the actions of the players and the different participants shape what the world is. We're working on a next game. This is an artist rendering. I'm not going to tell you more than that. If you want to follow what we're up to, you can scan this QR code. I'm going to give you a little bit of time. Since not many people are doing it, it must be because you don't have a lot of time. So I'm going to give you more time. Okay. Since not many people are doing it, it must be because you don't have a lot of time. So I'm going to give you more time. Okay. Are you all done? Great. And then the last thing is we slash Rasmus, one person from Moving Castles. Oh, yeah, I want to credit GVN and Rasmus, my two co-founders, and then also Agnes Cameron and Manus Niehoff, who worked on TCM. And this one was made by Bunker and Rasmus. And you can... It's shulgen.engineering, and you can upload your frogs and synthesize new materials. It's available now. And it integrates with ZooPass. Okay. The URL is shulgin.engineering. Wait. I don't know how to go back. Wait, wait. Okay, here. You have to read it quickly at the top. Here, shulgin. Okay, wait. Okay, one more time. You see it? Okay, here. You have to read it quickly at the top. Here, Shulgin. Okay, wait. Okay, one more time. You see it? Okay, I'll do... Okay, I have a lot of time left, so I'm kind of... But I think that maybe it's time for questions. Did we get questions? There are questions? Good. Because we have a lot of time left. How do I do this? Alright. A big applause for Arb. Who, despite being sick, came here and presented all of this. Don't be shy. Please bring more questions. Because the first one is not really a question. It's just like, thank you. And then the second one would be what's your favorite material from the map? I think the first question is really important. And I like it a lot, I must say. Thank you whoever asked that. I agree. I am persevering. Persevering. My favorite material from the map? I don't know. There's a lot of good ones. I will think about it a little bit. Does anyone in the audience have any favorite materials? Adrenochrome is pretty good. Neuralink is good, yeah. Yeah, the mold stuff is also pretty good. There's a whole, like, I like the whole oil, what's it called, material map unlock. That was great. Petrochemicals. Electronics was great. Petrochemicals. Electronics was good. Why Redstone? Yes, of course. I didn't mention that. We worked together with the beautiful Lattice company, and this was built on mud. And we used their chain called Redstone as well, which worked really well. I mean, we like working in mud, and Redstone worked well for, you know, it came with the thing and it was good. I'm not super opinionated about chains. I think it worked really well and we like working with the Lattice team. Who's the best player? I think that this is an interesting question. We had a leaderboard, but I think the best players were the ones that started creating their own stuff. I think Frolic built in the API, which was cool. That was a great player achievement. And I think that the players who extended the game and extended Logix were the best players. Are there other projects? Okay, here's a lot of stuff here. How did you build it? Mud. So the ones on the top are the ones with the most upvotes. So what percentage in character are you still in right now? That's a very good question. I don't know. I think the fever is pushing me more into character. Here, obviously, blockchain constraint options for game loops as well as developer resources independent of these two. Is there a central mechanic game loop you find interesting from a narrative perspective? I think that it's really hard to build game loops or games on chain. I think you can do a lot of classical games when you start doing obfuscation or ZK stuff. Otherwise, it really limits the game space. I think that TCM worked for the time we ran it, but it also was basically a capture with a faucet at the end. And then we did a bunch of wheeling and dealing around that to extend the game life of that. I'm currently, I find like games that have optimal strategies but that change based on player behavior really interesting. And I also like social oracle games like the beautiful Dress to Impress game on Roblox, which is a game where you dress up and you perform a fashion show and then people rate you for it. That's a great game. The next would be, are there any other projects that are along the same lines of a merchant blockchain narrative? I don't know. Are there? Does anyone know? Do you know any, Jibian? Are there any more? All of them trying? Yeah, we're all trying. I think that maybe games are too afraid to lean into the failures. I think that maybe is a unique thing about our game. And I think if more people were able to do that, we could have more emergent narratives. I think that's basically the somewhat chaotic kind of core message of my talk is that, you know, the people feel like they have agency over worlds that they can break them. If you don't allow that, you're just creating a little sandbox that people can play in, but you don't allow them to change or break the sandbox. Really, the most interesting narratives are the ones that subvert the expectations of the game makers, and then become part of the narrative. All right. Then the one that actually interests me a lot is like, how did you build it, the whole thing? Yeah, it was built using MUD by a great team, us at Moving Castles, and together with the support of the MUD team. And I think it looks good because we leaned into the kind of limitations of the medium. Yeah. Thank you so much. So what was the funniest hack then that happened? Yeah, I think the faucet hack, the, I mean, yeah, there was all kinds kinds it was like a lot of people like trying to break the game and then me kind of begging in dms for them to return the funds that happened a bunch of times the good thing is we we just were the central bank for the token anyway so when someone like hacked or tried to speculate on something we just increased inflation so it was worthless so like we did a bunch of stuff like this and uh that was fun uh we basically the the the trick is just to just demotivate so much that they give back them the tokens",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1cXPZD6cWdMNr2QSeVuUQ8-WSQ_YhrCRA6-l3ClLl2n0",
      "resources_slides": "https://drive.google.com/file/d/1JqJ4gKILsbR7oAM4Z6wVL0u5iDi-uDgZ/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "wizard-build-your-own-p-iop-protocol-in-15-min",
      "sourceId": "W78CYD",
      "title": "Wizard: build your own P-IOP protocol in 15 min!",
      "description": "Wizard is a new open-source framework allowing you to write your own ZK proving scheme. Wizard is one of the backbones of Linea zkEVM's prover and it can be used to implement advanced protocols easily. In this session I will guide you through an implementation of Plonk using just a few lines of code.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Frameworks,SNARK,polynomial-iop,Frameworks,Protocol Design,SNARK",
      "keywords": "Polynomial-IOP",
      "duration": 1471,
      "language": "en",
      "sources_swarmHash": "d38438171620ecd34967ddd26ca2f7cf37da87509735a26d94d1c7bfff1a4873",
      "sources_youtubeId": "4N43UH5hb14",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346da49dbb7a90e1d16a65",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67346da49dbb7a90e1d16a65.vtt",
      "transcript_text": " Transcription by ESO. Translation by — to you the tech we developed to develop the proof system of Linear. Linear actually uses many different proof systems for many different use cases. It is a complex system. What I'm going to present to you is the main one that we use for proving the execution, so concretely proving EVM execution. So, I shall present myself. I'm Alexandre. I've been working in this space for seven years now, and been focused on cryptography for five years now. I've been working on INEA since the beginning, even doing research on roll-up even before that. So, yeah, just to talk a little bit about myself. Okay, so I'd like to involve the audience a little bit. Who knows what a ZK-EVM is? All right, so that's a good half, a good third. So a ZK-EVM layer is a layer two solution which is whose purpose is to help Ethereum scalability. The idea is, instead of sending transactions on the Ethereum mainnet, you send them to a third party, Linear. Linear runs an Ethereum virtual machine, will process the transaction. It will be processed separately from the mainnet. So the contracts that are deployed on mainnet are not the same, necessarily the same as the ones that are deployed on linear. The states are separated. But we can bridge between the two. I'm not going to describe much more how we bridge between Ethereum and linear. But what is important to remember here is that we basically bundle the execution of many transactions and submit one proof at the end. We give you one number, every finalization finalizes on average 65,000 transactions at once. And since we are using snark proof systems, meaning they have a very short verifier time which is mostly independent from how many transactions are executed, that's how we get scalability. Verifying the final proof which is a Planck proof takes a few milliseconds but it's for 65,000 transactions. So that's why it's much cheaper and that's how ZK EVMs can achieve much lower gas price. All right. So maybe a few words on ZKP and SNARK. ZKP is, I mean, usually we say ZKP but most of the time what we use in production are ZKPs, I mean, usually we say ZKP, but most of the time what we use in production are ZK argument of knowledge. And in order for a protocol to be a ZK argument of knowledge, it needs three properties. You need zero knowledge. Zero knowledge means that the proof does not reveal more than what it should reveal. You need to have completeness, meaning that if you want to prove something that is true, you should always be capable of generating the proof. Meaning, for instance, if you want to know that you proved the square root of some number, it should work for every number and not just one number. And the most important one is argument is knowledge soundness, or essentially computational knowledge soundness. It is how you say that you know, that you prove that you know what you're proving you know. And so, as I mentioned earlier, we are building a ZKVM, but really the ZK is not about zero knowledge or hiding things it's really about proving that we know valid execution traces. We'll explain more on that later but especially we need more properties. We want the verification to be succinct, to be small and we want also the protocol to be non-interactive. It should be a single proof, a single message, and from that we should be capable of verifying a proof. Okay, so let's apply all that to the EVM. The Ethereum virtual machine is a state machine on which we can execute and a transaction in GVM is like an instruction from the user of that virtual machine. It is specialized for running smart contracts. It has a lot and a lot of features that interact in a complex way between each other. And not every computation that are easy to do on the EVM are necessarily easy to prove. So for instance, I'm thinking about the Ketchak hash function that every smart contract uses all the time, like it's free. It's really not free to prove. And so if we want to solve that problem, so of course we need to deal with the inherent complexity of the EVM, but we also need a proof system that is flexible enough to solve all the problems that we need to solve to execute the EVM. Concretely, in order to prove the EVM, we have traces of execution that are instantiated by polynomials. We call them colon in our framework. And it could be summed up as a collection of errors that communicate with each other by sub-argument. And there are many different sub-arguments that are possible to use. We can have lookups. We can have projection queries. We also support variants of lookups that we call conditional lookups or fractional lookups, we can have projection queries, we also support variants of lookups that we call conditional lookups or fractional lookups. So in order to make this work, we need a proof system that is really flexible and can deal with all the polymorphism that is inherent to proving linear zarythmetization. Okay, so in order to do that, we designed the wizard framework, which is the main gateway between describing the constraints to represent EVM and actually proving things. It has a very neat particularity in that in the wizard framework, you write protocols in an ideal model and you don't have to worry about how you're going to commit to things or which arguments you're going to use. You just say what you want and then you have a list of techniques like a menu which you apply to that protocol statement and it will create for you a proof system, as complicated as you need it to be. And so that's perfect for us. It turns out that this is how, in academia, they describe complex protocols. Like if you take, for example, the GROSS-16 proof system, if you had a look at the paper before, you would see that they say, okay, so this is the constraint system at the beginning, then we do something that is called QAP, and then we do something that is called NILP, and then it continues, and at the end you have a concrete proof system, which is a GROSS-16 proof system, but that does not translate into the implementation. People just take the final protocol at the end that you apply after every step of compilation of growth 16, and there is a growth 16 implementation. So what we do is that we actually implement every possible step so that this step can be reused for other proof systems, and we don't have to really mentally work out the whole protocol. In the case of linear, it would just be impossible. And on top of that, we made it in such a way that if anybody wants to add their own compilers or do their own tweak or add their own type of constraints, the framework will allow that without changing the core of it. All right. allow that without changing the core of it. Alright, so as I mentioned earlier, the proof system, the protocol that you are going to construct, you have to describe it in an ideal model. And this ideal model involves what we call the Wizard Oracle. The Wizard Oracle is, from the point of view of the protocol designer, a trusted third party. It knows everything. It remembers everything. It does computation for free. It is always honest. It's like something you really... If it existed in reality, we would not need cryptography. So that does not mean that what we build will not be secure. It just means that the Oracle will concretely be instantiated by something else in the future as we compile the protocol. So the protocol can also be described in a multi-round fashion. I mentioned at the beginning that we need non-interactivity, but the protocol sort this out using the Fiat-Shamir trick. It puts some limitation. It means that the verifier can only send random challenges to the prover, but that's a common limitation that every protocol has nowadays. It's very uncommon to be in a contrary situation. And so essentially, the prover can use the oracle by sending a big, large message to it, and the oracle will just remember and notify the verifier that, hey, the prover did this part of the work. You can ask questions. So the verifier can ask questions, and the oracle responds to the question without needing to do any computation. It's like a godlike entity, and it is always honest. So you don't have to worry about him lying. So as I mentioned the prover, the verifier and the oracle can send messages to each other and here comes the first primitive of the framework which is what we call colons. So colons can be of any sort. We have what we call committed column. Committed column means it is sent to the oracle. And basically being sent to the oracle means that the prover cannot change its mind about what was sent to the oracle. You can only send something once to the oracle. Otherwise, you're cheating. And the protocol will always ensure that. But on top of that we have what we call pre-computed columns. So pre-computed columns, they can be of two types. They can be sent to the verifier or sent to the oracle and they are known beforehand. So that's something that is part of the protocol description, actually. They always have the same values. You can think, for instance, the Planck circuit description, which is instantiated by several polynomials. We are going to see how we can implement Planck in 10 minutes. So I'm just putting myself forward a little bit. You need those columns as part of the proving keys, and they describe the plumb circuits. And on top of that, you can send proofs, and proofs means a message that is sent directly to the verifier. And there are other types. Actually, we have eight types of different variants of column type. The columns also have a predefined size. It can be one. It can be one, it has a power of two, it's due to a limitation, a current limitation in the framework, and they have a round assignment. And the round number is essentially describing at which round of interaction the column is associated by the prover. That's for the main part. And then, as I said, for some columns that are sent to the oracle, or for some groups of columns that are sent to the oracle, the verifier can ask questions about these columns to the oracle. So that's what we call queries. It's a common term used in academia. If you know about FRI, they do random position opening queries, so that's what they mean when they mean query. In polynomial IOP protocol, there would be univariate openings. In our framework, query stands for at the same time constraints. It would be questions that have a yes or no answer, like is this value the square of this other value? The answer can always be yes or no. Most of the time, it is served as a constraint. And here we describe it as a query. And we also have open questions that are like polynomial opening, position opening, and so these expect a response from the oracle that is other than yes or no. So we support many, many different types of queries. It can be lookups, it can be univariate evaluation, it can be inner product between several columns. So essentially, most of the folklore is there. And we implemented it because we needed it for the concrete implementation of vortex and linear arithmetization. All right. So as I said, once you have a protocol description, the only thing you need to do is to describe how you want to go from this description in an ideal world with ideal oracle into a concrete protocol that is secure in the standard model. So here is the base description that allows us to go from without IOP to polynomial IOP at the end. But in practice, this would not be sufficient. We would also need a polynomial commitment to turn this into a concrete protocol. So this part of the code does not describe how we do the polynomial commitment, but how we go to this point. Okay, so now let's get onto a practical example. So here is the Planck constraints description. So we have a set of columns. Qs are describing a Planck circuit. Xa, Xb, XC are describing the witness. And so usually we add another column on the right that is for the public inputs and that we are going to use. On top of that, Planck has some copy constraints which can be instantiated by a permutation argument, which I'm going to show you how to do. Okay, so let's implement Plonk. So as I mentioned, we need to define our protocol, then we can compile it, and after we can run it, so running the prover and verifying it. We can also automatically recurse it, but we are not going to cover that today. All right. Okay, so first of all, defining the protocol. This is done by specifying a function. So the whole framework is in Go. Most of the prover stack of linear is using Gnark, and the linear prover is also implemented in Go, as it is also relying on Gnark's implementation. So the defined function has this simple signature, and the builder is an object that is going to store everything we said to declare an entity in the protocol. So either queries, columns, or so on. And we can also specify checks to be done by the verifier. So let's go into that. OK. So here are the verifier. So let's go into there. Okay, so here are the columns description. So you can recognize the column that we saw at the beginning. So the queues column are for the circuit description. They should be the same no matter what we try to prove. So they go into pre-computed. The XXBXC are commitment, they have to be sent to the oracle and PI for the public input is inserted as a proof object because it has to be revealed to the prover. It's a bit counterintuitive that we call that proof but proof means a message sent to the pro community. It's part of the proof. Even if it's a nonsense from an academia perspective. And also a number of public inputs. Because the PI is, colon is larger than the actual number of public inputs, because every colon should have the same size. And also the value of the queue should be known beforehand, of course, because that's the circuit description. Okay. So now we can declare the queries. So on your right you have a global constraint, which is an arithmetic expression that has to vanish on all the rows of every column that it's touching. We can recognize the equation of the Planck gate constraints at the beginning. And we have a fixed permutation, which is instantiated by some forced permutation that has the concatenation of XA, XB, XC invariant. And that's how Planck proves the copy constraints. Then finally, we need to add a verifier check. This is to ensure that the PI that is sent to the verifier is well formed and that it should be padded on the right with zeros. Okay. So now once we have that, we can compile that into an actual protocol. So here I added the part that converts the PIOP into a concrete protocol because I added the vortex.compile, vortex being the polynomial commitment that we use. And now we just have to run it. So the only thing we need to specify is how concretely we are going to assign our columns. Because this is the only thing that is unknown at this stage, after reading the protocol description. So, yeah. We just provide it and we assign it. It's four lines of code. And so, yeah. Now, so we have some things that allow us to write Plunk constraints manually. But I don't know if you have tried writing Plunk circuit by hand, but this is really difficult. And it turns out that Gnark offers a very nice front-end to write circuits. So let's just write a wrapper of what we just wrote using Gnark so that we can use a Gnark circuit description. So I did the implementation. It was a bit longer than 100 lines, but it was essentially a few automated stuff. Okay, so let's do a circuit. So let's use Fibonacci as a use case. So my circuit, you have two values, U0, U1 as input, and you want to have the 50th number of the Fibonacci sequence generated by U0 and U1. U0 and U1 being public parameters. So on your right, you have the circuit writing in NARC, so you can see that it's fairly easy and much simpler than writing a circuit by hand. And then we just have to run it. And that's it. You just create your proof function that is explaining how to assign the colon. You run wizard.prove, and it's going to generate a proof for you using vertex-threaded polynomial commitment, and you can verify that in one line. All right. I have six seconds for the polynomial commitment, and you can verify that in one line. All right. I have six seconds for the future improvement, so we want to add more queries, and we think we can also remove the necessity to specify runs in the protocol as it should be inferred automatically. All right. That's it. You can check out the code here. Amazing, Alexandre. You can check out the code here. Yeah. Amazing. Thanks so much for the great introduction of Wizard. So a reminder that if you scan the QR code, you will attend the session, and you can ask questions, and you can also claim an NFT. And you can also vote. So if you have a question that you really want it to be answered, vote for them. So let's start with the top one. Does WSIR support lookup tables? And can it be used to implement lookup tables based on CKVMs? Absolutely. So the way you would do it is, for instance, say, so what you can do, first of all, if you want to do a range check, so that's a big use case for lookup table, you already have a range query. So you just take one column and you say, I have this query that just enforces the whole column to be within bound, and that's all you do. A second way, if you want to do more complicated range checks, like XOR, for instance, then you would have to specify three columns for your XOR. One column for the left side, the right side, and one for the result. And in this column, you put all the possibilities. So maybe say for 8 bits to 8 bits, you would have 2 to the 16th possibility. So you write down all of that in your table. And then you create a lookup constraint between this table and a triplet of columns for which you want to enforce XOR constraint. And you can also add a conditional lookups. You can have a fourth column that contains zeros or one and that activates the XOR constraints or not. Great, thanks for the answer. The next question is can you create different custom gates and at which instance do you decide which row corresponds to each kind of gate? So when you generate a global constraint this is essentially what is your custom gate then it's going to apply over everything but the wizard framework is more abstract than this. Essentially, there is a general technique to do it, which is to say that you add a selectors column that says which constraint is going to apply for each, and you have some product of your constraints custom gate expression multiplied by an indicative that asks whether this constraint is active here or not, and you would merge everything into a single global constraint in the end. So yes, you could implement custom gate. Actually, that's what they do all the time when they specify the EVM. Great. The next question is about recursion. So is recursion something that would be implemented in Wizard, or would it be separately like a commitment? So there is a separate way you can do recursion. So we do it inside of the Wizard at the same time and outside. The first way, we have a compilation step that is called self-recursion that usually goes just after vertex. The text of vertex proof and re-arithmetize it. And we can do proofs of that again, and we repeat, and we can shrink the proof. That's because vertex, as a single polynomial commitment, has a square root, very fair time. But applying log-login application of self-recursion, you get constant size proof. Great. And the last question that we have in the queue, at least for now, is that it's great that you can define an ideal protocol programmatically, and it seems that that does make it easy. Does that make it easy or possible to support automated formal verification or UC proofs of security? I'm not too sure what it would entail exactly to formally verify. So we could formally verify the standard set of compilers that we have. I think this is at least a necessity. But then there is a protocol description. It should be formally verified. This, I don't know how to do it. I don't know and I can't tell you how to make it easy but it would be a great use case I agree it would be great maybe after that conversation",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:30:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1FkV9X3aQwU20vdTZXHXBpHGRAISg06VrxYifChRhnIo",
      "resources_slides": "https://drive.google.com/file/d/18BC5R9QvxaV67ChCL7bKhOa1dXXomtfA/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "latency-advantage-in-cex-dex-arbitrage",
      "sourceId": "RPMHLF",
      "title": "Latency Advantage in CEX-DEX Arbitrage",
      "description": "We study the effects of having latency advantage in the CEX-DEX arbitrage in the first-come first-serve transaction ordering policies. We search for optimal strategies for a trader that owns such advantage. To find optimal strategies, we simulate price changes on CEX using real data and assume DEX price does not change in the latency advantage interval. We find that optimal strategy can even be to trade right away as soon as the price difference crosses a threshold where trading is profitable",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Rollups,Economics,MEV,AMMs,programming,dynamic,AMMs,Economics,MEV,Rollups",
      "keywords": "Optimal,Stopping;,Dynamic,Programming;",
      "duration": 562,
      "language": "en",
      "sources_swarmHash": "373cd46037e49db5d6daf67c74b2a86e58ffe6dbe5747c1938a919384ce69f95",
      "sources_youtubeId": "9r7hmxQ8DTA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:40:00.000Z",
      "slot_end": "2024-11-13T08:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1CjpmVDcW4MOjilttmNcrYu_KP0rC8ud1_BjudHV_ntI",
      "resources_slides": "https://drive.google.com/file/d/1le5l7TiUtjNS4T1T41z89p_iQAH7np6R/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "sszb-a-high-performance-ssz-implementation-in-rust",
      "sourceId": "M3SK39",
      "title": "Sszb: A High Performance SSZ Implementation in Rust",
      "description": "This talk goes over my EPF project for the SSZ ecosystem:\r\n\r\n- a benchmarking suite for the various rust SSZ implementations in the ecosystem to properly evaluate performance and point developers to which library they should use.\r\n- a high performance ssz implementation that's faster than existing libraries in the ecosystem",
      "track": "[CLS] EPF Day",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core,Protocol",
      "keywords": "serialization,ssz,rust",
      "duration": 849,
      "language": "en",
      "sources_swarmHash": "4cff4a6eb8f2f4ec6d0f9fb9efaa5a524fff05ba39fc73cf45ace5648e60cf18",
      "sources_youtubeId": "WIu4PGDZOqI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673480e89dbb7a90e1c6fbd5",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:45:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1-4E6jtMXWSHSGuL8JFQX16HGIrgdIQ5cWNLRXq-ty9I",
      "resources_slides": "https://drive.google.com/file/d/13FerR8YUacQpSD2TZC5voy8_RqId7w9Z/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "amms-as-managed-customized-portfolios",
      "sourceId": "RMNX33",
      "title": "AMMs as Managed, Customized Portfolios",
      "description": "When you provide liquidity to a Uniswap or Balancer pool, what financial product are you actually buying? This talk considers automated market makers from the perspective of liquidity providers. We first mathematically describe the underlying financial derivative that LP positions represent. Then, we show how to use AMMs to construct custom financial derivatives, specified by their payoff function, and discuss implications.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Trading,AMMs,liquidity,provisioning,AMMs,Trading",
      "keywords": "finance,liquidity provision",
      "duration": 608,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "7H2CPeN7VcI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673470e09dbb7a90e101bd24",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:50:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1I12hDyQcy5XvNa2BYACXq82Cc7f1_Kw_-YiA4Yo9Lkw",
      "resources_slides": "https://drive.google.com/file/d/1XWhkj_T3ODhFNvr53VnEEVkAp2w0sHUf/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "designing-conditional-markets-and-futarchy",
      "sourceId": "EWJNVJ",
      "title": "Designing Conditional Markets and Futarchy",
      "description": "Conditional markets allow predicting outcomes from potential decisions, enabling what is called futarchy governance, but key design questions remain open. We'll examine specific challenges: aligning founders with investors in protocols, encouraging meaningful participation in decentralized governance, and integrating futarchy modules into existing governance systems.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "market,prediction,DAO,Futarchy,Public good",
      "keywords": "Prediction,markets",
      "duration": 1519,
      "language": "en",
      "sources_swarmHash": "5e074344751472dc78fed2d0f7259d0d3f840e54a5bd75116712a1e19a7cdcc9",
      "sources_youtubeId": "iEjrdYReNnc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67346f489dbb7a90e1e59ed9",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:50:00.000Z",
      "slot_end": "2024-11-13T09:20:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1xu1ruVYDwVrtPaBTfIRAfXMJa5j_5CZosQxtJM57H9c",
      "resources_slides": "https://drive.google.com/file/d/1vqfv0UsbmEkDyDq88Di2oNwny9_fCMcK/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "you-know-whats-going-to-get-us-from-web2-to-web3-therapy",
      "sourceId": "LUKWAM",
      "title": "You know what’s going to get us from web2 to web3? Therapy",
      "description": "2024 has been about thinking how we avoid recreating the same systems just \"over here\". And it has to start with our intentions and our ability to make decisions from a better place vs continuing to be influenced by scarcity mindsets, disregulated nervous systems and a burntout collective. \r\n\r\nI delve deeper into this here https://pop.mirror.xyz/JoTHH4cSRw967mphJqur6hWS6vQx0q89ee0WnO1o63g",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "future",
      "keywords": "thriving,mental health,future",
      "duration": 531,
      "language": "en",
      "sources_swarmHash": "b56e50859f10264bce39a4458b8d038188b99b991e4359c0f173ef425205fdfe",
      "sources_youtubeId": "mKDf6mBemhg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T08:50:00.000Z",
      "slot_end": "2024-11-13T09:00:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1gUdSnWcxJdTYFT1JrkVP_VWgSxrlBCcEuwRk8pzgBSA",
      "resources_slides": "https://drive.google.com/file/d/1zFPvbGgF5CPfM28jf1rsGDOIroDLyADW/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "building-a-smart-passkey-wallet-with-aa",
      "sourceId": "WC7LEV",
      "title": "Building a Smart Passkey Wallet with AA",
      "description": "Passkeys are now one of the most popular tools in building good UX in Ethereum ecosystem. In this workshop, I will build a secure simple smart wallet utilizing passkeys and start sending basic transactions on L2s. During 2h worksop, we will try to cover lots of topics related to wallet usability and achieve a great example. The power of Account Abstraction with RIP-7212 will be clearly visible.",
      "track": "Usability",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Live Coding,Security,User Experience,Account Abstraction,hardware,Account Abstraction,Live Coding,User Experience",
      "keywords": "Passkeys,Hardware Security",
      "duration": 5276,
      "language": "en",
      "sources_swarmHash": "c8e7cf0e66e2b6ebf630142f0086fc2793594a167b7034952d9b193355db6732",
      "sources_youtubeId": "IQ8J0wTHk98",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734821d9dbb7a90e1d7f8b2",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1ED-jzkjpVBh8C5Ixuhx0_o1QSI84E5mcPX2UeHxX4D8",
      "resources_slides": "https://drive.google.com/file/d/1EEwPJeQFOU8QEkIkC5yA3XA1_mTxSX2M/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "debugging-data-for-ethereum-ethdebugformat-overview-and-project-status",
      "sourceId": "JLWSZ7",
      "title": "Debugging data for Ethereum – ethdebug/format Overview and Project status",
      "description": "Building debuggers for EVM languages is challenging, time-consuming, and brittle because compilers do not provide enough information to enable robust tooling. The **ethdebug format** project, sponsored by Solidity, seeks to address this concern by designing a standards-track collection of schemas for expressing high-level language semantics in connection with low-level machine code.\r\n\r\nPlease attend this talk to learn about the status of this effort and a brief overview of its components.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Tooling,Best Practices,debugging,Best Practices,Developer Infrastructure,Tooling",
      "keywords": "Debugging",
      "duration": 1462,
      "language": "en",
      "sources_swarmHash": "79193ebc09b0960748ebf82d2e76ad97264a5015d5954e41f04c3a0e30de541f",
      "sources_youtubeId": "E84_YWgExaU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347e409dbb7a90e1a811ca",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:30:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1hKCNu1k-EbMC3GsA0i_-SO8vLwgPTyED9D91FSwTjoU",
      "resources_slides": "https://drive.google.com/file/d/1nV0Io6JsOXdUOHC0-wbyVmhdMxYXhdSZ/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "deep-dive-the-lp-pricing",
      "sourceId": "CDPRCK",
      "title": "Deep Dive the LP Pricing",
      "description": "Accurate and robust oracle pricing is the backbone of DeFi. However, LP token prices can easily be manipulated if not calculated correctly.\r\nIn this talk, I will focus on how to calculate a \"fair price\" for LP tokens, ensuring security and accuracy. This includes LP token pricing for various protocols such as Uniswap V2, Uniswap V3, Trader Joe v2, Curve – sharing insights and implementations from my experience developing Alpha Homora, Stella, INIT Capital and INFINIT.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Mechanism design,Economics,defi,Economics,Mechanism design,Security",
      "keywords": "Math,Oracle price,DeFi",
      "duration": 513,
      "language": "en",
      "sources_swarmHash": "18cb2e5a31b31eb913a189c43e2749552a2d9e9cfad7263004d8713319c268e5",
      "sources_youtubeId": "zsohxOn91vc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734717f9dbb7a90e112656b",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1c2MfQGdbJapup-3V1uRqWXcF71JAgZPMM_0mp-IIXL8",
      "resources_slides": "https://drive.google.com/file/d/189jGPVhFdFZ3JXuGocjTHjHtGjw2xpAW/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "dont-get-rekt-practical-threat-detection-for-users-and-devs",
      "sourceId": "Y7QGNQ",
      "title": "Don’t get rekt: practical threat detection for users and devs",
      "description": "Learn to uncover, and protect against, weaponized repositories, sites and tools targeting web3 users, devs & researchers. With examples and hands-on exercises, the session begins with topics like detecting suspicious activity in sites, handling wallet secrets & signatures, decoding calldata of malicious txs, and simulating them to avoid attacks. To then cover more advanced techniques to spot harmful backdoors in code repositories and services that can impact on devs & users’ safety.",
      "track": "Security",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": true,
      "tags": "Tooling,Security,phishing,Security,Tooling",
      "keywords": "user safety,developer safety,phishing",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1iQKRk0GBHlEdWgzH2yQxE2MJqGiiPO9fQI4PkTbLKOk",
      "resources_slides": "https://drive.google.com/file/d/14b5knPYTUrt1SokifYXSD0KHvLjLAU4m/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "from-bottlenecks-to-breakthroughs-optimizing-zkevm-provers",
      "sourceId": "LT8BTE",
      "title": "From Bottlenecks to Breakthroughs: Optimizing zkEVM Provers",
      "description": "In this session, we introduce how we optimized zkEVM provers in production to significantly reduce prover costs, a major expense in running zkEVM. Topics include diagnosing zkEVM bottlenecks using CPU and memory profiling, leveraging DAGs for parallelization, and efficient memory management with a memory pool, fine-tuned garbage collection, and in-memory swapping for gigantic memory usage. These optimizations reduced zkEVM prover runtime by 75%, representing a substantial performance gain.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,ZK-EVMs,Open Source Software,optimization,Layer 2s,Open Source Software,ZK-EVMs",
      "keywords": "Performance,Optimization",
      "duration": 1395,
      "language": "en",
      "sources_swarmHash": "301fa43a7bfaa31972ff77b24bb260e2d672d4919d0603965f4abcc4e9fb5a6f",
      "sources_youtubeId": "aNF-BM6v-tI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734723d9dbb7a90e11a9ad5",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1uTR60xRfzUI21BwpSkQ39uJtzxKc0DLJd2BqZBQisTI",
      "resources_slides": "https://drive.google.com/file/d/1ABZFd7lDMOd0POVL1TnBpaiAe4Bm9zeD/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "harry-p",
      "sourceId": "LXJJDW",
      "title": "Harry P",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T10:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1wNma7KIt9CoI1JayWZB-MIf47ge7DGlMJ3Ev9Il3pdE",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "impossibility-within-dynamically-available-protocols",
      "sourceId": "SUNDNH",
      "title": "Impossibility within Dynamically Available Protocols",
      "description": "This talk will be about dynamically available protocols and their properties. LMD-GHOST which is the fork choice rule for Ethereum consensus currently can face ex-ante and re-org attacks. GoldFish and other protocols aim to fix this but they themselves then face problems with asynchrony resilience and subcommittees. \r\nI also want to present possible solutions to these issues and establish some impossibility results that might be useful in consensus research for path towards single slot finality.",
      "track": "[CLS] EPF Day",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus Mechanisms,Finality,Single-slot Finality",
      "keywords": "Dynamic,Availability",
      "duration": 847,
      "language": "en",
      "sources_swarmHash": "0e4939a7f956db573c7fdbe8e3f1c28adcd318a6659903b592bd01ac09bde4a5",
      "sources_youtubeId": "ZZS5hKyxqOQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347eef9dbb7a90e1ae0ce5",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1_2sjOdakXbWTFCsQUBSCgpvHSd9_OwHcKRN41aiBnJc",
      "resources_slides": "https://drive.google.com/file/d/1BBizpHatfuYq__EhDqW4oVpuZHF84Qey/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "lunarpunk-endgame",
      "sourceId": "EVHFWA",
      "title": "Lunarpunk Endgame",
      "description": "Global surveillance is a static world where change is surpressed and society cannot evolve. In contrast, an anonymity-enhanced world resembles a forest. New civilizational experiments blossom like flowers, radiating outward from the freedom-fighters of the future.\r\n\r\nThe lunarpunk end game is to enable a new ecology of social orders. This talk will describe the grand vision of lunarpunk: multipolar space-faring civilization, human speciation, and the reproduction life throughout the cosmos.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "Network State,Anonymity,Autonomous World,lunarpunk,Anonymity,Autonomous World,Network State",
      "keywords": "Lunarpunk",
      "duration": 1589,
      "language": "en",
      "sources_swarmHash": "62abf23b929c7511d2d6ad9d0fd17dbae55f874642acb021ab38481533536a9b",
      "sources_youtubeId": "NmrpTB-mfQQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673476a39dbb7a90e1389485",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1pdPYWGnlJDvugH2zzLYqzKQrvDlutN5EGd8EBIpbeR4",
      "resources_slides": "https://drive.google.com/file/d/1hwOwdlsXkK71jSAqasECVPl7YHbztM7e/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "navigating-stablecoin-yields-and-risks",
      "sourceId": "YT9SMK",
      "title": "Navigating Stablecoin Yields and Risks",
      "description": "This panel brings DeFi experts together to discuss stablecoin risks, including economic risks related to stabilisation methods, technical risks of smart contracts, and regulatory challenges. We will discuss solutions that can help mitigate risks in this rapidly evolving space and the challenges of promoting risk-driven decisions over trend-driven ones.",
      "track": "Cryptoeconomics",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Frameworks,Best Practices,defi,Best Practices,Frameworks",
      "keywords": "Stablecoin,DeFi",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "0162dccab9a5b86cc3b69290b4f0d831598fc81773c7ecd199c88267fcfe7814",
      "sources_youtubeId": "AsJS4vv0J8Y",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T10:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/15OlMPy7qIjacZlozudJLl0FrCp0kPt_kx5nIRNHipwE",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "realigning-with-ethereum-from-l1-to-l2",
      "sourceId": "PSSQCK",
      "title": "(Re)aligning with Ethereum: From L1 to L2",
      "description": "In this round table, Justin Drake and Marek Olszewski will explore the rational and tangible pros and cons of (re) launching an Ethereum L2. They will explore the why and how of launching an Ethereum L2 from a technical and ecosystem perspective.",
      "track": "Layer 2",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": true,
      "tags": "Layer 1,Layer 2s,Values,EVM,Layer 1,Layer 2s,Values",
      "keywords": "Transition,Ethereum Allignment,EVM",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T10:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1JF1fLnBMiSF5FSuifcPd7xXZqFJpC793NAwW7MxdqhM",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "the-verkle-advantage",
      "sourceId": "YLBEZN",
      "title": "The verkle advantage",
      "description": "This talk provides a comprehensive overview of the achievements by the stateless development effort, over the past year. It will explore some of the discoveries we made while implementing verkle trees, that improve the user and developer experience of Ethereum.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Protocol Design,Verkle trees,stateless,Core Protocol,Protocol Design,Verkle trees",
      "keywords": "stateless",
      "duration": 1543,
      "language": "en",
      "sources_swarmHash": "5a0b9f1615e20eb0e9597edb51957d0bf0f2f906610c445999fac2dd23a18440",
      "sources_youtubeId": "f0e3ulrO9Ik",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673471199dbb7a90e107f5eb",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1zs9ePGkdyS7IfCoOeK_dArKiELQYjDXk5L-A70d7Gf4",
      "resources_slides": "https://drive.google.com/file/d/1NEivVAmjpzZFmsI8R1DFTzcQZOwB4pgC/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "trust-zones-why-daos-will-be-the-best-organizations-ever-created",
      "sourceId": "R9ENCP",
      "title": "Trust Zones: Why DAOs will be the best organizations ever created",
      "description": "This talk introduces the theory of Trust Zones. Every Trust Zone is a unique blend of constraints,  reputation requirements, and accountability measures, within which an agent can operate on behalf of an organization to further its goals.\r\n\r\nI will contend that the operational management of all organizations can be described as creating new Trust Zones and adjusting their parameters. And further, that DAOs and other onchain organizations can do this better than any other organizational form.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,Governance,trusted,DAO,Governance",
      "keywords": "Trust",
      "duration": 505,
      "language": "en",
      "sources_swarmHash": "852ff1461e28d565f245830a4e9ecb44fed61bff2192ef6686cd69d515928f99",
      "sources_youtubeId": "tu6t6GdLyCg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:00:00.000Z",
      "slot_end": "2024-11-13T09:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/11gK41qto_r77F_waBaxEdW2JoYIgXHs4mVHzUzI_OaU",
      "resources_slides": "https://drive.google.com/file/d/1rtywjF61qKmajtac50arBNQksOE8sbvx/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "governance-innovation-analysis-on-voter-behavior-in-blockchain-governance",
      "sourceId": "ZKNSAL",
      "title": "Governance Innovation: Analysis on Voter Behavior in Blockchain Governance",
      "description": "As the first comprehensive examination of voter behavior in Web3, the following research explores two significant blockchain ecosystems, Curve Finance and Polkadot, using a novel quantitative methodology to decompose and highlight governance patterns.\r\n\r\nThe presented analysis shows, among other findings, a significant influence of market conditions on voter tendencies, diverse patterns relating to voting power accumulation, and a potential effect of financial incentives on voter participation.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Permissionless,Coordination,Governance,Decentralization,Game Theory,Tokenomics,voting,analytics,Coordination,Decentralization,Game Theory,Governance,Permissionless,Tokenomics",
      "keywords": "Vote Escrow,Funding Allocation,Voter Analytics",
      "duration": 535,
      "language": "en",
      "sources_swarmHash": "8a6871d32bd8b80aedb0c02d15838b45bb8315fb3f851587e4f2362a09ca2690",
      "sources_youtubeId": "wLw9Xvigdqs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:10:00.000Z",
      "slot_end": "2024-11-13T09:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1hyhPIjZoL4CayjCbBks0Dvhf-v1OSKN0dKkek0vNdSE",
      "resources_slides": "https://drive.google.com/file/d/1ysoWwrm94ML3YCcOHq2Fhl2vnvZ-PMPO/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "time-is-all-you-need-optimizing-dutch-auctions-on-arbitrum",
      "sourceId": "QNSX9R",
      "title": "Time is all you need: optimizing Dutch auctions on Arbitrum",
      "description": "Dutch auctions are a common approach in MEV-mitigating mechanism designs. However, little work has been done in exploring optimal auction execution times.  Using simulations, we demonstrate how optimizing for a key metric — wait time — can achieve optimal execution without the complexity of existing systems.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization Improvements,Layer 2s,Mechanism design,MEV,auction,dutch,Decentralization Improvements,Layer 2s,Mechanism design,MEV",
      "keywords": "Dutch,auctions",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "7b4f3808fd7baa654f358d3a9d534587739f8c60e00696cfd4ee495f27ccbbf3",
      "sources_youtubeId": "eq2AbGusaJY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:10:00.000Z",
      "slot_end": "2024-11-13T09:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1DhrF39oif7Piw0FK877aPOnLTq12Z7iwOXeKa33SnVU",
      "resources_slides": "https://drive.google.com/file/d/1tANSlnRSSqUAGImjL6DqB5mPekqZvCvI/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "epf-day-panel",
      "sourceId": "ZMRJ9B",
      "title": "EPF Day Panel",
      "description": "Panel with former fellows who became core devs and mentors",
      "track": "[CLS] EPF Day",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 2235,
      "language": "en",
      "sources_swarmHash": "072589e67adaa227348834bef25064203fc523216871a6bae08780ba110064ef",
      "sources_youtubeId": "BT1mIVNNOts",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347fbe9dbb7a90e1b8c58d",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:15:00.000Z",
      "slot_end": "2024-11-13T10:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1zfYthY0BXd-oH251a-aAijUCaZpAylXrBQ0_5terdHk",
      "resources_slides": "https://drive.google.com/file/d/19AAp3P2RFPU44UpizTvD0eu-QrHTb_nX/view",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "inclusion-list-inevitable-tradeoffs",
      "sourceId": "XEE9EG",
      "title": "Inclusion List Inevitable Tradeoffs",
      "description": "Inclusion lists have been a popular topic over the years, with various versions emerging, such as EIP-7547 and FOCIL. All these inclusion lists are constrained by a common trade-off: the Ethereum slot time. This talk explores the details of this trade-off and examines whether there is a \"best\" solution given these constraints.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization Improvements,Censorship Resistance,inclusivity,lists,Censorship Resistance,Decentralization Improvements",
      "keywords": "inclusion,list",
      "duration": 426,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "GKBV62BamGo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673473f09dbb7a90e128f649",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673473f09dbb7a90e128f649.vtt",
      "transcript_text": " So this one is more again on inclusion this and it's more on the engineering side that as someone that has been focusing on inclusion this over the last year and I also started looking at fossil more and more and this is where my perspective in terms of engineering challenge with regards to inclusion is. So today, where we are, right? So today we have Ethereum slot and each slot is 12 seconds. And what are the constraints within the slot? So as a proposer, I want to propose a block. And I would hate to have my block gets reorg and that's not nice So I want to propose My block on the strongest head that's possible right and after I propose a block everyone else on the network that's running a node will verify the block and compute what is the head and As a tester their job is to attest to the head of the block So this is where the for choice follows if you use lmdGhost, and then as an aggregator, as optional, you aggregate attestations, and then the proposer essentially follows the votes for the next slot, and then it builds on top of the head. And then because of timing game, you can see there is a phenomenon that today, the attester caught up is at four seconds. So basically everything is pushed towards a four second mark. And that's kind of the equilibrium right now that everything happens within three to four seconds. And then between four to 12 seconds of nothing typically happens unless you are the next law proposer, you are listening to transactions, and you are building blood. So where does inclusion list fit in into this picture? So here I'm speaking in terms of Fossil. So Fossil is a EIP 7805 I believe, but go search it if you don't know what Fossil is. So Fossil is probably the best inclusion list design that I have seen so far that is mostly bully-proof, in my opinion. And then it has the same slot property. And then, so what Fossil does essentially is that it essentially allows secondary runs of proposals that are allowed to send their local block and then such that it can train the next slot proposer, essentially. So where does that leave us in the picture, right? So because of that, we have to essentially add this proposer in the middle of the 12 slots. That means that as a proposer for the inclusion disk, I have to essentially verify the block beforehand, such that I want to essentially propose the block beforehand such that I want to essentially propose the best inclusion disk effort, right? And then as a constraint, the next slot builder or the proposer, I have to essentially pack the inclusion disk into the block. And then if I miss inclusion disk, then I may miss my block. And also as a tester, I want to make sure that the block satisfies the inclusion this. So there are three more constraints as a builder, as a proposer, as a tester, which I cover here. So where are some parameters that we can play in terms of trade-off? So for example, how big is the size of an inclusion disk? If the inclusion disk size is so small that it may not be useful, but the inclusion disk size is too big, then you open up network for DOS concerns. What is the size of the inclusion disk committee? Because we want committee size to be reasonable, but then if the size is too big again you open up network does concern and then how much overlapping are there within the inclusion this and then what is the satisfactory rule right so as a proposal for the next slot as a tester i'm verifying the block like what like basically like how much of the inclusion this the proposal has to satisfy for the block block to be valid. So what are the concerns? So first concern, I think, is the increase of bandwidth and compute for node. Like, depends on how big your inclusion disk size is. And then, again, the second concern is that proposer, like, how much time do I have to build the block? And then, as a tester, how much time do I have to verify the block? So here are some open questions for us to study if we're interested in this inclusion disk space. How compatible it is with the future roadmap, such as peer-dos, such as EPPS? How does inclusion disk work with account abstraction? And then how can we add block transactions into the inclusion disk such that it doesn't open up those concerns? And then how can we better utilize local mempool for inclusion disk? Maybe we can just essentially send the transaction hash instead of sending the full transactions. Finally, will there be other protocol market for inclusion disk? And it's something that we need to study more. So yeah, if you're interested to contribute, hit up julian and hit up me and then yeah definitely i'm definitely very excited about this inclusion this design space thank you thank you so now we have time for a few Q&A. Please raise your hand if you have any questions. OK. No questions? OK, it seems that there's a question here. Yeah, super interesting. I had a question. So I don't know if you can answer this but um in any capacity are you thinking about doing fossil or inclusion lists for l2s like arbitrum or yeah and how is that different from wasn't the eip and what could be on mainnet right so there too today is most of the literature or all of the literature today they have just one sequencer right so the sequencer definitely have a lot of power say today if you want to force your transactions in there like if sequencer ignores you there's nothing you can do but then there's a lot of people say well you can force transaction through layer 1 but that's also not nice because you have to wait like 24 hours right but then like I think like decentralized sequencer kind of self-set if you assume an honest majority so I would say the space in terms of censorship resistance on there too it's it's definitely very different on there one because on there too you can essentially having like 1 million validators you could just have like 10 sequencers and then trust like honest majority and then as long as you assume some of them are honest, and they were, like basically, basically they have to include your transactions. Any other question? Okay, well, thank you very much for your talk. Please give some applause to Terence.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:20:00.000Z",
      "slot_end": "2024-11-13T09:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/18aJAdqUOqTUSwaSiW85kTjIKaVx1BRU7lQDigrzc_wc",
      "resources_slides": "https://drive.google.com/file/d/1uLT2CwpWgLHUSedTDr-OdnBBtuXhAQ9a/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-next-generation-of-governors-will-be-modular",
      "sourceId": "DEAUWE",
      "title": "The next generation of governors will be modular!",
      "description": "Onchain governance is one of the main non-financial usecases of ethereum. Still, innovation in that space is slow, and deployed solution are still very much tighted to financial assets. In order to move away from that situation, and build more powerfull governance solution, we need to build a more modular and evolutive approach.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Design,modular,Design,Governance",
      "keywords": "Smart contracts,modularity",
      "duration": 418,
      "language": "en",
      "sources_swarmHash": "712084596ebb0aeddfcee323eece11a1914339db0b1d9170b841199a20de0882",
      "sources_youtubeId": "iyWhVEouHn4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734849a9dbb7a90e1fb0634",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:20:00.000Z",
      "slot_end": "2024-11-13T09:30:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1DnvD2EnuiJkqkdlnAA1h6CZl0zqKU90ShcgX4KV0SrE",
      "resources_slides": "https://drive.google.com/file/d/1FJadPAlern7uyE_qNEQJQaA-HQPwNLpI/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "voting-with-time-commitment",
      "sourceId": "7V7QNK",
      "title": "Voting with time commitment",
      "description": "Token-based voting mechanisms employed by DAOs can encounter three potential problems: plutocracy, Sybil attacks and vote buying. If one were to design a voting mechanism from scratch, how does one ensure that these issues are addressed adequately down the road? This talk aims to provide some intuition for the trade-offs faced when tackling these problems in general, and the role of time commitment in alleviating these issues, in particular.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Mechanism design,voting,Governance,Mechanism design",
      "keywords": "Voting",
      "duration": 1534,
      "language": "en",
      "sources_swarmHash": "ae548aae445f4151f042de6c1fc3c06468c5bc76bf04a3bf33063c4dbff22215",
      "sources_youtubeId": "CYrmSPVuGqs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673476f19dbb7a90e139f8dc",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:20:00.000Z",
      "slot_end": "2024-11-13T09:50:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1P434UTSmq4E68DmH8ddDjupGoA0DAAfW5KIZ-umwqaM",
      "resources_slides": "https://drive.google.com/file/d/1ioMqFQEfmNl59Tzdg4sHBg-fwiRr9Yep/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "building-a-social-app-with-spend-permissions",
      "sourceId": "GP9RAJ",
      "title": "Building a Social App With Spend Permissions",
      "description": "Join our hands-on workshop on building a social app with spend permissions!\r\n\r\nIn this workshop, we'll walk through:\r\n- Writing smart contracts for your social app.\r\n- Creating a frontend for your app.\r\n- Creating and using spend permissions to send transactions without popups.\r\n- Using paymasters to sponsor gas for your users.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Use Cases,Social,Account Abstraction,key,session,Account Abstraction,Social,Use Cases",
      "keywords": "",
      "duration": 2355,
      "language": "en",
      "sources_swarmHash": "85731d7d095e0a0ccb4971810cd60fd9d6a4b6fcdca38c001542ca062bdb96ef",
      "sources_youtubeId": "9daSgrLWIgY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347f6a9dbb7a90e1b460fc",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:30:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1IMXFflR1DsQZPhVlnc9Ss-Xp6JJcahFgzp1FXWS8ldw",
      "resources_slides": "https://drive.google.com/file/d/1fLFFH5GzzYRIMrTxirIFVdltv2GIXTXA/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "darkfi-kills-glowies-intro-to-the-worlds-first-anon-dao-anon-markets-anon-chat-the-coming-regfi-vs-darkfi-split-the-dark-forest-and-secure-freedom-for-sovereign-society",
      "sourceId": "FKED87",
      "title": "DarkFi kills glowies: intro to the world's first anon DAO, anon markets, anon chat, the coming RegFi vs DarkFi split, the dark forest and secure freedom for sovereign society.",
      "description": "The FBI director gave a speech on the \"Going Dark problem\" saying that mass usage of crypto threatens to create dark zones online where law enforcement cannot enter.\r\n\r\nDarkFi created the world's first anon DAO, after our experience on AssangeDAO which raised 55 million and bankrolled Assange's freedom.\r\n\r\nWe have also made the world's strongest anon chat, and the only p2p chat which is actually used. As well as task managers and anon markets. DarkFi delivers.\r\n\r\nFight back and lets gain our freedom!",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Anonymity,ZKP,agorism,Anonymity,Privacy,ZKP",
      "keywords": "crypto-anarchy,agorism",
      "duration": 1209,
      "language": "en",
      "sources_swarmHash": "35935a9b46ad253cf227167671ae96624ef153e85cf54b1f0fd1a02dbfb710a5",
      "sources_youtubeId": "0xEaNNSEk8A",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673477539dbb7a90e13b9365",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673477539dbb7a90e13b9365.vtt",
      "transcript_text": " Okay, I will start now. So, my name is Amir Taki. I'm a free software developer for 20 years and a core Bitcoin developer from 2010 to 2015. I wrote the BIP1. I made the first UK Bitcoin exchange. I'm 10 years plus working on privacy technology. I made the first privacy implementations in Bitcoin, CoinJoin, Stealth Addresses. In 2015, I joined the YPG, and I fought on the front line against ISIS. I was three years working in Syria for the local revolutionary government. And in this talk, I'm going to talk about DarkFi. So DarkFi is an L1 for anonymous smart contracts. It's like, in the same way Monero is the anonymous version of Bitcoin, DarkFi is the anonymous version of Ethereum. So I will also today also talk about the app that we're building for users. Like, what are we putting into users' hands. So let's begin. Let's have a look. So okay, that was actually, let's just go back here. So let's start with the name, DarkFi. DarkFi's name actually comes from 2013, where there was a project that we founded called DarkWallet. It was founded by me and Cody Wilson, who invented the 3D-printed gun, by Vitalik and a group of Spanish hackers. You can actually go on YouTube and you can find the initial video, which is an important piece of Bitcoin history. But the name itself comes from a speech by the director of the FBI. And the director of the FBI said that if people begin using cryptography widely, if everybody starts adopting cryptography wildly widely if the everybody starts adopting cryptography it threatens to create these dark zones in the internet where law enforcement cannot penetrate the going dark problem and the court is he goes you know the law has not kept pace with technology and this has created a public safety problem. So you might go okay well why do we need the cypherpunk values? Cypherpunk values, what are they? Number one is anonymity and I'm not talking about when I say people go oh our project is private, like oh we provide user, no I mean strong anonymity in the sense that if they come for you they cannot find out who you are you are completely protected and there is no gray zone there there is only the project your project is either anonymous or it's not anonymous also censorship resistance what that means is a 100% pure free speech. You can say whatever you want and nobody can shut you down. There is no compromises with free speech. It is absolute. Third property, sovereignty. What that means is we run the infrastructure. We own the infrastructure. They cannot shut it down. It's like in our hands, in the hands of the community. So why do you need these things? Well, imagine a scenario where an authoritarian government comes into power and they begin oppressing your friends and putting them in jail. And you need to mobilize to create a parallel free society. What do you do? nhw mewn gaeaf ac mae angen i chi ddyfodolu i greu cymdeithas llaw ar gyfer cyflawni. Beth ydych chi'n ei wneud? Rydych chi'n creu DAO anonynus, rydych chi'n cyfathrebu anonynus, rydych chi'n cyflawni arian resources to people for different projects. You coordinate the work using anonymous infrastructure, anonymous task managers, and then people get paid a salary. They get paid money. They need to cash out. They need to do OTC. They need to trade it so they can pay for stuff. And people using this infrastructure are able to mobilize. And, you know, what I'm talking about is not a theoretical thing. It's a real thing. The time is coming. And we all feel that within the next five years or so, there is going to be some major event of huge macro, you know, economic or political significance. Right now, they are putting devs in jail for making anonymous software. And when they are doing that, they're trying to frighten people into compliance. And it's like, you know, if I say something and, you know, I'm just saying, you know, commenting about something and you suddenly go, I suddenly go, or someone says to me something, I suddenly go, oh yeah, blah, no. And I react really aggressively and you go, or someone says to me something, I suddenly go, oh yeah, blah, no, blah, blah, blah. And I react really aggressively. And you go, whoa, why is he reacting so aggressively? That is not a sign of strength. That is not a sign of power. So when they are oppressing people and putting people in jail for making anonymous software, that is not, we are not meant to back off from that. That is a signal of where they're weak we're meant to hit them harder we're meant to push further and you know people always go to us oh you know what about this project what about this other project you know Darkfire has no competition because you know there's all these projects go oh yeah I'm a privacy project but I'll join our discord you know check out our Google Docs they are putting people in jail they're putting people in jail and if you are using discord then you are obviously not afraid and if you're not afraid you're probably not doing anything important dydw i ddim yn ffurfio eich bod chi ddim yn gwneud unrhyw beth o bwysig. Yn Lloegr, fe wnes i weld y diwrnod diwethaf, un mhobwys, aeth i brotest ac es i ar y sain a dweud bod y gwleidyddion yn corwpt, for three years. I even read now that they are reducing the time served for criminals in jail from 80% to 60% to make more space in jail because they expect more people to go to jail now, more higher capacity because of these new speech laws. How crazy is that? They're putting people in jail. And people are really suffering right now. The elites are trying to divide society, separate people from each other. They're actively hostile to the interests of the people. fee and we have not in in after billions of dollars of VC money you know the VCs they get the money from central banks you know they get it from their their friends in Wall Street and you know they're not going to they don't put it into disrupting the status quo but then this industry which was meant to change things to have an impact goes to those same guys ends up you know kowtowing before them trying to ask ask, oh, please, sir, give me some funds. And that's why we haven't been able to deliver. That's why projects are going, join our Discord, check out our Google Docs. We haven't even built a chat app after so many years. And now you're going on Twitter and you're seeing a project going, oh, I think the future is AI, conscious agents, communicating, allocating research. It's like, have you built any of that? No. You haven't built anything. It's just talking rubbish. We haven't built the basics that people need. And then people go, okay, you know, Dark Five, what you're building, what about criminals? What about the criminals you know everybody all the movies is is always is all about like you know mafias and criminals and gangsters there's no miss that's not there is a the reason why people are you know like watching this stuff and they're going oh you know I actually sympathize more with the criminals",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:30:00.000Z",
      "slot_end": "2024-11-13T09:50:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1p6CtJjA99UENn3f3VpXSbI_lYWQj6O34OdWgb8FUKiE",
      "resources_slides": "https://drive.google.com/file/d/15yF5An8NMTymdQGfyw5QLfKyqHfcfqPT/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "protocol-alignment-governing-like-a-protocol",
      "sourceId": "JDKAJD",
      "title": "Protocol Alignment: Governing like a Protocol",
      "description": "We define a protocol as aligned when all stakeholders in its network agree:\r\n1. The protocol’s objectives\r\n2. How to measure progress toward objectives\r\n3. How to achieve the objectives\r\n\r\nIn this talk, we'll explore both new and old decentralized mechanisms that governance leads and protocol designers can leverage to address misalignment in governance.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Futarchy,Mechanism design,Futarchy,Governance,Mechanism design",
      "keywords": "n/a",
      "duration": 548,
      "language": "en",
      "sources_swarmHash": "cb195897f2e256e070d9110b111d2d7e584f889225a323b751e7cc280d7e8864",
      "sources_youtubeId": "I_lMVnDgxvk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734816a9dbb7a90e1cffacb",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:30:00.000Z",
      "slot_end": "2024-11-13T09:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1n1_ahUlOLb7iuUb9uaTE_CyPbh0s7FZKpQGTyQ4xxps",
      "resources_slides": "https://drive.google.com/file/d/1g4W7rGdWR-rMu4mKY_KsdIgc3j_c-HiK/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "today-verkle-tomorrow-zk-everything-stateless-everything-lightclient",
      "sourceId": "Z8EEGW",
      "title": "Today Verkle + Tomorrow ZK = Everything Stateless, Everything Lightclient",
      "description": "Statelessness could be one of the biggest unlocks in the Ethereum ecosystem, allowing the protocol to scale massively without giving away control and access to big entities, all while providing some real 'teeth' to the light client ecosystem.\r\n\r\nIn this talk, we’ll see how stateless clients enable immediate scalability and decentralization benefits, and how combining statelessness with ZKing the state transitions unlocks Ethereum’s long-term vision.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Light Clients,Zero-Knowledge,statelessness,Light Clients,Zero-Knowledge",
      "keywords": "statelessness",
      "duration": 1464,
      "language": "en",
      "sources_swarmHash": "c3c8d6808c0b093ef71c7ebcba97b19a2528e60002141e344b7674df85b5c061",
      "sources_youtubeId": "oRiShQ5LPqw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673478069dbb7a90e13fb370",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:30:00.000Z",
      "slot_end": "2024-11-13T10:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1vOoQZu3TYR_edc7RAy-eEqHYRvkAPSwPJBk3veKBxRM",
      "resources_slides": "https://drive.google.com/file/d/1AepsIidX_uKP6DX2o-1d1xEoEpxr5zP2/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "top-opcode-offenders-in-the-zkevm",
      "sourceId": "DJL7RP",
      "title": "Top opcode offenders in the zkEVM",
      "description": "One of the challenges for any L2 is to reflect accurately the cost for each opcode in zk-resources.\r\nEthereum L1 reflects the resource cost in term of GAS but lately it has been proposed chnages in opcode GAS cost to fit the zk-world to make Ethreum L1 more aligned to L2 or even with enshrined zk-rollups.\r\nIn this talk, I will explain the worst performance opcodes when comparing its GAS cost Vs zk-resources cost in Polygon zkEVM in typical transactions (erc20 trannsfers, swaps, ...)",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Layer 2s,Zk Rollups,top,offenders,Core Protocol,Layer 2s,Zk Rollups",
      "keywords": "zk-resources,GAS costs,top offenders",
      "duration": 1362,
      "language": "en",
      "sources_swarmHash": "1cf0c4cf5a7f3375b701fee34d27087aef6897bea41bd54d5ce1afdf40e7e878",
      "sources_youtubeId": "doQJosAFOaM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673477c09dbb7a90e13d8506",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:30:00.000Z",
      "slot_end": "2024-11-13T10:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1NcWox_AiyJE1F6zW2KLfOoCFpaY0DVyowm34wlSdbao",
      "resources_slides": "https://drive.google.com/file/d/1yKgFK_UEKkiCR8OZviogOzogWiNHCu8d/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "local-build-why-language-is-key-to-decentralization",
      "sourceId": "UHVBNL",
      "title": "Local Build: Why language is key to decentralization",
      "description": "Localization is not a “nice to have” for decentralization: it is a core requirement.\r\n\r\nOver 50% of ETH nodes are between the US and Germany. 90% of stablecoins are USD-pegged. The world we’re creating is stifled by the one that already exists. \r\n\r\nTo be credibly decentralized, Ethereum must be built and secured in the human languages of people outside of the current paradigm. This talk will highlight web3-native problems and tangible solutions in l10n, from the technical to the organizational.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization Improvements,Languages,User Experience,localization,l10n,Decentralization Improvements,Languages,User Experience",
      "keywords": "Internationalization,Localization",
      "duration": 572,
      "language": "en",
      "sources_swarmHash": "291a51deef43112c4bd20554687baa9e61d8f77a35566b6d264ee3da2af0d9d3",
      "sources_youtubeId": "oaztuq6hO4c",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673482ed9dbb7a90e1e580fb",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:40:00.000Z",
      "slot_end": "2024-11-13T09:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1zMgBNNs4mjcJlQvsWzcG-01qBLosEtl3W_zPUteNz-0",
      "resources_slides": "https://drive.google.com/file/d/1aLLAARYtpdgsuKe760boUWNpb4Dgfm6v/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "from-auctions-to-zk-an-educational-tour-of-mpc-tools",
      "sourceId": "7TRTQW",
      "title": "From Auctions to ZK: An Educational Tour of MPC Tools",
      "description": "Ethereum made a significant contribution to the Cypherpunk agenda by removing central points of trust, allowing us to gain accountability, yet losing us any semblance of privacy that we had. There is hope at hand for privacy, but hope, in this case, is rather technical.\r\nThis talk aims to bring you up to scratch on privacy preserving tools while discussing S{N,T}ARKS, TEEs, FHE, how MPC elevates them in a decentralized setting, and highlighting their use from Auctions to ZK, from the 90s til now.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zero-Knowledge,MPC,Homomorphic Encryption,confidentiality,computation,Homomorphic Encryption,MPC,Zero-Knowledge",
      "keywords": "Confidential,computing",
      "duration": 1533,
      "language": "en",
      "sources_swarmHash": "026bba89d2edb936b13ef6502e54fd362b2d4d890f0e4ca06732592db8ed78d2",
      "sources_youtubeId": "fC44nTlYz4w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347dda9dbb7a90e1a56a9d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:50:00.000Z",
      "slot_end": "2024-11-13T10:20:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1VLWGFuzmpGa1l5aa_6_T3lRO-nTsPDh5IXDg9sFoZM8",
      "resources_slides": "https://drive.google.com/file/d/16vdhB82UKPQJTowHLmV5SXn2si4cRwa0/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "the-fixed-rate-flywheel",
      "sourceId": "WYWLXV",
      "title": "The Fixed Rate Flywheel",
      "description": "In the rapidly evolving landscape of modern DeFi, fixed-rate protocols have emerged as a critical component, bridging the gap between traditional finance stability and DeFi innovation. This panel introduces \"The Fixed Rate Flywheel,\" a powerful concept illustrating how fixed rate markets fuel variable lending, create hedging opportunities, and generate high-yield products. Join us to hear experts from DELV Tech, Morpho Labs, Phoenix Labs, and Gauntlet talk about the next evolution of DeFi.",
      "track": "Cryptoeconomics",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "fixed,rate",
      "keywords": "DeFi,Fixed Rates",
      "duration": 3282,
      "language": "en",
      "sources_swarmHash": "fb92f51e567f58d5275601ba18e31fd5273866fca5fa0ff9479ca09ee5036cdd",
      "sources_youtubeId": "RLzBsBudpFA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673485c89dbb7a90e106942c",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T09:50:00.000Z",
      "slot_end": "2024-11-13T10:50:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1ng1HvT-kAE4r-IB_k-m3qkQnZ9PMYl3wwR_zkEmF4Fg",
      "resources_slides": "https://drive.google.com/file/d/1KWVG3Uic3DfGAsnTkx4ZCMIcsBygR6oC/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "announcing-the-argot-collective",
      "sourceId": "DA7STK",
      "title": "Announcing the Argot Collective",
      "description": "The Argot Collective (argot.org) is a new non-profit organization maintaining Ethereum's core infrastructure formed by 25 former EF employees. \r\nOperating democratically and transparently, we aim to provide stable, long-term support for crucial projects, free from commercial pressures.\r\n\r\nIt's the new home for the projects:\r\n- Act\r\n- EthDebug\r\n- Fe\r\n- Hevm\r\n- Solidity\r\n- Sourcify \r\n\r\nThis talk will be the first introduction of this new organization to the ecosystem",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Languages,Open Source Software",
      "keywords": "Organizations,Solidity",
      "duration": 1441,
      "language": "en",
      "sources_swarmHash": "46c6b46ba9a805c1dc6d4e3710332d2de476aca7634f9b1811692955f7a29057",
      "sources_youtubeId": "2O64s6oOl2g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347f3b9dbb7a90e1b14908",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:00:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1wCMf3indiDC3mcME4zorE86wItPgSN0glxPKvU3HC_U",
      "resources_slides": "https://drive.google.com/file/d/1OT6HV0X4le8Xjral_uN644NslSIIgGAH/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "synto-nikka",
      "sourceId": "ZBSJDY",
      "title": "Synto Nikka",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:00:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1qlDffU55LOyqC5g5m_XelYjXsBTWIYahAHtzcqgHwic",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "the-daos-of-the-east",
      "sourceId": "BUKGLV",
      "title": "The DAOs of the East",
      "description": "DAOs are growing fast in East Asia, and they work very differently from DAOs in the West. From regional revitalization in Japan to Taiwan's digital ministry to the Chinese diaspora, I'll cover many examples and what they mean for the global community of DAOs.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,Collective Intelligence,Regulation,asia,Collective Intelligence,DAO",
      "keywords": "Asia",
      "duration": 1454,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "VK3Uv1G_hHI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:00:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/185nuWRZn9PaXkbj3mmudjiul9XhVrRireCzXcJBlu4Y",
      "resources_slides": "https://drive.google.com/file/d/1J_r5wl8YGbGtaLxHKFm4LNZoRVCyC0QK/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "the-future-of-layer-2-research-development-and-next-gen-technologies",
      "sourceId": "PJQQSR",
      "title": "The Future of Layer 2: Research, Development, and Next-Gen Technologies",
      "description": "Discussion around L2 blockchain research and development. What are the major challenges for L2s to advance, and what solutions are being explored? What will the L2 space look like next year and beyond? The talk will be illustrated with examples from Arbitrum’s research and development.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Scalability,arbitrum,Layer 2s,Scalability",
      "keywords": "Arbitrum",
      "duration": 1539,
      "language": "en",
      "sources_swarmHash": "04837782db7800ae0149069f9ac27bfc65f9d8593413bf75bb9314b9c9604a2f",
      "sources_youtubeId": "6GHjgjD9Va8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:00:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1j5n0blTsDLltg5bxumMOQ0zvAqbfL-faBMhuzsnBX3k",
      "resources_slides": "https://drive.google.com/file/d/1bBY4w4qCHkXs8_-PjrCuA-rFAxny5H8A/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "the-verge-is-not-going-to-break-your-contracts",
      "sourceId": "NJXNE3",
      "title": "The verge is (not) going to break your contracts!",
      "description": "The verge is comming, and with it a new pricing model for storage. This breaks many assumption that compilers have been doing for years. We'll see how part and future contracts are going to be affected, and what design should be favored in anticipation of the verge.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Verkle trees,Libraries,Best Practices,compilers,Best Practices,Libraries,Verkle trees",
      "keywords": "compiler",
      "duration": 1140,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "SAhp3LgbMYo ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "",
      "transcript_text": "",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:00:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1qXCj-zxWc3N3cgUT-kq17kAdjRXdLfCUoe5VGTpy0TE",
      "resources_slides": "https://drive.google.com/file/d/1p_t4oRwZR0YvwZMVfL4JO7tTbgI06sAf/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "unchained-index-a-purposefully-designed-schelling-point-a-native-web3-api",
      "sourceId": "VBUJML",
      "title": "Unchained Index: A Purposefully Designed Schelling Point: A native Web3 API",
      "description": "The Unchained Index smart contract, part of TrueBlocks, acts as a purposefully-designed Schelling Point, creating a decentralized, permissionless store for blockchain index data. In this talk, we generalize the Unchained Index to show it can serve as a repository for other datasets such as event signatures and address labels. We contend we can replace costly APIs with a robust, reproducible public good, enhancing data accessibility & decentralization.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Decentralization,Ethereum for Good,Coordination,Decentralization,Ethereum for Good",
      "keywords": "none",
      "duration": 612,
      "language": "en",
      "sources_swarmHash": "ea604c4fb470594534b4c6e9037f54f969d7e9fad9537949cba6906a31938188",
      "sources_youtubeId": "bfFZzY0h9qQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348bb99dbb7a90e154aafc",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:00:00.000Z",
      "slot_end": "2024-11-13T10:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/12qCfXtoD8E9oGVRdfTgU97VfTsXFeb1ceIy1bYwWAV0",
      "resources_slides": "https://drive.google.com/file/d/1faF3jCQjTHS6t7lAc2cR-LskKPNY9mwL/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "bringing-peer-to-peer-networks-to-all-the-peers",
      "sourceId": "PREYYS",
      "title": "Bringing peer-to-peer networks to ALL the peers",
      "description": "The p2p networks of the Ethereum ecosystem generally draw the line to server nodes. True end users devices: mobiles, laptops, browsers, are excluded and use centralised APIs and gateways to access the p2p network. Removing sovereignty, censorship-resistance and privacy in the process.\r\n\r\nIn this lightning talk, we’ll review everything that can go wrong when trying to include resource restricted devices in a peer-to-peer network, using the most popular tools and libraries.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization Improvements,Privacy,Censorship Resistance,resiliency,Censorship Resistance,Decentralization Improvements,Privacy",
      "keywords": "Sovereignty,peer-to-peer networks,resilience",
      "duration": 604,
      "language": "en",
      "sources_swarmHash": "a8dd8f3845f3154ccc04afef7a8d6fb793e689af52a7dadaf9bd8132da224854",
      "sources_youtubeId": "8qAn_5NTQZY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67347ed79dbb7a90e1acfae5",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:10:00.000Z",
      "slot_end": "2024-11-13T10:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/15T911YKp9NooTa41RChSG4jfO2xC3VEveRPwe9SbKcc",
      "resources_slides": "https://drive.google.com/file/d/1yjjc__aTeG8PJhisw5J2XfyrdF0tu2OI/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "indexing-entire-24-billion-transactions-on-ethereum-in-10-hours",
      "sourceId": "QEDEUG",
      "title": "Indexing Entire 2.4 Billion Transactions on Ethereum in 10 Hours",
      "description": "This talk covers learnings from building a general-purpose indexer which index every single transaction since genesis. There is also technical decisions when we have to deal with 7 billions records of data and how to process all of those data in less than half a day. Additionally, we will discuss the difference between batch data processing and real-time data processing, sharing best practices and strategies for both approaches.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Scalability,Event monitoring,data,processor,Architecture,Event monitoring,Scalability",
      "keywords": "Data,Processing",
      "duration": 509,
      "language": "en",
      "sources_swarmHash": "acedb4b51b4007f8a151c15a8ebd2d0e0ca17cb2dfe9172b2556b692d4a55d05",
      "sources_youtubeId": "MQsj9MWBz1M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734879d9dbb7a90e1230047",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:10:00.000Z",
      "slot_end": "2024-11-13T10:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1e7StVYyUS6PD_m8Qka4g3W8mafU8txCAZgD9XA95sSI",
      "resources_slides": "https://drive.google.com/file/d/13pfCL_lp6_NyfbqNJeY_vD-0n1l-4NfP/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "beyond-technology-ethereum-as-the-philosophy",
      "sourceId": "TZV7RV",
      "title": "Beyond technology: Ethereum as the philosophy",
      "description": "In this talk, we dive into the philosophical foundations of Ethereum, bridging blockchain principles with ideas from ancient Greek philosophy to contemporary thought. We’ll unpack Ethereum's conceptual roots—shaped by cyberpunk, solarpunk, crypto-anarchism, technolibertarianism, and more—to reveal how each contributes to Ethereum’s vision of a reimagined society.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Values,Solarpunk,Ethereum for Good,intellectual,engagement,Ethereum for Good,Solarpunk,Values",
      "keywords": "philosophy,technophilosophy,intellectual engagement",
      "duration": 496,
      "language": "en",
      "sources_swarmHash": "e19f5fc32d1a5089d8018f712e7d0b670d892704a4a0f3fa9018d3fc5a719c62",
      "sources_youtubeId": "Wan90qeRCxA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67349ada9dbb7a90e11d7cab",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67349ada9dbb7a90e11d7cab.vtt",
      "transcript_text": " Hello everyone, I'm Sasha and I'm here to talk about Ethereum not just as a technology but as a philosophy. During these few minutes we'll journey through the history of thought, exploring how ancient and modern ideas align with what Ethereum brings to our world today. But before we dive in, a quick disclaimer, as philosophical concepts I'll touch upon have been simplified, and the majority of philosophers will discuss never-before-seen blockchain, and linking their insights to Ethereum is quite an interpretive intellectual exercise. Well, we're all aware of how Ethereum is transforming our economy, society and governance, but its impact goes even deeper as Ethereum also challenges us to think about core philosophical dimensions from the nature of reality and truth to ethics and art. Ethereum Core extends beyond decentralization, programmability, and automation. It embraces resilience, open collaboration. It values inclusivity and sovereignty rooted in a vision of social economic freedom. Ethereum aspires to create a borderless network where innovation thrives and hierarchies dissolve. These fundamental principles resonate deeply with timeless ideas explored by philosophers across the history. Thus, philosophical concepts like decentralization and consensus appeared in Greek city-state or polis. Plato envisions them in his works. Also, trustlessness or parisian was another core idea, as well as privacy, with Aristotle's distinction between public, polis, and private oikos spheres. Furthermore, blockchain's mutability mirrors determinism and the medieval pursuit of eternal truth and decentralization feudal hierarchies. The Renaissance introduces a concept of security, an idea that became central to Ethereum's framework, and, for example, Giordano Bruno's idea about universe as a living organism may match the Ethereum's infinite garden metaphor. Early modern philosophy, spanning the Enlightenment and beyond, emphasizes reason, individual freedom, and skepticism toward traditional authority. Thinkers like Jean-Jacques Rousseau or John Locke championed personal liberty, autonomy and property rights. Also, this era focuses on industrialization, finding its modern reflection in smart contracts. As we progress through history, anarchism seen in Daoist and DeFi celebrates community-driven, intermediary free systems. ProDON's idea of industrial democracy and Kropotkin's mutual aid resonate here. As does libertarian socialism through Daoist that balance egalitarian P2P interactions with social justice initiatives like UBI. Pragmatist thinkers like John Dewey underscore cooperation and practical solutions, action-based outcomes. Contemporary philosophy brings fresh new perspectives. Thus, phenomenology, which emphasizes individual freedom, aligns with Ethereum's focus on personalized interactions. Then, emphasizing freedom, choice, and responsibility, existentialism matches with Ethereum, which enables autonomous decisions and accountability. Analytic philosophy in Wittgenstein's call for clarity echoes in Ethereum's language for secure exchanges. And of course, postmodernism, which critiques fixed hierarchies, resonates with Ethereum ethos. Worth noting that today's thinkers bring diverse perspectives on blockchain. Some like Zizek critique it while others acknowledge its transformative potential. Now I want to briefly talk about Ethereum's philosophy, philosophical DNA as I said. I believe, and I think many of us here believe, that Ethereum embodies a convergence of multiple philosophical movements which form its philosophical DNA, blending in many fascinating ways, starting from cyberpunk and cypherpunk, which sound similar, but each hold unique vision of freedom and autonomy. While cyberpunk imagines rebellion against centralization, cypherpunk provides the cryptographic tools for privacy. Other influences are cyber-utopianism, which sees technology as a path to democracy. It views Ethereum as a path to democracy. It views Ethereum as a tool for open, accessible systems... that empower individuals worldwide. And crypto-anarchism, which emphasizes privacy and autonomy... resisting state control through cryptography. Then, there is... Ethereum also draws on techno-libertarianism and posthumanism. In essence, the first highlights the theorem's role in personal autonomy, while the second envisions it as a transformative medium reshaping identity and community. Next are object-oriented ontology and defensive accelerationism. Object-oriented ontology is a part of a popular speculative realism movement. It views all entities, human and non-human, as equally significant. In this perspective, every block, smart contract, and code line on Ethereum has its own agency, acting within a flat network of relationships. And defensive accelerationism, as many of you know, supported by Vitalik Buterin, it calls for responsible tech progress, balancing innovation with ethical responsibility. Lastly, Solarpunk and Lunarpunk, they represent Ethereum's dual potential, sustainable open future alongside robust privacy-focused protections. Also, to further frame Ethereum as philosophy pluriverse, an ecosystem of diverse interconnected realities where multiple systems, cultures, and values coexist and interact. Then, epistemologically, Ethereum offers trustless knowledge, a paradigm where truth is verified collectively. Then, in terms of ethics, Ethereum introduces technetics. In terms of teleology, one of the goals of Ethereum is a tool for digital liberation. Mainly Ethereum isn't just a technology, it's a living philosophy, an experiment in rethinking trust and freedom, please invite us to ask what happens when the code carries our values and when autonomy, privacy, and decentralization become foundational. Together, we're shaping a new philosophical paradigm. Thank you. Thank you. Thank you so much, Sasha. That was great. I don't think we have time for Q&A, so thank you so much. And we've got about two minutes until our next...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:20:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1_ySmTRldIj5s-VJNLFK0CcREWHnRA9KeGMc5zS6taAk",
      "resources_slides": "https://drive.google.com/file/d/1Hs8eFPigMsB051sylBZWQk4x4a5HPC40/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "cypherpunk-is-slow-not-hyper-financialized-and-unlike-twitter",
      "sourceId": "QPQZJR",
      "title": "Cypherpunk is slow, not hyper-financialized and unlike Twitter",
      "description": "In this lightning talk I will present three major directions that we need to tackle to make Ethereum Cypherpunk:\r\n1. Against popular trends, I call for increasing block time (instead of making it faster) to increase resilience via better DVT and mixnets - both are struggling with low latency blocks\r\n2. Let's revive the Ethereum world computer, not just financial infrastructure and their implications\r\n3. Rethink [d]app UX entirely - how does resilient human interaction feel like in the digital era?",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "latency,Censorship Resistance,Ethereum Roadmap,Not financial",
      "keywords": "Latency",
      "duration": 467,
      "language": "en",
      "sources_swarmHash": "81e87623a13ffdd562e5c7608e9481ec24aa6405629b9a8fedfa843deab3d387",
      "sources_youtubeId": "zIqVFYQqPk0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67349e5c9dbb7a90e13140a5",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:20:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1oHb6j9HUcr5SBg9cKc9eUxdiZdwushIlE08dKhrQ1zE",
      "resources_slides": "https://drive.google.com/file/d/1qJHjV9G5v7I6s3DSDZb7gyTx-QOO-Nql/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "txain-discover-the-next-generation-of-blockchain-exploration",
      "sourceId": "WRGHRM",
      "title": "TXain: Discover the Next Generation of Blockchain Exploration",
      "description": "Discover TXain, the next generation blockchain explorer designed to elevate your blockchain experience. Join us as we delve into our key features: an intuitive UI, real-time data, advanced search capabilities, and in-depth analytics. As a new startup, we’re committed to performance and information clarity, ensuring seamless navigation and comprehensive insights. Learn how TXain is set to redefine blockchain exploration, providing the tools you need to explore, analyze, and understand the blockch",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "data,real-time",
      "keywords": "blockchain explorer,user experience,Real-Time Data",
      "duration": 426,
      "language": "en",
      "sources_swarmHash": "c7b6d89b21dc79bb6f3eaeb558d3882f666c486f9c880a25afae3b3dcac9a1df",
      "sources_youtubeId": "4NJZijpEH6A",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348d8a9dbb7a90e16d35b3",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:20:00.000Z",
      "slot_end": "2024-11-13T10:30:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1_ATKYtQF_Q_hjc85bqwcab990AdWWjiO8FiSDVR2BMg",
      "resources_slides": "https://drive.google.com/file/d/1BHD2qn-t9w9iIIhYcwc2J6psxsyh0iTk/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "build-your-first-onchain-game-with-mud-in-5-minutes",
      "sourceId": "FRVJHK",
      "title": "Build your first onchain game with MUD in 5 minutes",
      "description": "Have you ever wanted to build a game that runs fully onchain, but didn’t know where to start?\r\n\r\nIn this lightning talk, Alvarius will walk you through the basics of MUD, an application framework designed for games and autonomous worlds, and build a minimal onchain game from scratch.\r\n\r\nLearn the basics, learn the tooling, and learn where to go to keep learning and building after this session.",
      "track": "Developer Experience",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Frameworks,Gaming,Autonomous World,mud,Autonomous World,Frameworks,Gaming",
      "keywords": "MUD",
      "duration": 418,
      "language": "en",
      "sources_swarmHash": "a70a986e206d0d7674fa83c328e9afe6fe1e6665599b677b101637cf6478f48c",
      "sources_youtubeId": "twSYvUk55TQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348dfc9dbb7a90e1732e65",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T10:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1qMDjbZEumhcxlbrMh8-E5iJpvSVc-VXecukqL_lGC4I",
      "resources_slides": "https://drive.google.com/file/d/1o_LnE8NiMCuiDYzLw5aDNkoXd09a8DOn/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "cheatcalls-eip",
      "sourceId": "FSSDUT",
      "title": "Cheatcalls EIP",
      "description": "Development nodes, such as Anvil, Hardhat, and Tenderly, provide specialized methods to manipulate blockchain state. For example, the `anvil_setCode` method allows arbitrary overriding smart contract code. \r\n\r\nUnfortunately, each node implements a slightly different set of methods with varying behaviours, resulting in wasted development hours and potential vendor lock-in.\r\n\r\nIn my talk, I introduce a new EIP that proposes standardized `cheat_*` methods with well-defined interfaces and behaviour.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,DevEx,Best Practices,hardhat,Best Practices,Developer Infrastructure,DevEx",
      "keywords": "EIP,anvil,hardhat",
      "duration": 1459,
      "language": "en",
      "sources_swarmHash": "0cb6ab2a62dfb516dfe5874f72d5dd2c24314f94c4351a4db19f278ec45af1cf",
      "sources_youtubeId": "uG5SEBzh3qg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673486969dbb7a90e11222dd",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1SPNfEMjAph1OpaPequc8om3JZMEGtbmn17fCEmGQhuE",
      "resources_slides": "https://drive.google.com/file/d/1h_UlftTJ5OCCWsd8hA0KSPJ27dDQwJsi/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "cypherpunk-for-centuries-coordination-and-secrecy-across-the-ages",
      "sourceId": "NMKQYY",
      "title": "Cypherpunk for Centuries: Coordination and Secrecy Across the Ages",
      "description": "Join Evin McMullen for an adventure through the historical ledger, learning from ancient examples of human coordination, governance and selective disclosure technologies whose principles are reflected in the onchain experiences we know and love today. \r\n\r\nPull up a chair, anon. Class is in session, so let’s explore the core Ethereum Values and context in which we live, and what came before us, through the lens of tech that led to the modern cypherpunk movement.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Network State,Security,culture,Governance,Network State,Security",
      "keywords": "History,Culture",
      "duration": 330,
      "language": "en",
      "sources_swarmHash": "4c878ebdfca963d75a8fbe08904ffc6ce431bbc2f152f4aca3eaed01a2cd0010",
      "sources_youtubeId": "XJKbwYhs3j0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67349ecd9dbb7a90e132caa2",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T10:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1zKy1Wacd_g6VIy9gBPNTLczV1UoUIzGVToCNnN39u1c",
      "resources_slides": "https://drive.google.com/file/d/1qp0D9X40TlsiZ36J7E1P3YhccrliNn6u/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "panel-source-code-verification",
      "sourceId": "UJJPSH",
      "title": "Panel: Source Code Verification",
      "description": "Source code verification is the basis of trustlessness and transparency in blockchains.\r\nMany projects do source code verification but there hasn't been much collaboration and public interaction. The panel will bring members from the new collective \"Verifier Alliance\" together to create an open discussion.\r\n\r\nTopics include open-data and open-source, standardization, future challenges like state and data growth, multichain, monetization, and financial sustainability",
      "track": "Developer Experience",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,User Experience,blocks,explorer,Developer Infrastructure,User Experience",
      "keywords": "Source Code Verification,Block Explorers",
      "duration": 241,
      "language": "en",
      "sources_swarmHash": "d6d57f8aec03fc28074f6e2d132ac76edc081fbb1a655a7ed4b49af8c342d5d4",
      "sources_youtubeId": "XBYmL8ICFyQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348f1d9dbb7a90e1860f67",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1q-4HjJon6v4PjMBDOXwQwQS2B6fgTj_TjlTh6teEZd0",
      "resources_slides": "https://drive.google.com/file/d/1F5Krv5veS-IABDwYpfWBM73Z1Su47EmK/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "smart-contracts-with-privacy-case-study-buying-renewable-power",
      "sourceId": "F9PWUP",
      "title": "Smart Contracts with Privacy - Case Study - Buying Renewable Power",
      "description": "Getting the world’s industries to switch to renewable power is immensely important for our planet’s future, but renewable power purchasing agreements turn out to be complicated to manage and administer.  Buyers and sellers must interact indirectly through the electricity market and agreements contain complex rules.  Keeping track of these is complicated and expensive - UNLESS you have a blockchain-based smart contract.  This is how we did it, using ZK for privacy, on chain!",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Zero-Knowledge,Use Cases,enterprise,Privacy,Use Cases,Zero-Knowledge",
      "keywords": "Enterprise",
      "duration": 1440,
      "language": "en",
      "sources_swarmHash": "3d1b5977c282fddf02aa30d70116459a362882bb2c1df1029d7cba048a07cf9b",
      "sources_youtubeId": "mFX6m60ceIY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67349b569dbb7a90e11ec407",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1iPCFSCb5vpiqtzwoYxszBwbVcjQ5iI86jv7FH1Uo3E8",
      "resources_slides": "https://drive.google.com/file/d/1Y_rxgbY335V1M40cjFY71QkPdI8XdPf0/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "the-future-of-light-clients",
      "sourceId": "UL8U8B",
      "title": "The Future of Light Clients",
      "description": "Ethereum has achieved a remarkable feat: production-ready light clients. There are now at least seven light client projects active on Ethereum today.\r\n\r\nHowever, light clients have kept up with Ethereum’s future, Layer 2s. Implementations for layer 2s have been mostly overlooked. This is due to both the low prioritization of work on light clients and significant technical challenges. In this talk, we will discuss the path to layer 2 light clients and our work to bring them to production in Helios.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Light Clients",
      "keywords": "",
      "duration": 1503,
      "language": "en",
      "sources_swarmHash": "ae42d7faa5d49909983fcbdfc21c6fb48c8961506f5fd90a364aa222c9eea601",
      "sources_youtubeId": "SrZBfwnjf7M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673486b89dbb7a90e1136426",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/11L_sO6Usnx1os7aiKFPC2mNm1diDnV9Hlo7PETnsic8",
      "resources_slides": "https://drive.google.com/file/d/1jADpteHZL5COexS36m7GhgUqaR7Fa8rw/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "the-rise-of-appchains-from-l2s-to-rollup-clusters",
      "sourceId": "SEARYQ",
      "title": "The rise of Appchains: from L2s to Rollup Clusters",
      "description": "Ethereum's rollup-centric approach has led to the emergence of L2 Rollup Clusters reducing fees but creating fragmented liquidity and a less seamless user experience. Third-party bridges, though helpful, are cumbersome, vulnerable to hacks ($2B losses to date), and costly, leading to high fees. In this keynote, Alex will discuss how native interoperability, with ZK at its core, can resolve fragmentation, enabling Clusters to collaborate instead of competing for users and liquidity, ultimately dr",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,Appchains,Zero-Knowledge,interoperability,Appchains,Ethereum Roadmap,Zero-Knowledge",
      "keywords": "Fragmentation,UX,interoperability,Rollup Clusters,L2",
      "duration": 1508,
      "language": "en",
      "sources_swarmHash": "67989f0d9198656929f445c0b928d7b40f9c288f16a42d67c8773572544bef03",
      "sources_youtubeId": "HHl2iOgP4FA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673498739dbb7a90e1d3f53f",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:30:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1WOJXGXgVk5LDrCpMtULqypFYqyEzI5whhM4XbIRAcVA",
      "resources_slides": "https://drive.google.com/file/d/1TcKl3w5dbErFKy6Uyaq_Yk9I_JTyZBPV/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "ethereums-ultimate-gift-will-be-birthing-digital-matter",
      "sourceId": "XSCFZR",
      "title": "Ethereum's Ultimate Gift Will Be Birthing Digital Matter",
      "description": "Bitcoin created Digital Gold, intangible yet valued like real gold. Ethereum will birth Digital Worlds which culture will treat as real. Unlike Bitcoin's scarce digital coins and tamper-proof IOUs, these worlds will have scarce digital matter and tamper-proof physics. Within them, inhabitants will use primitives like smart items to build economies and civilizations with society-shifting GDPs.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Gaming,Use Cases",
      "keywords": "",
      "duration": 1025,
      "language": "en",
      "sources_swarmHash": "b2b83d28ec83257b3cf45d5993e985e81d999f07117256d8a488907ff1599d39",
      "sources_youtubeId": "ZwkGlbjT1SQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734901b9dbb7a90e1916dd8",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:40:00.000Z",
      "slot_end": "2024-11-13T11:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/15oxvM3TxOCUK4NDmYqvX1h3RKEylrnyt66ZdyLe_RR0",
      "resources_slides": "https://drive.google.com/file/d/1AzSsLIGm8Zem7ED3gLXlLUIeifzZ7Dcy/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "our-cypherpunk-approach-to-self-sovereign-digital-identity-does-not-work-in-real-world",
      "sourceId": "USJSPF",
      "title": "Our (Cypherpunk) approach to Self-Sovereign Digital Identity does not work in real world",
      "description": "For years our community is using cryptography and privacy-enhancing technologies trying to build solutions that will bring people control over their digital identities. How far have we got?\r\n\r\nThis talk would like to expose a gap that exists between our Cypherpunk approach to SSI and what a real world project needs / wants / can do.\r\n\r\nIf we want our SSI solutions to bring control over their digital identities back to people, it seems we need to take a different approach.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "ssi,Digital Sovereignty,Identity,Privacy",
      "keywords": "ssi",
      "duration": 442,
      "language": "en",
      "sources_swarmHash": "9310afeb1f82b238e351a86f537a6eb43ee59f033defe00ad24d7a0cfec43778",
      "sources_youtubeId": "7q2YD5QUHmo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67349f549dbb7a90e13a18b4",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:40:00.000Z",
      "slot_end": "2024-11-13T10:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1tieWVdz2ClCZUAnL4cwbHgtEkk_tNIfgbdodCv6BfoY",
      "resources_slides": "https://drive.google.com/file/d/1bCRsuuduwwQ4gaPIm-pDY3ZyV6QWIHqB/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "demand-based-recurring-fees-in-practice",
      "sourceId": "GWBWPE",
      "title": "Demand-based recurring fees in practice",
      "description": "ALL 4 letter .COMs have been taken since 2013. Yet most only have a few natural buyers; hence, speculation doesn't make that market more efficient.\r\n\r\nYet, in crypto-economics, we can already transcend private property to deter the monopolization of digital assets like domains. \r\n\r\nThis talk explores solutions from Weyl, Posner, and Henry George. We'll show how pricing and allocative efficiency can be improved through Georgist land value tax for assets like real estate, domain names, or ad space.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Economics,Mechanism design,Quadratic Voting",
      "keywords": "pricing",
      "duration": 1338,
      "language": "en",
      "sources_swarmHash": "ed8bc9d962b6e970fc945d4aa1df2b5dee1b721a79c9be789ca4d8d245456693",
      "sources_youtubeId": "pjcP-P7q5mU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348b6f9dbb7a90e151420e",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:50:00.000Z",
      "slot_end": "2024-11-13T11:20:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/15PZ749rPc9HedXMUE_qdwIMFPhSIfM_Qt1GSmEy4JsU",
      "resources_slides": "https://drive.google.com/file/d/1IObHgeQDaeL0hcSxTbumWTZSaud--NSd/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "visual-code-of-cypherpunk-and-lessons-from-subcultural-aesthetics-we-should-remember-on-the-road-to-mass-adoption",
      "sourceId": "ZAYEXK",
      "title": "Visual code of cypherpunk, and lessons from subcultural aesthetics we should remember on the road to mass adoption",
      "description": "I want to take builders on the turbulent ride through how subcultural and social movements used their visual codes when spreading globally, and what design tasks are still ahead of us on the way to making Ethereum cypherpunk again and onboarding the next billion users to Web3 at the same time.\r\n\r\nThis ride will include three stops:\r\n1. waving one's emotional state into the collective identity\r\n2. using shared aesthetics as a signal of belonging\r\n3. coordinating a collective design process.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Identity,Design,communication,Coordination,Design,Identity",
      "keywords": "culture,aesthetics,communication",
      "duration": 758,
      "language": "en",
      "sources_swarmHash": "3dd85d3f006a11a867fed6fa39a4901b593753cc0b383f9af7098ac9914b54ce",
      "sources_youtubeId": "aUkVqsDW6t4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67388bae1b0f83434d2b7c95",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67388bae1b0f83434d2b7c95.vtt",
      "transcript_text": " Let's see in the wipe. So we all want mass adoption for our products and we want this products to be loved and used by many. And yet we have one heavy anchor that will always hold us back from mass adoption – a visual language or cypherpunk. Let me show you what's happening. Visual language consists of two core layers. The symbols – they carry meaning and cultural baggage. And the styles – Styles evoke emotions. They always go together when it comes to how we respond to visual messaging. A positive symbol in a positive style evokes joy. A positive symbol in a negative style makes rather negative impact. Think about toys in the horror movies. A negative symbol in a positive style can cause confusion, but rather it brings amusement. And think about Halloween. And a negative symbol in a negative style evokes nothing like fear and disgust. People try to distance themselves from everything that lives in this quarter. Well, adoption lives there. How do you think where cyberpunk is? Let's see. The first symbol, an anonymity mask, traces back to guy folks who earned a reputation of a failed terrorist and actually never fought for democracy. Well, Alan Moore tried to recover this image in the comics V for Vendetta, making it a symbol of vigilantism, but later Hollywood successfully shifted the message, adding a narrative with a fear, creating taglines, and a lot of blood. Today, it's a symbol of cypher funks. Now, here you already see a second symbol, hoods, that was originally associated with monks, the knowledge keepers, which is a good beginning. But later, they became a symbol of secret societies, many of which practice ritual murder and dark magic, if such a thing exists. And recently Hollywood dressed all the assassins in hoods, adding a nice layer of death and blood to the overall image. Today hoods is a symbol of cypherpunks. Well, here you already see the next symbol, which is a common line romance, which again by Hollywood, in a form of digital reign, together with the techno style, presented technology as a global threat to everyday life. And the last style I would like to mention is glitch effect, which was a way for early internet artists to present, to visualize a system vulnerability and the beauty of breaking it. But for normal people, the glitch effect evokes the same feeling as breaking glass. Now, how do you think where on this matrix cipherfunks are? Well, fortunately, we have a century of subcultural movements that can teach us how to ease this tension between our heritage and the past and the mass adoption needs. So here are three main lessons I want cypherpunks to hear. First, choose joy as a strategy. Cypherpunk looked like it was designed for post-apocalypse. But privacy should not look like paranoia. It should look like freedom. Just look how pride movement turned the protest into a celebration. They didn't promote a better hiding spaces. They made visibility powerful and joyful itself. And you may say, meh, rainbows. But people love rainbows. Ethereum love rainbows. So we already want this joy. Next lesson, aim for simplicity. Cyberpunk visuals require technical tools to reproduce them. But if your grandma and even you cannot draw the symbol with anything that you have in your hands right now, it will not spread. Extinction Rebellion teaches us how to do. They created a symbol which a child could draw in seconds and every human could reproduce with anything they have around. They later gave this symbol to communities and communities style it with their own wipes to represent their emotions around the movement and the movement went global. And last example, embrace evolution. Black panthers used powerful yet very aggressive imagery, usually containing weapons. Because of this aggression, the movement didn't achieve the reach they hoped for, but the movement did not die. The new generation learned from the past and took over and moved from displaying weapons to displaying words, from we protect ourselves to we deserve to be here. And they reached millions and more. So if I were here, an incarnation of all the subcultural movements from the past, I would ask our ecosystem to not make Ethereum cypherpunk again, but to make it a new cypherpunk. And maybe, just maybe, it will also have a different name. Thank you. Thank you very much. So we have a bit of time to ask a few questions. Raise your hand if you have any questions. Over here. Who's doing it right? Who can you call out or are there any examples that you've seen this week of different ways to evoke this sentiment in our ecosystem visually? I mean, who is doing the right cypher form style or who is doing the right in general? The visual styling that you've described and the evolution of that expression, have you seen any of it in person? We are here right now. So we have some more questions over there. Hello. Thank you for the talk. So my question is, from the negative examples that you presented, it seems like a lot of them were kind of, you know, serious, tough, masculinity focused. And crypto as an industry is also quite masculine in terms of just people who tend to, you know, run the companies. If we gather up everyone who works in crypto, mostly it's men. uh one of the positive examples that he had was you know the pride flag for example which is something that is quite challenging to traditional masculinity so how much do you think that plays into the current branding issues that crypto experiences thanks well I'm so glad you asked this because like maybe 20 minutes before the talk I just cut off one slide because I needed to fit in five minutes. But after the glitch effect, what I wanted to say is the cypher fund imagery has a very heavy gender coding and is all white male narrative, which is exactly what you tell about. Well, all those movements that I showed, they actually show the equality of everyone. And what I really love in Ethereum here right now in DEF CON is we have more of non-white people. So it's really good that white people are not dominating here. Also, if you study cyberpunk movement itself, there were a lot of regional directions who actually didn't go for this american uh white narrative cyberpunk and actually uh for the original cyberpunk movement hollywood did a lot of bad job for us because as much as we were trying to narrate uh well developers don't do a lot of good job for the visual communication so they did a lot of good job for the visual communication, so they did a lot of good stuff for vision building, but Hollywood knows how to show masses what exactly the government wants to see, and in general, nothing goes on the screen which can harm the government. So even V for Vendetta, the author of V for Vendetta wasn't involved in the movies because he didn't agree with the flatness and bloodness of the movie creation. The original comics had a very deep narrative that you actually need to see where he comes from. Did I answer your question? Thank you. So there's a last question over there. Okay if I may I would like to have like a very small comment. I absolutely do agree with you that the imagery we are using is holding us back and this is like spot-on well job, but the job well done. But none of the images that you showed are actually connected to the original cypherpunk movement because they didn't have any image. It was just a mailing list, a very traditional mailing list that didn't even allow to attach any imagery. The closest they were getting to having any visual identity is when three of the people from the original cypherpunk movement were featured on the cover of Wire magazine and they were wearing masks, but they were very different masks. They have nothing to do with Guy Fawkes and it was not their idea to wear the mask. That's one thing. Second, they used one pretty cool image that became really associated with them, but not with the movement, but one of the campaigns that they were running. In the early 90s, there was this danger that U.S. government will impose a surveillance mechanism on a communication with, like, Clipper project. And what cypherpunks did was, like, basically they hijacked Intel logo, and instead of Intel inside, they put Clipper inside. And this became, like, a sticker that they were distributing in many, many places. And so this was, like, quite successful. But everything else that you showed has absolutely nothing to do with cyberpunks. And one last thing, cyberpunks, the name itself, it's not that they created. They were not considering themselves to be a counter-cultural movement. It was basically a joke from someone who was looking at them and commenting how they are like approaching things. But they were not punkish at all. Absolutely. Yes. And that's how branding works. It's not what you say about you. It's what others say about you. So I encourage you, open your laptop and Google hacker icon and see what you have in dance just google cypher funks and see what google gives you and it's yeah that's the whole point there the world and we know which powers are engineering the reputation of cypher funks which is going to the minds and hearts of people outside of crypto bubble. And the call is not being passive around this, but actually do actions to create the image that we want to have, not what happens because some other very...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T10:50:00.000Z",
      "slot_end": "2024-11-13T11:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1JfZtSjos8JrMCOBp9B9xIaU5dMAfVMzayGYW7eA5F7Q",
      "resources_slides": "https://drive.google.com/file/d/1on4DVwDiZF20dNebVIDLXMFlMb70govU/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "defragmenting-ethereum-interoperability-and-the-superchain",
      "sourceId": "YEQLR8",
      "title": "Defragmenting Ethereum - Interoperability and the Superchain",
      "description": "With the proliferation of L2s and Dencun (4844), Ethereum has scaled. However, we have a new challenge -- fragmentation.\r\n\r\nNow we're introducing various interoperability standards across Ethereum and Superchain ecosystem from intents to low latency cross chain bridging primitives. What are these standards and what will enable? How can we create scalable and composable blockspace which enables application developers to onboard the rest of the internet?",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "optimism",
      "keywords": "superchain,OP Stack,optimism",
      "duration": 1528,
      "language": "en",
      "sources_swarmHash": "99cad33e6038d1d9265e2534b867e4232d0cb8977c8b66ec412976ded79e29c4",
      "sources_youtubeId": "tjXe8xxWCWU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673499049dbb7a90e1dd0381",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:00:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/18NUBFhBTGUc1VCTGb7xM78rgqQTtMu78w-hWIYbTYxA",
      "resources_slides": "https://drive.google.com/file/d/13OHA0aPhVbPt4qitwe7HAI7-fNqOkRv-/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "hardhat-3-preview-overhauled-and-rust-powered",
      "sourceId": "QZYQYE",
      "title": "Hardhat 3 Preview: Overhauled & Rust-Powered",
      "description": "The Hardhat team has been working continuously over the past two years to redesign and rewrite Hardhat from the ground up, including a major migration to Rust. This talk will explore the problems and solutions that the upcoming release of Hardhat 3 will focus on: performance, Solidity tests, correct L2 network simulation, and a comprehensive deployment system.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Tooling,DevEx,solidity,Developer Infrastructure,DevEx,Tooling",
      "keywords": "Hardhat,Solidity",
      "duration": 1620,
      "language": "en",
      "sources_swarmHash": "fbf71c10e089f8db1849642e0dee7c93c96327b3a698e74257918c7cc10f9742",
      "sources_youtubeId": "slSwrZTwNn4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348e149dbb7a90e175a3e3",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67348e149dbb7a90e175a3e3.vtt",
      "transcript_text": " Hey everyone, thanks for being here. My name is Patricio, I'm the co-founder and CTO of Nomic Foundation, which is a non-profit dedicated to Ethereum developers like you. At Nomic, what we do is building open source infrastructure and tooling for the Ethereum community. And the most well-known of our tools is HardHat. And during this presentation, what we are or I'm going to talk about is HardHat v3 or HardHat 3, the new version of HardHat, which is still in development because it's a whole overhaul of HardHat v3, or HardHat 3, the new version of HardHat, which is still in development, because it's a whole overhaul of HardHat with tons of changes. But first, let's take a look at HardHat 2 for a second, especially its limitations and why we decided to rewrite it. The first one, and main one, is that Ethereum changed a ton during the lifetime of Hard Hat. We started working on it almost seven years ago, about 2018. And back then, Ethereum was a single chain, a way more simple ecosystem. There was mainnet and a few testnets. Those testnets aren't even alive nowadays. And the applications were simpler, those test nets aren't even alive nowadays, and the applications were simpler, right? There wasn't even stable coins back then. Now, applications are larger, and Ethereum turned to be a single chain into an ever-increasing ecosystem of different chains, each of them with a slightly different behavior. Another problem that we have in Hard Hat 2, or Hat, is that we brought everything in JavaScript, including our network simulator, and that had several performance issues. We also focused a ton on the Node.js ecosystem because we envisioned building a platform for others to be able to customize and extend their own setups, but that focus on JavaScript only also meant that you could only write tests in JavaScript or TypeScript. And the final limitation is that when we designed Hard Hat, there was only one chain mainnet. We designed it in a way where for every single hard hat process, there is a single chain, a single network connection, you just import hard hat, you have your network connected there, most of the time it's going to be simulated by us, and all the boilerplate, all of your libraries, all your plugins get configured for you, but it's a single chain. As soon as you want to work with multiple chains, you hit this limitation and you start making workarounds. We created RPCs for switching the hard forks or forking other networks. People use different configurations and things like that. So, HARDCAD 3, we rewrote it from scratch to circumvent these limitations and offer more functionality. And how does it look? Well, it looks a bit like this. It's a bit like hard hat. It's still hard hat, but hard hat from the future or more technical hard hat. But at the end, it's just a hard hat, right? Like, it feels and looks like hard hat. So, as I mentioned, it's a complete revamp of the product. The scope is massive. Hard hat grew during these six, seven years. It has tons and tons of clients and functionality. And I can't cover everything here. I'm only going to focus on this list of things, which are the network simulator that we rewrote in Rust, the Solidity Test support that is added in hard hat 3, our deployment solutions.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:00:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1XDRIhALcLD_91krtX14MMkCYoXRCN3nZ_oia1tIdaLw",
      "resources_slides": "https://drive.google.com/file/d/1uwxr-7RfoVcDG2v2wG1TSWv3mR-v8G_E/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "making-defensive-technology-offensive-how-to-get-cypherpunk-ideals-to-the-masses",
      "sourceId": "RGMXQ7",
      "title": "Making defensive technology offensive: How to get cypherpunk ideals to the masses",
      "description": "Cryptography is an inherently defensive tool; it hides your information from adversaries. This is crucial to prevent censorship or monitoring of your data. But it's often sold to consumers with fearmongering about all-powerful malicious actors, which is often ignored by all except the privacy-conscious. We explore real-life examples of offensive cryptographic affordances like interoperability, efficiency, and user consent as stronger motivations for the masses to migrate to cypherpunk tech.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Frameworks,Values,Use cases of cryptography,messaging,Frameworks,Use cases of cryptography,Values",
      "keywords": "d/acc,adoption,messaging",
      "duration": 352,
      "language": "en",
      "sources_swarmHash": "879b660e0ee34065abe3a04185d46bd2bb368af1a1b90dea0e9fd1d28e14b236",
      "sources_youtubeId": "W5bRYUO-Wk8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734a08f9dbb7a90e1469361",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:00:00.000Z",
      "slot_end": "2024-11-13T11:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1osFBDl_IG67iwDmsSkuzzcHEUPFlkirPaPwWwqi5bwE",
      "resources_slides": "https://drive.google.com/file/d/1-h1NWuXSUWcHu_GcLi3i6U7YExAqhCKL/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "tending-the-infinite-garden-organizational-culture-in-the-ethereum-ecosystem",
      "sourceId": "U7SNLQ",
      "title": "Tending the Infinite Garden: Organizational Culture in the Ethereum Ecosystem",
      "description": "This presentation will discuss the findings of the academic paper \"Tending the Infinite Garden: Organisational Culture in the Ethereum Ecosystem\" by Dr. Paul-Dylan-Ennis and Ann Brody. Our study examines the decision-making processes fundamental to Ethereum's protocol governance, drawing on interviews with Ethereum's core developers. We identify a central worldview in Ethereum known as the \"Infinite Garden\" and discuss how Ethereum's social layer is crucial for upholding cypherpunk values.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": true,
      "doNotRecord": false,
      "tags": "value",
      "keywords": "Ethereum,Core,Development;,Social,Layer;,Governance;,Values",
      "duration": 1427,
      "language": "en",
      "sources_swarmHash": "6cd713334783bb75c3c9510a70b0a320d2f16dfb69b2c7e997c0f0a2504db504",
      "sources_youtubeId": "GAAi4ysKV_c",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67349bef9dbb7a90e120f3ee",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:00:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1f-XpVYzA-AiFID7laGqTa-L6kAXqGezXQRCWQw-a-L4",
      "resources_slides": "https://drive.google.com/file/d/1SSaeXlM1yqA642cTndIn_PNnqTFqbnC0/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "unlocking-new-possibilities-with-stateless-architecture-in-layer-2",
      "sourceId": "NGZBJL",
      "title": "Unlocking New Possibilities with Stateless Architecture in Layer 2",
      "description": "Explore the potential of stateless architecture in Layer 2 solutions. As Layer 2 technologies evolve, we will discuss the fundamental trade-offs and present how combining client-side Zero-Knowledge Proofs (ZKPs) with stateless architecture enhances efficiency. This session will highlight innovative possibilities not yet widely discussed in the Ethereum community, showing how this approach can revolutionize scalability, security, and privacy.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "statelessness",
      "keywords": "Privacy,Scalability,Statelessness",
      "duration": 1383,
      "language": "en",
      "sources_swarmHash": "8bea63f73194ad7063441d20c9051702aad4fa50e03192aada5cbe1f8a7cb960",
      "sources_youtubeId": "hfj9QaGoZrs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348d529dbb7a90e16bc9cb",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:00:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1CkoCHWyFJ_4IDI_puC1cfrAXBQJADtCY7bYExgXn3xQ",
      "resources_slides": "https://drive.google.com/file/d/1A7B1aIm34k2UlI83nMR1NnaTkfOmDce8/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "civic-tech-meets-dao-lessons-from-japans-largest-digital-public-goods-community",
      "sourceId": "G7CUCX",
      "title": "Civic Tech Meets DAO: Lessons from Japan's Largest Digital Public Goods Community",
      "description": "Code for Japan, Japan's largest civic tech community, is implementing DAO concepts and Ethereum coordination mechanisms to enhance its decentralized structure and sustain open-source projects. This talk explores our journey in applying on-chain governance to an established volunteer-based community, highlighting our approaches to contribution visualization and project support using NFTs and QF. We'll share key challenges, solutions, and learnings from bridging web2 and web3 in civic tech.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DAO,Quadratic Voting,Public good,plurality,DAO,Public good,Quadratic Voting",
      "keywords": "civic tech,Plurality",
      "duration": 1451,
      "language": "en",
      "sources_swarmHash": "190cfee6da59a09210da795f2ff2d6925a9a2ea36382e77b0b80ef454219c5d2",
      "sources_youtubeId": "KbMl1CGelHw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673499019dbb7a90e1dce9b3",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:10:00.000Z",
      "slot_end": "2024-11-13T11:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1HA7lnd6KnPUYw130uBGO9g_dh4nt0Km9EV8l4rT8-rU",
      "resources_slides": "https://drive.google.com/file/d/1IBUShmsrSzsHLEqzScR9VhPrTgnw9o6u/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "from-packets-to-privacy-understanding-and-evolving-network-security",
      "sourceId": "XYRFXT",
      "title": "From Packets to Privacy: Understanding and Evolving Network Security",
      "description": "This talk will provide a comprehensive journey through the fundamentals of network communication, explore the workings and risks of Virtual Private Networks (VPNs), and dive into the world of Mixnets. We’ll discuss how decentralized Mixnets can offer privacy by default, potentially eliminating the need for traditional VPNs.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Anonymity,Censorship Resistance,vpn,Anonymity,Censorship Resistance,Privacy",
      "keywords": "Mixnet,VPN",
      "duration": 529,
      "language": "en",
      "sources_swarmHash": "60a1a240af7b45bf3220aa7ef64b4613aeae26d9f09b1dfdab8177ceb235c87b",
      "sources_youtubeId": "7FyShvrYnHk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734a0f89dbb7a90e1476ca9",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:10:00.000Z",
      "slot_end": "2024-11-13T11:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12nsOv8WsOMt_04w0HJeyZq7caYnELYCEfrMGbVYyAGM",
      "resources_slides": "https://drive.google.com/file/d/1NFhSRmRF62Lhw3fNnMGGmcnYG12wLRyn/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "comparing-slashing-penalties-on-proof-of-stake-networks",
      "sourceId": "YJZT3K",
      "title": "Comparing Slashing Penalties on Proof-of-Stake Networks",
      "description": "With the support of the Ethereum Foundation, we have performed an analysis of slashing penalties on the seventy largest proof-of-stake cryptocurrency networks. Using insights from institutional economics and game theory, we consider variance in slashing penalties in terms of the conditions that trigger slashing, the magnitude of penalties contemplated, and the limited cases where human judgment plays a role in determining such penalties.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Game Theory,Economics,slashing,Economics,Game Theory,Governance",
      "keywords": "Slashing",
      "duration": 374,
      "language": "en",
      "sources_swarmHash": "00baaaa90215078e8748e07c20e343af0c5212f5ea255d441498ef851c8df27f",
      "sources_youtubeId": "frKyGtUTvi0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67348e1b9dbb7a90e1763ec0",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:20:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1U1W0kONT5CqQY5olh7ieFlQWhiN9s7HXZjQqM1AuBzg",
      "resources_slides": "https://drive.google.com/file/d/1VypQo2Sue78xs-FzT2lTVNhVS2IGabwD/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "solarpunk-vs-lunarpunk-the-evolution-and-integration-of-these-movements",
      "sourceId": "SFY3FB",
      "title": "Solarpunk vs. Lunarpunk: The Evolution and Integration of these Movements",
      "description": "In this talk, I will explore how the ideals of solarpunk and lunarpunk can be integrated to address privacy, inclusivity, and justice.  We will explain how combining the strengths of both movements we can potentially create a cohesive vision for a sustainable, equitable, and free future.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Anonymity,Solarpunk,Ethereum for Good,Social,culture,Anonymity,Coordination,Ethereum for Good,Social,Solarpunk",
      "keywords": "Lunarpunk,Culture",
      "duration": 567,
      "language": "en",
      "sources_swarmHash": "ec3e42c5c5bdbed8c1e3b858f53d8832afe881cea558c321fb3a1c657e542700",
      "sources_youtubeId": "2SYWYVJonuk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6734a1589dbb7a90e1486e16",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:20:00.000Z",
      "slot_end": "2024-11-13T11:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Zg48147sw4ud8uPsdsYKyuXSSdSVDoJZ0LSxumOJZ4o",
      "resources_slides": "https://drive.google.com/file/d/1s9s8u7UDCQiJ9PxVxcUCmqjI_7EbqWPF/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "alaska",
      "sourceId": "JWDJNQ",
      "title": "Alaska",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:30:00.000Z",
      "slot_end": "2024-11-13T12:30:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1mO_TORHBpzzpI0gU0stdNMv8Ew2DbX2vskYfKX4MClo",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "white-rabbit-world-premiere",
      "sourceId": "7CFGTS",
      "title": "White Rabbit World Premiere",
      "description": "White Rabbit is the first crowdfunded anime on Ethereum. It is about the metaphorical journey of going down the crypto rabbit hole. White Rabbit follows Mirai, who embarks on a path to discover the meaning of free will and self-sovereignty. There will be a seed phrase scavenger hunt in the final act of the film.\r\n\r\nDirected by pplpleasr and Maciej Kuciara, run time 30 minutes",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "Beginner",
      "audience": "Design",
      "featured": true,
      "doNotRecord": false,
      "tags": "Account,Abstraction",
      "keywords": "animation,film,nft",
      "duration": 2331,
      "language": "en",
      "sources_swarmHash": "537d2d5f2354801bf1e3b64510b42e28f61350e8051c9eb4e09dca64a9516975",
      "sources_youtubeId": "kuQMm0J1SK8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673499879dbb7a90e1ea2b83",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T11:30:00.000Z",
      "slot_end": "2024-11-13T12:15:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1IhRTtp7JRxxcgFhG5DluJWQD1KNt28d8UsxmQ7icfhc",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "ktv-winners",
      "sourceId": "UYQFMA",
      "title": "KTV Winners",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-13T12:30:00.000Z",
      "slot_end": "2024-11-13T12:45:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1cuZ-hN8gOGCEQohCTOeJVPCVT4HAWbG9sWbvDGpwg_Y",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    }
  ],
  "Day 3": [
    {
      "id": "speed-hacking-challenge",
      "sourceId": "RSYU7K",
      "title": "Speed Hacking Challenge",
      "description": "​Prize Pool: $50,000\r\n\r\n​A High-Stakes Speed Hacking/ CTF Challenge\r\nAre you ready to dive headfirst into a thrilling web3 adventure? Join us for ETH Escape, a heart-pounding Speed Hacking & Capture the Flag (CTF) challenge designed to test your coding skills and problem-solving abilities on Ethereum.\r\n\r\nhttps://lu.ma/viyjky8t",
      "track": "[CLS] ETH Escape - Speed Hacking Challenge",
      "type": "Mixed Formats",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 8930,
      "language": "en",
      "sources_swarmHash": "bc7eb48bd3a673d7dc1c4deec5b1496ac7f9e02af789aead2f3e359692311eab",
      "sources_youtubeId": "CRMPai0pUpw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc095982f234a126a635e",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673cc095982f234a126a635e.vtt",
      "transcript_text": " Thank you. Thank you. Thank you. Thank you. 15 minutes in, 90 minutes remain. No solves yet. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. All right. We have our first solve here at minute 76. Ddust2, Ddust2, where are you? Congratulations on getting this started. Kicked off here in the finals. You've got the lead. How does it feel? A little bit nervous, actually. A little bit nervous. What is your hacking spirit animal that you try to inhabit while you hack? Alright, we'll go with the elephant. They're very smart. Thank you. Thank you. Thank you. Second solve, NP Hard. We've got two people up on the leaderboard to take the top two spots so far with 72 minutes left. NP Hard, where are you? Remind me. Alright, good work. What did you eat for breakfast that's making you so successful today? Power of Corgi. Alright, love it. Power of Corgi, and the shirt says it all. Let's see, you can jump into the number three spot. I'm betting it's happening soon, and then it's gonna be a race so you could finish the next two challenges. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. In the leaderboard, third solve. Looks like they went with a different challenge to start. So one, now in first place, currently poised for $10,000 rewards. We've got DDoS2 in second place, currently poised for $10,000 rewards. We've got DDoS 2 in second place, currently poised for 7,500 rewards, and NP-Hard for 5,000 in rewards, currently as it stands. Where is one located? Top of the leaderboard. Where are you? Is that you? All right, they're staying in on. Okay, fair enough. 65 minutes remain. Remember if we have three people solve all three, that will end the clock before the remaining 64 minutes. away guys. Thank you. Thank you. Thank you. Thank you. One hour remaining. One hour remaining. One hour remaining. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. We've got a real race for the prizes now, Rage. Congrats for getting on the board. We've got three folks who have solved one particular challenge, and then one at the top of the leaderboard who solved a different challenge. So wide open for who's going to get the rewards but again as it stands now we've got one taking home ten thousand dollars in addition to their winnings already 7500 for ddust2 and np hard would get five thousand dollars in this round so anybody's game still 51 minutes and 45 seconds left. Keep hustling. Everybody's doing a great job. Thank you. Another update to the leaderboard. One pushing ahead by solving the second challenge. So on their way, it's possibly capturing the $10,000 grand prize. A reminder that places four through 14 in the finals all receive the same amount. So if we have a top three that solve all three before the timer's up, the finals will be over. Thank you. Thank you. Welcome to the Leaderboard Mage Intern. It's heating up. Who can get into those top three spots for the big prize money? 47 minutes to go. Thank you. Thank you. Bill joins the leaderboard in position two to shake things up a bit in the top three. MP Hard drops to number four. So good work, Bill, getting in there for the $7,500 spot as of right now. Thank you. Thank you. Thank you. Thank you. Welcome to the leaderboard, Slipper. Slipper with two solves very quickly. So put them into second place now over Bill. So it's heating up. We've got two folks with two solves. The first one to get the last one is gonna take home the $10,000 prize unless someone can solve two before they get to that last one. With third place wide open still, but being held by Bill as we speak. Just under 40 minutes left of the final round coming into it before in case it ends as people start to solve things. I want to give a big thank you to the Ethereum Foundation. I want to give a huge thank you to our friends here at Friendly Maltese Citizens putting together these awesome challenges. We appreciate all the hard work you did on that. And then we also want to give a huge shout out to the upcoming Ethereum Protocol Attackathon with rewards up to 1.5 million and the wonderful support that we've all received from Bybit, Wormhole, Arbitrum, The Graph, GMX, and Base. But thank you again to friendly Maltese citizens. You guys did a great job to create a great event. So keep it up, everyone. Keep your heads down. Coming in on the home stretch here. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. 30 minutes remaining, 30 minutes remaining. Thank you. Thank you. Bye. Thank you. Thank you. Thank you. Thank you. Thank you. The only change to the leaderboard we have is Bill getting a second solve a few minutes ago. That means that our 1, 2, and 3 right now are all racing to get that third solve. First one to do it gets the $10,000 grand prize. Coming down to the wire, we have 22 minutes left. Three people in the driver's seat to stay in one two and three and Plenty others behind you can certainly get up there as well. So keep it up. Let's see what happens Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. All right, players, we have 15 minutes left. And as a token of my appreciation for letting me bother you all day while you're trying to focus, we will release a hint right now to see who can try to take home the prize in the last 15 minutes. Thank you. There were typos in the first hint, so there's a second hint out there now to clarify. Thank you. As it stands now, we still have one in first place with $10,000 prize. As it stands now, $7,500 to Slipper if things stand as they are. And Bill would take home $5,000 prize as it stands now. $7,500 to Slipper if things stand as they are and Bill would take home $5,000. The rest of the pool up to the rest. However, it does seem that if there's one solution to one of these, to one of the ones that has not been solved yet, that person can easily jump into first place if there's no other solvers. So anybody's game in this room in the last 12 minutes, good luck. Thank you. Thank you. Ten minutes. Thank you. Thank you. Thank you. Thank you. Thank you. Five minutes remaining, but we can now say with confidence that Slipper is the inaugural champion of the DevCon CTF and will take home the grand prize of $10,000 in addition to what he's already made today. So $10,000 plus what he's already made and the physical gift we'll be giving him. Huge hand for his work today. There's still a few minutes left to see who can take the other spots because it's wide open with one more solve for pretty much anyone on the board right now. Thank you. Thank you. Thank you. Five seconds. In five seconds will anybody be able to get a solvent. Thank you. All right, pencils down. A big round of applause to the whole group here. Everyone in this room should be pretty proud. That was a lot of people that came in this room and did some speed hacking today. And you are the best among them today. A huge congrats to Slipper. Where are you? Slipper, come on up for a minute. Congratulations on winning the whole thing. I have a question for you. You took a little bit of a mitigated risk there in holding two answers at the same time it looked like you submitted them back to back. Was that intentional strategy to put pressure on one? So I saw that I had already submitted the one solution, but it turns out I was not shown on the scoreboard. So I just realized I didn't submit the one successfully. So it looked like a brilliant strategy that was actually a brilliant accident. Congratulations, you did a fantastic job today. How do you think you'll spend the money? How much would I get? $5,000, because you and I will split $5,000. No, $10,000 plus what you already got from the previous round. So you'll get a little over $10,000 and a gift from us at the awards ceremony later. I haven't decided yet. But I would thank ChatGPT for helping me. So I would pay for them. So a nice donation should go to ChatGPT for helping during the competition. Congratulations to you for being the inaugural winner. I want to shout out really quickly again our great friends over here, the friendly Maltese citizens for putting these challenges together. I think they did a really great job of putting challenges that were of all difficulty types and managing the event and the platform. So we really appreciate their help here, our team here as well. I want to thank the Ethereum Foundation for allowing us to run this, the first one they've done, I believe, at DevCon, and hopefully not the last, and for being our partner in doing so. And I really want to thank the, as you know, the upcoming attack-a-thon of the auto competition for the theory and protocol, the entire things in scope. Saw a lot of really talented people in this room today who I think could do a really great job working on this project that starts in about 12 or 11 days. Eight weeks of hunting, a ton of educational materials to get you up to speed, and the sponsors of that program who are supporting it we'd also like to thank, which is Bybit, Wormhole, Arbitrum, The Graph, GMX, and BASE. So a huge round of applause I also want to I want to congratulate our second and third place winners for respectively earning seventy five hundred dollars and five thousand dollars so that's one where's one raise your hand one stand up one congratulations Congratulations. And then Bill. Where's Bill? Bill, congratulations. Winning $5,000 plus what you did earlier today. So really great work by all of you. I think you're all coming out in the black today by getting some rewards and cash. We also have a reward for you you later which we'll do at the Award ceremony here After we do a little bit of a fireside chat Regarding security So hopefully you'll stick around for that so you can collect Your gear and then get to Our team with Your wallet numbers if you haven't already done that So you can collect your USDC in the Next few weeks, the last thing is a lot Of the folks in this room, which is amazing, didn't even sign up until they walked up today and just came in and dominated. So really great job on you all for showing up, coming through here, getting into the finals, making a few dollars. And because of that, I don't believe you got swag packs. So we have swag packs for you as well. Ash if you could raise your hand over there He has them for you. So make sure you collect those as well before you before you depart But yeah, thank you all for participating Find anyone in an immunify shirt to give some feedback to because we want to make sure we give feedback to the etherium foundation So when they run these again, they can kind of decide, you know What the best are out to do and get some good feedback from you all of what's too hard, what you would have changed and we'll try to adapt it that way. Thanks for participating in the first one and have a great day. Stay tuned for the fireside chat. Thanks gentlemen. Thank you. Thank you. Thank you. so Thank you. All right. We'll start the fireside chat here in about three minutes, folks. Thank you. Thank you. Thank you. Thank you. Thank you. All right, folks, we're going to get started here in a minute. If we could actually just get folks to sit down to the left or the right and not in front of the camera that would be great. Thank you. Just a reminder to all finalists who just participated. Quick reminder to all finalists who just participated. Please stay here for the award ceremony so we can give you your gift. And ensure that you've talked to our team and given us your wallet address so we could deliver your reward money. All right. With that, I think we'll start to get the fireside chat here going since we got the live stream. And what I think we'll do first to make it quite easy on ourselves is just introduce who we have here you've heard me talking all day so enough of me but I'm Mike O'Keefe I head up sales and customer success at Immunify which is CrowdSec bug bounty programs and audit competitions and I will let's just move left to right to start. Alright hey everyone my, everyone. My name is Michael Llewellyn. I'm the head of solutions architecture at OpenZeppelin. I focus a lot on working with our security audit team with some of our top clients, working on kind of weird, out-of-the-ordinary sort of audit requests, maybe things that are outside the EBM, and then also do a lot of work on security operations for DAOs, security councils, and lots of other very particular things for the industry. Hello, my name is Neville. So I'm a program analysis expert primarily in management in B-Dub. So I'm also a co-founder. My time is spread throughout, you know, like engineering, management, sales, and a lot of operations as a co-founder. But yeah, I'm a security researcher at heart. Hey, I'm Luna. I'm the co-founder and CEO of Zellick. My background is in vulnerability research, reverse engineering. So I used to do iOS zero-click. That's the kind of background I have and then yeah nowadays I mainly just do Zollock with my co-founder Jazzy we recently acquired a company called Code Farina you might have heard of it I worked very closely with that team as well now so that's kind of what I do yeah I'm Pietro I'm a lead auditor at Chain Security and we are an auditing company based in Zurich who specializes in complex DeFi projects and we've audited a bunch of projects that you might know, Maker, Curve, Lido, Polymarket. We've got bugs if you want later. We find them and we package them. And we specialize on EVM ecosystem. Awesome. And so thank you all for joining the panel. Thanks for coming here today. Nice way to end the long day of deep focus these folks had. And I think it's been great because I think Immunify has worked with all of you in different capacities throughout the years. So it's a great collection of folks here. I'm going to start with one that's a little off script that we hadn't talked about yet. It's Pietro. Pietro, you participated today. What did you think of the CTF? What did you think of the strength of the challenges and how you felt you performed? Yeah, so I'm not too satisfied with how I performed. I review code eight hours a day all year, but I very rarely deploy anything, very rarely interact with RPCs myself. So I found the bug quite rapidly. I was in round three, and I think they were very easy to spot. But then I had some trouble with the on-chain part. It was an interesting challenge still, but I'm curious to see what the other rounds were about. There were six with different challenges, and the finals also seemed very interesting, but I haven't read the code, so I'm curious to see that. Perhaps a team format would work better for you next time. You spot the bugs, and then you hand it off to a teammate. Yeah, probably, or just taking a couple of hours to get used to deploying stuff beforehand also would have worked. Well, we were thrilled to have you participate. A lot of the audience today, a lot of the participants in the CTF today, which was fantastic, were developers, security researchers, auditors. We had a really great mix and many others, I'm sure, as well. So I think keeping that in mind, keeping the audience in mind when we go through the next few questions over the next 15 minutes or so, we can kind of cater to those audiences however you please and whatever angle you guys want to come at it from. But the first thing I'd like to ask is a lot of these folks are involved in CrowdSec security. That's kind of what the, you know, what the Ethereum protocol attack-a-thon will be about. I'm curious how you all see CrowdSec security next to your businesses and how you operate. And maybe where I'll start actually is with you, because you all recently acquired Coderina. so you obviously see some value next to the audit So i'd love to hear your perspective Yeah, i mean the main reason we acquired code arena is we just think we can deliver like better experience better security for our customers The main thing that i would show is this concept that we have introduced called audits plus So when you have a consultative audit you're're going to get, like, you know, a small number of very highly skilled people, and they're going to look at the most important parts of the protocol, but it's fundamentally a time-boxed audit, right? So they're going to get maybe one week, two weeks, three weeks to look at this thing, so they can't afford to look at every single, like, gas optimization that there could be, or like, oh, no, this function's not documented, they need to go find and make sure that the critical invariance of the protocol are firm. Now, on the other hand, a lot of times you do want to get information about all the possible things that could go wrong, and not just that, you might want to have, you know, hundreds or thousands of eyes on the code base, because that is what you want before you not just, you know, ship it to mainnet, but also before you put it in front of a bug bounty, because that would be very expensive. The earlier you catch a bug in your development lifecycle, the cheaper it is, whether that be at the integration test or the unit test or the code review, internal audit. The earlier you catch these things, it's cheaper. So you want to get a consultative audit because you can have a firm authority say like, okay, yeah, we looked at it and we're pretty sure it's secure. And then you want the additional reassurance of like, okay, not just that, we've got a crowd of people that looked at it and it's definitely been very thoroughly picked apart. So because of that, we think that consultative audits and competitive audits or, you know, crowd security, these two go hand in hand in creating something that's extremely high assurance. So that's kind of what we think. We think that crowd security is very important. Yeah, that's why we call it crowd arena. Wonderful. I'll open it up to the rest of the panel for anyone that's got a response to this question as well. Well, crowds, this kind of security is really, really effective. And it's really enabled by the community. I mean, listen, everyone is motivated to learn new technologies, make a name for themselves, and also do the right thing, and finally make a little bit of money. And especially for bug bounties, which is the last line of defense, I mean, having participated in bug bounties myself, I feel that you are highly motivated, you know, like to spend a whole night looking for a bug or something like that. So, yeah, it is highly effective, you know, in certain contexts, but it really depends who's there, you know, looking at your code. To be fair, White Hat program is the last line of defense? Right, but that's something that I wouldn't do because of legal issues and things like that. Sorry, I was just posting. I can't help myself. Pre-deployment, post-deployment. Yeah. It's a model we've definitely seen a lot of our clients start to work with. Like we do mainly consultative audits, slash private audits, lots of different names for it. We expect to continue to see demand, but we've definitely seen a lot of clients that are specifically integrating us into a broader process where we're typically doing a private audit or consultative audit, and then they plan to later go to a crowdsourced competition. So it's good from the perspective if we did, by some chance, which is always some chance, miss something, like there is additional layers of security that are going to potentially catch that, so it's not all on us. And potentially there's several private audits as well. And then afterwards, we've actually found ourselves working more and more with clients in the fixed review process of competitive audits because of the fact that you can't have a lot of issues coming in. Even if they're low, that could be applied to the code base, and having someone who's already seen that code base and picked it apart a bit can help the fix review process as well and ensure the code is as secure as possible before it goes to mainnet. I do expect this is going to be a model that continues to grow, and there's going to be probably a lot of coordination between both private and competitive audits and the ways that clients use them. Yeah, so we work with traditional audits, but we also recommend a competitive audit if you can also afford it. We have seen the class of bugs that are code are not necessarily the same between the two. Competitive audits don't guarantee that you will have good people, but in general you have a high probability that people who are skilled at what you're doing will look at it and they will find specific things in which they are very proficient. What we do is really more trying to cover everything. And we try to give guarantees in the sense that we know what we understand and we know what we don't understand at the end of an audit. With a competitive audit, it's harder to know what's been seen and what's not been seen. Yeah. And one thing we really like about the collaboration between an audit firm, like OpenZeppelin, Chain Security, or Zelleck, for example, and a competitive audit is that, for example, you do the private audit and then the researchers who are assigned on that private audit, now they have a very high context understanding of the code base, and they can prepare the notes that then arm the, or equip the community auditors to be even more effective than they would if they had just been going in blind, so to speak. They can be like, hey, watch out for these parts. These parts are especially important to check, but of course, don't let that rabbit hole you. One more thing I'd add is if you start with a private audit first, you're more likely to be told you're not ready, go back and try again. As opposed to a competition that's just gonna say, all right, it's payday. So, probably do the private art first. 400 issues. So true. Excellent point. So I'll skip around a little bit, especially for the audience who I know is here since I surveyed them earlier. Particularly for developers, what are some of the best practices you see as auditors that you wish were more widely adopted? And why do you think they may not be widely adopted? So I think developers often think about security in a later stage of a project. And they're not thinking about their code in an adversarial way from the get-go. They should do more code reviews, internals. They should do more internal audits in a way where some part of the team tries to break the protocol. I think this is very useful. Yeah Two things you can do today To improve your security Is 100% code coverage I know you guys out there You guys do not have 100% code coverage I know And the other thing is Write your functions I read this great blog post by I think it was Nillian I think they're totally right You should do function requirements and then checks and variants and then protocol and variants at the end. So you should just assert that the, like if you have like a, you know, CFAMM, just assert the constant function is still constant at the end. Or like, you know, like the whatever solvent, that kind of thing. If you just throw in a bunch of assertions at the end, it can just kill a lot of attacks, or at least mitigate them. What might have been a crit might now be a medium. Yeah, so one other thing that I wish people do a bit more is more static analysis and more formal verification. We don't see this as much, probably because it's a little bit hard to use some of these tools. Also, these tools haven't evolved, haven't benefited from enough money for R&D at this point. And also, the underlying language changes so much, so it's hard to make them really, really spot on. But yeah, it might also be a cultural thing, and this also like this is like offloaded to towards the auditor so the auditors do use these tools but like the developers themselves don't use them as much currently yeah i definitely put like a big plus on invariants and just like doing it throughout the process like both defining them building a threat model around it testing against it and even when you're going to production like monitoring and looking for for flaws in the invariance occurring later. Another thing that I think doesn't happen nearly enough is for people to include the access control and deployment process and other things, if not in the scope of the audit, then at the very least being very thorough about how that's going to impact the code. We have actually seen things that we were basically putting down as a credible vulnerability because we saw in the documentation about how they were going to deploy the code in a way that was basically going to wreck them. They actually were arguing with us saying, oh, it's not in the code. I'm like, well, it's in the docs that was in the code that was in the scope and it would have wrecked your protocol so we're going to count it. I think people, including the deployment process, is almost part of the code and scripting it and being very careful about how the keys that are signing deployments being done, using a multi-sig or something else to handle that process. And yeah, never putting yourself in a position where some EOA key on some developer's laptop is hopefully not going to get phished and wreck the protocol. And I wanted to add on to what Pietro had to say. It is true, I think all of us will empathize with the fact that a lot of teams, essentially, this is very common, and I totally understand why they feel that way, because they just need to, you know, go to market. They think of security in, like, a, oh, we'll just do it at the end kind of way. But that's really bad. It's actually more expensive. it's more slow because you're going to get an audit by one of us because all these people here are very good firms. And we're going to tell you, like, hey, your design does not work. You're going to have to rewrite this entire component, come back in two months after you've rewritten it. So before you thought you were going to launch in December and now you're launching in March. So it's like tax planning. It's like post hoc tax planning. It doesn't work. It's really bad. You should think about your security as you are building your protocol at the design phase. Yeah. Yeah. And something you said earlier is the biggest ROI splash you're going to get is catching that stuff to the left of the audit beforehand, which is not as sexy as the bug bounty business or just giving it to the auditors or all of that stuff. But I equated it to the job of Homeland Security. The best job you do to the left of the audit is never going to be talked about, but it's the highest ROI and the most difficult to calculate, avoiding those situations. Sorry. A lot of the work you do initially is also relevant to security, even if it's not security-oriented sometimes. Like, good design is something that makes security assessment much easier, even if you have been thinking about security the whole time. So I would say being good at software development helps security in general, yes. That dovetails nicely into another question I want to ask the panel here, and then maybe we'll have time for a question or two from the audience if they have some, which is talking to some folks here today who are developers early in their projects, some of them being first or second time through this process. which is talking to some folks here today who are developers early in their projects, some of them being first or second time through this process. What's the best piece of security advice you can give them at the earliest stage of their journey of creating something new? Yeah. Oh, sorry. Michael, did you want to go? Yeah, like figure out what you want your protocol to do and what you want it to not do and write them down and then like look at them while you're developing your code and then add tests for them, add assertions for them. Like we find this to be a very common problem in our audits is that we talk to the customer like, okay, so like what are you worried about? And they're like, well, you know, we don't want to get hacked. I'm like, okay, what does that mean? Right? You should know what it means for your protocol to get hacked. So then you can, like, actually think about like, okay, well, I'm worried about this or that. Basically having a real threat model. I'll add to that. Like, definitely 100% threat modeling is, like, where you should start. I think, because the other thing is, like, people like security has to scale with the project. Like, are you going to be able to afford to have, like, a full monitoring infrastructure set up, like, going to deployment if you're, like, you know, you don't have any TVL, you don't have a lot of things. Like, we will still say, like, yeah, you should probably do that. But if you, like, literally can't, you don't have the resources, like, a lot of teams security infrastructure support. But then suddenly in a couple of months, TVL has grown, it's gotten massive, there's a lot more value at stake. They never really thought through how are we going to upscale our security. I've seen people that will build up their access control and hard code in something like an EOA owner and they're like, well, maybe we should transfer it to a multi-signal. And they're like, actually, we don't know how to do that. We didn't prepare for it. That's a particularly egregious example that does exist in a billion-dollar protocol. Try looking for it, where someone just hard-coded the owner, and you just hope that whoever has that key never gets kidnapped or decides to go rogue. But just in general, be prepared to scale up your security as you have the resources't spend on it today, plan to know, okay, at some point at this level of TVL, let's start taking whatever profits might come from that and invest it in making our protocol more secure. Because most of y'all here, unless you win enough from this competition to pay for a whole audit, you can't get your code audited from a top tier firm, but maybe at some point you do or want to. And be prepared to upgrade and other things. Yeah, like you want to have escape hatches for yourself. It's kind of like lawyering stuff up. Like, has anyone here worked with lawyers before? They're like so expensive. It's like so annoying. So you tell them like, okay, okay, we don't need like this like frigging Cayman Islands, Panama thing. Just like make me the Delaware LLC and just we'll fix it later if it gets really big. But these things are, you know, lawyers are good at like, okay, yeah, we'll just move all this stuff over, it'll be fine. They're good at having these escape hatches for you because people have been making legal contracts for a long time. Think about your security in ways so that you have escape hatches for, okay, now my thing has scaled, how I'm going to keep this secure as it's growing. But you don't want to be dumping tons of money on making your protocol super fancy, secure, early on, because you probably don't need it. Yeah, I would say start really simple. You might have a lot of very good ideas, but really make the minimal protocol you can do, and you will save a lot of money on security, on audits. Reuse valid components that have been audited previously and understand how they work and the audit process will be much simpler for everybody one final thing that I'd like to add is probably what you're gonna be building 99% of the time it's not gonna be original like a lot of people have built something which is similar and you can look at what happened to that thing probably it failed probably it had some security issues probably it had some economic which is similar, and you can look at what happened to that thing. Probably it failed. Probably it had some security issues. Probably it had some economic issues, right, if it's a Web3 project. So go back, see what happened, how it failed, and make sure that your design at the very early stages and also your requirements are such that this will not happen. That's obviously very hard, but if you do it at an early stage, it's going to be much cheaper. Excellent. Any questions from the audience? Any that we could squeeze in one or two questions if people have it? Hi. So I'm building a debit card solution where I'm assuming that once a transaction is signed and went to the network, it will be successful because the whole thing is standardized. But on the parallel, I am running a consensus model which will monitor those transactions if those are successful or not. And if it fails, it will trigger the system and act accordingly. So what things I should keep in mind, like what could be the security threat which I could face? Yeah. So just understand that you're trying to build a system where someone signs a transaction, submits it, succeeds or fails, and this is just for a normal payment. So what we are trying to do, we are providing the transaction time to four to five seconds. So once the transaction is signed and sent to the network, we assume that that will be successful. But on the parallel, we are running a consensus model that will monitor those transactions and build a consensus that the transaction is successful or not. If it's successful, we are good to go. But if it fails, we will trigger the system and that system will act accordingly. Like if it fails due to network congestion or some logic error or something. So what other things i should keep in mind like what other Scenarios of which transaction could fail or how? It's hard to say exactly because i think there's a lot more i Would need to know about the system. But just off the top of my head, I would say there could be an instance where the way that you're, like the node that you're monitoring it from is somehow down. Or like I guess be prepared for whatever infrastructure you're relying on to be able to confirm success or failure. You have like a backup plan if it goes down or you're, like, ensure that someone isn't able to exploit that, making some very general assumptions here. Like, we are using blockless protocol, which have different nodes. And I think those nodes are building the consensus. Not a single node is building the consensus or a proof that a transaction is failed or successful. So I think that wouldn't be the case. But there could be something of which I should be aware about. Again, I'm trying to second guess about how your system works. But one of the things that comes up is whether the consensus algorithm has a finality gadget, whether you can actually tell whether a transaction has been finalized or not, because otherwise you'd need to wait a lot more time than four seconds. Four seconds does sound really really low though. Thank you. Time, one more question if anyone in the audience has one? One question over to the left there. Hi. So I'm not a security researcher myself, but I've seen a lot of hacks that are due to consensus mechanisms and sometimes mass social engineering of a protocol. I was wondering what do you think about how we can basically build mechanisms that are more hard-coded to prevent such exploits? More hard-coded? Yeah, because basically the human factor is more unpredictable. How can we hard-code consensus to be less... Maybe it's a difficult question. When you say consensus, do you mean like, for example, a multi-sig, does that count as consensus? Yes, and also at scale like for a DAO. For a DAO? Yeah, so transparency is very important here because it makes it a lot easier to see what you're actually voting for, for example. I think one notable example was there was one for Tornado Cache where they just proposed a thing and they were like, yeah, it totally does this, and it just did something else. So yeah, monitoring your DAO is pretty good, transparency. Another thing is just time locks. So make it so it's easy to cancel a thing that's in the pending state in the time lock. So this way there's a longer window for things that are potentially breaking to get canceled and noticed. Of course the downside there is you get a lot more latency, so if you have some critical thing that needs to get jammed through, it may make it hard to do that timely. So that's another trade-off. Do you guys have any ideas? Yeah, we've had some really fun experience, and I say fun in a sarcastic way, with DAOs that were under some form of governance attack. In one case, a whale buying a lot of tokens and trying to push through a proposal on compound that would have given them an even larger part of the treasury and then control the DAO. The way they did it was very tricky. They took a bunch of tokens they had purchased, they spread amongst a bunch of accounts. We did actually have security monitoring that picked this up as strange activity. But even then, it's like, okay, so who is a genuine new delegate and who's one of these many accounts controlled by this one entity? There's very few ways to do that in a permissionless system because it's essentially a civil attack in that sense as long as they have the resources to allocate. I think at least being aware, I think ultimately having a reputation in the ecosystem and then having some checks and balances on them and recovery mechanisms. In that case, having a multi-sig that can veto proposals if they're somehow violating the DAO but you have very specific rules, that multi-sig is elected by the DAO, they're accountable. That's where I'm starting to go towards, especially for DAOs. And for multisigs themselves, I think definitely having strong reputation. Possibly even KYC, it kind of depends. People are controversial around that. But I think if you can, if it's appropriate to say, we know these are individual people, do a signing ceremony where they're all in different places and they can verify they're all different people it's not like one guy controlling four or five different keys yeah I think it's like there's no good technical solution to these things these are just like okay having strong social mechanisms to recognize who's a good actor and who's not because it's not as easy as like Ethereum where it's like okay we're running a bunch of nodes that are all coming to deterministic answers And the general idea is just like make sure you have enough notes that they can always outvote the bad notes on the deterministic Answer with multi sigs and nows these are not determined because these are not deterministic answers are like yes This is good. Yes. This is bad. So there does have to be a lot more work on the consensus of who are the actors, why are they allowed to participate in the system, and how do you root out bad actors and prevent them from gaining control. As a shitpost, you could take the MakerDAO approach and you could make the decisions deterministic by offloading it to AI. All right, end of shitpost. It's also an interesting experiment anyways. I'm curious how it goes. So another mechanism I've seen is to make the quorum of a governance mechanism increase in size, the more contentious the vote is, so that the simple majority is not enough. So it's a variable quorum. Yeah, of course, it's like a tradeoff between how easy it is to get stuff done, same with time locks, and how safe it is. Yeah. I mean, on one end of the spectrum, it's just everything's hard-coded. On the other end of the spectrum, it's just like whatever goes. Any other final comments from the panel here today? I know we're a little bit over time, but I want to thank you all for joining. For anyone in the audience who hasn't had an opportunity to work with either Immunify or any of these firms. These are some of our favorites in the industry. There are many others that are great too, but I was really happy to put together a panel with this group because it's really great. Can't go wrong with anyone sitting up here. Anything? Any last words, panel? I think everyone on this panel is really awesome. Likewise. I agree. Very nice of you. Thank you all for joining us. Really appreciate it and hope everyone has a great rest of the week. And with that, I can let the panel go and I think we owe some rewards to our CTF participants today. Thank you all again. Vielen Dank. Thank you. Close out the day. We have here our awards for our wonderful CTF participants. We're going to go ahead and start with Slipper again. What's that? I'll let Slipper show you. All right. So in addition to your cash rewards, again, make sure that you've given your wallet information, your correct wallet information, double, triple check it, so you get your correct payment. But if Slipper is still here, we do have a physical gift for you if you wanna come up and grab it. Slipper, are you still here? Come on up real quick. So in addition to the cash, congratulations again on being the first one. Maybe you can show the rest of the participants what we'll be giving out here to the finalists. So in addition to the cash prize, we've got a nice set of Beats by Dre Studio Pro headphones for everybody to take home. And make sure you get your Immunify swag bag as well, commemorating the event here today. Let me go through and call names up to come get your stuff. I'm going to go in a random order now just because my spreadsheet is a little screwy. Wait, no, something's wrong. All right. Billy, is Billy here? Billy, come on up. Receive your gift or I can bring it to you. Billy? Here you are. Sir, congratulations. A great job done by you today. Chain security also left a little bit of swag here too, so feel free to grab something there. All right. Next, we have Tony. Tony, come on up. Congratulations to you. Way to be a finalist here in the CTF. A lot of hours spent here today. Heads down. Next, we're looking for Naoya. I hope I'm pronouncing that correctly. Is Naoya here? Naoya. Maybe that's the better pronunciation. All right, we'll hold that one off to the side. . I can call it last name. Is that all? I'm getting good username. But are you sure that's it? Yeah, this is from Mixi. No, no, no, thank you. Did you want to go next? All right, yep. Harry. Harry? All right. Congrats, great work today. Good work. Chi-Sue. Chi-Sue. Chi-Sue. Chi-Sue. Great work. You can give us one of these, j or rkskek. We've got somebody that's going by the name of just j. J. The email is rkskek and some numbers. Oh, you got one right. Okay. So that's okay. It's all the same. That's fine. Yep. So that means we're missing a bill still then right? Yeah, it's all good. Yeah, just, you can just, yeah, it's all the same. We can just pass them out. Great. All right. Perfect. Rage next. Rage. Rage, I love the hat. Good work. All right. There we go. Already new. Just need a bag. Congratulations. Great work. Great work. Thank you. great work mage intern mage intern great work M4K2. Congratulations. Vlad. Vlad, are you still here? All right, Vlad. Vlad, I think you even won your group, too, I believe. So congratulations. Nipun. Nipun. Nipun. Great work. The early leader stays on the leaderboard. And Valera, last but not least. Valera. Congratulations. Great work. Great work. Once again, thanks everybody for coming through. Thanks for participating in the CTF. I know it was a lot of hard focus work in one room all day. Please look forward to the $1.5 million Ethereum Protocol Attackathon that will be hosted soon and with the sponsors for that program. And again, a big thanks to the Ethereum Foundation for allowing us to run this event here at DEVCON and the friendly Maltese citizens. Okonami, are you still here? Okonami? Minamoi. Minimoi. And take care. Have a great rest of your week and enjoy the rest of DevCon. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:30:00.000Z",
      "slot_end": "2024-11-14T08:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1TFlUSOJNbrhtdG-u3_ajWbpR--vyfBXX6KSwtcFkFI0",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "better-privacy-defaults-for-your-users-with-stealth-addresses",
      "sourceId": "JWMCXJ",
      "title": "Better privacy defaults for your users with stealth addresses",
      "description": "Stealth addresses can be used to give users better privacy from payments to identity. In this workshop we will run through practical use cases of stealth addresses today and how they relate to different types of products.\r\n\r\nWe will then have a short coding session where we will implement stealth addresses to generate private Safe signers from scratch.",
      "track": "Cypherpunk & Privacy",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Live Coding,Privacy,User Experience,stealth,address,Live Coding,Privacy,User Experience",
      "keywords": "Stealth,Addresses",
      "duration": 5069,
      "language": "en",
      "sources_swarmHash": "d32d9e330397b274c8351fcbc952c9ab3143f9db4d3e5b70a65ad986f133bc72",
      "sources_youtubeId": "ZRoIovLDZFM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673588f79dbb7a90e14bdf0d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T04:15:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1yyp2Kb948zJmCqxO8kfDdZZrs259maV0t-DrGiabFXw",
      "resources_slides": "https://drive.google.com/file/d/1RfH9E6WKIZDKBZcOA1godhob3qqo29Rv/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "building-a-basic-smart-contract-with-scaffold-eth",
      "sourceId": "339THC",
      "title": "Building a Basic Smart Contract with Scaffold-ETH",
      "description": "In this session, Kevin will help you create a basic smart contract using Scaffold-ETHv2. It's a hands-on way to learn the basics of writing and deploying contracts on Ethereum.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Tooling,react,Developer Infrastructure,Tooling",
      "keywords": "Solidity,NextJS,React",
      "duration": 4819,
      "language": "en",
      "sources_swarmHash": "0e001e2036f4a3ce845bd26e911d6bcd837f9e9239c209cad237fbc9609baf79",
      "sources_youtubeId": "u-kIPcapQ2E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735cd319dbb7a90e1cee6cd",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T04:45:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1V3HDkZ9JwUiU5yIZOaYoLXG69XdykTD5DfEAoqdLSV0",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "evm-object-format-eof-managing-the-bytecode-chaos",
      "sourceId": "UU9BTK",
      "title": "EVM Object Format (EOF): Managing the Bytecode Chaos",
      "description": "Currently, EVM bytecode, while being powerful and simple, is lacking structure. This leads to many complexities when introducing new EIPs and maintaining backwards compatibility.\r\n\r\nIn this talk, we illustrate some use cases of the EVM Object Format (EOF). Next, we provide a quick overview of the main changes introduced by the EOF and related EIPs, along with code examples. Finally, we discuss potential benefits and drawbacks that could arise with the introduction of EOF",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,Protocol Design,Security,upgrade,Ethereum Roadmap,Protocol Design,Security",
      "keywords": "EOF,EIP,upgrades",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "9cde575870d589f4c0332dd64baf91d16d9b34b3e7ce8ae9eb17a332db3219a9",
      "sources_youtubeId": "WKVgCoNp39g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/11DBlWa1M4JLbQS2Ik4OU6nxzNoPj1nINFepAqbY2qIk",
      "resources_slides": "https://drive.google.com/file/d/1F38P9FcBhg245q1oDEN1fk90xsPVNBVC/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "how-to-hallucinate-a-server",
      "sourceId": "QNFTYG",
      "title": "How To Hallucinate A Server",
      "description": "A Hallucinated Server is a virtual server whose execution is cryptographically simulated by users, using \"multiplayer\" privacy technologies like multi-party computation or fully homomorphic encryption. Today, thanks to recent advancements in MPC and FHE, we have the technology to build the first fully Turing-complete hallucinated servers. We discuss the underlying technologies, and how we've used them to build several proof-of-concept applications.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Homomorphic Encryption,MPC",
      "keywords": "MPFHE,Hallucinated Server",
      "duration": 1366,
      "language": "en",
      "sources_swarmHash": "3a887806e3d602aeb13383ce6beea87c101495b2a54f22bfc5f8a9bf48c4e0b5",
      "sources_youtubeId": "FvLpfGP8Lrk",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1wOtuuxn-pV_UdYT74yaBeuoxLyXyxkk_KW0-5GBqLJk",
      "resources_slides": "https://drive.google.com/file/d/1VNKkS-0x_dF9c7qLCNhHe2Rt51k3Kwo9/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "smart-accounts-need-smart-sessions",
      "sourceId": "SJDY99",
      "title": "Smart Accounts need Smart Sessions",
      "description": "The world of dapps is evolving and wallets are becoming smarter. This is powered by developments in Smart Accounts which unlock more user-friendly experiences. Learn about how WalletConnect is introducing Smart Sessions and walkthrough all the standards (EIPs, ERCs and CAIPs) that will make the future of wallet UX possible.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "interoperability",
      "keywords": "standards,wallets,interoperability",
      "duration": 1802,
      "language": "en",
      "sources_swarmHash": "06f9344ba6e1d54564b078134d5ad55ec3e142a2bb173b240d8df7aa64772788",
      "sources_youtubeId": "GeYbDsOW4hQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67356fb59dbb7a90e189a824",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673572269dbb7a90e19a37f9.vtt",
      "transcript_text": " In numbers, the total size of the game is about a total of 150 bytes. That's the entire state of the game. The front end is built in React and Phaser. The source code for the back end is about 500 or so lines of C++, not including boilerplate. That might be something like half of that. And it took about a month for a team of devs and collaborators from Gauss Labs and Xerox PARC and PSE to build this together. So why am I showing you this? Why am I showing you this incredibly simple game that a kid could have probably built as their first programming project? Well, essentially, the interesting thing about this game is that the back end of the game is running entirely inside of fully homomorphic encryption. And to our knowledge, this is the first time a multi-user application with a back end that is running inside of FHE has actually been built. So that means that all of the state of this game is encrypted using FHE, and all player actions and everything happening inside of the game is encrypted using FHE, and all player actions and everything happening inside of the game is happening inside of FHE. Thank you. So one question you might ask is why bother running this inside of FHE, right? FHE is super expensive. Well, one thread that we've been pulling at in the last year is this idea that technologies like FHE can enable us to run what we think about as hallucinated servers. So let me describe what I mean by that. Today, if a group of us wanted to come together and to build some sort of application that all of us might use, like let's say a social network for DevCon attendees, the way we would do that today is that we would have someone rent out an AWS server, write some sort of back-end code, and then deploy that back-end code to the web server, and then each of us would, using our computer or using our browser or client or whatever else, talk to this back-end server, making API requests to update and retrieve the state of the application. making API requests to update and retrieve the state of the application. Now using technologies like programmable cryptography, another way we can imagine doing this sort of thing opens up. So we can imagine in a world with programmable cryptography that rather than there existing a specific physical server with a physical footprint that's running all of the computations of the application, instead every participant of the application might store something like a cryptographic shard of the overall state. And using technologies like multi-party computation or fully homomorphic encryption, we could cryptographically simulate the execution of this virtual machine using things like ZK-proofs or FHE to advance the state of this arbitrary machine using things like zk proofs or fhe to advance the state of This arbitrary computation one step at a time ensuring its consistency ensuring that everybody is only you know Having access to the data that they're supposed to be able to While doing so in a in a decentralized multi-party way without needing to rely on a physical server anywhere that actually is the source of truth for the system. So, you know, this sort of opens up a lot of interesting questions. What if our digital services ran as these distributed hallucinated computations between just the relevant parties? We could imagine having this abstraction for a server where instead of, you know, servers run by Zuck, we have a server made of math that's perfectly secure, privacy preserving, verifiable, interoperable with every other service built in this way, etc. Of course, right now we're very early on our journey. So in terms of the game in numbers, in order to run this extraordinarily simple game, 150 bytes of state with four frogs on a 32 by 32 grid, we are using nine machines to coordinate a variety of different MPCs together. So we have four MacBooks downstairs, and we also have five 192 core AWS machines in the cloud, costing us about $200 an hour to run this game. Every binary gate involved in the execution of any operation takes about 10 milliseconds to evaluate, which is about a 1 billion times overhead on top of ordinary computation. And for every bit of plain text state in this game, this bit will blow up to about 3,000 bits. Actually, I think this might be 3,000 bytes of ciphertext. I need to check on that, but it's a huge overhead. So the way that I think about what's going on is it's sort of like we've built almost this particle accelerator and spent enormous amount of resources just so that we can suspend in the middle of the cryptographic ether for a brief instant something that looks like the Higgs boson, and we can sort of hold that",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1Xn-t83UrHqZiD2z9Y1uuRL-w6SCGvLF-dX6-cK0TwYM",
      "resources_slides": "https://drive.google.com/file/d/1TDKtJEBB4kjXWteZ20IloEGdAG-cuZEE/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "the-age-of-account-abstraction-opportunities-and-challenges",
      "sourceId": "EPN9S7",
      "title": "The Age of Account Abstraction: Opportunities and Challenges",
      "description": "In a world where the web3 user experience is streamlined through account abstraction, complexities like gas and multiple L1s/L2s are hidden from users. This talk explores the competitive dynamics likely to develop at each layer of the stack (layers, DeFi protocols, intent protocols) and the strategies that might be employed to succeed. Join me to delve into the transformative impact of making Web3 seamless and accessible, and understand how to navigate and thrive in this evolving landscape.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Account Abstraction,Intents,specialisation,layer,Account Abstraction,Intents,Layer 2s",
      "keywords": "Protocol competition,User growth,Layer specialisation",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "7737be5eba2099cd02775d6c06240621bd43800d2d436c6647d2411b7f3cd8cc",
      "sources_youtubeId": "dNeCG5Za_hM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T02:55:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/17eyZChjX1qpt1_WuQIDXpXi06_RixZQtwAbNNS22vqU",
      "resources_slides": "https://drive.google.com/file/d/1StaC6tHfqm3FvyU52I3Z1SfxsnSyWWwc/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "utilizing-national-ids-in-the-ethereum-ecosystem",
      "sourceId": "PR78EL",
      "title": "Utilizing national IDs in the Ethereum ecosystem",
      "description": "This panel brings together developers of MynaWallet, Anon-Aadhaar, Proof of Passport and zkPassport, who are exploring and developing applications that utilize government-issued IDs in the Ethereum ecosystem. We will discuss the characteristics of each ID system and what functions can be realized using tech stacks in the Ethereum ecosystem and cryptographic technology.",
      "track": "Real World Ethereum",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Civil Resistance,Privacy,Identity,Civil Resistance,Identity,Privacy",
      "keywords": "National IDs,Selective Disclosure",
      "duration": 3351,
      "language": "en",
      "sources_swarmHash": "5f450cd1ae6a875ce249bdf3b65ee1d5e4b524568296c784d24f0c3b908c3845",
      "sources_youtubeId": "XsQ_DiECL0I",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736baf49dbb7a90e12ccb78",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T03:45:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1DNOsJyO6qTZrHr9rXUHPF9-HZEOF4NkaTmABCndOG0g",
      "resources_slides": "https://drive.google.com/file/d/1gAnV6ub_U-4kKpkth9CHr5JYDssps6Hh/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "vlsmsanalyzing-faulty-distributed-systems",
      "sourceId": "AKRLKH",
      "title": "VLSMs—analyzing faulty distributed systems",
      "description": "Validating Labeled State transition and Message production systems (VLSMs) provide a general approach to modeling and verifying faulty distributed systems. With formal definitions of validation and equivocation, we are able to prove that for systems of validators, the impact of Byzantine components is indistinguishable from the effect of the introduction of corresponding equivocating components. All of the results presented in this talk have been formalized and checked in the Coq proof assistant",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Distributed validator technology,Formal Verification,correct-by-construction,Consensus,Distributed validator technology,Formal Verification",
      "keywords": "Correct-by-construction",
      "duration": 1787,
      "language": "en",
      "sources_swarmHash": "faf3bda887c2724dd5bd923f3f360e2226fc675126f2a2e5d499b3311e2a1db3",
      "sources_youtubeId": "loyKzWQlyEo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67358e2b9dbb7a90e1a57339",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67358e2b9dbb7a90e1a57339.vtt",
      "transcript_text": " Testing, testing. Hi, everyone. Hi, hi. Yeah, I'll take this mic, actually. Thank you for coming to this early morning talk. It's going to be a talk on formal verification and like some basic methods in distributed systems and reasoning about faulty distributed systems. All of the, everything presented here has been formally verified by the team at Runtime Verification and is, like, available to, like, click and check. And you can look at the proofs in the talk, but this talk is not going to be focused on the proofs, more just definitions and theorems and not really walking through the proofs. But you can check them out. And there's a... So I've separated a bunch of sections, a section on validation theory, a section on equivocation theory, and then on relating and reducing Byzantine faults to equivocation faults in the context of validators. So let's kick it off. So let's talk about validation. And so here's the slides where I guess you'll... So this is the name of the paper. Validating labeled state transition message production systems. So it's for modeling distributed systems, faulty distributed systems, and all these amazing people worked on it for a long time. Here you can scan this QR code and pull up the PDF if you like. I'll show this also again later. So here's the validation theory section outline. Basically, we're going to go through the definition of this model, its compositions, and then the definition of validator, and then move on to the equivocation section. So here's the first definition here. So a VLSM is a tuple, you know, as we often like to define these things. It's sort of like a state transition system like you're normally used to, except for it has a few other things, like a label, which is also not too unconventional. But it has a first order message set in the definition. It has initial states and initial messages. And then there's a transition function that takes labels, states, and messages, optional messages, and gives us states and optional messages. This is like the state transition slash message receipt and production function that describes for a particular VLSM, sort of what's happening computationally, you can imagine. And then they're also equipped with another thing, which is why they're called validating, this beta, which is a validity condition. And it basically says, it's going to be valid to transition on a label from a state given a message. So when you're in a particular state and you receive a message and you might want to transition, there might be multiple possible transitions each identified by a label. And some of them might be basically banned. Even though they're defined by the transition, the validity condition won't let you take that transition. So this is like a state transition system with native messages, with initial state messages, and with a transition function that's totally defined over this domain. And then we sort of restrict it, effectively making it partial with this validity condition. This is like the validation condition. Basically, we're going to imagine that these things don't transition, even though the transition is defined, when the validation condition is not satisfied. And so that's the definition, but actually, I haven't told you how to get the states and messages, but it's sort of what you would expect. You start at the initial states and the initial messages, and then we build up this fixed point by taking the union of all the states that you get from the transitions and all the messages that you get from the transitions and transitioning from those states using those messages. So basically starting from the initial states and messages, transitioning and sending all the messages that are produced from there and receiving them and doing it over and over again, basically, until you even get stuff like a node in an early state receiving a message from some other state. This really is a big fix point. And so there is an interesting thing that can sort of happen, which is the validity condition can be satisfied even when an input message is invalid. So we have to slightly distinguish between just the validity condition being satisfied and the actual trace being valid. The trace is valid only if all the input messages are also valid. So, you know, no garbage in sort of allowed in a valid message. So that's basically the definition of VLSM. And then they have a pretty natural way to compose them. And so, now I'm going to go through that definition. Unless someone has, like, a question now before the composition. Okay. So, basically, what we're going to do is we're going to take a disjoint sum of the components for the label. So we identify the label, which component it is. The state is a tuple of the component states. The initial states is a tuple of the initial states. And then we have like a union of the messages for the initial messages. And then here, actually, we're composing of VLSMs that have the same message type. This is just so that all our transitions are defined and everything. And to let them send messages to and from each other, so they're not just independently. Like if that was a disjoint union, they wouldn't be communicating. So we have a disjoint union for the labels, tuple for the state, and then a regular union for the messages. And then in the free composition, we have a transition that basically just affects only one component exactly according to the transition that would have had before the composition and checks the validity condition only of that component exactly like it was before. So very little, basically nothing being done by the composition except for transitioning the individual components and checking their individual composition, sorry, their individual validity conditions individually. There's no composition-wise constraints, but they can message each other in this thing. And then, you know, note also that this is also VLSM, and that's why it's like sort of a composable model, you know, same type of definition and everything. Yeah? So the split up of the transition functions and the validity where they are purely mathematical? So the question is, is the split up between the transition and the validity purely mathematical? I mean, I guess it's for the sake of convenience when dealing with the math and... So, but, I mean, more more traditionally in math you'd use like a partial function I guess whereas here we're about to get into this conversation about validation and distributed systems and what can happen in a distributed systems that you might not be able to tell locally about and so it gets there's there there is a reason why we are thinking about the validation at different levels. And actually, and that actually does sort of spell it out. Basically, there's going to be, actually, the next slide, we're going to apply, we're going to talk about constraint compositions where there's an additional constraint on the, so basically, like, this constraint composition just basically conjuncts a constraint on top of the, uh, validity, on top of the validity constraint of the free composition, which just is the individual constraints applied independently. So this, this, this composition constraint, um, you know, lets us sort of analyze, uh, things a little bit more conveniently than just a partial function approach. I guess you want to be able to see that a transition would be possible, but it's not there. Yes, so the question is if we're doing this to try to see if a transaction is possible but not valid. And yeah, you're very much going in the right direction. And we're getting there to try to see if a transaction is possible but not valid, and yeah, you're very much going in the right direction. And we're getting there with this definition here. So, this is the definition of validator. It's a very natural, simple definition of validator, which is kind of useful in many different contexts. So, in a... And so, the components are a validator for the composition basically they're checking if they're truly a part of that composition and they're sort of you know only transitioning as if they are a part of that composition even though they don't know it per se so let me just go through this definition so basically a component in a constrained composition is a validator. If any transition that that component can make can be lifted to a valid transition in that composition. So if the component has a valid transition, then if a validator has a transition, which it can take, then there's also a transition in the composite system where that validator can take that transition. It sounds kind of... It sounds weird, but basically it's not. The local condition lifts to a distributed one. And so basically, this local component is checking about a condition that's distributed across the whole composition. And that's sort of non-trivial because of information disparity between the nodes. So there's a message here being received, and this is the message being sent, and this is the label, and this is a constrained transition, which doesn't necessarily mean that M is valid. However, it's lifted to a valid transition with M being received. So basically, the validity condition of the component is enough to guarantee the validity of the message received in the composition. So basically the component locally is able to verify whether the message has this distributed property. And that's why it's called the validator. Because it's basically able to check something that's outside of its scope. And it's again defined here with respect to a particular composition and a particular constraint on that composition. So you might have different validators for lots of different distributed settings. But we're specifically going to be interested and focused on equivocation for a reason that I've already talked about, but it sort of reveals at the end why equivocation is so particularly interesting to look at. So the equivocation theory section, talk about evidence, and then using evidence to describe compensation constraints that limit and validity conditions that limit equivocation. And then we'll talk about models of equivocation. And then all this will tie in nicely when we start talking about Byzantine faults. So this is sort of what we're used to seeing in blockchains when it comes to slashing conditions. This is a starting point, or was a starting point in our proof of stake research. Basically, when messages have the same sender, they've been, they're like collected by the same node, or like in the same smart contract, you can imagine. And they basically could not have possibly been produced by the sender in a single round of the protocol. So if you run a trace of these things, there isn't a single trace where those two messages are produced by that node. And so this is evidence of equivocation this is somehow we have two messages that couldn't have been produced by their sender and we have them sort of in the same state this is sort of a sort of faulty behavior and this is a local evidence and here's an interesting definition. Yeah, yeah, of course. Sorry. If we don't have a history, how do we check that? What? Yes. So that's a good question. I mean, I think it's undecidable in general, but the question is whether it could not have been possibly produced. So basically you need to sort of quantify where all traces and say there is no trace where these two messages can be produced. So in practice, you know, we have lots of simplifying assumptions like you're guessing, you know, we have lots of simplifying assumptions like you're guessing, you know? But the definition doesn't say how, you know, how we can come to this decision about whether a message could have been produced. You know, it's okay. That's actually another nice thing about having the validity conditions as sort of like predicates. You can have undefined, sorry, undecidable conditions, whereas, you know, if you're using partial functions, that would be an issue. So there's global evidence is a little bit more interesting and a little bit more, maybe a little more decidable, right? Because you, in this, we have like a sort of global view of the trace. So, we can basically check that So... Yeah, so... So this is getting into some later content that I was hoping to, I think I've slightly misordered this. But anyways, if you have a God's eye view of a VLSM trace, and you have a message that wasn't sent by a component, but it was received by some component, that's an equivocation. Sorry. Denise, can you go ahead? No? But I think it would be like a equivocation or a thought-form equivocation. Mm-hmm. Yeah. So here's a theorem, right? That the local equivocation is always going to be less than the global equivocation. And all these are checked in the theorem provers, but you can sort of imagine why that is. And basically, we can use these global and local definitions of equivocations to limit the equivocations to create basically a composition constraint where the faults are limited. We can easily just say, okay, well, there shouldn't be any equivocation and talk about DSM traces where there aren't any equivocations. And we also use the full node assumption to reduce the amount of equivocations because then you can only sort of get an equivocation from the sender of a message because you've already received all of its dependencies. We can limit equivocations to just a subset. And we can also assign weights to the nodes and then limit the equivocations by their total weight. These are, like, example conditions in composition constraints or a local constraint. So this is a composition constraint for a validator on a global constraint that looks like this. And so, like, in this particular example, this validator is just checking the local equivocation weight and if it's less than T when the composition constraint is checking the global equivocation. And the validator property is basically that from the local one, there should be a state where the global one is also satisfied. Basically, the lifting property of the valid state from the local to the distributed property. So, you know, this was talking about basically what equivocation looks like and how to detect it and therefore how to talk about, you know, non-constructively traces that have limited equivocation. But we do have a very nice constructive sort of approach to where we can describe equivocators and basically there's two models for equivocation. There's a state equivocator, which basically splits its current state up or has many states for the same validator. And it can do that by forking or by starting new machines. And it also has... And there's also the message equivocation model, where instead of the state splitting and having multiple copies of a validator, validators can receive messages that haven't been sent. And sort of this sort of is what we're observing in that definition of global equivocation. And it turns out that these two things are equivalent, actually. The traces that you can get from the state equivocations and the message equivocations are the same. Whether you are receiving messages that haven't been sent or splitting up states, if you project down to those equivoc up states, if you, like, project down to those equivocator states, we get exactly the same traces. And it's kind of interesting, basically, like, splitting a timeline and communicating across timelines end up producing exactly the same states. And so, these two are models of the same phenomenon equivocation. And that's why we have those two definitions there where one of them seems a little bit different than the other. You know, somehow two messages that couldn't have been produced in a single trace evokes a state equivocation, and a message that hasn't been sent yet to being received evokes the message equivocation. But they are equivalent. So that's a pretty cool result that's going to be useful later. But basically, to repeat it, the models of equivocation that split the state and models of equivocation that allow communication from other traces lead to the same traces for validators for a limited equivocation. So that means that when you have evidence of equivocation being produced, you can produce that evidence either with state equivocation or message equivocation, and you get exactly the same state, exactly the same evidence. Great. So that's the first two sections. Any questions before the next one? Excuse me. So here we go. Yeah, please. Your microphone is off, sir. Can you try it? I guess in the previous discussion, you kind of assumed finite branching, which means that you cannot make infinitely many copies at the same time. No, we have an unbounded, we have like a, a list, like, unbounded list of copies. Okay, but still finite, right? Yeah, finite, but finite unbounded, yeah. Yeah, because when it comes to infinite messages and states... Yeah, that's a good question. I think we have possibly infinite... traces, but not states and messages? At the moment. Yeah, I guess so. Sorry about that. Yeah. We'll get there. Yeah, so now basically we're gonna do, yeah, go ahead. Yeah, please use the mic here. Microphone, microphone. Yeah, please use the mic here. Microphone, microphone. Just hold it closer. Oh, no, never mind. Sorry. Hi. Can you hear me? Yeah, that's great. This is great. Sorry. I think the states can be infinite but not reachable. It's a matter of which are the reachable states. But it matters how many labels you have and that gives you how many moves you can do. But in reality, yes, it's bounded and, yeah. Great. So, let's move to the Byzantine faults. So, basically, we can model Byzantine faults in VLSM by replacing a node with a node that basically has a free behavior that uses labels to send and receive any message at any time. The important behavior is that it can send any message at any time, basically modeling someone that can send any sort of malformed and invalid message at any point. And we do have a little bit of constraints, which is that we don't let them forge messages on other nodes, and we do have a full node assumption. But, you know, they can send any message, you know, signed by them from them, basically, without forged messages inside. And so, we can replace equivocation limited validators with Byzantine components and find that they have exactly the same traces. The ones that aren't replaced have the same traces. So if you have a trace in the equivocation-limited composition, where some set B of validators is Byzantine, they have the same traces as if they're composed with equivocators instead. And basically that's because of the validator property. So if you have a validator property on receiving a message from a Byzantine sender, that means that there is a composite state where that sender, as an equivocator, can validly send that message. Because here we're validating for a limited equivocation setting, so some amount of equivocation is valid in that setting. And so we can replace these Byzantine nodes with equivocating nodes. And then look at the traces of the validators that aren't equivocating and show that they have exactly the same traces. Denise, do you have a question? Okay. And the same result also holds for weight-limited equivocation model. So it's not just for a fixed set, but for under T-limited equivocation weight. All the behaviors of non-equivocating components due to equivocating components is exactly replicated by Byzantine behavior with the same T limit. And so that basically means that under the limited less than T weight equivocation, we get all of the same traces for the validators as limited less than T-weight Byzantine faults. So that's sort of the sort of magical way that we can not use Byzantine fault tolerance. Basically, for these equivocation-limited validators, equivocation faults are exactly as expressive as Byzantine faults because by validating for that limited faulty setting, you know, they're restricting their transitions a lot. And if a Byzantine fault, a Byzantine node can send some malformed message that they receive, that means that that transition can be lifted to a valid state in the composition under the limited T equivocation condition, which means that there are nodes in the composition distributed that satisfy that less than T threshold, but, you know, aren't Byzantine nodes, but equivocation nodes. And then, you know, like putting those transitions together to get traces, we can rebuild exactly the same traces. And so basically this forms an alternative for analyzing faulty distributed systems to Byzantine fault tolerance. And quite simply, you know, by studying equivocation limiting and equivocation faults instead of, and equivocation faults instead of Byzantine faults. So somehow equivocation faults are like a special kind of fault where if you validate for limiting equivocation, that's just as good as validating for limited Byzantine faults. Oh, sorry. That's just as good as, sorry, that actually lets you throw out Byzantine fault tolerance analysis altogether when thinking about, like, what traces you could go to. You can sort of just go to the protocol-defined ones, and it doesn't really matter what the Byzantine nodes do. They're basically just protocol- equivocators as far as the analyst is concerned. And so instead of having misbehaving nodes, they just have either like a state replicator or a message passer that sort of crosses timelines. Which is sort of much more tamed types and well defined behavior. So later we're gonna relax the full node assumption and treat synchronization faults. I'm out of time. Thank you so much. Thanks for coming. Really appreciate it. If you have any questions you can find me outside later. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1neM1-qHBPiHQ47mw5gGhxKmdlAYMtpZujIccA88zZM8",
      "resources_slides": "https://drive.google.com/file/d/1thif2hdl5jczmisfpTqoFwra7_H0YoUg/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "why-ethereums-issuance-policy-is-redacted",
      "sourceId": "39HYEG",
      "title": "Why Ethereum's Issuance Policy is [redacted]?",
      "description": "This talk explores the status quo of staking economics, its drawbacks as we see them and what the future of staking economics could look like.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "ACD,Staking,Economics,ACD,Economics,Staking",
      "keywords": "none",
      "duration": 1694,
      "language": "en",
      "sources_swarmHash": "0118431db811d58100ca9b2f46e6661abd7baab68e55a3055f821c149862bfa0",
      "sources_youtubeId": "cUgKXBq017g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736ecef1b0f83434d629108",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:45:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1H2muDBPNRQn-IIusKik3f5fD_tsi9lmseX7GwmbUAh8",
      "resources_slides": "https://drive.google.com/file/d/1VXeY7G-WzJNouMEPJ447HTB7-aTCvfmR/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "hacking-thai-beats-cities-and-dances",
      "sourceId": "NM8B9E",
      "title": "Hacking Thai Beats, Cities & Dances",
      "description": "Can we inspire Thai builders to be more creative through hacking our own culture? Stories of an algorithmic Thai music festival in Thailand's oldest museum, an open-source hackathon to improve the city of Bangkok, an interactive art performance that blends algorithms with traditional Thai dance; and how you can build better builder communities by inter-disciplinary thinking.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Art,FOSS,Live Coding",
      "keywords": "",
      "duration": 522,
      "language": "en",
      "sources_swarmHash": "c127a713d0d245b536157b346a5bb540daeb2d2951f255a26af1b86af9a7f766",
      "sources_youtubeId": "WrWIehDVA8E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67356bfe9dbb7a90e1565e6b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67356bfe9dbb7a90e1565e6b.vtt",
      "transcript_text": " Hello, my name is Phum and I'm a software engineer at Metabase and I work on open source analytics. But in my free time, I'm actually the co-founder of an open source initiative in Thailand called Creator's Garden, where we basically work on open source projects and create events to explore things like synthetic biology and topics like philosophy. But we try to explore those mainly with coding. So while working on these projects in Creator's Garden, I got asked two questions mainly. First is basically how do we get Thai people or specifically Thai builders to be creative. And second thing is how do you bring builders to your spaces? For example, if you have basically an Ethereum space or Web3 space, how do you get people hooked or interested? We have been hosting these events in Thailand for about eight years now, so since I was in middle school, actually. And I'm going to talk about two of these events that kind of gives me this idea on how to engage with builders. So first is on hacking Thai beats. So this is a project I work with Thailand's oldest museum. So I was working with this guy, Khun Hanoi, and he might look very traditional to you, like a Thai musician, but don't let that fool you. He's actually an AI researcher at TikTok and at Google. So he was working on a program that would be able to synthesize music out of AI. So I was thinking, what if we would take Thai instruments, for example, the pinai and granade, and we would be Thai instruments, for example, the PNI and RANAT, and we would be able to use computer and coding to basically synthesize sounds out of nothing? So that's where the idea comes in. What if you can use algorithmic audio synthesis to basically create an event where musicians can come together? But instead of instruments, they do coding. And instead of Western music, they do coding. And instead of Western music, they do Thai music. So in this event, we actually did a big projection mapping onto the wall of Thailand's oldest museum. And people were using things like AI synthesis tools and algorithmic coding tools where they can basically build sounds out of nothing. And I think this is one of the most important lessons we know over the years, which is it's very important not to only have a strong domain or a strong theme or a strong tool, but to combine that. For example, a lot of hacker house, they only focus on the technology. For example, they focus on Ethereum, but they didn't really let people know what they can build with it. In contrast, a lot of hacker houses, they focus on themes. So, for example, finance, but they don't really see what other interesting tools you can interact with that. So, with the juxtaposition, it kind of helps a lot. So, Seymour Papert kind of puts it best when he says, people really learn best when you build things that you're interested in. He was talking about children, but this really applies to anyone. So that kind of brings me to my second project, Hacking Thai Cities. So everyone knows, like, Bangkok is a great city, but there's one catch. Well, you know, there's, like, the air we have right here is probably not the best, and maybe the traffic is not the lightest. And the mayor of Bangkok didn't actually just stop and do nothing. He actually hosted a hackathon, actually was done about three years in a row now, and I kind of like that idea. But the problem is, in a lot of these hackathons, the idea is to stop and really goes nowhere. So I was thinking what happens if we use the power of open source? I know the space for Ethereum really got this far because it's open source, so why not the whole city? Why not make an open source city? So we started with the idea of basically letting people come together, but they don't have to formally form a team, but rather an open source project where anyone can join anytime. If you're interested in someone, well, just open a PR. And we started by basically bringing the domain experts in Bangkok, people who have the data of the whole city, like what are the problems in the city, what are the areas that have flooded, and the distribution of those people, as well as people who have accessibility problems, like people who are in a wheelchair, for example. And I think the greatest thing about open source is it lets everyone contribute. So it follows the principle of low floor, high ceiling, and white walls. Low floor means you can contribute anytime. If you know how to do translations, you can do it. But there's also very high ceiling. So if you're an AI researcher, you can kickstart a new project. And it has white walls. So no matter if you're a scientist, if you're a researcher, well, there's a place for you to contribute. And this brings very interesting projects in the hackathon we did. So for example, there were a lot of motorcycles that were parking near basically the sidewalks, and you cannot basically get over that, and it was really annoying. So someone developed a software that was computer vision to check that and automatically report that and get a bounty. So in Bangkok, we have that bounty system where you can make money by just catching the people who are on the sidewalks, the motorcycles, and you can automate that with computer vision or automating the tree detection of every tree in Bangkok. So we did that. And the last project I want to share with you is hacking Thai dance. In 1923, there was this photo, which is the first ever photo taken of the Thai dance. And the interesting thing is it's 100 years later, but nothing has changed. We're still teaching dance the same way in school. And to be honest, a lot of kids like my age, they don't really get interested in dance anymore. It's unlike K-pop where you're able to hack. So we were thinking, can we use AI to basically change that? So we think of the project where you can generate new dance out of old dance moves, like for example, the 59 principal dances. So we were turning this into an art performance called Cyber Subin, which lets human dancers kind of dance with AI. So we built a software that would try to learn the characteristics of Thai dance. The way we did this is we started basically a group of builders, but also a group of Thai choreographers. And we were going through the old documents of how people would decompose the dance that makes us learn that there are six principles in a Thai dance for example the energy in your body and the circle and curve that surrounds you and from the knowledge we gained from the choreographer we were able to build a software that about allows us to basically generate new dance out of old ones by tweaking the parameters. So we think of things as basically a set of animation transformations where you'd have the original dance, for example, the dance of Te Panom, which looks like this. And then you generally run that through a series of programs that would generate very new dance that you can command and we turn this into an interface where dancers can use to basically generate dances and we put this all together in a dance performance so we just show this up in bali uh i think that was indonesia and also in taipei taiwan so dance performance, we got a lot of people like upstage to kind of join in the fun and play around with Thai dance. So generally, I think this brings me like all the three projects to a final conclusion, which is if your culture or your domain is very hackable, for example, if you make finance like very hackable or you make Thai culture, there are some elements where people can go in and can use their tools or use their technologies to hack. Then it makes it really fun. So compare, let's say, a Thai student who were learning the Thai dance in school and they were really bored because there's nothing new to make. Versus like kids who know how to use AI and they're able to construct new dances and have fun with it. Yeah, so I would like to say thank you to my team in Cyber Subin for the dance production. And thank you to you too for listening. This is it. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:55:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/16NvToD2NQxicsfxWktPRLuxSt7qrL73mCEcujVhk_i0",
      "resources_slides": "https://drive.google.com/file/d/1IQQP48SNOBU7GhDHArFHe0HDDud7GMcM/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "product-led-blockchain-development",
      "sourceId": "8YS9LW",
      "title": "Product-Led Blockchain Development",
      "description": "As teams spin up new app-specific rollups and L2s, we've moved into an era of product-led blockchain development. In this model, developers are not only building the first product or client to leverage their protocol, but establishing what ‘product-defined blockspace’ means. \r\n\r\nIn this talk, I go over the history of product-led growth, how it evolved to product-led protocol development in web3, and finally, what product-led blockchain development means in the context of app-specific rollups.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "development,product",
      "keywords": "usability,product development",
      "duration": 519,
      "language": "en",
      "sources_swarmHash": "e673aab0a55e626aee65b66612cfff54e5bc047220abde3acee5e741714ea2b5",
      "sources_youtubeId": "RIyvlEGFyHo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67356d759dbb7a90e16a807e",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67356d759dbb7a90e16a807e.vtt",
      "transcript_text": " Hello everyone, my name is Gregory Rocco as mentioned and I tried to figure out what I could do in a short amount of time for a lightning talk. I felt like you'd have a lot of highly specific technical talks, so I figured I'd give product a go when it comes to giving one of these. So the title itself is Product-Led Blockchain Development, so it's kind of covering the whole lineage of going from product-led growth for startups to product-led protocol development, which was originally coined by Dan Romero a couple of years ago when thinking about Forecaster, and then finally product-led blockchain development, which is this new era of everyone seemingly wanting their own block space for some strange reason, but we'll figure it out eventually. But the subtitle here is Why You're Wrong and the Market is Right deals specifically with making sure that whenever you think about product development, protocol development, or even blockchain development, that you are serving some customer need and not your own interests to a certain degree. You have to be interested in what you're doing, but at the same time, you have to serve a market. And a disclaimer to this talk is that I was a founder, and I was wrong plenty of times. I specifically started my career in this space in decentralized identity, which namely is a hard space to crack when it comes to product development. And probably there are just very few companies and protocols that have done it. Maybe ENS is the closest to product market fit when it comes to identity. But user-controlled credentials generally were quite tough. Going back into the core basics of product development, one of the kind of core pillars there is product-led growth. Namely, that's whenever you build a product, you specifically have to make sure that customer needs and customer experience are at the forefront, dictating everything that you do and become the full kind of motion for what you do. So feedback comes in, you then take that feedback as a product developer, and that's what influences engineering, design, messaging, and everything in between. It shouldn't come from, oh, just one more feature and all these users are going to use it. It never works out that way. It's specifically in the case of someone requesting a feature from what you build. You bring something to market, you get your initial set of users, and you're constantly using that messaging to inform what you're going to do and all the decisions you're going to make when thinking about product development. This was the framework for a long time when it came to startups. It's what I was indoctrinated with when I went through Y Combinator. And then later in the space, since we're all kind of of building these emerging protocols and blockchains, the framework had to be changed a little bit because it was a bit of like a blue ocean space when it came to protocol development. Kind of fast forward a little bit later and you have the situations where developers do developer-y things, where the developer might have an interesting idea and might have some problems bringing it to market specifically because they're just kind of handing a protocol over to someone and saying, here, build on this. Build all these great ideas on my protocol. It'll be great. But the problem is it's not taking those first principles into account. And a couple of years ago, Dan Romero from Forecaster put out a post called Product-Led Protocol Development, and it specifically dealt with the idea that as a protocol developer, you're building the first apps and clients that leverage your own protocol and are then taking the feedback from those to inform protocol development. Namely, in his case, there are three core tenets of product-led protocol development. The first is that users use apps and clients, they don't use protocols. So let's use Farcaster for the sake of the example. Users are using Warpcast. They're kind of making posts. They're interacting with the application. On the back end, it's Farcaster, the protocol, that's kind of informing all of these interactions. The second thing is that a developer worth their salt will make sure that they're concerned with meaningful daily active users instead of the coolest technology that they want to use. A developer that's building for an audience specifically and not experimenting. And the third and final part of it is that protocols are valued based on the clients and app success that build on said protocol. So it's always making sure that apps and clients are at the center here. And then if you're building a protocol, you are defining the blueprint for how that protocol should be used by being informed of what's happening on these apps and clients that people are actually using. And then finally, to round it out, complication requires a bit of order. Recently in the space, we're finding protocols wanting to have their own block space, building app-specific roll-ups. And I think just the bottom line of all this is just making sure that if you kind of go down this route, that you have to be the block space you want to see in the world. You have to make sure that when you're building this, you are building the killer app, leveraging this block space, that you're not going the usual route of building this thing and saying, hey, everyone, we're going to get a growth marketer to bring a million developers through the door, and then they're going to build a whole bunch of stuff on our new protocol, and it's going to be great. It's like, no, you're going to be building the app that specifically leverages this for what you designed it to do. And if it doesn't work out well, then you might need to hit the drawing board again. But it's all specifically going back to this core idea of product-led growth, where you are presenting something to the market. You're presenting the end users with something to use, they are then informing what happens with that and everything in between. So kind of returning to first principles, I encourage experimentation, but at the end of the experimentation, if you do wish to bring something to market, please keep designing and developing with users in mind and making sure those users are able to give you feedback to inform the next stages of what you're building. So thanks. Thanks a lot, Gregory. And now we have a little bit of time for questions. Wow, someone's ready. I can see that. All right, let me get this box to you. Ready? And go. Hello, thank you. I'm curious to have your opinion about the fact, don't you think the space is even designed to not incentivize competition between different roll-ups? Because they're all looking to attract liquidity, attract users, and just grow individually. And so this doesn't go in pair with taking care of every user, because they compete for users, they compete for liquidity, and there is a lack of interoperability, really. So what are your thoughts on this? So in the case of very app-specific case, you're then competing with others. We don't need all those roll-ups. No, we don't. It's unnecessary. I think I'm in agreement with you. We don't to a certain extent, but making sure that if you were building something to then build in in those use cases at the same time. I might not be understanding the question correctly. Well, I think the space is designed in a way like everyone wants to top their own roll-up, get users, get liquidity, but at the end, we're not building something that actually can be useful for people. And it's also sometimes just serving the ecosystem. But to an extent, so if there is a dominant use case that's an app-specific roll-up, then good for them. I hope it amasses the most amount of liquidity in that own way and dominates that space if it's providing a good enough of experience. Like, I want to allow the ability for anyone to create that app-specific roll-up, but at the same time, I want someone to win. And that's okay because it means users getting the value. And if that app-specific roll-up turns around and does something against user wants, there's another app-specific roll-up kind of waiting for that, or waiting for that user base. OK, thank you. Is there another question? Because otherwise I was another one here. Yes, there are others. So you essentially mentioned that developers should focus on building a product that the market wants. What makes it, and we see that most companies actually are having a hard time doing this. What do you think makes it so difficult to apply that very easy, fundamental, first principle thinking? I think being in the space for a number of years, everyone, there's a number of years, everyone, there's a lot of cases where people aren't working deeply enough or pinpoint enough on a very specific issue. Namely that, good example, let's use a hypothetical of social protocols, or someone building a chain for all the social protocols. They're thinking mostly about, this is again a hypothetical, but like we're going to amass all the social protocols. They're going to be using our chain. There are different clients and all that without building the very specific client themselves and saying we're going to be offering this experience first as a first-class citizen. They're thinking too much about offer it to everyone before let's get the experience right and figure out how to then open it up. And I think that's been a part of the thinking or an issue with the thinking for a long time. It's ambitious and it's very like a good thing overall, but at the same time it becomes difficult to provide that very narrow experience or solve that very specific problem before thinking too widely. And I think you have to balance both of that when developing and designing.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T02:55:00.000Z",
      "slot_end": "2024-11-14T03:05:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1aMtbpw97Q1DjqYA3pKLPTVpJ9vWOJoduN-rGCXYlHck",
      "resources_slides": "https://drive.google.com/file/d/1PUCNi_DKlJJuey7QpQLcAhHDq9kjGi4l/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "fossify-yourself-for-privacy-and-security",
      "sourceId": "TW7QGF",
      "title": "FOSSify yourself for privacy and security",
      "description": "You will leave this workshop at least a bit more cypherpunk than when you came. The session will introduce FOSS stack of tools for all platforms. We will discuss free operating systems, GNU/Linux distros, GrapheneOS, secure communication, browsing, hardware options and secure environment for handling your crypto or Ethereum validators.\r\nThe workshop is interactive and open to anyone to participate. Join us to find free and open solutions to your problems or come to share your favorite foss tools!",
      "track": "Cypherpunk & Privacy",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": true,
      "tags": "Privacy,Security,self,hosting,Privacy,Security",
      "keywords": "free software,degoogle,self hosting",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1PShw8A7XomH3DtlwmgLZcgMrPY11XvLp_EuNeSwghoQ",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "mud-past-present-and-future",
      "sourceId": "FE9L3P",
      "title": "MUD: Past, present, and future",
      "description": "MUD--an open-source engine for autonomous worlds--was released two years ago in DEVCON Bogotá. Since then, it has gone through many iterations and helped many developers build their onchain games and worlds. Two years later, MUD core developer Alvarius will take stock of where we are and what the future holds for MUD.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Frameworks,Gaming,Tooling",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "6053e69151ed013c3c0ca3815d2a365dd08d3ba0696adcecafc8f53b1ffcd65a",
      "sources_youtubeId": "AbHC8FVGxeU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:00:00.000Z",
      "slot_end": "2024-11-14T03:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1OeTy66nVePoVL95ayNDdQbYFQRdNCNjTM0xMIccPtWE",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "the-dacc-vision-balancing-progress-and-protection",
      "sourceId": "AA8SRQ",
      "title": "The d/acc Vision: Balancing Progress and Protection",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 594,
      "language": "en",
      "sources_swarmHash": "b77c636e49391f4c713aa60f1572f8527d82b94618ed7b10dd27f83b94af084c",
      "sources_youtubeId": "pTpURVxj9hk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67356cb99dbb7a90e15f86d1",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67356cb99dbb7a90e15f86d1.vtt",
      "transcript_text": " I will get straight into the one slide that I have today. So last year I wrote this post that introduced this concept of DIAC and in general was basically responding to a lot of the discussion that was happening at the time around techno-optimism, the need to be more optimistic about technology and to appreciate the very positive impact that technology actually has had over the past 10 millennia, and also the extremely positive potential that it has over the next century. And at the same time take seriously some of the risks and also really focus on which technologies are going to do the best possible job of giving us the kind of future that we want while at the same time minimizing and even actively fighting against the risks. One of the arguments that I made is basically that there is this important phenomenon which is the offense-defense balance, right? Basically is it easier to attack or is it easier to defend? And if you have an environment where it's easy to attack, then you almost inevitably have this kind of dark Hobbesian choice between a very powerful sovereign and a very destructive state of anarchy. And it just inherently creates all kinds of very bad political effects on top of creating constant ongoing risk of suffering, right? And at the same time, this is all happening under this backdrop of this discussion about is AI itself, and particularly super intelligent AI, this very big and massive risk? If it is, should we try to slow it down? But then if we do slow it down, then slowing it down forever basically just creates a world that becomes more and more unstable. And so what is the actually resilient and actually long-term stable future that we could be aiming for? And I think this to me is a near and mid-term part of the answer. So in the post that I made last year, I split up defensive technology into four categories. So the first split was the split between the world of atoms and the world of bits. So very famous distinction, might as well just grab it and use it. And within the world of atoms, split it up between macro and micro. And so macro defense, basically you think about physical resilience. And we'll have some very interesting talks later today, including topics on how to massively reduce casualties if some kind of extremely terrible disaster happens to humanity. Micro-defense basically means biodefense, so anti-pandemics. And both myself and a huge number of other really fascinating people will be talking about how we can make our environments vastly more resilient against actually existing and even potential pandemics all without requiring significant changes in individual behavior so you might be pleased to know that this room is actually already passively airborne disease resistant so for example over here you have this i mean box, and the box is HIPAA filter. We have a bunch of these all around the room. And so basically this room becomes vastly more safe against COVID, against any kind of future airborne virus that comes up without requiring anyone to actually notice. And this is one example of the kinds of open and widely accessible, easily deployable, and freedom-preserving technologies that we can deploy that can give humanity an orders of magnitude boost in its resistance against these kinds of threats. These are things that we are not doing today, and these are things that with a surprisingly low amount of investment we could actually do a much better job at. So then on the other side we have the world of bits. And the world of bits, this is a distinction that is I believe unique to me. But I talk about the distinction between cyber defense and info defense. Cyber defense is defense against threats where all reasonable people agree who the attacker is, right? So if a DeFi protocol gets hacked, then obviously the DeFi smart contract doesn't agree that it got hacked, because if it did, it wouldn't have sent any money out. But all reasonable people looking at the situation will agree that, well, yes, there is a hacker, or at the very least, someone who used a very unintended mechanism to get coins out of that system. And so we can talk about cryptography, we can talk about formal verification, blockchains, zero-knowledge proofs, what I call the Egyptian God Protocols, FHE and obfuscation, and also another important thing, hardware security, and actually deploying all of those technologies and actually even applying them at the level of operating systems and making sure that things are much safer than they would be today. And then info defense. So this is where reasonable people can actually disagree on who the attacker is. So one man's misinformation is another man's unjustly suppressed valid point and we need technology to actually help filter through this information and help people identify what kinds of content are more likely to be actually positive and what kinds of content are more likely to be misinformative, misleading, things that even they themselves, if they better understood it, would not want to see, and do so in a way that does not involve empowering a centralized elite that decides on behalf of everyone else what is good and true and what is bad and false. Now, that is all a vision of defense. I think one thing that is also important to talk about here is I added a third dimension in the year since then, right? And I call this the survive-thrive dimension. And here we talk about not just the technologies to protect against the bad stuff, but also actually enabling the positive future, right? So in the bio side, we have longevity. Who here is excited about longevity? So now longevity, and then beside longevity, we also have BCI, and BCI is kind of conveniently beside longevity and open decentralized compute, right? So on the left is making sure a compute is safe, or at the top, making sure a compute is safe. At the bottom is making sure a compute is amazing. And compute and biology together, we get BCI acceleration. And I think one of the really important points that we're going to make is that there's actually a lot of ties between these different spaces. It's all one very big integrated field. A lot of the viral persistence research that's being done in the context of long COVID right now actually has a lot of tie-ins that are very applicable to aging, right? So there's recent new theories that viral persistence actually is a thing that's a very big contributor to Alzheimer's. And then BCI, better medical technology, it contributes to fighting diseases, it can contribute to living longer under, quote, normal conditions, and it's also a BCI accelerator. Now, physical abundance. So we want the big silver punk cities, we want housing to be affordable. We want, you know, housing to follow Moore's Law instead of following Eroom's Law, for those who know what those are. And then, you know, we want to go to space. We want everything about our physical environments to not just be resilient but also be affordable and amazing. And finally, collaboration technology. So even in situations where people are not attacking each other, where you actually do have reasonable people and communities that have high trust between each other but they do want to be able to more quickly and more effectively agree on things and come to new consensus if we want agree on things and come to new consensus if we want decentralized collectives to like actually be able to act like life players and make bold choices and keep adapting themselves to rapidly changing circumstances without that collapsing into a dictatorship then much more powerful collaboration technology. So this includes all of the stuff we've been doing around public goods funding. It includes quadratic voting, other forms of voting, futarchy, all kinds of different governance ideas. So basically I think there are natural tie-ins at all sides of this between technologies that can create an environment where everything is much more safe and much more resilient by default and an environment where we have open, distributed and widely available progress for everyone. And so this is what all of the speeches that we're going to see today are going to be about. And I'm very excited to be with you and listen to them. Thank you. And so now I'll introduce our next speaker, Eli Dorado.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:00:00.000Z",
      "slot_end": "2024-11-14T03:10:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/105T9qheqDS91uBB6zsLjkTZKkqteIemZL0l9pkz8eJo",
      "resources_slides": "https://drive.google.com/file/d/1GnSqMMj86XIKIrrDYqp1bTaWUvTFJ3uR/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "the-open-source-orchestra",
      "sourceId": "9PWLBV",
      "title": "The Open Source Orchestra",
      "description": "Member of the Open Source Orchestra",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:00:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1MLErEiLaty6zwbafFEy3AROdYSwqpoEoEBnY5JL_9YY",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "chain-abstracted-smart-accounts-how-to-build-amazing-web3-ux-in-2024",
      "sourceId": "E7QLZJ",
      "title": "Chain-abstracted Smart Accounts -- How to Build Amazing Web3 UX in 2024",
      "description": "Chain abstraction (CA) and account abstraction (AA) have been two of the hottest topics in Web3, but few people know how to use them together.\r\n\r\nIn this talk, I will explain how developers can build amazing Web3 experiences by combining AA with CA in the form of \"chain-abstracted smart accounts\" -- smart accounts that can spend their balances on any chain without bridging.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,User Experience,Account Abstraction,chain,abstraction,Account Abstraction,Cross-L2,User Experience",
      "keywords": "Chain,Abstraction",
      "duration": 516,
      "language": "en",
      "sources_swarmHash": "a50d966f5d76b48008c986aca879f466e282828b70da8935c2dde90de5afc221",
      "sources_youtubeId": "oiwLnbe--XE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67356f5f9dbb7a90e1835056",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:05:00.000Z",
      "slot_end": "2024-11-14T03:15:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1OMXnYKdkNqOzuJsNTBVLxJNmMzaz6upJphPKMzWFHU8",
      "resources_slides": "https://drive.google.com/file/d/1v_BxfyvCMCWG-ZIoON9KEFQrbEB1yjkN/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-political-economy-of-dacc",
      "sourceId": "AXX3JD",
      "title": "The political economy of d/acc",
      "description": "The dynamics behind d/acc are not new. Economic history is full of examples of the private provision of public goods. If we want to reduce AI risks while preserving freedom from centralized control, it's worth thinking carefully about the different ways humans have solved isomorphic problems in the past, and how the same tools could apply today.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Public,good",
      "keywords": "d/acc",
      "duration": 1079,
      "language": "en",
      "sources_swarmHash": "44f39b5a61d0278c154c160191866ad38d028d9a4d0677cb0457ded12fe5dd30",
      "sources_youtubeId": "Ukm0tcoedeg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67356f0f9dbb7a90e17febf4",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67356f0f9dbb7a90e17febf4.vtt",
      "transcript_text": " All right. Everybody, I'm thrilled to be here. I am Eli Dorado. I'm Chief Economist at the Abundance Institute, and it's a real pleasure to be here to talk about DIAC. I'm approaching this through my training as a sort of political economist, and I'm going to give a high-level overview of how I see this space. level overview of how I see this space. I want to start by talking about our instincts for both control and freedom. We all have these instincts. And in the last few years with AI innovation, we've seen this kind of like control narrative or this kind of control instinct be on display. Like we need an enforced pause on AI development. We need to shut down data centers. We need a kill switch. We need regulation. We need to restrict model releases or we need to impose bans on open source AI models or export controls. And sort of the counter reaction to that is like, no, we need freedom of speech. Are we really gonna regulate matrix multiplication? And this is a slippery slope to totalitarianism. And then I think that there's an interesting conclusion at the end that, therefore, I'm gonna not believe the premise. I'm not going to believe that there is any risk here. And I think that this conclusion is very natural. It's really bad epistemic hygiene, but it's common and we need to acknowledge it. And you see this in completely different contexts with things like climate. People will say oh, I just don't believe in it, because they don't want to, like, accept the sort of control-oriented beliefs of the other side. So just because freedom is good doesn't mean that the problem raised by the anti-freedomites is fake, right? that the problem raised by the anti-freedomites is fake, right? So, DIAC approaches both AI safety and biosafety from the perspective that they are genuine public goods. But crucially, just because we acknowledge the existence of safety as a public good doesn't mean that we think the right approach is to generate it through top-down coercive means, right? acknowledge the existence of safety as a public good doesn't mean that we think the right approach is To generate it through top-down coercive means right? We're gonna take both freedom and public goods seriously, and I think that's the ethos behind The act and about a lot of it theory um in general So that brings us to the next part of the talk the private provision of public goods So So that brings us to the next part of the talk, the private provision of public goods. So what is a public good? In classical economic terms, a public good is one that is neither excludable nor rivalrous. So like the canonical example is the lighthouse. A lighthouse on a coastline provides guidance for all the ships, whether or not they pay, right? So a lighthouse is not an excludable good. And at the same time, one ship relying on the lighthouse for guidance does not diminish the ability of any other ship to rely on the lighthouse for guidance. So the lighthouse is non-rivalrous. So game theoretically here, we have a free rider problem, and we should not expect the lighthouse to be produced by the market, right? You must rely on governments to produce lighthouses. So one of the great economists of the 20th century, Ronald Coase, got interested in this example, and he decided to actually collect data on lighthouses in Britain. And astonishingly, he found that every single one of them was privately constructed. So we have a free rider problem. It's not supposed to happen. But every single lighthouse in Britain was privately constructed. So how can this be? What Coase found is that lighthouses were vertically integrated with ports. Coase found is that lighthouses were vertically integrated with ports. So the port operator would build a lighthouse so that people could come to their port. And then they would recoup the costs through port fees. And what this example shows is that creative structuring can turn a public good problem into a private good provision. public good problem into a private good provision. Another closely related idea is a common pool resource, right? With the classic example being irrigation systems. So irrigation systems are arguably one of the things that led to the creation of states in the first place. So Carl Witt Vogel, in his book Oriental Despotism, advances the concept of a hydraulic empire a powerful state as in Egypt or Mesopotamia or China that's built around irrigation and maintenance of this common pool resource is a public good and it's worth it to accept some coercion if it means you get the benefits of irrigation so just like with the lighthouse there's a twist so later scholars if it means you get the benefits of irrigation. So just like with the lighthouse, there's a twist. So later scholars, particularly Eleanor Ostrom, figured out that many complex irrigation systems, like in the Philippines or in this one in Valencia, Spain, were cooperatively maintained without an autocrat. This one goes back a thousand years. And so she won a Nobel Prize for figuring out the conditions under which such cooperation is possible. Another example of non-coercive public good provision is standard setting. So standards are a public good, but because the value of the standard goes up when other people use it, there's an incentive to cooperate in standard production, standard development. So one of the explicit powers reserved for the government in the US Constitution is to develop standards, but it turns out that private standard setting happens all the time. This is an example from IETF, but we're here at DevCon, and Ethereum itself involves a lot of cooperatively produced standards. I'm not going to do a long spiel on each of these modalities, but there's a lot of different ways in which public good production can be incentivized without relying on sort of centralized top-down government coercion. We've talked about the first three already. You can think of a lot of open-source software development as scratching an itch. Sometimes people do things to accumulate prestige. Sometimes people give money to causes they believe in. My organization, the Abundance Institute, is a 501 nonprofit. Our budget is entirely funded by donors who get very little public recognition. Usually they don't want it. And they give just because they believe in the mission. Norms like politeness or not engaging in petty theft are also a source of public goods. And of course, mechanism design is an important one for Ethereum. We can think of consensus as a public good and the consensus protocol as a mechanism that incentivizes coming to consensus. So public goods, like AI safety and biosafety, can be provided non-coercively without government involvement, and it happens all the time, and it's possible that by some accountings, most public goods are in fact privately produced. And so now I want to show the flip side. When public goods, and particularly safety, is provided coercively top-down by the state, the results are often pretty bad. So let's look first at nuclear energy. In the 1950s and 60s, we experienced cost reductions as we deployed more nuclear plants. So this is common in industry. It's called the learning rate. And then in the 1970s, the trend reversed. Costs started to explode. Initially it began in the early 1970s with environmental regulation and then it accelerated in response to the Three Mile Island incident in 1979, which by the way, nobody was harmed by that incident at all. Another domain, which Vitalik referenced, in which safety regulation has had enormous costs is pharmaceutical development. And by the way, this is adjusted for inflation, just like the last chart was also. So these higher costs of bringing drugs to market represent thousands of missing drugs. We're missing thousands of drugs. And people die because of the lack of drugs. But these people are not identifiable victims. And my friend and former professor Alex Tabarrok calls it the invisible graveyard of people people dying because of lack of drugs brought to market Aside from nuclear and pharmaceuticals the other major industry that is regulated via like pre-market approval is Aviation so everyone thought thought after World War II that there was going to be a growing personal aviation industry, that people coming back from World War II knew how to fly, they could buy surplus military airplanes, and we would all just have essentially flying cars, just personal aircraft that would take us wherever we wanted to go. And it could have happened, but we had increased regulation at a couple different points along the way here in the mid-1960s and early 80s, and this strangled the industry. Unfortunately, FAA has recognized that they've gone too far with personal aircraft regulation. They're beginning to actually loosen the rules. And something that's super interesting is that they actually think that loosening the rules for a category called light sport aircraft will on net increase safety through both innovation, so the more you innovate, the safer the aircraft get, but also through changing the composition of the fleet and sort of increasing the use of manufactured airplanes. This is a way that deregulation can lead to actually higher levels of safety. And it's interesting to see a major federal regulator recognize that. So there's a lot of reasons, a lot of different examples in which coercive safety regulation doesn't actually make us safer. So we talked about nuclear, but let's compare it to coal. it to coal. So we have, nuclear is highly regulated, highly safe. And meanwhile, we have coal plants still operating and contributing to thousands and thousands of deaths. I estimate in the US, we have about 16,000 deaths a year from coal emissions, which is like five 9-11s every year. And everyone's apparently okay with that, but not with lowering the bar for nuclear plants. You know, experimental drugs certainly can be unsafe, but a lot of times drugs are substitutes for other medical procedures. And if you have to have a surgery or a hospital stay, those are risky also. And we don't balance the risks in those kinds of... We don't make that comparison in our regulatory system. One thing that strikes me as really interesting is in Europe, you can buy a contact lens from a vending machine. But in the U.S., you have to go, it's not safe to do that. So you have to go to an optometrist and get a prescription first before you're allowed to do that. So it's thinly veiled protectionism. There's research that shows, by an economist named Parker Rogers, that shows that deregulated and down-regulated medical devices are actually safer than the more regulated device types. He looks at cases where device types are, where the regulation changes on the device types, and he finds out that the less regulated types increase entry into the market, they increase competition, and ultimately they turn out to be both safer and cheaper. And I already mentioned, the over-regulation of manufactured aircraft means that people select for experimental aircraft, a lot of times amateur, home-built aircraft. So the background here, among all this carnival of stagnant negative effects from safety regulation, is what's called the Great Stagnation. This is a period in the U.S. and much of the West known as the Great Stagnation. And if we hadn't experienced it beginning around 1973, the U.S. would be about twice as rich today as it is. So it's a major economic event. And a lot of my work focuses on ending the Great Stagnation by enabling innovation in sort of these four sectors, health, housing, energy, transportation, which together make up about half of the economy in the U.S. And I think of stagnation as the result of sort of three separate pathologies, but like one of them is the way that we do safety regulation in the West. I think the other is sort of more generalized vitocracy and then protectionism. But today we're talking about safety. And so, yeah, so safety, the way we, this sort of top-down model of safety is a major cause, I think. So it's worth thinking about an alternative to that, and that alternative, I believe, is to build. I think that the responsibility is on us to show that there is another way if we don't have a good answer for how to deal with real or perceived risks in a decentralized and non-coercive way people will continue to agitate for the coercive way right and? And, you know, I'm showing these modalities, again, for the different ways that we can sort of privately or non-coercively or non-top-down provide public goods, and we can think about how we can apply each of these to different aspects of the public goods problem, right? So, you know, maybe there are safety resources that can be created, that can be done just via firms that can internalize things. Maybe there's a risk that AI will be able to use sophisticated phishing attacks that need to be addressed. And we can do that through Better protocols new standards authentication authenticated communication standards Maybe air filtration systems as Vitalik mentioned can protect against Biological attacks, but also provide ancillary benefits in the form of reduced illness so that they don't require like mandates or, you know, top-down regulation. So I think another aspect of the solution, of the build solution, is to focus on concrete and near-term problems. I think a lot of attention is focused on the most fanciful scenarios, but a lot of the near-term solutions also have a bearing on those risks. And so working on things that are concrete and yield tangible benefits now can, you know, if those other more extreme scenarios ever materialize, we can be more prepared for them than we otherwise would be by focusing on stuff that's more near-term, concrete, tangible. And finally, I think a lot of times people think about, you know, what can go wrong with new technologies, AI and biotech in particular, right? And I would think that this needs to be balanced by what can go right. And I don't mean that in a trite way that shuts down thinking about risks, but as a counterweight to the panic that sometimes people feel and as a motivation to build the future and to build an exciting future that we all want.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:10:00.000Z",
      "slot_end": "2024-11-14T03:30:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ark5gHHkzTiHgbw7rdgfM5t6pIra-jjXvX-Qq1FPlRk",
      "resources_slides": "https://drive.google.com/file/d/1txb1RD4YC1AqdjMaBEM3TEpGSqu3BFac/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "aw-rpg-mud-day-demo",
      "sourceId": "JGPNWL",
      "title": "AW RPG - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\n\r\nWe make onchain RPG mechanisms to be used by traditional game dev. When separated, each mechanism stands alone as its own game. When combined, AW!\r\n\r\nSo, five games, five sets of RPG mechanisms:\r\n- Game 1: Base-Defense\r\n- Game 2: Web2 game modded onchain\r\n- Game 3: Turn-based roguelike\r\n- Game 4: Shoot-em-all\r\n- Game 5: Open-World RPG",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Gaming,Tooling",
      "keywords": "n/a",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "1cbd645a323feea8c19756e32111149addcdcc452b7be4f218b3a6273bb1f0c0",
      "sources_youtubeId": "27XdaX2PiHU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:20:00.000Z",
      "slot_end": "2024-11-14T03:25:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1EKpvJeOeAM34stz5C81nwEHi_an7BtgW7b7GpbmcS4w",
      "resources_slides": "https://drive.google.com/file/d/1YuyQJ18PX4fBDVf2Uzu4trmnNDoBki2X/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "play-a-massive-onchain-war-game-mud-day-demo",
      "sourceId": "PG3VAG",
      "title": "Play a massive onchain war game! - MUD Day Demo",
      "description": "Play Battle for Blockchain, an onchain war game with us. Become the commander of armies and storm your enemies. Collaborate with friends to obliterate opponents and win fortune.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Coordination,Gaming",
      "keywords": ",",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "b3fc8cfcda25f5b407661fc4c11e4aeea06f84745c1626e046e3a58c220d1b10",
      "sources_youtubeId": "KXWmTDAetZ4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:25:00.000Z",
      "slot_end": "2024-11-14T03:30:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1UNKZFzRMqNLX4iLJO6NRMaXRhwd2RgXojdoLtHJGj3w",
      "resources_slides": "https://drive.google.com/file/d/1zmnBTcdVjBa5XA1J2HcvN4gzaYVViryh/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "achieving-chain-abstraction-through-transaction-orchestration",
      "sourceId": "7BGJZW",
      "title": "Achieving Chain Abstraction through Transaction Orchestration",
      "description": "Achieving chain abstracted experiences will require the ability to execute multiple transactions across multiple blockchains as a single \"action\", ideally with a single signature used for permissioning. \r\n\r\nIn this talk I'll explore the concepts of transaction orchestration and single-signature, multi-chain permissioning. I will present what are the benefits, how to create such systems and what are the drawbacks (e.g. soft atomicity, stale permissions, etc...)",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Account Abstraction,Intents,transaction,orchestration,Account Abstraction,Developer Infrastructure,Intents",
      "keywords": "Chain Abstraction,Transaction Orchestration",
      "duration": 575,
      "language": "en",
      "sources_swarmHash": "1691042d52c26591b6bf3ef128d97f50483dd912c8ae264796b4f21b97d66188",
      "sources_youtubeId": "84L9qDxioZ8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735719f9dbb7a90e19870fb",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735719f9dbb7a90e19870fb.vtt",
      "transcript_text": " . Good morning everyone. Glad to be here. As you see, I renamed my presentation, but it's the same thing, I promise. So we basically just rebranded some things. I applied here as co-founder at Cluster, but now I work as VP product at Byconomy. So without further ado, we only have five minutes, so let's get started. So one of the things that we basically deal with today is, let's call it, monolithic execution. So let's call it like this. You have your normal blockchain transaction, you encode the transaction, you sign it, and you send it to an RPC node, and the RPC node sends it to a block builder who then, you know, it goes to a miner or a validator or a sequencer, they include it into the blockchain, and that's it. That's monolithic. And some of the features of the monolithic environments are they're unicast, like one RPC node communicates to one blockchain. They're isomorphic in a sense that an EVM node doesn't understand an SVM transaction or like a near transaction or a WASM transaction or whatever. Signature scheme coupled, you know, your Ethereum node really does only use one signature scheme for everything. They're non-composable. So imagine if you are pre-signing two transactions, so one and two, you can't pre-sign that the second transaction takes as its input the output from the first transaction. They're non-collaborative. If you have multiple transactions that you want to execute, first of all, you can't even bundle them in a single object. But even if you could, there is no way for a collaborative execution where, let's say, a solver solves for a fraction of your transaction, then a bundler solves for the next fraction of your transaction, etc. So yeah, these are the problems with monolithic execution. And that's where we go into this modular execution, what we use to call transaction orchestration. Modular execution environment, in essence, is using super transactions. And a super transaction is a single data object which can contain the payment information for how you want to pay for this super transaction, user ops, intents, off-chain actions, ZK proofs. It's modular, it's extensible, and it's also recursive. So what does this enable? So this enables essentially a node to commit to multiple actions to the user, and the user permissions these multiple actions with a single signature. It requires smart contract accounts. It relies on ERC 7579 validator module. It encodes all of this into a Merkle tree. Then you sign the Merkle tree root hash. And after you've signed the Merkle tree root hash, this essentially gives permission to all of the leafs to get executed. What's the cool thing about modular execution environments? Well, like one of the coolest things is that they are modular, obviously, but they are also recursive. What does this mean? So let's say I'm a node which has committed to execute something for you. And I don't know how to execute the entire thing that you have given me. Maybe it has like an intent to move funds from optimism to base. And then it has a requirement for two or three user ops uh to execute a certain you know let's call it like this uh intent uh user op first remove my supply from ave on optimism then an intent uh move this usdc from optimism to base and then a user op supply to aveave on base, which is basically, let's call it a rebalancing and Aave position in a single transaction, super transaction. And this is kind of, let's say this can be done by a single node, but for just for an example, let's say a thought experiment, let's say that the node doesn't know how to trigger an intent. It doesn't know how to trigger an intent. It doesn't know how to front liquidity. What can happen essentially is that one of the leaves of this Merkle tree can be the root of another Merkle tree, and then one node can commit to another. So essentially this is a data structure for a new type of transaction, which is recursive by default, and which has multiple elements of execution by default. recursive by default and which has multiple elements of execution by default. We are trying to essentially push this to become one of the main, if not the main, data model for transactions within a chain abstracted world because it bridges gaps between... So like the leaf can be a Solana transaction, it can be an Ethereum transaction, it can be a Vazen transaction, payment can be like an on-chain payment, it can be an off-chain payment. Off-chain actions, for example, you can trigger a call to some API, then require this to generate a ZKTLS proof that something has been executed. So it's very extensible, very modular. It really effortlessly achieves all the elements of chain abstraction, like gas abstraction, single signature, multi-chain stuff. And yeah, also what we are building is a modular execution environment, which is a permissionless execution layer for all of this. But I wanted to keep this technical and not shielding our own things. So yeah, this is it for super transactions and modular execution environments. Thank you, Mislav. And now we have time for questions. Raise your hand if you have a question. I know you do. Don't be shy. Yes. All right. So I'm going to throw you the box. Get ready to catch it. Yeah. And go. OK. How do you ensure transaction atomicity in environment? Let's say you did a transaction on Optimus and Mainnet, and then something happened to Optimus, and someone claimed that the rollup was faulty and the block reverted, what happens to the transaction which was settled before? So the super transactions and modular execution environments are just technical concepts. They don't really guarantee or not guarantee atomicity. So a example of a modular execution environment can be one which guarantees atomicity. So we are currently working with certain block builders who can build blocks on top of multiple blockchains at the same time. So you can get a pre-confirmation whether or not it's going to get executed everywhere or nowhere. A lot of things don't really require this type of hard atomicity. Like, for example, this case that I said with Aave. The worst case, if a piece of the transaction fails, you'll just retry the transaction on the destination chain. And then, so even like a super transaction can contain also retry instructions. So you can pre-sign all of the retry instructions that are needed. In our experience, like 99% of everything goes through. But yeah, for certain cases, let's call it maybe trading or DeFi positions where maybe you want to do a cross-chain flash loan or something like that, where atomicity is very important. There we are already in active conversations with certain projects which do block building on top of multiple roll-ups at the same time. And then you basically get a pre-commitment to inclusion of all of the transactions within the super transaction hash. Thanks. We already have another question lined up. Yeah, my question actually was the same as about the automaticity. So in general, it's a convenient way to put all transactions together. And do you have some ways to prescribe a sequence of transactions? So let's say this transaction should happen before this one or some mechanisms like this. So in general, they are sequenced by default um so a node will not execute a transaction if it simulates a revert and also there is like this entire trustless protocol with staking and slashing that actually ensures the liveness of all of this but let's call it like if you are bridging to base and like the transaction on base requires 100 USDC, until you have 100 USDC on base, like it will not, like the node itself will not execute that piece of the transaction. The secondary part which you can do is you can set conditions with on-chain variables. So for example, let's call it transaction on optimism triggers a cross-chain message onto base, which updates an on-chain variable, which gives permission to the second transaction in a row. So we are actively working with a few cross-chain messaging protocols to enable this functionality. The entire idea is that you should be able to write, let's call them smart contracts in only front-end code, where it's like you put, you know, if-else conditionals, just call this transaction, if output is bigger than this, then write to this chain, set this transaction. We're also building an SDK called abstract.js that's going to enable all of this. But so essentially, the nodes use reverts for sequencing. So until the transaction reverts, nothing gets executed. And then you can use on-chain variables to control the sequence flow. but let's call it for this Ave example and for a lot of other examples that we have, sequencing happens automatically because like until conditions are met nothing gets executed.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T03:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1doK-azcslBW5RIq_sNRQH7NJp6c4MBm2nfjksEAeGgw",
      "resources_slides": "https://drive.google.com/file/d/1a6bHriAXYwl5hU7BeJcSIx17Nu9W72sL/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "biomes-mud-day-demo",
      "sourceId": "GGANKV",
      "title": "Biomes - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\n\r\nBiomes is growing the virtual world with the largest GDP. As a fully onchain 3D voxel world, every single action in Biomes -- mining, building, crafting, even moving -- is a transaction on the Redstone L2.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,Autonomous World,Autonomous World,Gaming",
      "keywords": "",
      "duration": 330,
      "language": "en",
      "sources_swarmHash": "5ebf0f6f2bdec9c8761b67e000f21af843089ccb91434362289f2ba85d3aca4d",
      "sources_youtubeId": "gp9jYC1tQWQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673586cf9dbb7a90e115c97d",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673586cf9dbb7a90e115c97d.vtt",
      "transcript_text": " Okay, it's working. So yeah, hey everyone, I'm Devani from Biomes, and I'll start again. So we believe a crazy new asset class enabled by digital matter and smart items will unlock autonomous worlds. So if you haven't heard about biomes, biomes feels and looks just like Minecraft with everything on chain for the first time ever. So not just assets but items, players, physics, all the actions, even movement is fully on chain and this is not on a testnet, it is live on Redstone, the L2 Ethereum main net, you can try today. So if I just, you know, I can move around in the world, I can WASD around, I can mine, I can build, I can craft. All the things you'd expect from Minecraft we have working, and it feels just like Minecraft. We almost parody UX. Okay, so then what's different? So the main thing that's different inside of the world is smart items. Smart items are basically items that have regular functionality in the game, like a chest where you can store items, take stuff out. But you can augment them with a smart contract. And now that item will behave based on the smart contract. So as a concrete example, we have a Uniswap-like chest, which is basically a chest with a Uniswap smart contract, which makes it so you can only take items in and out if you transfer tokens. And now you have basically like a third-party shop that behaves based on this smart contract. And the design space for these smart contracts is kind of vast because it's a regular smart contract that any player can write. So already since like offer release a couple weeks ago, we've seen a couple of smart items being created inside of the world. So I'll just go around the world and show you a couple. So the first one I mentioned was that Uniswap like smart item. And so this is one of the main bazaars in the game where people are buying and selling items. And as you can see, when I go up to a chest, I can't just take the items out. I still have to interact via this UI, which lets me pick how many wood I want to buy, which is the Sakura wood, which is this new rare wood, rare religion that's forming inside the community. And you can see it's giving me the current dynamic price for this wood. So now I'm going to go into spectator mode so I can fly around and show you a couple of more interesting smart items. So just beside this like main bazaar, one of the gaming guilds who's been playing actively has created their competitor bazaar right beside us. It's kind of like the shoosie swap of biomes. They have their own pricing and fee mechanism but they're doing a similar thing of buying and selling items using a chest. Now, you might ask, well, why can't I just mine this chest and take all the items? And that's because we have another smart items, which is a force field tower. And when you have a force field tower, you can, again, use a smart contract to control permissions for who can build and mine inside of your area. And the main thing to know is that compared to other decentralized projects where they have land sales for NFTs that control permissions, with biomes, you can't control the land forever. To control your land, you have to use this in-game item, which is the force field tower, and you have to charge it using batteries, which is another in-game item. And so all of these smart items function via batteries, and as long as the batteries charge, it will work, but players can rebel by damaging the battery and making the smart item no longer work. So just two other cool, very interesting smart items players have created. One of them is this sakura holy tree. So there's four kinds of wood in Biome, but one of them, which is the sakura wood, has become kind of memed and everyone sort of is rallying around it. And so this player wants to build this giant monument of the biggest sakura tree. But he needs a lot of sakura logs and they're hard to get. They take over a year to grow. And so he's created this chest where you donated 100 like Sakura logs, which he'll use to create this tree, and in return he mints you this like NFT, we call them passes inside of biomes, and this pass gates access to his, like to use all of the, like go inside of his buildings, all the events he's going to run, and so by donating your wood, you're sort of getting access to all of those things he's going to use in the future. In a similar sense, another gaming guild has created their own hotel where they sort of have beds and rent out for people to sleep inside. And to access it, to become a part of their guild, you have to get to their chest, and you have to, again, transfer a couple of items to get to their chest, and you have to, again, transfer a couple of items to get their pass, and once you have their pass, you can now access all of their things. The last thing I'll show is not a, like a pass, it is a pass, but it's not about just transferring items, but it's actually about parkour, because now we have, like, real-time movement, you can actually, oh, it's, the chunk itself is not loading. But over here, we have basically a place where players can play parkour. And when you get to the top of the parkour, you get to, again, mint a pass by interacting with a chest. And only players who get to the top can interact with it because that's where the actual chest is. And yeah, so that's pretty much it. We think smart items can birth an asset class enabled by and on the virtual world's physics. We have a talk later in the afternoon, so come by to hear more about it. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T03:35:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1DMxoqy08a2IwA26zMJNDXvac90qjFcdm4KPkNLwWrl0",
      "resources_slides": "https://drive.google.com/file/d/1N6IpyuXCx9Niw7GYss4y2VXs1126PLuE/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "decentralizing-the-internets-collaboration-layer",
      "sourceId": "NZMSMG",
      "title": "Decentralizing the Internet's collaboration layer",
      "description": "Over half of the world’s Internet users trust one closed-source, centralized app suite for their daily knowledge creation and collaboration: Google Workspace.\r\n\r\nAs a core part of what people use the Internet for, it should offer similar robustness as the Internet does through sufficient decentralization. The decentralized stack required for such apps is now possible. The talk explore this stack and introduces examples of dapps we built with it incl. this year's Devcon's collaboration stack.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Privacy,Use Cases,mutual,aid,Coordination,Privacy,Use Cases",
      "keywords": "mutual,aid",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "67d13d6f3f5b77d476ea538e34aa83aba2921660fa653b0adc74a46bb181e4b0",
      "sources_youtubeId": "btaNddkfyLg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T03:50:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1XQpLsYFcvAaRsWM6b13TUaTHGrXpSSKJ4fVPEoKkJfw",
      "resources_slides": "https://drive.google.com/file/d/1nUVGIbij3EuHqzjsqCgaqo4TqIh0MOGX/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "energy-renewal-sound-healing",
      "sourceId": "7DEDKP",
      "title": "Energy Renewal (Sound Healing)",
      "description": "By master Ice \r\nThis session helps you rest deeply, reset your energy, and find inner peace.\r\n- Recharge and relax with gentle sounds of gongs and bowls\r\n-  a short guided meditation. \r\n\r\nNov 14 10:30 -11:15",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T04:15:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/1FvG19MBxNr-yTjRDpb3Z4gWrJzfSxAeauH5sxykoiLg",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "finding-rough-consensus-on-issuance",
      "sourceId": "GSKJK8",
      "title": "Finding rough consensus on issuance",
      "description": "lido and ef researchers agree on far more than people think. this talk is an attempt to synthesize and explain my take on the big picture as simply as possible with plenty of humour.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "ethereum,Economics,Ethereum Roadmap,Politics",
      "keywords": "Issuance,Lido,Ethereum",
      "duration": 1540,
      "language": "en",
      "sources_swarmHash": "606060e792644aaedb6856326c95c995d3a5181cdd1ed28148e53af3a8009506",
      "sources_youtubeId": "7TM2YL4ZRNI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736eee41b0f83434d94c564",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1UZfs00-12fFWsIVRmhuoFq4kD-ulyhRRnI5VXPFmdeQ",
      "resources_slides": "https://drive.google.com/file/d/1G_ebeSAhDqQtnNGD04MxXfLQUPfqeRED/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "io",
      "sourceId": "9BQWGB",
      "title": "iO",
      "description": "It will be worth it ;)",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "46d5ef22074d469f164fac15b71964a1b097a2df621f13d511931dc51491d35a",
      "sources_youtubeId": "5hDj0TB8s18",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1RcEikB5_ALOwZaJQaAvBqDR_O7aF9ycww9YUXYxXCFA",
      "resources_slides": "https://drive.google.com/file/d/15ywtmf8B9_utODU2EwKL2EnT1_0AcOqJ/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "neurotech-humanitys-next-frontier",
      "sourceId": "GMSXUV",
      "title": "Neurotech - humanity’s next frontier",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 617,
      "language": "en",
      "sources_swarmHash": "363f6e1847ca2d59e02168e9772ee69e6a04064044d084783d5277e0122d51ec",
      "sources_youtubeId": "aM62xYINTx4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673574789dbb7a90e1a6c27d",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T03:38:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/17GDo2qkBsW9cNEfQVEMckFKyyYZZ0KEwY1Wo37pv0iM",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "pessimists-archive-presents-the-franklin-fallacy-why-we-misjudge-new-technologies",
      "sourceId": "W7MVPA",
      "title": "Pessimists Archive Presents: The Franklin Fallacy, Why We Misjudge New Technologies",
      "description": "People often dismiss emerging technologies by focusing only on their current limitations, overlooking their potential evolution. This tendency, seen throughout history—from the telegraph to Ethereum—stems from what can be called “The Franklin Fallacy.” When asked about the purpose of a hot air balloon, Benjamin Franklin famously responded, \"What good is a newborn baby?\" highlighting how judging a technology in its infancy is shortsighted. This talk explores the psychology of this fallacy.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "e/acc,Marketing",
      "keywords": "Technological,Acceptance",
      "duration": 945,
      "language": "en",
      "sources_swarmHash": "48acde54ba63807192aacaefd33d64c8c49f1880cf8670d34a0f7f3be2d030bd",
      "sources_youtubeId": "CSLqxWBcM-0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673573b19dbb7a90e1a51919",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1BYWK_IatacBdd2r84kKv_IWDoGpsDqXH7RNIaxf7qqQ",
      "resources_slides": "https://drive.google.com/file/d/1UExUiTCTXVQvasENYO3EP-CJp6BnWhgF/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "reading-ethereums-tea-leaves-with-xatu-data",
      "sourceId": "LGXA3Q",
      "title": "Reading Ethereum's Tea Leaves with Xatu data",
      "description": "Demonstrate how we collect data from the Ethereum network and how it's used for upgrades, research, and analytics. We'll then run through some examples of how to use the tools and public datasets yourself.",
      "track": "Core Protocol",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Consensus,Testing,observability,Consensus,Layer 1,Testing",
      "keywords": "Data,Analysis,Observability",
      "duration": 3344,
      "language": "en",
      "sources_swarmHash": "223c4c92e673157087235386107c1c6f5cfca28a376877be1a72c3d4a7f311a9",
      "sources_youtubeId": "MKZ7tFBMrsk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67358b949dbb7a90e19ee9d9",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67358b949dbb7a90e19ee9d9.vtt",
      "transcript_text": " Hey, good morning everyone. Thank you for coming to our workshop today. Before we start, if you have any questions just throw your hand up. It'll be pretty relaxed, so yeah. Just look into the workshop overview. Introduction, that's us right now. We're going to go into Xar2 Genesis, see sort of how it happened, why it happened, and then we'll look into the data sets that have emerged since. We'll then progress into how we're using the data, how others are using the data. Leo over here will then present. He's from the Megalabs team. He'll present the big blocks test that happened last year, and then we'll have a live tutorial from Tony. He'll go through how he uses Paisa to run his analysis. Sweet. So a little intro on who we are. I'm Sam. That's Andrew. He's hiding. We've both been doing DevOps on the PandaOps team for the last few years. And we both have a deep appreciation for things like Ethereum and observability. As for our team, ETH Panda Ops, we started out in 2021, being thrown straight into the deep end with the merge. We're embedded in the F and do DevOps for the protocol. And we post blogs semi-frequently on our website. We try to keep them really high signal. They're the best way to keep up to date with us. Scan the QR code and it'll jump you straight there. Our team has a pretty wide range of projects cooking at all times. You've probably come across a couple of them. For example, if you've ever run a node, you've probably checkpoint synced from an endpoint that was running checkpoint Z. You may have also seen Parry and Barnabas whipping core devs in the core dev calls. Yeah, there's a lot of stuff going on. So let's move on to the workshop. To set the scene, it's late 2022 to the workshop. To set the scene, it's late 2022 and the merger's just happened. We've switched from proof of work to proof of stake, but in doing so, our consensus mechanism has become a lot more sensitive to time. Suddenly the when of things happening has become a lot more important. And this data at a global and network level doesn't really exist. It's easy to check that a block was seen, but it's much harder to see when that block was actually seen in Sydney compared to Berlin, for example. And now that we're clear from the merge, researchers start hacking away. They want to upgrade the beacon chain. But yeah, they need this timing data to validate their ideas. And they start capturing it themselves. Varying scales, different implementations. It's hard to expose, it's hard to validate. There's a bit of potential errors in there. So we started to brainstorm ideas on how to solve our problems. We needed to somehow integrate with existing beacon node implementations as neither of us were really too keen to implement a full beacon node. As DevOps engineers, we'd usually just implement a few Prometheus metrics, put the feet up and call it a day. But millisecond-level precision is pretty important, so that rules out Prometheus metrics. Log aggregation was also another option, but it's definitely a moving target. These things change all the time. It's not really something that the client devs really pay too much attention to. And we'd also just have to turn on debug-level logging. We'd be throwing the kitchen sink at our log aggregation pipeline, and it would potentially be an unreliable result anyway. So that rules out logs. So we started to look at other options. Turns out that the beacon API has this thing called the event stream. You can subscribe to it, and when the beacon node sees things or does things, it will emit an event. So blocks, attestations, voluntary exits, everything. It was all there. The beautiful thing is that the beacon node implementation all supported this endpoint in a standardized fashion. So what we landed on was Zatu. We used Go, gRPC, and we thought that it would be responsible for just collecting Ethereum timing data. It definitely wasn't going to be trying to store or query that data, but the plan was to derive events and ship them somewhere else. To do this, we initially created two modules. Zatu's server would create events from other modules, would collect events from other modules and send them somewhere else, and Zatu's Sentry was our first module. It would run as a sidecar next to every beacon node, modules and send them somewhere else. And Zartu Sentry was our first module. It would run as a sidecar next to every beacon node, subscribe to events, and send them off to Zartu server. We wanted to make sure that all of the events followed the same structure so that it was much easier to add new events into the future. And also, really importantly, since this is like a distributed system, we wanted to make it clear how much you could trust the data. So data coming from a client is not necessarily trusted, but if it's been derived by Xar2's server, something that we control, maybe you can trust it a bit more. This example event is for one of our Xaru sentry nodes running on mainnet, subscribing to a beacon node, and a new block has just come in. I've redacted a couple of the fields, but yeah, that's the general idea. That's all great, but we still hadn't really solved where to send the data. And it turns out it was a lot of data.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ii_t0zNEsYz1aRQml-w9fPgG3GbBAXs49o3KIFZpdCM",
      "resources_slides": "https://drive.google.com/file/d/1eCQVPBN0ksItKaKScucOWbSIZTH7q2qs/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "zoom-in-on-eof-stack-validation",
      "sourceId": "YYGYGF",
      "title": "Zoom in on EOF stack validation",
      "description": "Deep dive into EIP-5450: EOF stack validation spec and explaining some of the rationale behind it.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,eof,Core,Protocol",
      "keywords": "EVM,EOF",
      "duration": 1485,
      "language": "en",
      "sources_swarmHash": "d5182e8c0b90b2ac33f823220b4300a06bc0c0713de0715bda2313ea4d8fe5eb",
      "sources_youtubeId": "80szRrNW0MM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357ab39dbb7a90e1d98e37",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:30:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1d8txUWtGhcQzZvxbPw_N_fi_3997eaZr5RJ2nDVrHkg",
      "resources_slides": "https://drive.google.com/file/d/19U0kZKB4CH4sSd1WW9TZbzRf156Ey42t/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "exploring-mud-worlds-with-blockscout",
      "sourceId": "QTLXWL",
      "title": "Exploring MUD worlds with Blockscout",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\nShowcase of the Blockscout features that help users and developers explore any MUD world on-chain.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Appchains,Interface",
      "keywords": "Block,Explorers",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "841831dffa918db69a90da87a1750660825bfa2fe51a60e3a0df71c3ebe110e1",
      "sources_youtubeId": "rB9lwWtNgtI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:35:00.000Z",
      "slot_end": "2024-11-14T03:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1K-pTNAyptFNuvxYIVpjPCZK8H_NM7O6asR23AlFcIro",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "neurons-to-networks-whole-brain-emulation",
      "sourceId": "ZMP7AG",
      "title": "Neurons to Networks: Whole Brain Emulation",
      "description": "The pursuit of whole brain emulation (WBE) represents one of humanity's most ambitious scientific endeavors, requiring unprecedented coordination between neuroscience, computer science, and institutional frameworks. This talk examines the evolving landscape of WBE research through the lens of institutional support mechanisms, with particular focus on the pioneering role of the Foresight Institute in fostering early discourse around brain emulation technologies (Fellowships, Prizes, Grants)",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "DeSci",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "2b833cbe82a2cdc4ecf891423a6de6042b4b61c5725a4c3152630f81f792fcf4",
      "sources_youtubeId": "Q5o_IDX9H8U",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:38:00.000Z",
      "slot_end": "2024-11-14T03:46:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/10jF6UddyGhtID4JHs8yJOU2RqS0OrJMOgHWiqU5tBx0",
      "resources_slides": "https://drive.google.com/file/d/1IzxXQxixFVzZx9Y1fb_LHA7JiMFwX4Wc/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "cafecosmos-mud-day-demo",
      "sourceId": "FJLAMZ",
      "title": "CafeCosmos - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\n\r\nCafeCosmos is a fully onchain cafe building tycoon. Players are able to earn through redistributive DeFi contracts by speculating on the best earning recipes. \r\n\r\nBuild, Farm, Cook, Earn.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Gaming,Protocol Design",
      "keywords": "Token Engineering,GameFi",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "e4419e8fd54099cbd7c22435a9391dc9f0acb4ac88e3cd4199213dbda44bfb0d",
      "sources_youtubeId": "j-Na9bPZEhI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:40:00.000Z",
      "slot_end": "2024-11-14T03:45:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1PDrU1-lLeAQqLQ4AlBYvxRmDQ_iMUPXlUDleh2GhdJo",
      "resources_slides": "https://drive.google.com/file/d/17m1CF7Q4z3X-oZAl4nZxIk2BMpvz8Gnc/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "chain-abstraction-is-risk-abstraction",
      "sourceId": "E3XUE3",
      "title": "Chain abstraction is risk abstraction",
      "description": "We'll explore the concept of chain abstraction, examine various approaches to it, delve into the associated risks for users, and define what abstraction should really be.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,Token bridging,Intents,chain,abstraction,Cross-L2,Intents,Token bridging",
      "keywords": "chain,abstraction",
      "duration": 533,
      "language": "en",
      "sources_swarmHash": "829115eeae40f6ca7edfae22d074db33d0635faff8f15655e7bf3cb78abd66b4",
      "sources_youtubeId": "yF2OGIh37Cw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673575189dbb7a90e1a97058",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:40:00.000Z",
      "slot_end": "2024-11-14T03:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1U-dbNKwiKAFUbasDggGI5sY4MPQrY0WG2flAU08jtEo",
      "resources_slides": "https://drive.google.com/file/d/12hKgy5lNVVdNQLwSglO5iK5PZYhOj6-b/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "neurotechnology-opportunities-and-challenges",
      "sourceId": "EJZNQX",
      "title": "Neurotechnology: Opportunities and Challenges",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 709,
      "language": "en",
      "sources_swarmHash": "852eaee2e37a13f88b9cbb6db73a2435d4ae6ac9829466f8639a54b6942172fc",
      "sources_youtubeId": "Q5o_IDX9H8U",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673575df9dbb7a90e1ac4199",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673575df9dbb7a90e1ac4199.vtt",
      "transcript_text": " Because you've had three talks so far. Okay, cool. Right, so kia ora, my name is Niamh, I am from Foresight Institute, I'm Chief of Innovation there, and I care a lot about whole brain emulation. So we are a non-profit in San Francisco, we're a 501c3 and we were co-founded by Eric Drexler who's the godfather of nanotechnology and Christine Pedersen who coined the term open source. We're 40 years old, we're currently an all-woman team and we're looking to accelerate whole brain emulation. So we have five key focus areas, nanotech, neurotech, biotech, space tech, and computation with an overarching track called existential hope, in which we hope to build out flourishing futures. As chief of innovation and strategy, I oversee the fellowships, the prizes, and the grants, and I also build out future programs, two of which we will be discussing here today. One is a lo-fi whole brain emulation mouse brain prize, which we'll be launching next year, as well as our whole brain emulation fast grants and grand prize, because we are looking to accelerate this field dramatically. So what is whole brain emulation? You've heard about it from Juan, but essentially it's the uploading of the brain onto the platform. Why do we want to do this? Because with the, you know, short timelines, we think that this is a really great way that we might be able to coordinate and cooperate rather than just, yeah, evolving essentially with AIs, which is what we want to do. We also know that neurotech is super cool and that it's going to help us with modern day science and medicine, cognitive enhancement, space tech, who doesn't want to jump in a spaceship right now and travel two million light years away? I do. I think it's kind of sad that I'm limited to my own physical being right now. And I think also that whole brain emulation is possibly going to help us with better understanding consciousness. Obviously, just like whole brain emulation is possibly going to help us with better understanding consciousness. Obviously, just like whole brain emulation has never been done before, the human brain has never been mapped before, consciousness itself has also never really been defined properly. So we've got a lot to cover, and this is why it really is the new frontier. So I joined Foresight two years ago. One of the first projects that I really built out was our first whole brain emulation workshop. This here was a pretty big deal because it was the very first in the world. And what we did is we brought together the original authors of the roadmap and asked them alongside AI alignment researchers and neuroscientists, like, why is this stagnated? The report was written 20 years ago. What's happened? Is this stagnated? The report was written 20 years ago, what's happened? Is this still relevant? And what bottlenecks do we need to overcome? The biggest thing for them was money. And it was also because money is really hard to get when there's a lot of stigma involved with whole brain emulation. It's kind of seen as a little bit weird, a little bit icky, even though as we've just listed, there's a lot of benefits to actually exploring this. And so that's what I can do and that's how I can help them. As you can see here, it's blown up at Foresight. Our community is like three times bigger than it has been and just keeps growing. We actually had to reject a bunch of people which is pretty sad to do from a workshop. We are already giving out with our AI safety grants money to help people that are working in our neurotech for AI, so safer AI with neurotech. These are some of our grantees. I don't know if you haven't read yet, but Mark posted on LessWrong. It's very, very good, so please take the time to read it. And this is Catalin's first whole brain emulation wet lab for mouse brains. It's a pretty big deal because it's the very first in Europe, and we help fund that with that. So we are doing all that we can to progress things, and we're building out as we go. Why is our community worth somewhat supporting? Because they're pretty cool, and they're pretty good at what they do, and they're all radical moonshots. shots. So we've now had two Nobel Prize winners come out of our Feynman Prizes. As you can see, we've got Sir Fraser Stott out here. He submitted an essay, the exact same essay nine years later went on to win a Nobel Prize. And then this year, we also had Nick Baker win a Nobel Prize with the same work that he submitted 20 years earlier. So a little bit more of a long game there, but we're with him all the way. And as you might see, Jean Herbet, who's a pretty big deal in longevity, the Amaranth Foundation named his work One of the Cornerstones, and it's about essentially replacement therapies for ageing brain tissue. So we're all on the cusp there. Great fellows, including some of our core community. I see Michael Andregs in the audience. He's working on hi-fi approaches to mouse brains at the moment. He's very good at hardware, so, you know, chat to him if you're interested. Please do. I've got some incredible fellows. This guy here, Akash, is working in India using wearables to better understand scent, which is pretty unmapped at the moment. But also these BCIs are directly capturing EEG data and training AI so that we have better diagnostic tooling for cancer, which is pretty cool. Samner Norman is working on ultrasound for BCIs with his technology and his FRO. Not only has he mapped the human brain as best as it has been yet with ultrasound, he's also managed to turn ultrasound, which is normally like the size of a fridge, into a teeny tiny chip, which is this size. And two weeks ago, I stood in my living room with this chip and found my own heart valve beating on my own TV screen with it, which is pretty huge. I mean, this is kind of what happens when you start exploring things, because you start seeing how innovation can just like fly, and that's pretty great for humanity. So there are a lot of enabling technologies already pushing toward whole brain emulation. If you haven't seen yet, we've just mapped the fruit fly brain for the first time. That's 138,000 neurons, which is a pretty big deal. You should watch the video on science if you haven't already. So that's why we think it's possible. We are launching next year a mouse brain lo-fi emulation prize. That will be a million dollars. A, that incentivises researchers to start working in this field. It also just helps progress things all together. We think the timeline for this to be one is two to three years, but actually, simply through positive competition, this might be one sooner than that, which would be even greater because when we're looking at shortening timelines, we need to do all that we can to find progress, and so that's what's happening there. We are hoping to launch a whole-brain emulation fast grants and grand prize. So similar to Metaculous grants and the COVID fast grants, we know that we can stimulate an industry this way. We can help people with their pre-R&D proofs so that they can then go to the VCs with the proof and actually get the money to build it out. But we need a little bit of help first. And so that's where we're hoping to lean in and be able to actually create space for this. And obviously, like, $20 million is, like, the minimum that we're working with right now. If we can grow that further, then we can obviously, like, help more people and accelerate this even further. So that's, like, a big deal for us. Also, a $20 million prize for whole brain emulation of a human brain is definitely going to get people up in Adam. That's what we think. So come chat to me because I know that we're pretty short on time. Sorry if I went a little bit long. If you'd like to know more about what we're doing and also want to get involved with whole brain emulation. There are a lot of risks but we do want to do it the right way and we think we have the community and the means to be that. Yeah, thank you. Thank you, Neve. Again, if you'd like to go outside and have anyone ask questions, please do. Next up, we have our first remote speaker of the day, Milan Czikovic, he's building a startup in Neurotech right now, so can't be with us today, but I would love to bring Milind onto the screen. We'll be ready to go. Thanks so much, Janine. Let's see, can I be heard? I heard a yes. Okay, excellent. Awesome. Well, I'm very sad I couldn't be there in person. The pictures have been absolutely amazing that I've been seeing, but it's an absolute pleasure to be here virtually. My name, as Janine said, is Milan Svitkovich. I'm the co-founder of Integral, a startup based in San Francisco, building deep brain implants to treat severe neurological and psychiatric disease. But I'm not here today on behalf of my company, but rather as an advocate for the entire space of neurotechnology development. I'm often asked to talk about why I think neurotechnology is so important, but you all are very lucky that you have already heard Juan and have Patrick later in this session who are making that case much more compellingly than I'm sure that I would. So I will just reiterate that I think neurotechnology is indeed a critical technology to ensure humanity's successful transition to an AI world. But moreover, even in a world where I didn't exist, I would contend that neurotechnology is definitionally the most transformative sector of technological progress for enhancing the human experience and improving human well-being in as much as the definitions of experience and well-being are that they are things produced by brains. Instead, though, I was asked to talk about opportunities and challenges. So let's get into that, and we'll see if my AV works. Did that slide change? I think that's a yes. All right. So those who aren't familiar, let me just give a far too brief and absolutely non-exhaustive context for the state of the field of neurotechnology as of 2024. So a neurotechnology is any tool that directly exogenously, so not thinking, but somehow externally observes or manipulates the state of biological nervous systems, and in particular the human brain is often the one we care about, although animal communication is obviously an exciting prospect. The definition that I just gave there encompasses a huge range of technologies and also goals that we might want to achieve with them. With respect to the many goals, Juan already went over some of this. And to be clear, I'm not intending anyone to be able to read the list on the left. I made it comically small to make a point, which is one of the actually kind of biggest opportunities and or challenges in the space, which is unlike a field like longevity or climate technology, where at least at a high level, I think there's some clarity and agreement on what the goal of the field is. Neurotechnology is a little bit more like the early days of the internet or speaking outside my expertise compared to those listening for sure. But what I imagined in the early days of the Ethereum ecosystem were like, which is everyone was really excited about building this platform, this fundamental technology, but it wasn't clear at the outset what all the killer use cases would be or even what would be possible or even what will be desirable. And neurotechnology is a little bit like that. It's something of a marketing problem for the field. We can talk about a lot of exciting use cases, but it doesn't pull the imagination to any one vision. So that's both an opportunity and a challenge, depending on how you look at things. On the right hand side, there are a number of existing and emerging technologies that we can use for observing and manipulating the brain. I'll just highlight some important ones and really not go into too much detail here at all. There's not time.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:46:00.000Z",
      "slot_end": "2024-11-14T03:58:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1SnBZ54kfiM59nvSu08JQKHZSM7N-GjLiVE_3-95veIw",
      "resources_slides": "https://drive.google.com/file/d/1zrzBFrutxefs6lbXLtltvt8vEzrjlv3N/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "chain-abstraction-but-i-want-to-know-where-my-tokens-are",
      "sourceId": "SDMYAY",
      "title": "Chain abstraction? But I want to know where my tokens are",
      "description": "As a space, we face a big problem: how should we think about where assets live. Is Eth different from oEth or aEth. Does it matter where Circle prints your USDC? Should chains delegated to the kind of infra that Google Cloud or AWS is today? Clearly, the fragmentations of USDC/USDT/xDAI and then all the L2s creates horrible UX. However, the underlying assets are different things and the chains they live on have completely different security guarantees. Let's fix this!",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,UI/UX,Accessibility,chain,abstraction,Accessibility,Cross-L2,UI/UX",
      "keywords": "UX,Chain abstraction",
      "duration": 568,
      "language": "en",
      "sources_swarmHash": "7cdf9445d78bbafc780fd73c8756dc53b50e6ba0f32b3cebe0f96a2fb5c337e7",
      "sources_youtubeId": "J_VTceUdfgs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673576739dbb7a90e1ae4a04",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673576739dbb7a90e1ae4a04.vtt",
      "transcript_text": " Hello. Hi. This is not going to be nice. I'm here to tell you that it will be your fault if things go wrong, and I will explain that in greater detail. I think it's such a nice segue to have Radina's talk earlier because exactly what I want to talk about is the responsibility you have when you abstract chains and when you curate the infrastructure that you have. So about me, I'm the peanut brain at Peanut Protocol and we've been doing chain abstraction when we still called it chain agnostic payments. So we do chain abstract payments. You can do a payment, let's say, in 10 USDT on Optimism, and someone can receive 10 USDT on Polygon. We don't use Intents. We don't use fancy solvers. We use simple bridges to make that happen. We've been around for a while now. So yeah, for generalized chain abstraction, I think it's important to ask, like, why should I even care where my tokens live? I think I personally care because the risks are very different to something like a traditional centralized financial institution. When you think about a bank, you have legal guarantees and you have bailouts that secure your funds and you don't need to worry about the exact capital, the underlying capital of the bank. You can be pretty sure that you are able to withdraw your funds in any mature economy. In crypto, well, we don't have legal documents to back us up. We have smart contracts. So I want to talk through two dilemmas. One is simple UX and very stripped-down UX and exposing all of the risks. So L2 beat, when bridge beat? I think it's, yeah, a slide that I stole from Radina that I briefly want to talk about. Look at all of the risks that are involved in such a simple thing as just having USDC on orderly. Well, getting USDC on orderly. You not only have all the circle risk, then you get like the mantle risk, get the canonical risk layer zero and so on. That's crazy. So let's make the pizza green. And let's talk through some ways we can communicate these risks to end users. So we have this first generation of chain-first UIs, which emphasize the chain, and then the assets. So that's like MetaMask, where you have to pick your chain, and then it only shows the assets on that specific chain. Then we have asset-first UIs. So Rabi, I think, is a really nice example, where it shows you the assets. It will even show you the network as separate entities, but it will show the asset first. It will show the network as a small thing in the back. Well, how should we do it as product people? What should a product person do? Should they just like slap thousands of warnings everywhere? Surely that's not a good way to onboard users. It gets even worse when we abstract everything and we just have like a final balance and we just assume that everything's going to route it everywhere, right? Yet we have to expose these things somehow, these things somehow, especially that with chain abstraction, essentially what happens, I would argue, is that the FAT protocol thesis kind of stops existing, and the middleware thesis plus the app thesis become much, much broader, which means that you are much, much more responsible for curating the infrastructure in the middle where you're using and what you're exposing to the end user. So there is this phenomenon, I think, that we'll see with mass onboarding and very simple apps where we'll have a race to the bottom in terms of not showing these risks, right? Like as a business, especially as a startup, I want to move fast and break things. And yeah, it is a disadvantage. It is objectively a business disadvantage to be like, warning, this thing is at risk. Warning, that thing is at risk. Which means that it's even more important to internally, as a product team, to vet the infrastructure that you're using and consult tools and people like L2Beat and maybe BridgeBeat in the future. One thing that I wanted to emphasize is that the market doesn't seem to price these risks strongly. Like one ETH on some random L2 is going to be exactly the same dollar value of ETH on mainnet. So it seems that the market just doesn't price these. Maybe because we haven't seen a catastrophic risk yet, a catastrophic failure yet. So yeah, if you ever want to chat about interoperability for payments, DM me, I will send you money. These are my details. Thank you, Conrad. And you can already talk to Conrad right now, if you ask a question. Incredible, right? You can ask your question by raising your hand and I'll get the box to you. Yes. Oh, that's far. Maybe it's easier if you do it from there. No, no, it's okay. You can stay. I like standing up anyway. So I understand about the counterparty risks of the bridge introducing different risks to the bridge asset. But in the far future, if we have soft smart contract security and all that, with like ZK or whatever people are using, do you foresee it being at risk parity across all the chain and therefore chain abstraction is viable? I think when we say chain abstraction, it's still some basket of chains that you as a product creator curate, right? So I think maybe there will be a world where all this tech will go down to zero. It's going to be very, very simple to have really, really, really good tech there where we don't need to do all these audits essentially. But yeah, it's up to you to curate. I think that's my main argument, yeah. All right, we have another question. But in the far future, yeah, sorry. For someone that's new into the space, me, here. For someone that's new into the space and building, doesn't have a plethora of knowledge of existing third-party risks that exist with bridges, how do you actually assess and price in that risk when building? Where's the best place to do that? Yeah, really, really good question. So EltaVita is a great place. I think my main message would be, again, it's you as the product owner or even like co-founders who have to make that decision about curation if you want to hide the risks. Like you're faced with this dilemma of like either you expose the risk and you are fair to the user and say, well, it's going to be your own fault. You're responsible, which creates worse UI. Or you curate it for them and then don't get surprised if people will spam your support email if they lose their funds and they will hunt you down. Cool. We have time for another question. Anyone wants to ask one? Maybe you do. Yes, you do. Fantastic. Get ready to catch and go. Good catch. Do you think this is going to create a barrier of entry for new L2s to enter because there's a risk in a new player? I think it's the opposite. So I think as with any research project, there's a first mover disadvantage because you're pouring in millions and millions into the biggest brains of the industry to design a new infrastructure and then anyone else can just fork that or build a very similar copy um yeah whilst inheriting all of the niceties of the first mover but you wouldn't apply for if they are using a new technology for example yeah sure if you're building something new then you need to convince everyone that it's good tech, and you need to really pitch yourself. I think that's also a message to all infra projects, like pitch to apps that you are the best option, the secure option for your users, because app developers have this responsibility towards their end users of curation.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:50:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1iiL0JiNnH0ChCkoh1IzT9f8BrqYd5kQugPQmJbbcZXo",
      "resources_slides": "https://drive.google.com/file/d/1_86XnoNJvrgp0_4z1JXExVEwqPfRfsri/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "porting-dark-forest-to-mud-mud-day-demo",
      "sourceId": "VBS9CJ",
      "title": "Porting Dark Forest to MUD - MUD Day Demo",
      "description": "We recently ported Dark Forest to the MUD engine and would like to share some of the insights we gained during this process with everyone.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "e3dd16f3151091dff7332eccf386599232b3603bebdd91e86bdccd3321a106e9",
      "sources_youtubeId": "pGz0lv8y74s",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:50:00.000Z",
      "slot_end": "2024-11-14T03:55:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/14aQQNVk55JWYMHYKeZITv12OkJVvgS-kWDNWXp6cpX4",
      "resources_slides": "https://drive.google.com/file/d/1XUdsAECewuDi5VDfG59fzYoPRWkPzIzz/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "trust-minimized-p2p-marketplaces-on-ethereum",
      "sourceId": "YPNBE8",
      "title": "Trust-minimized P2P marketplaces on Ethereum",
      "description": "Blockchains have enabled trustless and fast transaction settlement (i.e. stablecoins, DeFi). However, these existing use cases exist in parallel and are siloed off from the real world. With the maturation of ZK, MPC and other programmable crypto techniques, we are now able to connect data on the internet to blockchains in a trust minimized way for use in smart contracts. This talk will explore the massive design space unlocked for apps (i.e. trust minimized P2P marketplaces)",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Signatures,P2P finance,p2p,marketplace,P2P finance,Signatures,ZKP",
      "keywords": "TLSNotary,ZKEmail,P2P marketplaces",
      "duration": 422,
      "language": "en",
      "sources_swarmHash": "0e81f864032a7b77b620d0096f1bbc41df4651e639dd5006d81e82e88e1e33bc",
      "sources_youtubeId": "GvRhTfLx9w0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735781c9dbb7a90e1b5649c",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735781c9dbb7a90e1b5649c.vtt",
      "transcript_text": " Awesome. Hey, guys. I'm Richard. Today I'll be talking about trust-minimized P2P marketplaces on Ethereum. Go over the why, how, and some thoughts of looking ahead. So why are we here? Um, if we look at web 2 today, uh, data currently exists in these centralized data silos, whether it's fiat currency in your bank account, uh, your social graph, concert tickets, domains, et cetera. Um, these lead to high switching costs for users. For example, you can't bring your followers from Facebook to Twitter. And these also are very extractive to these users. For example, Ticketmaster charges, like, monopolistic prices because they own both the secondary and primary exchanges. And lastly, there's, like, closed APIs, which make it very difficult to interoperate between payment APIs, for example. Additionally, if you look at Web3 in our permissionless database, the problem is that it's really detached from the real world. If you look at your social graph, stable coins, like these DeFi instruments, and even ETH, all of it runs in a parallel system. In fact, most of DeFi today is self-referential. And so the solution here is to bring the real world to Ethereum. And the question is how? We can break through these kind of wall gardens using cryptography and also maybe TEs. So we can use ZK and MPC, these new technologies, to allow us to permissionlessly export data from these centralized data silos. Techniques such as ZK email and TLS notary, which also can help us redact any kind of private sensitive information prior to exporting. Check out some of the other talks on ZK email or TLS notary today if you want to learn more deeply. But yeah, now with these tools, we can kind of compose any kind of Web2 action with Web3 action, such as proving a fiat transfer using zk email to swap for some stable coins on chain or proving ticket transfer using TLS notary to swap for stable coins or swap domains for ETH, et cetera, or even put a social graph inside a TEE to do some DeFi stuff with. An example of this flow that is in production today is ZKP2P, the project I work on. The problem that it fixes some of the issues I mentioned earlier, such as the high fees. The construction for this is pretty simple, actually. A seller comes in, escrows some funds in a smart contract and provides a payment ID. A seller comes in, escrows some funds in a smart contract and provides a payment ID. A buyer comes in, pays the seller, and uses that proof to unlock the escrow contract. It kind of replaces Binance or any kind of centralized exchange with a smart contract, and replaces the manual process of unlocking the escrow with instant unlock upon satisfying a predicate. This construction extends beyond fiat. Simply replace the off-chain payment with a digital asset transfer and a proof of payment with a proof of asset transfer. Now you can kind of do P2P trading tickets, trading domains, trade social media handles, CSGO skins, gift cards and much more. You can even further extend this construction if you, if you trust TEs as a credit poll third party. You can kind of encumber a Web2 account, put it inside a TEE and act that, act as an escrow for the Web2 asset. And both seller and buyer send requests through the TEE. Uh, once the TEE owns both funds and the Web2 asset say tickets, uh, then they can proceed to unlock funds to both parties. Uh, yeah, now you kind of can see the Web2 account is fully programmable. You can do some weird stuff like make the TEE run an AMM pricing model for exam, for example. Yeah, in conclusion, design space is huge. To do trust-minimized marketplaces, the tech here is ready. Dev tooling will continue to only get better. And instead of only bringing Ethereum to the real world, I think it's time to bring some of the real world onto Ethereum. Uh, thank you, and that's it. Thank you Richard. Do we have any questions? Shy crowd today, come on guys. I don't even know if you have the ability to go back in slides. Sorry, is this better? Yeah, if you can't go back, that's okay. I was just gonna ask something about it, but it doesn't matter. I don't think we can. Maybe you can just going to ask something about it, but it doesn't matter. I don't think we can. Maybe you can just talk to it. Okay. Any particular use cases you're most excited about, like in terms of future of peer-to-peer marketplaces? I think there's just a lot of ideas out there. You can put almost any Internet data on-chain and use it. Trade any kind of Web2 asset or even data and anything that you think has value on an internet, you can kind of bring into on-chain to do cool stuff with. Yeah, love to see it. I mean, just like you said, Ticketmaster, I think a prime example of just like marketplaces that are absolutely using monopolistic pricing to just gouge consumers in what is becoming, yeah, a very much frictionless process to do straight Peter Peer. I guess a question, have you seen any regulatory issues? How are you thinking about navigating that? Yeah, I think there haven't been so far. I think P2P is a legally, I guess, gray area kind of thing. You are allowed to transact with your peer. For example, Facebook Marketplace is a huge P2P marketplace. And we're just basically enabling the coordination of that to be easier through on-chain means on Ethereum. Totally. All right. Thank you, Richard. Appreciate it. Give it up.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:50:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1_yxVcYnivrcVQGtbD7FmPQLfgJn75M9f-qQDTJJuPH8",
      "resources_slides": "https://drive.google.com/file/d/1jDL17wya4cBYIvGzW7VnOgyzS4FVjiaw/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "eve-frontier-mud-day-demo",
      "sourceId": "RMKJTL",
      "title": "EVE Frontier - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\n\r\nEVE Frontier, is a single-shard survival game from CCP Games—the creators of the legendary space-based MMO EVE Online.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": true,
      "tags": "Gaming,Autonomous World,Autonomous World,Gaming",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:55:00.000Z",
      "slot_end": "2024-11-14T04:00:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1uN2SOUzGZIHw0d3Pw3RkvvmxeEi6RqnN2J0-JbWUMHI",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "neuroai-for-ai-safety",
      "sourceId": "ANUNJW",
      "title": "NeuroAI for AI safety",
      "description": "Powerful unaligned AIs pose risks to humans. This talk will explore how neuroscience-inspired AI–or NeuroAI–can lead to a deeper understanding of the human brain, and help us build more secure AI. I’ll connect these ideas to d/acc, arguing that neuroAI can play an enabling role in creating technologies that are inherently defense-favoring and promote human well-being.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "d/acc",
      "duration": 476,
      "language": "en",
      "sources_swarmHash": "0f032086c3904fa7333685c2ecd4f5fe0ff0a750e1ffc2fea31c8898de4951fb",
      "sources_youtubeId": "5Wm5rR_L32g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673578c69dbb7a90e1b6afc1",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T03:58:00.000Z",
      "slot_end": "2024-11-14T04:06:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1c6dtMFBwrLngeeenxxoRO7mPchxToNPT-x38ox27h0o",
      "resources_slides": "https://drive.google.com/file/d/1QJGFrKubZn_dDlpbpLCHtzpxjZ1CbkVn/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "evmmax-fast-modular-arithmetic-in-evm",
      "sourceId": "7CWEHH",
      "title": "EVMMAX. Fast Modular Arithmetic in EVM",
      "description": "On the top of EVM Object Format we build an extension which allows contract developers to implement optimized advanced cryptography functions. This feature allows us to implement existing and future ECC precompiles counterparts directly in EVM. Adding new ECC functions (i.e. bls precompiles or  functions based on a new, unknown yet, elliptic curve) to the protocol won't require introducing new precompiles. It can be achieved easier and without any risk for the consensus.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,EVM,Cryptography",
      "keywords": "EOF,EVM",
      "duration": 1500,
      "language": "en",
      "sources_swarmHash": "774eda5d353b7a3c077cb4c10c27d5cc267a272e847e3d76cdd4032de56593fb",
      "sources_youtubeId": "4mGDMCdlzz4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357dc39dbb7a90e1f50a68",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67357dc39dbb7a90e1f50a68.vtt",
      "transcript_text": " I'm Radek from IPSIN Long Research and Development team at Ethereum Foundation and today I want to talk to you about some extension we're gonna add to the to the Ethereum virtual machine in the near future. So the idea has a couple of years already, and it was initiated by some core devs to improve the way how the cryptography is handled or cryptography-related functions are handled in the EVM. The main reason was to avoid adding or limit the need to adding new precompiles. But before we get into the details, I would like to make some simple introduction and explain the reasoning behind EVM Max. So what's the EVM Max? So EVM Max translates to EVMterior Virtual Machine Modular Arithmetic Extension. It's a set of new modular arithmetic instructions which support every sized-capped, which is important, odd modules. At the current spec, we define addition, subtractions, and multiplication. We also consider adding exponentiation, which is helpful for calculation some more advanced modular arithmetic functions like modular inversion or square root. One every important thing which should be noticed here is that this proposal is built on the top of the EVM object format, which we had just a presentation of in a minute. And also there was one presentation made by Dano on Tuesday. So EVM Max makes a usage of EOF immediate arguments and the validation of the immediate bytecode also. And it makes it possible to validate EVM Max code before the deployment of the contract to the chain. But I'm not going to get into details of this validation, because it's not the main topic of this presentation. But it's worth mentioning that EOF is a crucial dependency which makes EVM Max easier to implement in an efficient way. It doesn't mean that EOF is a dependency which has to be before. It just makes the implementation of EVX much easier in an efficient way. before, it just makes the implementation of EVX much easier in an efficient way. So, but where exactly EVM Max is located in the cryptography related EVM stack? So we have basic operations in the modular arithmetic on the bottom level, which are used to implement the second level, the elliptic curve cryptography primitives like point addition, multiplication and more advanced like pairing verification. And these primitives are used to implement ECC algorithms like signature verification and Zika related functions. But EVM-Max implements only the bottom one level on this diagram. In the Epsilon team we also use EVM max to implement the second level to make sure that the set of instructions of EVM max is offered and the API offers right write abstractions, write abstraction and efficiency. So, sorry, too fast. What is going on? Yeah, okay. So we now know what's EVM Maxis in general. So one of the reasons I already mentioned at the beginning of the presentation, but there are a couple more reasons I want to list. So in the Ethereum community, there is a need to make the implementation of the cryptography in EVM much easier and efficient. I'm just kidding. Easier and efficient. So. Yeah. So with EVM, we won't need to wait for a specific precompile to be delivered in a fork. We also would like EVMX to be a tool which allows to avoid adding new precompiles in the future. It will make the core devs' lives much easier because they won't need to maintain the very specialized cryptographic libraries in the EVM so they they will not have a headache like what exactly this function do I'm not the cryptographer exactly or we don't have a specialized cryptographer in the team so why do we need to maintain this complicated libraries, which we take basically very often from some external libraries, which are already implemented. But so EVM Max will deliver a tool which should make precompile for some reason will be still needed to be implemented and to use by the EVM, we can imagine that the EVM max bytecode can define the specification of the precompiles, how they should be implemented exactly. But let's get a little bit deeper into the EVM Max instruction details. So, we can split it into three different parts. So, the first part is responsible for setting up the EVM Max context. The second one is just a set of modular arithmetic instructions I already mentioned. And the third one is responsible for EVM, EVM max context communication. So let's get into details of these three parts. So first, SetupX creates the EVM max context if it doesn't exist yet, which means it initializes modulus values and allocates EVM max value slots in the dedicated for EVM max memory only these slots can be only accessed by the EVM max of code of course and Also initialize some specific constant values like for example R squared which must be which is used for Which is used by Montgomery form. In this context, if the context is already defined, so the setup just only switches to this already defined context. Second part are the arithmetic instructions. So basically, they perform the arithmetic operation according to their names, as you can see so everybody who who know a little bit about the arithmetic modular arithmetic should be able to know what's what they exactly do but it's worth noticing that they operate only on indexes of the slots in the EVM Max context. So the indexes are static and can be validated on the deployment before deployment of the contract to the mainnet, which allows to validate them exactly the same way as all the other",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1fh8W3duOjm-uN-PLpwXQdH39CtC5VtYT9yOjlpTE8hk",
      "resources_slides": "https://drive.google.com/file/d/10qL1fOoK-38pPgEH0jC_QpqqrsCvzhrX/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "for-the-kingdom-mud-day-demo",
      "sourceId": "FM3LCK",
      "title": "For The Kingdom - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\n\r\nFor The Kingdom (https://forthekingdom.xyz/) is a web-based MMORPG featuring a player-driven economy and worldbuilding, empowering players to be anyone they want to be.\r\n\r\nThe game is fully onchain, and currently live on the Redstone Garnet Testnet, using the MUD framework.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Gaming",
      "keywords": "fully,onchain,game",
      "duration": 350,
      "language": "en",
      "sources_swarmHash": "f72a715fadd0d0f0678b493f07a5e8630a9eba13d93187911740ef6769a6d966",
      "sources_youtubeId": "PRVgtqUH6_U",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735897b9dbb7a90e1550f24",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735897b9dbb7a90e1550f24.vtt",
      "transcript_text": " Translator's note Trinh will be demoing For The Kingdom, a browser-based RPG currently on testnet. Hello, okay, thanks guys. So yeah, super great, presenting our game right after E-Frontier, like just super. So okay, hi everyone. Thanks a lot for joining. My name is Tuyen, I'm the founder of Constant Maker Studio, and we're making a fully on-chain game called For the Kingdom. So For the Kingdom is an MMO that aims to become a community-owned world with a player-driven economy and world-building that empowers players to be anyone they want to be. Okay, so can you repeat the video for me? Thanks a lot. Okay, so our vision is to craft an expansive and persistent game world where players can freely experience it and take an active role in shaping the game's values. The core gameplay loop begins by first dropping you into a vast open world where you can explore and fight monsters for valuable materials, for other players to steal their items, or simply lead a peaceful life by chopping away some woods. The true goal of the game is becoming the best at what you do and contribute the most to your kingdom. Imagine you're the best blacksmith in the whole game, and only you can craft the most powerful weapon. It means your kingdom's warriors have an edge over their opponents, or you can even sell your weapons to other kingdoms as well for a profit. And so here I'll show you the first three minutes of our game via a recording, but you can definitely already play the game, even on your smartphone. So the first thing you do is to choose one of four available kingdoms and start building your character. So you can see here that that's basically me creating a character called Defcon for the win. I'll be able to customize the character's story by answering some of the random questions about the character's childhood or their personality. Your choices here determine your character's starting stats, like choosing that you stole a trinket from a rich woman when you were 10 will get you a boost in agility. The first thing you do in the game is claim a welcome package where you'll get all of the beginner items necessary to start to create, you know, doing anything. We have like some text to support onboarding but I'll just, you know, like skip it to get to the fun part. So right after that, the next thing you can do is to basically go into your character profile and equip a weapon so that you can prevent yourself from punching some monsters barehanded. So the next thing you can do is to browse some available quest lines. We have daily quests as well as story lines that you can explore through Kingdom specifics. Next up, you can move from your starting point to different parts of the map. Like here I'm moving to a grassland outside of my starting city. Moving right now kind of cost 15 seconds to compensate that right now we don't have an, like, you know, a zoom in map or an Intel map like DOFUS or ROCFU yet, but we'll get there sometime. You can see during that time that I've also equipped a battle skill. Next, you'll be able to engage in PvE in each tile. Like right now, I'm fighting a slime enemy. As the game is an auto battle and fully on chain, it means that right now the outcome is deterministic and the result of the battle is already decided once you click on the button. But the animation here is kind of like a good representation. Here you can also engage in PvP with different players in this game. For example, I can challenge literally the best player right now. And probably we'll get decimated in one move. Yep, that's because Soulmate or like Dudendi, I think that he's here. Thanks a lot for coming. Like you know, he is stoked with very, the map is right now very huge. And the main goal of this demo, basically, is to defeat Ignis, our first raid boss. It's a powerful dragon that can drop the best loot right now once defeated. So battling monsters, bosses, and, you know, farming resources will enable you to craft various items to get stronger. In the future, these items can be tokenized and sold to other players that cannot craft them. So, that's our demo. I'm just going to skip to... I have like five seconds left. I can't like, you know, can you change the, I mean, yeah, somehow I can't change it to, to the next slide. Like, can you manually change, like, you know, I don't have enough time yet. So if you can like, you know, go to the last slide. It's OK. Just five seconds on this. This is what we've been doing for the next build as well. We'll add more co-op features and basically a card demo game that we kind of stolen from Final Fantasy right now. And yeah, just can you move on to the last slide, please? So yeah, thank you, guys. The game is playable right now. And yeah, just can you move on to the last slide, please? So yeah, thank you, guys. The game is playable right now. Cross your eyes on mobile.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:05:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/19JLbZ-yVksBM4TM3ftOIccuAC6UgKAKtM0nVGAdWdQ4",
      "resources_slides": "https://drive.google.com/file/d/1QzU_GBZ2S1lsvtJWqpHYQkGn5ZJu7blZ/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "hallucinated-servers-another-prog-crypto-chip",
      "sourceId": "DYJ88A",
      "title": "hallucinated servers another prog crypto chip",
      "description": "An introduction to programmable cryptography, culminating in the dream of a \"hallucinated server\".",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": true,
      "tags": "Cryptography,MPC,fhe,Cryptography,MPC",
      "keywords": "Cyprography,fhe,mpc",
      "duration": 1396,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357c399dbb7a90e1e3ce59",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1vVTMx-WFRYRYIkDhxt9cWeLavDtiXTRNFX6sO0Z4Nyo",
      "resources_slides": "https://drive.google.com/file/d/1VX6NZ3kubScPXkijneNWfWVBYyYOA3r0/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "how-much-security-does-your-restaking-protocol-really-need",
      "sourceId": "QDDV9C",
      "title": "How much security does your restaking protocol really need?",
      "description": "Restaking protocols have aggregated millions of ETH with the hope of securing new infrastructure on Ethereum. These services, such as ZK provers and oracles, require restaking ETH to enforce custom slashing rules. But how much ETH do these services need? And how much risk do these services place on Ethereum L1? We will formulate a mathematical model for answering these questions and present an empirical analysis of cascading risks from restaking services to Ethereum, with a positive outlook!",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking,Censorship Resistance,Economics,Restaking,proof-of,Censorship Resistance,Economics,Restaking",
      "keywords": "Matching Markets,Proof of Stake",
      "duration": 1521,
      "language": "en",
      "sources_swarmHash": "5fe34799b6933fd32c8f52dc76f488c7257cafc29d4fe0ab5a8a4564e3294d0d",
      "sources_youtubeId": "ikYZ2dMUTyw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735812c9dbb7a90e14536da",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736ea1d1b0f83434d40f7be.vtt",
      "transcript_text": " Thank you, Miss Purple. Yeah, so the title of the talk is Protocol Guild Funding the Ethereum Commons. My name is Trent Van Epps. I'm a member and one of the organizers of this tool. And just a shout out to the DEF CON organizers. I've presented at a number of events in the Ethereum community and it never gets old. So thank you for putting on an incredible event. And hi to any family or people that are watching on the live stream. So let's start off with some crowd engagement. Raise your hand if you think that there are 25 to 100 Ethereum core contributors. Show of hands. Okay, and by core contributors, these are people who are working on client development, research, people that help with testnets, people that do support for testing and security, specifically related to Ethereum mainnet. That's what I mean by core protocol contributor. All right. So not too many hands. Raise your hand if you think it's 100 to 300, roughly. Okay. Okay. And then raise your hand if you think it's the last option, 300 to 1,000. All right. Most of you were right, so good job. I think roughly, in my estimation, there's probably 100 to 300 people who are involved, mostly in a full-time and probably some in a part-time capacity, working on the software that underpins Ethereum. Another question. Raise your hand if you wanted to buy some of them coffees. How many of them could you name today? So, 1 to 10. How many of you could name 1 to 10, you think? Okay, put your hand down if you thought of Vitalik or Justin. Okay. Alright, how about 10 to 30? You think you could name 10 to 30 core Ethereum contributors? And then finally, 30 to 100. All right, if your hand is still up, come talk to me. I want to meet you after this, if I don't already know you. Why does this matter? This matters because Ethereum is a software commons. And the term I like to use, if you're not familiar with it, is commons. And the definition I use is, this is a system of peer production which manages shared resources. And this is really important for how Ethereum is stewarded over time to think of it in this commons frame. And when we know a system's peer contributors, we can do specific things with it. For one, we can get more certainty around the nature of the shared resources that it produces, and specifically the production process. Secondly, we can fund their work, help to avoid capture from malicious entities coming and engaging with the stewardship process. And finally, we can stabilize this contributor set with particular incentives. And maybe if you are familiar with the commons term, you've heard of it in the context of a fishery or a forest or a pasture a pasture that people share together like natural resources that people figure out how to maintain a standard of the resource and then they all benefit collectively but this can also be applied to digital goods or intangible goods like the internet is a common infrastructure that we all use today we're using it right now for the live stream, and I would be surprised if there's many people who don't use the internet every day, but they don't necessarily think about it as this shared infrastructure. Similarly, for something like Wikipedia or Linux, this is software infrastructure. Software is oftentimes produced within this commons context. And when we think about Ethereum, we might think about different parts of it, but we don't necessarily think about it as this holistic commons kind of mechanism. So the three resource types, from my perspective, I think there are three main ones. The first is the network and derivative forms of it. So the chain state, the chain history, different forks of mainnet. And layer twos would, I think, fit into this resource type of a distributed system. Secondly, the asset in the bottom right side of the diagram. Ether, we all use it for paying for transactions. And then it can be put into different forms, whether wrapped or restaked or bridged to different networks. And then finally, one that maybe you don't think about as being a resource type is media. And this can be the specifications which define bits of the protocol or research about what Ethereum can look like in the future, software generally. But it also includes things like the transcripts of governance calls, all Cordev's calls, or the EIPs which describe future changes. I bucket these all within the term media. Hopefully these three resource types make sense to you as well. And all three of them are interwoven in a very unique way. They can't necessarily exist without the other, and so there's this interlocking where each of them refers to and is informed by the other three. Each of these resources is also produced through protocols, and by protocols here I mean like rule sets or guidelines for how the resource comes to be. So for the network, there's specific rule sets, technical rule sets for the fork choice rule or blobs if layer twos are using EIP4844 or P2P gossip for nodes coming to consensus and passing data to each other. The asset, you know, it's minted through proof-of-stake issuance, and then it's destroyed as part of the 1559 burn. These are rule sets which describe how the asset, this resource, operates. And then finally, the media is produced through open-source norms, permissive licensing, public-by public by default governance, and rough consensus. And these rule sets are really important for defining how the actors engaging in the production of this work really operate. It defines what they can do and what they can't, the constraints which guide their behavior. So this brings us to the question of who and in what context is the software media actually produced? Some of you, I know, had your hand raised until the very end, but sometimes this isn't necessarily something we think about. Sometimes, because Ethereum is so dependable, we don't necessarily think about something which just works by default. But it really is an interesting question to start to consider is who's actually making this stuff? And this is a map. I'm not going to go too deep into it, but this is my perspective of who's actually working on mainnet these days. A range of different project sizes, different project types, there's commercial entities, there's non-profits like the Ethereum Foundation. It's a really broad array. Today, probably around 10 different, 12 different independent institutions, and I would say maybe 30 different projects within that. And maybe you knew some of these, but maybe there's others that you don't. And in the middle, there's like this whole range of other org affiliations. Hopefully, if you're not listed here, you don't get offended. Consider yourself part of the middle there. And this is what comprises the main net stewardship, this set of projects that make up the software that then powers the network that we all use in some way or are aware of, right? So what Protocol Guild does is it takes the individuals that are working within these team contexts, within these projects, and it surfaces them to a different level of legibility. So it makes them visible from a longer distance, let's say. So the Protocol Guild mechanism binds these individuals together in a collective mechanism that then you can do interesting things with. You can fund it, you can set common standards. We'll get into that. So Protocol Guild today is a list of 187 of these main net contributors. And collectively, they have more than 600 years of cumulative contributions, which is, when you put it that way, it's pretty incredible, right? You have people who've been here a long time, people who have just shown up. But together, it's a lot of brainpower that's working on this software. And generally, you can also think of it as just a collective funding mechanism, like I mentioned. There are a couple of things which make Protocol Guild very, very unique in the landscape of the Ethereum space, specifically regarding funding protocols. And I'm going to go through them here. The first is that it has a very narrow mandate. It's only concerned with maintaining the mechanism and driving funding to it. It doesn't engage in protocol governance. It doesn't direct the day-to-day efforts of these individuals. It has a very narrow mandate for what it's supposed to do, and that way we can make sure that we're focused on that mission and we don't have this scope, which expands over time and then the mechanism loses its focus. It also focuses on a very narrow domain, specifically the core protocol set of contributors. It doesn't think of anything much beyond that and tries to approximate where the edge or where the boundary condition of the edge of the main net protocol actually is. Whereas things like Gitcoin or Optimism Retrofunding, they have a much broader domain that they try to put funding to. Protocol Guild specifically focuses on the individuals doing this specific work. The second thing which is unique is it has an open membership of individuals. So to compare it to like a non-profit or a for-profit entity, which, you know, they hire and fire people depending on their budgets, we've committed to having an open membership. So if you meet the eligibility requirements, you show up and, you know, you've been doing this work for six months and you expect to continue doing it, the eligibility is obligated to add you. And this is how we build up legitimacy over time by not necessarily gating the membership, which could be dangerous long term. And we focus on individuals instead of what many mechanisms do for their funding approach, which is focusing on the team or the project itself, we think it's really important to return as much agency to the people themselves that are working on this software and make sure that they have as much flexibility for how they engage in this commons production process. Third, it has a comprehensive quarterly curation. So we're regularly updating the mechanism. The set isn't static. Every quarter, we go around and try to find anyone that's missed that now meets the eligibility, and we add them to the membership if they want to. Ultimately, it's opt-in. We do have core contributors who decide not to be part of this. It's totally fine. So it's an opt-in mechanism. Into this particular domain and not necessarily seeing somebody who's doing valuable work that's maybe lower profile or harder to make legible at a certain scale. The fourth unique aspect for this mechanism, this tool, is that all of the funding goes through a four-year on-chain vesting contract. And this is different. It's an opinionated frame for how funding should be distributed. We use time waiting to allocate funding so that if the earlier you show up and the longer you stick around, the more funding you'll get through the mechanism. And this creates assurances in a number of ways for both the funders, the members, and the community that's observing this mechanism and engaging with it. Funders know for sure that anything that they allocate will be there in the future. It's going to sit in this immutable contract on-chain, and they know for sure that it's going to vest linearly each block i can't run off with it they can't take it back so there's a there's a nice characteristic that we get by having it in this immutable contract on chain um and then for for members they know for sure that there's going to be funding available to them if they stick up and show around and all of these touch in some way, again, the commons frame. Software doesn't spring from the earth like these natural resources, a fishery or a forest. It doesn't come out of the ground naturally. Of course, you can enhance the development. You can make sure it has good conditions. But software requires humans to create it and steward it long term. And these really recognize the commons nature. It's going to take time to do things. Ethereum is where we are today, but there's still a lot to do over the next five to ten years. And the way we've structured this mechanism deeply considers the reality that this stuff takes time and you need people there to steward this common software over time. And so to that point, we can look at all this on-chain stuff through a Dune dashboard. If you're familiar, I recommend go check out our Dune dashboard. We try to surface as much information as we can there. So we've been building Protocol Guild, myself and a number of others and the rest of the membership since 2021. And today we have $57,000 per year that goes to the median member. I know we can project out, you can see in the bottom left, there's a big lump sum of about $40 million that vests over four years. However, markets are volatile. I can't really guarantee that that level of funding will necessarily be available at that US dollar value. So yeah, this is a great accomplishment, but I think we probably need to 10x this, and I'll get into a little bit more around the incentives and why we think this needs to be higher. But definitely go check out the Dune dashboard. It's a great way that we can lean into this radical transparency because all of the funding coming in, all of the funding going out, the memberships change, and number of individuals, it's all tracked in this Dune dashboard. So please do check it out. And now that this thing exists, we've been able to fundraise to it. And we've been really honored to have many large projects and small projects throughout the Ethereum community recognize the value of this common scale mechanism. And I just want to give a shout out to them really quickly. So massive, massive thank you to EtherFi, Tycho, Layer Zero, the Arbitrum community, Lido, Optimism, Uniswap, ENS, ZK-Sync, and MolochDAO for really seeing the value in this kind of thing and not just making a future commitment or saying, you know, this is a good idea, but actually committing significant funding to it. So yeah, thank you. And other things we can do with the mechanism now that it's on chain and leaning into this transparency is we can start to say, okay, as funding is coming in, how can we ensure that the distribution of funding is separated out or we have a good diversity of assets that are funding these individuals doing the work. So we have this diversity score that compares it relative to all past funding. So another cool thing that we can do when the funding lives on chain. And one thing we introduced earlier this year is the 1% pledge. I recommend go and read the full post, tim.mirror.xyz. It's a great post. But one of the things that's really great is this diagram on the right, which visually displays the incentive imbalance that you have to work on the core protocol. So in the top right, you have a crypto project founder, right? They have a lot of risk going into something like this, but there's potential for significant reward. And when you're working on the L1 work, it's stable. You'll get a salary. You might get equity in some company, but you don't have this exposure to the broader ecosystem success or there's not going to be a new token for the Ethereum blockchain. Some people have the misconception that people who are core developers, they've been around since the ICO, and they just have a huge stack of ETH. I'm here to tell you this is not the case. Many people, for whatever reason, they didn't have it. Or if they did, they've sold it since then to pay for living expenses. And there's always new people showing up. So just to set that straight, core devs are relatively undercompensated to the broader industry. And one thing we can do through the mechanism is shift the incentive imbalance. We're not going to ever be able to match what you could get at a, let's say, a VC startup or a layer two that has a new token. Some of these newer projects that you join early, it's clear that you can have significant incentives. You'll never really be able to match that fully, but we believe it's important to at least slide slightly up on the reward curve and give people more consistency while maintaining the same level of risk. Because ultimately, at the end of the day, this work is really undervalued and less visible than a lot of the more high-profile stuff. So another shout-out. There's been a couple of projects which have really leapt into this idea of the 1% pledge. And again, shout-out to Etherfy, Pondau, and Tyco Labs for early on in this year saying, you know, we recognize the importance of this thing. We're going to donate 1% of our tokens to the mechanism, and that's been the bulk of the funding this year. So really thankful that they have taken this early thing and been some of the first people to actually engage with the mechanism in this way. And like I said, we do need significant funding to come into this mechanism to rebalance the incentives. There's risks around the commons being captured long term. This is something you always have to be vigilant about and paying attention to, because as I said, the software doesn't just come from the ground, fully formed. It's a human process. It's a political and social process to maintain and steward this over time. And like I said, there's still so much to do in Ethereum that we need to start putting ourselves in a posture of recognizing that over time, the commons needs to be maintained and stewarded and funded. And while there are risks, there's also significant successes in the natural commons space. Fishing practices, communal rice terraces, different farming systems have lasted for thousands of years. And then in the digital context, the internet, Linux, Wikipedia, they've had early successes, but we can start to see the challenges that they're experiencing as larger pools of capital show up on the edges or the margins of the commons production system and the challenges that they're experiencing now. And so we should really start to think of this. Next year, Ethereum turns 10, which is incredible to think about. There's still so much to do. But what can we do to start to take on this posture of funding the commons at the scale where it's best recognized and suited to fund these individuals who are doing this important work? So I'll end with this. Is it a core dev UBI? Is it a software standards org? Is it a union? Is it a compensation package? Probably to some degree, it has little bits of these woven together. But above all, it's a call to action. It's an invitation for the broader community to take on the responsibility to recognize their role of participants in this ecosystem. You know, the EF is not going to be the sole funder. It never was, but it should not be looked to as, you know, it's just the EF will take care of it or, you know, these large organizations will take care of it. All of you, if you're part of a project or even as individuals, you have an obligation to think about deeply the ways that we steward and we participate in these commons production processes. So my DMs are open if you think this applies to you. Thank you. Thank you. Thank you so much for that. And on your statement, actually, you mentioned that core devs are not paid enough compared to industry. So how much is enough? That's a good question. I think I showed $57,000 per the median member. They do get salaries in addition to this as part of their hosting org, whether non-profit or for-profit. So this is in addition to that. It depends on what part of the market cycle we're in, but we're aiming to 10x this and provide at least 500,000 to the median members. Some people will get a lot more, and some people will get a lot less, but this is kind of what we're aiming for for now. And yeah, keep in mind that any funding that goes into it is distributed over four years. So it's not, you might see this really large number on the Dune dashboard, but keep in mind that a lot of these tokens are very volatile and it'll shift over time. And who decides who's eligible for the Protocol Guild? So the members have an eligibility framework, which we try to make as explicit as possible with the awareness that, you know, things change over time, right? I keep referencing how the commons is stewarded over time. What we think is the edge today may not be the edge of Ethereum core protocol in 10 years, right? We need to have the humility to recognize, okay, we think we have a good picture now, but it's always going to change. It's always going to shift a little bit. So the members themselves are deeply engaged in this decision-making process around where the edge of eligibility is. Yeah. What benefits do donators have for giving away 1% of their token supply for free. What's the pledge used for? So for free is maybe a bit of a bad way to think about it because what you get is the continued operation of Ethereum, right? The maintenance of this software, which you're building a business on, you're building a project on. And again, like I mentioned, we often take Ethereum mainnet for granted because it's so stable and because it really doesn't have issues and in addition to that We're we're getting much over time. We've getting much much better at Doing network upgrades improving the scalability improving usability and security So people often take all of this work which happens maybe behind the scenes or out of the public visibility, they take it for granted. So I wouldn't say you're getting it for free, like you just contribute it and you don't really get anything in return. You're getting something in return already. So I'd first say that. But the money, so what is the pledge used for? It goes directly to the individuals that are listed. There's no discretionary budget where we decide, okay, we're going to fund this particular project or this software initiative. If you meet the eligibility, you get a weight, and then the funding goes directly to you. And we don't make any obligation. You could start to get into the ideas of credible neutrality. You don't want funders coming up and saying, I'll give you this if you do this. We don't make any guarantees to people that contribute funding. We only have time for one more question. How do you make sure contributions are fairly rewarded, especially when individual impact can be so varied? Fair is subjective at the end of the day, right? So we take an opinionated stance that ultimately it's the collective body of software developers that ultimately steward Ethereum in the long term. Of course there are people that are objectively more impactful or more valuable to the commons, but what we can do is pull up everybody together. The rising tide lifts all boats. And that's really what we're focused on. Yeah. All right. Thank you once again. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1pXSBtge-cUH6xweP8_EkxdNV7HFwwguB4oabzfh2UJ4",
      "resources_slides": "https://drive.google.com/file/d/1k3AfN23Q4M-YJRYtzJqKfmAqQIY2BM_p/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "keynote-make-ethereum-cypherpunk-again-why-we-need-privacy",
      "sourceId": "NKMLNG",
      "title": "Keynote: Make Ethereum Cypherpunk Again: Why we need privacy",
      "description": "The Web3 revolution seeks to address the sins of Web2. However, in doing so, it’s created an even worse outcome for users  - users’ data is publicly available and makes them vulnerable to state-level censorship and adverse actions.\r\n\r\nThis talk will address the philosophical as well as practical considerations of privacy in Web3. \r\nPrivacy is an industry-wide issue and sits at the heart of all that is Web3. Understanding why privacy matters involves recognizing that it is not an isolated concept bu",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": true,
      "doNotRecord": false,
      "tags": "Zk Rollups,Privacy,cypherpunk,Privacy,Zk Rollups",
      "keywords": "cypherpunk",
      "duration": 1572,
      "language": "en",
      "sources_swarmHash": "20a3cf340f6c43d5173a021c482ae5b81d70ebc0be6456fad506132345e86310",
      "sources_youtubeId": "Sod3t2JdmOg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673581319dbb7a90e145ffc9",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1ReFBU_bsCAkpa9iAfYEJf0LER_SIpmsSyIlr2UIGBVw",
      "resources_slides": "https://drive.google.com/file/d/13AKv6FlPxWv_JfmZ5AAsdwdcscSQMrbr/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "maximum-viable-security-mvs-a-new-issuance-framework",
      "sourceId": "KWUF3N",
      "title": "Maximum Viable Security (MVS) – a new issuance framework",
      "description": "We derive a new framework for analyzing Ethereum Issuance, based on Ethereum's core values: security and neutrality. Upon discussing various attacks on Ethereum, we study future growth projections and the importance of diverse validator set, and conclude that Ethereum's defendability is the key factor for issuance policy evaluation. Via MVS, we show how the current issuance reduction proposal is dangerous, based on the future staked ETH concentration with CEXs & impact on solo stakers.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking,Validator Experience,Security,composability,validator,set,Security,Staking,Validator Experience",
      "keywords": "neutrality,autonomy,validator set composition",
      "duration": 1597,
      "language": "en",
      "sources_swarmHash": "cda7c749cb12987f113639b78de67d0d7154fb91ffdc537cde8b5578b7373bb1",
      "sources_youtubeId": "Cjr2ZEzNocc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc18e982f234a1281817c",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1ykeBOYepaHLNtCV-zLYv6QDLjqI6Dn-EYre6XtHK8lo",
      "resources_slides": "https://drive.google.com/file/d/1R3_alpfLWqPKWaxhOHg1yCZLx7mJHmVT/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "quarkid-bringing-south-america-on-chain-with-ssi-and-account-abstraction",
      "sourceId": "QXCTMB",
      "title": "QuarkID: Bringing South America on-chain with SSI and account Abstraction",
      "description": "QuarkID is a Self-Sovereign Identity protocol bringing millions of South American citizens on-chain. Citizens in Buenos Aires, Argentina, Monterrey, and Nuevo Leon, Mexico, are using government SSI deployed on ZKsync Era through the QuarkID protocol. Driver's licenses, birth certificates, and over 50 different credentials are secured by Ethereum technology in the world’s first case of governments using Ethereum’s permissionless blockchain to meet their identity needs.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "2FA,Account Abstraction,Identity,Open Source Software,Political systems,Politics,Public good,Use Cases,Validiums,Zero-Knowledge,ZK-EVMs,ZKP",
      "keywords": "Sovereign",
      "duration": 1183,
      "language": "en",
      "sources_swarmHash": "b778c903f53cea71812ed118675883effd57f913af3ef91ebe387eecd6c274a6",
      "sources_youtubeId": "5c43HjCcZeg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736bbe99dbb7a90e1323fe9",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736bbe99dbb7a90e1323fe9.vtt",
      "transcript_text": " Thanks so much for being here. Let me share with you the... My name is Diego Fernandez, I'm the co-creator of QuarkID. And let me start by showing some hard facts about what QuarkID is. First, it's a self-sovereign identity protocol. It's open source under Apache 2.0 license. It's a digital public good enrolled in the Digital Public Good Alliance. It was launched in October the 21st in Buenos Aires City with 3 million inhabitants, and we onboarded almost 165K users in less than one month. And we have 80 million users in the confirmed pipeline, and that comes from three cities in Argentina and in Mexico, four subnational states, both in Argentina and Mexico, who are doing two pilot projects with national governments in El Salvador and in Argentina. And we have onboarded also two major banks, which is Banco Macro, the second biggest retail bank in Argentina, and Banco Santander, as well as one of the biggest telco companies in America, which is Tomex, with more than 100 million users. Why did Buenos Aires City start the QuarkID journey? Our vision was first governments should pay the friction to create highways of trust because it will bring enormous value to society. And that highways of trust should be open, non-permissioned and available without any government oversight to everyone. And that should give users the ability to decide how, when, with whom and what information to share. And we did this in collaboration between governments and startups. And we found and we discovered that that creates a thriving web-free ecosystem that we believe will bring prosperity to society. What were, in my perspective, the three key decisions that we needed to make? Well, the first, decentralized versus centralized. The second, open non-permission versus private permission. And the third, open source versus proprietary. And this is pretty important because governments, unfortunately, usually choose the downside of these options. Centralized, private and permissioned, and proprietary. And we believe that that sort of waves are starting to change. And of course, we chose decentralized, open permission, and open source. Very briefly, what's QuarkID tech stack? QuarkID again is a protocol which runs on top of the CKSync stack which uses Ethereum as a security layer. But when you are planning to deploy a system to millions of people using decentralized non-permission technology, you face an evil dilemma, which is users use today centralized Web2 applications, which have an amazing UX. And when we migrate to Web3 world, we're very familiar with that, UX is so terrible. Just claiming an NFT requires you to go back and forth in wallets and signing transactions. It's so hard, so hard. UX rules the game. If we don't have a simple approach to UX, so hard, so hard. UX rules the game. If we don't have a simple approach to UX, there will be no adoption. What is the evil dilemma solution that we found? Well, we give users the choice. You can download the open source wallet from GitHub even and compile it yourself, or you can download it from the App Store. It's an open source, non-permission self-custodial wallet. But we implemented QuarkKD technology within the government application, which is running on decentralized railways, utilizing CK-SYNC era and Ethereum, but it's a self-custodial wallet for users which are not so used to deal with this type of technologies. Our way of thinking is, hey, what application will use my 85-year-old mother when she wants to download a new application in her iPhone, she calls me and says, hey, son, this application is safe. That's the level of users that you need to understand that we will be a substantial part of your user base. So we need to give users the choice of having a very simple custodial solution that runs, again, on decentralized railways. Or you can go full decentralized encrypted. That's your choosing, not ours. And it's always a use case. And I think that we as a community need to understand that people don't buy technology. Decision makers in governments and institutions, they don't buy technology. They mostly don't care about technology. They buy use cases that solve real world problems. We need to think, and that's my perspective, as technology, we need to think in technology as a tool, as a means to an end, not an end in itself. From real world decision makers, if we have the use case, which is pretty simple, a policeman, and that is happening in Buenos Aires today, a policeman, instead of asking you for a piece of plastic, shows you a QR code, you scan it with your wallet, and that gives the policeman the proof that you're able to thrive in. For decision makers, for a major, a governor, the president of a bank or whatever, 99% of the decision relies on use case. Of course, we use Ethereum as a security layer, CK-SYNC to provide CK proofs. We have native account of fraction. I will follow up on that because it's pretty important. And users control their information, not institutions. But decision makers will see use case. We love technology. Decision makers make decisions based on use case. And the question is, how do we get billions of people on chain? Today, according to statistics, we have almost 600 millions of people. There are 8 billion people in humanity. How do we solve this problem? I like to think that the Quark ID mission is adoption, adoption, adoption. This is a very famous quote that Steve Ballmer gave once, you know, this champion all over the place, we probably all saw that. And of course he was saying developers, developers, developers, and many of us are developers here and of course developers are crucial. But when Steve Ballmer was saying developers, developers, developers, his company, Microsoft, had 95% of the user base using Windows. So then it's easy. He already had gained adoption. We as a community haven't yet and we need to get there. And in my perspective, and this is kind of radical, we need to try and horse the system. And I don't mean this in a bad way. We need to find a way to gain adoption through the system, not against the system. And I am positive that that Trojan horse, that positive Trojan horse, is self-sovereign identity using account abstraction. And why is that? Because, let me go back there. Account abstraction is crucial because each user, when he gets an identity minted in QuarkID, they are using account abstraction and they have an account in CK-Sync era. It's as straightforward as that. And that enables a lot of use cases. And I was discussing the other day, next year, Buenos Aires City will be piloting the chance of giving subsidies. I mean, the city gives out subsidies to several different things, something like $35 million a year to different type of beneficiaries. Buenos Aires is evaluating the chance of giving subsidies using their identity wallets, using a token. And there is a great advantage when you're dealing with subsidies. And that is being done because of account abstractions. That's a great opportunity. And why are governments so, so important? Because they provide us, or they may provide us, with a reverse adoption life cycle. If governments can't help to create the necessary railways of trust, we as an industry should help governments to achieve this goal. And for me that is crucial. We need to focus on that. We need to understand that there are champions, there are government champions, there are institution champions out there that have shared the same ethos that we share, that believe in a future with self-custodial basis, decentralized, non-permissioned, censorship resistant and they need help from our side and we should provide their help. I firmly believe that a decentralized, non-permission, open-source future is possible. And I'm convinced that we need to do that with governments and with institutions and not against them. And that's our goal. Thanks so much. That was quick, man. I got 10 minutes left. We can do a lot of Q&A. We can definitely do a lot of Q&A. We can definitely do a lot of Q&A. Well, feel free to send all your questions here, but let me go to the first one as well. How does the open source nature of the project impact business model and perhaps monetization? No, there's no business model for Croquet. This is an open source protocol, which is being used by several governments. It's not a business. I see. Going through the top questions, is it possible to interoperate with other protocols? Yeah, definitely. I mean, two things here. Of course, we... Over here. Great, thanks. Sorry. So, of course, we have two things there. First is the W4C standards, which define DIDs, which is Decentralized Identifiers and Verified Credentials. And that gives you a level of interoperability. But, and I'm happy to say this, this is the first time in any DEVCON that we have a specific workshop of self-sovereign identity. And all of us in the industry are meeting today this afternoon in order to precisely work on interoperability with the cash from privado and so on and we think that there's a bright future in doing that. All right next question why Buenos Aires West start there and I can recall that you are planning to expand to Argentina, Mexico and Colombia. Perhaps you talk about the country expansion plans and the different regulation challenges in each jurisdiction as well. I have a very particular view on regulations. First, Buenos Aires is an extremely crypto-friendly city. You have a great pool of talent. Many of the most important projects in the Ethereum ecosystem and in the crypto ecosystem came out from Buenos Aires, just to mention a few. Open Zeppelin, NAMIC Foundation, I don't know, Crecimiento and Aleph and so on. We have very, very important projects coming out from Buenos Aires. Decentraland, one of the founders of Sandbox. I could keep on mentioning a lot of projects. So Buenos Aires has this amazing talent pool. We are so focused in the crypto industry. And when I, as my former role of Secretary of Innovation of the city, made a call out to the startups and to the innovators in Buenos Aires, everyone started collaborating and we came up with this solution which we're really proud of. Okay. Crypto-friendly city. Where else besides Buenos Aires? Well, I mean, Buenos Aires is a crypto-friendly city, of course, and we are working as a community very, very strongly and now Milagros from Crescimiento will share that with you. We think that Buenos Aires and Argentina could become one of the first, if not the first, crypto country. We're struggling to do that, we're sort of working to do that, and of course we'll need your help. All right. Can you describe your level of partnership with other protocols, maybe some strategic planning, maybe other companies that you're looking to collaborate with? Well, I mean, of course, and we shared that yesterday in a panel that we were with Evan and Anthony from Privado ID. Something like three to four years ago, there was basically nobody speaking about identity but Privado and Disco XYZ by that time. And our vision is there are 8 billion people in the world. There's so, so, so much space that we should work together. Okay. There's a question that we'd like to ask. Sure. The answer there is, where are the credentials stored? On-chain or off-chain? If off-chain, if on-chain, how do you handle? Privacy risk. No, not on-chain. Not on-chain. Never, never, ever, never, ever, ever you can put personal information on chain. The technology behind the standards of digital identifiers and verified credentials work this way. What you have anchored on chain is a digital identifier. That's the only thing which is anchored on chain. Credentials are minted and stored off chain. So just to give an example, you have the DID of the City of Buenos Aires or any other government or institution, and you have your own DID. So when you want to mint a new credential, and those are anchored in our case in CK Sinqueira, and when you need to mint a new credential, you ask the issuer to give you that credential. That happens on a peer-to-peer basis using a protocol which is named Ditcom. And what you are doing is you're signing a JSON digitally signed by this DID in favor of this DID. And that is stored in the personal device that the users have in their hand. So when you need to verify that credential, what you do is you check that the credential is valid, that is not new, that is not void and that it was signed by the issuer in favor of the holder. So you rely on on-chain security but the data is store-of-chain. How does QuarkID plan to maintain its technological advantage in the long term? Well, basically, as any other open source protocol, expanding our user base generated interest. The amazing thing is that when you do this again reverse adoption lifecycle and you start to have millions and millions and millions of users, developers start to build on top of it and we have several startups and several companies building on top of the protocol and even expanding it. Just to give an idea, when we were involved in the, this is a very small thing but it's quite nice when we did the the left pop-up city in august the the passport for entering lf was a credential minted in qwerkyd and one amazing guy his name tule he's not, I guess, did this extension of the protocol, connecting it with an Arduino device in order to open a door. So when you access the hub, you scan that, show your credential, and the door got open, which was so nice. And that is, I mean, the community building and developing. Can we squeeze in a question about competition? How is this project different from ProvidoID or PolygonID? I don't know in such a detail the stack of ProvidoID and how they are working now with this Coxyz because they recently merged. But again, we of course believe in competition and we respect each other very much. And as I said before, I don't think... You need to have the technological basics that you need to implement. But this thing is not about technology. This thing uses technology to gain adoption. So what's our focus? Adoption, adoption, adoption. And perhaps on the final question on adoption, since we have a few more minutes remaining as well. Can Argentina's adoption by both its government and citizens serve as a model for other Latin American countries? I wouldn't say serve as a model, but when we're dealing with such border innovations, and the other day I was thinking, isn't there a more obvious case where innovation is needed than identity? We are sending or trying to send rockets to Mars. Elon Musk is catching rockets falling out of space in mid-air. We're discussing artificial general intelligence models, if they're going to rule or not the world. And when we need to identify ourselves, we pick a piece of plastic and start doing like this on camera. And my God, that's so stupid, so stupid. We need to change that. Now, of course, it's identity. Institutions can feel fear. And finding some governments and big institutions as Banco Santander and MACRO that were able of taking the first steps, of course, makes every other one in the place, hey, come on, take a look at those guys. They did it. Why don't we? Alrighty. So that concludes our session. Can you do a quick wrap-up about what you do? Come on, take a look at those guys. They did it. I don't win. All righty. So that concludes our session. Can you do a quick wrap-up about what you do, a key takeaway, maybe a high-level description, and for the audience before we conclude the session? Yeah, of course. I mean, for me, the most important part is if we are going to achieve what we think is the ethos and we feel the ethos of this community, of having decentralized non-permission trust and value being exchanged over the Ethereum network, identity is the way to go. Governments and institutions are the adoption path for going from 600 million to 7, 5, 6, whatever billion people in the next 3, 4 to 5 years. And QuarkID and my team is trying to achieve that or help to achieve that with all of the other players in the industry. Thank you so, so much. Thank you. Please give a round of applause to Diego. That is QuarkID bringing South America...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1nZf4Y4ZKlAYK_rEfdGkjjq6S4WGbMxpwSUXYgi9pq-M",
      "resources_slides": "https://drive.google.com/file/d/1u5k2F3znVW_oBeyBhxBascv83UEbEwQb/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "remix-team-jazz-jam",
      "sourceId": "DFPGS9",
      "title": "Remix Team Jazz Jam",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T06:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/15rbpsHykfj3g9nCDSuig1Spz-H0RvoW5Qg7cgHOO95M",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "the-end-game-wallet-when-does-abstraction-go-too-far",
      "sourceId": "ZTMLMQ",
      "title": "The End Game Wallet: When Does Abstraction Go Too Far?",
      "description": "Chain abstraction has taken the front seat. As innovations continue, it's becoming increasingly stark that we will eventually approach a world where third-party solvers fulfill most transactions. The core protocol is also changing to cater to further abstractions even at the validator level. The question remains, how far are we willing to go in the name of efficiency, and optimizations, to which a user can't use Ethereum without third parties?",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Values,UI/UX,UI/UX,Values",
      "keywords": "n/a",
      "duration": 602,
      "language": "en",
      "sources_swarmHash": "bbf4c10d801934b671c122c84ecccd4d41134ab3102920d95762ec1e24fd214a",
      "sources_youtubeId": "FBB5YWMQ56s",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673579d19dbb7a90e1d1825d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:00:00.000Z",
      "slot_end": "2024-11-14T04:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Yvp0nywauCOnCqYI14BUwqz77qWUB-SScBhTcicFGtg",
      "resources_slides": "https://drive.google.com/file/d/17ukjHSYJPsyATV_bLOgr9PxLFKpdeebW/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "popcraft-mud-day-demo",
      "sourceId": "UDJFDV",
      "title": "PopCraft - MUD Day Demo",
      "description": "This is a project demo for MUD Day CLS: onchain games and non-financial applications. PopCraft is a fully on-chain casual click-based game integrating gameplay with financial elements. Currently in single-player mode, it plans to expand to multiplayer. Built on composability, PopCraft uses PixeLAW, TCM, and Redswap. In-game item issuance and trading are decentralized, transparent, and open, allowing seamless integration of any ERC-20 token projects and DEX.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Gaming,Not financial",
      "keywords": "Fully,on-chain,game",
      "duration": 296,
      "language": "en",
      "sources_swarmHash": "aff1af8d2c9754f3f42ee7832466c60ad90eb9ccfc155e90e1ece97bd6932049",
      "sources_youtubeId": "fV_xf0pac6k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735888f9dbb7a90e149563b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735888f9dbb7a90e149563b.vtt",
      "transcript_text": " Hello everyone, I'm Seki from the Matican team. Today I will show our latest 4-length game, Popcraft. Okay. I always ask myself some questions about the necessity, enjoyment, benefits, timing, and the contribution, but using blockchain in this field. So what's the outcome? In this field, a lot of people have to do a lot of exploring. And we want to do something different. So we chose the cross and composability as our core start point, which lead us to create PopCraft. It's a fully composable game, which makes cross-embedded compatibility no longer just a meme. So the gameplay is just like PopStar. You can even rewind by eliminating all items or bots in 90 seconds. Multiple adjacent items can be clicked to eliminate directly. Sequel items need to buy a program to eliminate. Okay. Okay. This is a video, but I cannot to play it. Okay. What is PopCraft? Simply put, PopCraft equals to pixel long plus this custom scene plus RedSwap. OK. So how we build it? Actually, we build two things based on mount. IEVM compatible Pixelon and PopCraft. OK, we just use Pixelon as a game board for Puffcraft, which gives game very excellent composability at a core level. Okay, so we use this custom machine as a game item factory. When we play this custom machine, we think how we can use this material more useful. We end up, came up with an idea of, we just use this material in another game, yeah. Okay, we use Reswarp as a game marketplace. Okay, Reswarp as a game marketplace. Restwarp is DX or Resto, and it makes the game problem price very transparent. So, the problem now is live on Resto mainnet. You can play it anytime. Okay. Also, you can play beyond Popcraft. You can play Popcraft, purchase game proper in Popcraft. Raise the price of the game prop. You can also play this custom machine, produce machine and produce material and sell them on RedSwap. You can also just be a trader on RedSwap, buying and selling T7 material, they are all ERC20 tokens. Okay, you can do, choose any of the above, all of the above. Okay, composability in PopGraft. Okay, we can replace this custom machine as any other project which has ESC20 tokens as a game prop and a game item. Also, we can replace RedShop with any other DEX. One more thing. I think we need to make crypto vegetable in 4.2 game. We might have to do another thing, but we also need to do one thing. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:05:00.000Z",
      "slot_end": "2024-11-14T04:10:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/12K7Vn_cc7jQu6WzJS3EQxpVW_8a_ylzYwi82LxCmSBw",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "reading-before-writing-an-approach-to-brain-interfaces",
      "sourceId": "AECBRW",
      "title": "Reading Before Writing: An Approach to Brain Interfaces",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 512,
      "language": "en",
      "sources_swarmHash": "9a8cb184078eae668de170bf56770a6a31c5bcf2c71b1a6a9fa068ba7a9ae576",
      "sources_youtubeId": "LYfOHxvgApA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357a999dbb7a90e1d8dcbc",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:06:00.000Z",
      "slot_end": "2024-11-14T04:13:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1yeFg5w90FisDwxUH5GvlEr2tT53GBDkWeBVcOZI2p7c",
      "resources_slides": "https://drive.google.com/file/d/1JspEHBhgNXpk5SWCNFeXr4DLW63ZWIXF/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "project-mirage-mud-day-demo",
      "sourceId": "BVANRC",
      "title": "Project Mirage - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications. Project Mirage is an onchain island management game where players build, expand and trade their islands.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 206,
      "language": "en",
      "sources_swarmHash": "447ea7e3caa545f9b9abe80a90fee1ee9304096a65c62f14a199a99ec90b97e2",
      "sources_youtubeId": "YEZN8zdHRx4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673591ec9dbb7a90e1f51b8b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673591ec9dbb7a90e1f51b8b.vtt",
      "transcript_text": " Steal someone's identity Xavier? No, no, okay. Hey, my name is Xavier. Closer, closer. Closer, okay. Hey, my name is Xavier. I'm the owner of Project Mirage. Project Mirage is an island building game, and we are using Redstone Garnet, testnet, and we are building Foley Ahmad. So let me demo the game. So when you begin, it's not like that. It will be like an empty island with a city hall in the middle. You have to pave road. Yeah, you have to pave road and add more buildings. You can see there are drop downs. We have a road pavement. You can drag. You can save the road and add more buildings. There are various categories of buildings. There are residential. The first thing you need to build is like resident buildings to add like laborers, but you also can build like different commercial, industrial, civic landscape to like fulfill certain requirement and produce certain resources. So for example, I can put like a house here and put like a commercial building nearby. You can view the details by clicking into the buildings. We have some red ones here. Oh, now, oh, level up. Unlock some buildings and put some names, like X island. By reaching certain levels, you will see something like this. This is my own island. So I have built like virus buildings. We also have like a functionality called summon. So I don't have like resource to summon any character right now, but you can view the character here. For example, I have this one. We have some description. And you can actually produce resource by assigning the character. So for example, this one, he can plus 4% onto the resources. Yeah. He can plus four percent Onto the resources. Yeah and also we have a functionality called shares so for example if I visit my co-workers Island I Can trade shares on her island for to become like a shareholder on different people's island you gain profit by gaining resources when the other owners produce resources. So you gain a certain percentage. And also you can trade shares. And with the priority of the island coming up, the share becomes more expensive. Yeah. We can also visit other people's island. There's a leaderboard on our game test. For example, this is one of our top players. And we can see his island's much larger than mine. Yeah, that's it. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:10:00.000Z",
      "slot_end": "2024-11-14T04:15:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1d-1krZg7I-YltJPVKWhfg0Tl6wSDlA4A7_wN3qi3s3M",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "putting-intents-and-users-together",
      "sourceId": "YUPJGZ",
      "title": "Putting Intents and Users Together",
      "description": "Intents represent a new approach to Web3 interactions. However, the transition from the existing structure to an intent-centric space remains uncertain unless we maintain user familiarity. We conducted experiments on user experience for intents and tested them with a focus group. This talk will explore how we can approach intents in a way that users will adopt more readily by leveraging the latest standards and EIPs, including EIP-7702, ERC-4337, ERC-7579, and ERC-7715.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Rollups,Account Abstraction,Intents,chain,abstraction,Account Abstraction,Intents,Rollups",
      "keywords": "Chain,Abstraction",
      "duration": 520,
      "language": "en",
      "sources_swarmHash": "289be16f743d082567c4698ba4f6e9e23627809b187c2df7857d8836eef1a707",
      "sources_youtubeId": "0FpMhUJTwA4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357c2b9dbb7a90e1e347be",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:10:00.000Z",
      "slot_end": "2024-11-14T04:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1oa41JFQPp-vuRePzM4jYH0K22uvY02iOso74y9q_Ryc",
      "resources_slides": "https://drive.google.com/file/d/1ozLbEySYeXshZwmeZlAuE0_WVV_kCYfo/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-longevity-acceleration-roadmap-a-technical-plan-to-solve-aging",
      "sourceId": "V9BA8B",
      "title": "The Longevity Acceleration Roadmap: A Technical Plan to Solve Aging",
      "description": "The Longevity Acceleration Roadmap: A Technical Plan to Solve Aging",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DeSci,e/acc",
      "keywords": "Longevity",
      "duration": 476,
      "language": "en",
      "sources_swarmHash": "23706b226f61f01a6d2ee5fa74716b3f1521fb6b40769f99b9662a2e37344e20",
      "sources_youtubeId": "yT7L3bPpbEw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357e839dbb7a90e1096ef2",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67357e839dbb7a90e1096ef2.vtt",
      "transcript_text": " Take it away. Hi, my name is Nathan Chang, and I'll be talking about the Longevity Acceleration Roadmap, a technical plan to solve aging. This roadmap is a foundational project at the LBF and it's intended to help guide newcomers to the longevity space. I can only do a speed run now, but if you're interested in the full thing, check out the link below. Okay, so the LBF is a non-profit that I co-founded to mobilize the world's top talent to work on solving aging. But why work on aging? Well, simply because life is good and death is bad. And if you ask what is the thing that causes the most death, it's aging. So by far, aging causes the most death. 75% of deaths worldwide are caused by aging. And this comes in the form of age-related diseases like heart disease, cancers, Alzheimer's, etc. And this is an emergency. All 8 billion humans are on an exponential trajectory towards physical and cognitive decline and ultimately death. But if we can solve aging, we can envision a future where people can live for as long as they want in peak health and function. But if you want to see this future personally, we need to do something about aging today. So where's the plan to solve aging? Well, when we first started, we couldn't find any clear plan to solve aging. So we had to make one on our own. And in our plan, we wanted to have three key features. So one, we wanted to exclusively focus on direct research and technology paths that could solve aging completely. And two, we wanted to focus on specific, well-defined key technical objectives. And three, we wanted it to be actionable. So have a good overview of projects in the space so that people could easily join or start them or fund them. Okay, so we got a lot of input from scientists, researchers, and entrepreneurs in the field and we identified three main strategies that could plausibly solve aging. So one is biostasis. This means pausing biological time and this is typically with cryopreservation at low temperature or chemical fixation. Two, replacement, so replacing old parts with young and most promising full body transplants plus gradual brain replacement. And three, advanced bioengineering, so understanding and modeling biology and aging and also developing genetic and cellular engineering tools. aging and also developing genetic and cellular engineering tools. Okay. So one mental model to kind of understand these three technology paths is a two by two matrix. So in one axis, you're trying to solve aging or by time. And the other axis, you're either trying to understand aging or bypass aging like the complexity of aging altogether. And just as a side note, some of these technologies can also be thought of general defensive biomedical technologies. So for example, in like replacement could be useful for things beyond aging. So if you got in a car accident, replacement could be life-saving. Okay, so let's take a look at biostasis. So the strategy here is to pause biological time until the future where medical that, but also to prevent thermal fracturing. So when you're trying to bring a body or a brain from low temperature back to room temperature. So there's a number of different projects and startups in the space, but there still needs to be a lot of work to be done. Okay, let's talk about replacement. So this strategy is really divided into two parts. So there's replacing the body with a clone body and knockout of the brain. But then, too, for the brain, you gradually replace the brain with young neural tissue graphs, bit by bit, kind of like in the style of the ship of Theseus. Now, in the body, this really has only become within reach since 2018, when Chinese scientists demonstrated the first successful cloning of primates. And it's plausible that this could be extended to humans. Okay, but you can create a clone body, but you don't want to create a brain at the same time a person so you'll need to construct a genetic construct to ensure that the capacity to have consciousness never forms in the cloned embryo. And there's actually already a natural proof of concept for this. It's a birth defect called hydra and encephaly, where cerebral hemispheres never form. And these bodies actually never have consciousness, but can develop to full maturity. And last, to put it together, you have to do some sort of head transplant. And these have been done experimentally in the past in mice, dogs, and monkeys, with survival in some cases lasting days to months, but still needs to be improved. Okay, so let's move on to the brain. As I said before, you can gradually replace the brain with engineered neural tissue graphs made from a patient's own iPSC stem cells. And neuroplasticity of the brain allows migration of brain functions away from damaged areas or if done slowly. And just recently, the US ARPA-H program hired the leading researcher in brain replacement, Jean Haber, to lead a $100 million moonshot program in this strategy. There's a number of key technical objectives in our V1 roadmap. Most of the hard stuff is in the brain, but also in spinal cord reconnection. And there's a number of startups working in this field, but most are still in stealth. Okay, we also asked researchers to estimate the time and cost to get a reasonable attempt on this roadmap, and they came up with a figure of $3.6 billion in about 10 years. they came up with a figure of $3.6 billion in about 10 years. Okay, last, bioengineering. So this is the longest and most uncertain road. It will require the convergence of four different prongs. So one, large-scale data collection, something like a protein data bank, but much larger scale. Two, computational modeling, which could include AI models. And then three, design of genetic or cellular interventions. And four, delivery of genetic interventions or cells. Okay, next steps. So clearly, there's a lot of work to be done to solve aging. But unfortunately, there's very few people or resources devoted to trying to solve aging. But unfortunately, there's very few people or resources devoted to trying to solve aging. So if you feel an intense desire to defeat aging, definitely join the LBF. We're the biggest community focused exclusively on building for indefinite lifespan extension. We run highly selective intensive workshop retreats. Our next one is in March in Berkeley, California. But fighting aging is not just for biotechs and scientists. We need everyone to get society aligned on allocating Apollo program level resources to fighting aging. And so if this resonates with you, I'd encourage you to also join the vitalism community where we're building a social political movement to make fighting aging and death humanity's number one priority. And our community is organizing the biggest longevity event in the world. It's going to be a two-month pop-up longevity city in Berkeley, California.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:13:00.000Z",
      "slot_end": "2024-11-14T04:20:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/160SSgpDZHkjg4YniAuH3mYD1hx7hZuv_Qp2ip0zoRso",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "biometric-proof-of-personhood-verification-in-worldcoin",
      "sourceId": "YSXLVA",
      "title": "Biometric proof of personhood verification in Worldcoin",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "db9c3bf3ac88fc0e1b02fbe543835a1658ae222287be34e7cb273c75741990e2",
      "sources_youtubeId": "q3rpu8aDRA8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:20:00.000Z",
      "slot_end": "2024-11-14T04:28:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/10po4XtVzHK_QqfTE4tTOIt7GHM_mPiDKzpxYrl2-TFc",
      "resources_slides": "https://drive.google.com/file/d/12YLD9pTQpr5tYwySzhEJ3buGlWUM9Xh9/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "exploring-various-approaches-to-achieve-effective-decentralization-for-intent-based-protocols",
      "sourceId": "LGZYYW",
      "title": "Exploring various approaches to achieve effective decentralization for Intent-Based protocols",
      "description": "Intents are emerging as the gold standard for transacting on-chain. However, they do come with decentralization trade-offs. In this talk, I'd like to present the status quo, various architectures, and new tradeoffs in terms of where they fit in the trilemma of fees, execution speed, and execution guarantees. The objective is to achieve maximum decentralization while maintaining a great UX and efficiency.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "TEE,Decentralization,Homomorphic Encryption,Intents,MPC,ZKP",
      "keywords": "TEE",
      "duration": 529,
      "language": "en",
      "sources_swarmHash": "e19db64e8f8dd4b52315070de1744acf9726b9198101d6cdbd72062edc4ee5b1",
      "sources_youtubeId": "Vy_-uON0FTg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67385ce81b0f83434dae2a14",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67385ce81b0f83434dae2a14.vtt",
      "transcript_text": " Fees, execution speed, and execution guarantees. So welcome to the stage, Munir. Everyone. Okay. Everyone? Everyone? Okay. I'm glad to be here. It seems like this is an intense morning, and a very intense one. Sam Munir from Lata Labs. We're known for building PowerSwap, a five-year-old protocol, and recently Portugus, that allowed us to launch parasoft delta so the first time paraswap is now in the intent space and and it's a great thing um so i guess you heard a lot about intense i believe it's now uh something that is second nature uh so i'm gonna jump directly to what are intense in the context of trading on chain. So it's basically one way of expressing an outcome without describing the details of that outcome in simple terms. So if you are a user, you want to swap a token A for a B, well, what you do, you sign an order or an intent. You send it to a backend API in general, like 99.99 of the time. But behind the scenes, there are what you call solvers, resolvers, market makers, agents, how we call them, that will compete for user flow and will bid on an auction. And the best bidder is going to be the winner. So we have two entities here. We have the user who have a desire, like I want to swap token A for B. We have solvers, agents, you name it, competing for user flow. And as I said, there is an API in between that does the whole work. Users still are mastering their key. The system is like work as an EDFI protocol, but the thing is that API, which acts as a central point of failure, if it stops working, the whole system is not useful, is not working at all. So that's a problem. And this is what we're working on in Porticus in trying to create an alternative. But the challenge is, and what we are also proposing, is a way so that we can still preserve the user experience, because if we introduce a fancy decentralized layer, let's say an app chain, well, we won't ask users now to interact with yet and yet another chain. That's not going to be great. It's not going to be good for UX. So the thing is, we keep that backend API, that centralized back-end API. But the thing is, now it's optional. So you can use it. You can run it yourself. So the user and the relayer can be the same thing. And that allows us to still give the user exactly the same experience, but still having a neutral network that can run this system of intent interactions. So the system looks like this. We have on the top left the user interaction. So it means user wants to do action XYZ. I talked about swap, but it can be I want to provide liquidity to Uniswap. I want to do a cross-chain liquidity providing on another AMM, or maybe I want to deposit my USDC on Aave and earn yield on another chain, or maybe completely abstracting that chain. I use a relayer. I can be my own relayer. It can run literally on my laptop. So I publish the auction or my intents on the network. We have an auction manager. This is the heart of the system that you're going to zoom into. The rest of the process is the same. We have a competition. The winner is going to win. Get to execute the user trade. Rest is not new. So we have multiple ways for achieving that. Unfortunately, we don't have a lot of time to get really deep. But there is the possibility of running the auction manager on an app chain. Something that's not new, we have like DYDX who are running the whole order book and the matching engine on an app chain. It's working well. The big limitation here is privacy, because we may be pre-licking MEV on the user side, but also on the agent side, on the solver side, like all the bits would be public. There are solutions yet, but here we get into more complexity, like commit and reveal schemes that can be a bit challenging as well, but it's one of the approaches. The other one is one I like a lot, is the TE, maybe, or MPC, or a combination of both, where we can have a bunch of nodes who are running on, if we go with TE, running on a trusted execution environment, so it means like a closed environment, a private environment, so we solve the privacy issue of like pre-licking of MEV, and the issue with TE is we rely on a specialized hardware, issue with MPC is a bit slower than TE. So maybe a combination of both could do the job. It's something that's still a work in progress or a research in progress. The other one is like the holy grail for computing on encrypted data that can be a perfect solution, but unfortunately, this is FHE, a full homomorphic encryption, where the nodes can compute on data fully blindly without having any idea of what's inside and getting to select, in this case, select the winning agents based on the best bidder in terms of price, gas, you name it. But the problem is FHE latency is not acceptable yet. It's way, way too slow. It can work on a batch computations and not in real time, where it's very important in the case of trading on chain and many other use cases. So that's maybe we still maybe a few years ahead. There is some work on building some ASICs, again, specialized hardware, which would become a limitation. But still, yeah, this could be the best solution at some point. But we're still a few years ahead. Thank you, guys. That's all I had. Happy to answer any questions. Thank you, Munir. All right. We have time for questions. Raise your hand if you want to ask one. You've been amazing in terms of asking questions so far. Let's keep it up. Let's keep going. Yes. All right. One, two, three. Is there a specific service which solves this intent problem usable by many DEXs, cross-chain swaps, or each build their own intent solvers? You mean on the product side or on the agent side? On the product side. Does it make sense to have a scalable product here, which is serving this intent services for many, many cross chain solutions? Yeah, for cross chain, I think it's becoming more and more like a normal thing between codes. So we're seeing more and more protocols normalizing cross-chain interactions. So if that's the question, I would say that's just a matter of time. We're not far away from having, making cross-chain is like the normal thing to do and thinking beyond cross-chain. If it's a question about having generalized purpose intents, it's something I believe less that it's going to be the case, because the complexity of each protocols makes them so different to have a general purpose solving mechanism. So if that's the question, I hope I answered. All right. Time for another question over there. So as someone who has not explored Intents yet, I want to ask how to participate as an agent or a solver in this network. Like, what exactly is a solver? Is it a piece of code or like a market maker bidding for the prices or such? Yeah, it's definitely too broad. It's always a piece of code anyways. Like someone has to write a code that will answer a user intent between code, like user desire to do something. The simplest case is I want to swap token A for B. And the one who provides the best price is going to be like coding and competing with others. Like you may run a fancy algorithm and connect it to many exchanges, the Texas buyer, of course, and running this algorithm that will answer the user intent. But it can be other things. Like nobody has a integration of Aave that will let any user put USDC cross-chain on another chain and earn a better APY. That's also a use case that can be fulfilled by an agent. All right.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:20:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1LaXJZlFuHU9E1WzvaA5EEprE3pUrcWOtPgm0VCJNCN8",
      "resources_slides": "https://drive.google.com/file/d/1zrsTtk7_Z99jlv7r-ZWdtJychKaQS8dq/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "txmonster-mud-day-demo",
      "sourceId": "3GSMUH",
      "title": "TxMonster - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications\r\n\r\nUsing MUD Dev to build \"Eat Sleep & Survive\" TxMonster on RedStone Chain",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "N/A",
      "duration": 288,
      "language": "en",
      "sources_swarmHash": "d78526fb87c5a752ac399b532bc16e395f7ee7caf593f8ae3c03e5b78e07e201",
      "sources_youtubeId": "P3x5UV39CCA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673587ef9dbb7a90e1311702",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673587ef9dbb7a90e1311702.vtt",
      "transcript_text": " What are you doing? You guys find here it's like Pokemon Go here? Okay, so this is a chance for us to play the game similar to Pokemon Go. Capture mantras on the text studio and then with the mat app. So, can I have a clicker? Where is it? No, here. Okay. Just a little bit. So I need a clicker here. So that's to be great today so I can show up in here and talk with you guys like a transfer to share the game. And how grateful is the MUD team and the Lattice and the rest of the supporters to build the game. Thanks again. So, please, I need a clicker like you. Okay. So we can do it. Okay, so you can just skip into the next slide. This is the game loop. So the next one. The next one. Okay. The text monitor is like the monster capturing. So that is the 3D web browser and we're doing with the Unity. So we are coming from the web tools like studio games. So we want to bring like the gaming experience inspired into the web free and the fully on chain games blockchain. So uh this one we bring like the whole new diversity like monster system so we can try to use and discovery with the like the sandbox and open world. Uh please next slide. Okay the game look will be very like simple. We see in here we build the characters, we explore the world, we just like taming the monster and then we like taking battle work and we gain access and then we can like build the character again. That's very simple game look so we're just following it. Okay, next one. Okay, we have like a lot of features in ecosystem but the most important thing here I want to emphasize is the FOMC mechanism. The first thing is like we talk about the game files, it's the ownership of the assets. But talking about the fully on-chain game, we need like all the logic on-chain. And lastly, and most important, is decentralized world bringing contribution with the community. Okay, please for the next one. Okay that is the gaming NFT contributing because we need to make sure that the people and the user need to play the game by the game itself not only about the finance thing. So we have it here the NFT the monster activities cosmetic and all of the things the user needs to contribute by the finance or by gaming experience, by goal for the game itself. Okay. And of course, uh, blockchain people own value not only for the game and also for like finance thing so we also have the further value. So looking for it and we need the next one. OK, this is our emphasize things that you see in the future. We have the user-generated content. It means, for example, we have our own project with access. And then the community, they can create their own access, make this one, and then put it into the game. But in extra, you can own your sub-client. And you just use access, and then you can bring the sub contract into the game. But in Extra, you can own your sub-client. And you just like use access and then you can bring the sub-contract into the game. And then you also have like all other features in the game and go along with us. But most and important thing is like if you have the better team and you can build the client with our access, you can do that. So the UDC is the most important thing when you play the game. If you, anyone in here plays Steam community, you have",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:20:00.000Z",
      "slot_end": "2024-11-14T04:25:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/10U4OcgkMv_HGXoZzHe-sIP9e08AcMp-G142YBiu1DUM",
      "resources_slides": "https://drive.google.com/file/d/1sLQb1lSHs3xFGv9wziKI-TyiMJnZJpfd/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "ultimate-dominion-mud-day-demo",
      "sourceId": "GPQVMW",
      "title": "Ultimate Dominion - MUD Day Demo",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\n\r\nUltimate Dominion is a fully onchain text-based RPG. Explore the world, defeat monsters, collect, buy, and sell items.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,Autonomous World,Autonomous World,Gaming",
      "keywords": "",
      "duration": 329,
      "language": "en",
      "sources_swarmHash": "342168451b143f531922ea9d08f6e99c010b5fe7b227fa112be1f90f564d43c0",
      "sources_youtubeId": "yILE0MO7B2M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735907b9dbb7a90e1cac2fd",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735907b9dbb7a90e1cac2fd.vtt",
      "transcript_text": " Ritz Raspba Reviewer 1 Hello everybody, my name is Ritz Raspba. This is my first time at DevCon 19, and I'm really excited to show you what me and my team have worked up for you guys. We've been working on this game called Ultimate Dominion for the past six months. It's a fully on-chain text-based MMORPG. Now, I've been growing up a real fan of Skyrim, those types of games, and that really inspired my art going into this. I've been drawing knights and dragons since I was a kid, and I'm glad I'm able to have the same assets as the things I've been drawing and growing up with. Narrative is a really big part in this. I mean, I feel a lot of games in the last decade just haven't been, haven't really been focusing on that world building, haven't had a world that felt like we should fight for and narrative that we would want to push forward. A lot of games focus on building a game that feels like it's going to outlive the players, but we want to work on a game that feels like it's lived for and has had a history long before they've arrived. Now heading into that game, this is what we have. My character name is going to be Mr. Slay's lot. This is beta and I'm going to be doing this live for you guys. That's my profile picture. Deal with it. Moving forward, I'm going to be rolling my stats next. The stats determine what you're good at and what you're bad at. If you have high HP, then you're harder to kill. Higher strength means you swing your sword heavier. Agility, you're faster and better at running away from me. And intelligence gives you access to the strongest magic in the game. Let me actually roll my stats a little bit. Rolling into here, this world is going to be in a 10 by 10 grid. You're going to be moving about it more. Think of it like you're moving a piece in a chess board. Loading into it here, you're going to see to the right here, we're going to have several points of interest and a yellow border. This yellow border is going to be the player, the safe area. Your character is depicted as a dragon icon. And as you see as I move through here, the middle screen is going to be updating with whatever, with the players and the monsters that are currently there. This getting into the combat here. I'm going to be going up against a kobold scout. Oh. Apologies. So the items in this game are the main way that you're going to pack things and dealing damage. These individual items, as I equip them, and even these low-level items, they don't do a lot. They're mostly numbers and have small effects. However, when you couple them up with... when you have multiplayer and multiple people using several attacks, like think of a rogue with a smoke bomb and then your wizard casts Fireball and lights up the smoke bomb and does extra damage. We want to incentivize players and empower them to have cool interactions like that that we really can't anticipate. This sort of story building is really the... It's what we want to push. Here we have the chance to do some really lovely cooperative storytelling. We as developers want to empower players to have a narrative to push forward and to attack with. And that's the main dream. Right now with this beta, we're focusing on this main loop where you attack monsters, get their items, and sell them. And with that comes, well, that progression. But this is the meat and bones of the game. All this narrative stuff that we want to do, all this ambition, this crazy world that we want to build is great, but we want to have a good game that goes with it. I mean, I think it's really important. Getting into it, now I'm going to be getting into the combat here. Going up against a... There we go, I apologize for that. Getting into the combat shortly here, you're going to see that as I get into it, we're going to have these actionable items here are what I have to attack with. As I was saying, when you mix and match with multiplayer, you have a lot of combination and a lot of things that you can do. But ultimately, this is the game. If you want to support it and you want to see it live through, you can join the demo with the code we're going to have back here. And I can't wait to see you there, guys. My name is Ritz Rospa, and this was Ultimate Dominion. Thank you. Thank you, Ritz.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:25:00.000Z",
      "slot_end": "2024-11-14T04:30:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/13Uil3sm_cj9Qi6g5Yd7Wn1eUVWbT6tRsAAUDqNmNTmU",
      "resources_slides": "https://drive.google.com/file/d/1zYGJzeJokUlCNleWOVNAy1_3aG8x6C4-/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "circles-resilient-money",
      "sourceId": "EWTZTD",
      "title": "Circles - resilient money",
      "description": "Circles is an attempt to design money while maximizing decentralization and resilience. Furthermore it tries to find a balance between \"newcomers\", joining the social contract that any form of money represents, and those already in it.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "macro/micro economics,Public good,Solarpunk",
      "keywords": "fair,money",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "8852482be3807eba17d5f9debd1cfa66d74aa6a315b42cb50220516bec0a3ffa",
      "sources_youtubeId": "u257H6Plt-k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:28:00.000Z",
      "slot_end": "2024-11-14T04:42:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1M-GH6csygqzvZdnn2OpNm0N7Qq6jQ2Z9bbEUeyCxiOY",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "argentinas-opportunity-and-the-case-for-a-special-economic-zone",
      "sourceId": "EZSLBK",
      "title": "Argentina’s Opportunity & the Case for a Special Economic Zone",
      "description": "Argentina is uniquely positioned to capitalize on the technological and economic opportunities of this century.  \r\nThis talk presents a compelling case for the creation of a Special Economic Zone (SEZ) to drive technological growth and economic prosperity in the country.  Intended to push innovation and attract investment, a SEZ could address Argentina's current challenges, including inflation, infrastructure, and regulatory hurdles.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Economics,Frameworks,Network State,Political systems,Regulation",
      "keywords": "Special Economic Zone,SEZ",
      "duration": 520,
      "language": "en",
      "sources_swarmHash": "f004d1c7e3c87a361dc974de732c962f75168a14efd745e0c966b0fe6037dedc",
      "sources_youtubeId": "xiCG85_ChsM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736bd629dbb7a90e142cd38",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T04:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1ZI-uvSDiXNOMq2oSg_Q_XXuhL57THew5PgeViZ-WwC8",
      "resources_slides": "https://drive.google.com/file/d/17dVyvq0xPWrhK3JO0dj8f8VLIkfsa2wY/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "behind-zupass-applied-cryptography-for-consumers",
      "sourceId": "GEEXRU",
      "title": "Behind Zupass: Applied Cryptography For Consumers",
      "description": "Recent advancements in cryptography on consumer devices (like your mobile phone) and progress in developer tooling (Semaphore, SnarkJS) have led to the emergence of open-source projects such as Zupass, a personal \"cryptographic computer\" helping power Devcon.\r\n\r\nThis talk dives into the technical challenges behind scaling ZKPs to 10,000+ consumer devices and the history behind the project, from its inception at Zuzalu to its usage at recent Ethereum events (Devconnect, ProgCrypto, ETHBerlin).",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,ZKP,Use cases of cryptography,Mobile,proving,Identity,Use cases of cryptography,ZKP",
      "keywords": "Attestations,ZK Identity,Mobile Proving",
      "duration": 1501,
      "language": "en",
      "sources_swarmHash": "06fa912b31c5526d10f554221175ea2db40141a304aa57c4763c1d342f82c693",
      "sources_youtubeId": "7PPAIFVVgRU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673583b29dbb7a90e1c8d87a",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673583b29dbb7a90e1c8d87a.vtt",
      "transcript_text": " Hello, thanks Sam. As you said, there's going to be a bit of a surprise for you all. I know it's getting close to lunch, towards hungry times, so we'll make sure it's worth the wait. I'm Richard. I work at Xerox PARC, and I'm here to talk about ZooPass. But before I get into ZooPass, I actually want to first talk about Ethereum. We are an Ethereum conference. And one theme that we've seen popping up over the past, I think, few years especially, and few months even more",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1w_5QruyCi8qSHhXVnY97nh_BrNGlcqN8q30ZIT81j4g",
      "resources_slides": "https://drive.google.com/file/d/1t3zO4l83pBb8McR6Jzt-i4WsI8g-fTiT/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "cultivating-culture-in-web3-preserving-the-essence-as-we-evolve",
      "sourceId": "MZMQXY",
      "title": "Cultivating Culture in Web3: Preserving the Essence as We Evolve",
      "description": "Meaningful conversation around the importance of maintaining the unique culture of Ethereum, especially as we continue to grow and attract individuals from traditional  backgrounds. The chat can explore how to uphold the values and ethos that have shaped the web3 community + the higher human needs of belonging, connectedness, and purpose etc.\r\n\r\nThis would be between myself and Aya",
      "track": "Cypherpunk & Privacy",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "culture",
      "duration": 4078,
      "language": "en",
      "sources_swarmHash": "740106772d85d82c905abffd644ad4dd8989b47f54e200bd925ddd9cd9c70e3c",
      "sources_youtubeId": "N4_f93eEk8Y",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67359ead9dbb7a90e18ca137",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T05:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1MEHwnn1XVg3IxqYq8U8Z80rO7dw8-zksCQ9QTwsL6X8",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "open-tech-blockchain-and-settlement",
      "sourceId": "NGXHAA",
      "title": "Open Tech, Blockchain, and Settlement",
      "description": "In this talk, we discuss the what and why of open tech, starting with networking and the internet. Using the recurring progression of tech to openness, we explore the critical classes of commitments and settlement that enable blockchain to accelerate open coordination of finances, tech, and society.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "fork,Consensus,Coordination",
      "keywords": "Forking",
      "duration": 1599,
      "language": "en",
      "sources_swarmHash": "2afaade7fef485377dc90dcd4b79e1ba6952d8647f8ab9c7a51aac698beef1e3",
      "sources_youtubeId": "vfAgwVjLonc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735847d9dbb7a90e1e872a3",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1pAUfWWkDdvSVfjG3UFm9ChrQDu1XE0Ae1vmkkLNxV3A",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "practical-endgame-on-issuance-policy",
      "sourceId": "TQMWK9",
      "title": "Practical endgame on issuance policy",
      "description": "A practical endgame on issuance policy stops the growth in stake while guaranteeing proper consensus incentives and positive regular rewards to solo stakers. Viable reward curves for this endgame are presented. Motivations, impacts and potential downsides of an issuance reduction are in focus. A tangible framework is also introduced: never exceed an issuance rate of 0.5%. A stringent cap on issuance caps the inflation rate, solidifying ETH as trustless sound money with robust economic security.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Economics,Staking,Tokenomics",
      "keywords": "",
      "duration": 1636,
      "language": "en",
      "sources_swarmHash": "343fa2625e225c2ce506d01b26c6323f2618c82ca604b1fad8189db7465585aa",
      "sources_youtubeId": "m91Wu6-cdwk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc204982f234a128c12d5",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1xmwhrvV65FuGDVnNb8_zGgVoMM4-pg6gMEP0t1Iw-OU",
      "resources_slides": "https://drive.google.com/file/d/1s9Jmnf7bpHGGaWFAY2jUDxVK1Avusw3x/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "revm-endgame",
      "sourceId": "VEEYFZ",
      "title": "Revm Endgame",
      "description": "Revm is a critical component of the Ethereum ecosystem, used by builders, toolings and clients. It is an audited and proven library that is both fast and easy to use.\r\n\r\nAs more projects adopt Revm, I feel the increasing burden of making breaking changes and the need to consolidate its functionality. That’s why I am thinking about Revm Endgame, a solution to support experimentation, Layer 2 features, and EIPs without the need for repository forks.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Architecture,Public good,execution,client,Architecture,Core Protocol,Public good",
      "keywords": "EVM",
      "duration": 1484,
      "language": "en",
      "sources_swarmHash": "362ae064054b976732ffdfbf8338aa3a0c5d0b44e11c212fdb8e6ec7389eff92",
      "sources_youtubeId": "xRuDWTWuxKA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735836e9dbb7a90e1c0a9ff",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735836e9dbb7a90e1c0a9ff.vtt",
      "transcript_text": " The introduction. Yeah, in this talk I would like to talk about Endgame, the last step of RIVM project. It started a few years ago and I think I found the solution that's going to last for a while. Before that, let's talk about history and basic evolution of the project. It started in 2021. I took a break from the work. I basically went to the seaside, relaxed a little bit, and after I came back, I did basically want to do some project that's related to infrastructure, wanted to build something that uses EVM, and I started researching what's basically on the market. And there was like three things. Open Ethereum was the client that I worked on before and it was GPA license so it was not suitable. EVModim was the new one. It has some, it's built by an ex-colleague, it has some experimental and exotic Rust night features and it was like just interpreter. So in that So it was not enough for me. Sputnik VM, I was most interested in it. And it was older project, older EVM, but it was not that maintained. It was hard to use it. And if you want to do inspection, tracing, you need to basically build it in some different way. And that's very hard to use. So after making PR to the Sputnik to add some generics on the instruction set, basically after two weeks of no response, nothing, and seeing the project not moving anywhere, I closed that PR and made my own REVM. In the month I had some initial initial code that can be run. This PR is fun to look at that. By the end of 2021 I supported the latest forks, state tests for passing, it supported all precompiles, and most importantly, interface was simple. I wanted to have, from my experience with Open Ethereum, I wanted to have EVM where you settle the data and say, transect. And then basically it gives you the output of that execution. The database trait was the abstraction that allows you runtime fetching of account and storage. Environment was the way how you set transaction block, configuration, everything around it. And inspector was the good abstraction, very powerful one that allows you runtime inspection of your EVM when you it's run get it this PR it was optimized few times but this PR was most impactful it's basically 4x the performance it was no std from the start the idea was to use it in JavaScript or Wasm. It was MIT license, and by that year, I basically jumped to another project, and REVM became my hobby. First adopters came next year. Foundry was the biggest one, the first one. The way how I met Georgios was he tweeted if there is any EVMs out there and I responded on the tweet, hey, fresh feedback if you're interested and he was in the minutes my DMs, that's Georgios, he's basically amazing in that sense. They started integrated a few months, I think the Oliver did the first integration five months after the start, that was like March 2022. Harhead started to get their RAST backend, they published DDR this year. Builder Surges was one of the big users of the library and they would, the first one they sync from the Genesis to the tip of the mainnet and it was very nice to hear hey we are syncing everything is fine. By the end of 2022 all focus was supported, optimization was done, our REVM-GS was supported but removed by the end of the year, there is no traction. A few more optimizations were done and most importantly RET, the client, started to be made October 2022. At that point of time I joined Paradigm to work on the client. I think I had H on my open Ethereum times. Second year, RET are mostly basically focused on the client implementation. We needed to make something stable and we did it very fastly. Team was amazing. I planned to stay there just for a few months but I'm still there now in Intaka but either way same team. Shankar and Kakum were added and I moved on. This year, RET01 was released, big stability improvement, it was audited, it was the people liked it, people wanted to use it, it is amazing. REVM on its own hit its own milestone, it got audited. It was community driven by the company. And it's basically the guy that finds a lot of EVM bugs. There is blog post if you want to hear more. You have got supported. And start of the year, ZK EVM become a thing. This year, I could claim, I feel that it's okay to claim that, the REVM became the most popular EVM library out there. And it's become the critical component of Ethereum ecosystem. There's a few types of the REVM users that I can see. They're basically clients and chain. RET first, Helios, Delight client, Trin. As the execution clients, there are different chains, Optimus, Crawl, Binance, Polygon. They are all interested to have REVM in their code, or basically, in this sense, RET and in that connection, REVM in their code or basically in the sense RET and in that connection REVM. The tooling both the foundry hardheads are basically using REVM. Builder searches, they're a little bit private but always in my DMs are the big users of it and ZKEVM basically became the standard library that's used in that field. There is even a grant by EF Foundation that basically targets REVM formal verification with the usage of the RISC-V and ZKEVMs. This all affected the future of REV and how I look at it. So, the problems. First problem, how to do AIP testing. How to allow AIP champions to come to the code base, improve their thing, implement it, test it in the foundry if needed, and basically do their own thing and do their basically on their own time. One of the example is transient storage. I want to just move on that path. In July 24 I got issue in GitHub. Hey can you implement that? That was one of the AIP champions that wanted this included in Aravian for him to test it in the Foundry. AIP was not included in the any hard fork, there is no even notion for it to be included. So that was mostly it. Few months after, AAP got CFI'd so it potentially can be included in Shanghai, but either way there is not like strong guarantee. Shanghai happened, A AIP was not in sight. Next, Harfork, Cancun. July, PR was made for that AIP. August, it was merged. In March, the AIP landed on Manet. Approximately four months before that, the AIP should be made inside the clients for the testing, for the damn nets and for test nets at the end. Another example was 3074 and I want to say all the requests that champions made were reasonable. It is from their point of view but the question that I had is like should I have started working on the API right away when I first request comes? It's a little bit silly because I have different priorities. I cannot do everything. Should I just merge PR when it was made? The question with that is who is going to do maintenance? Who is going to remove it if the AIP was not included? Do I do it for every IP? That would be a lot of unnecessary code. And in the end who is going to maintain all of that? Second problem with the maintainers is how to add testing dev experimental features. One of those I always try to facilitate and enable big projects that depend on our EVM to give it support, to give a way to make tech functionality happen. Sometimes there's like features, Rust features that basically enable those things as configuration, but sometimes if this is only the EVM project simple EVM project our EVMC R55 integration that require different execution environment would not be possible and problem three is chain support most of the end chains have small difference and they're not like, they don't want to move away from EVM a lot. They're most like new chain spec hardbox, new transaction types, maybe few AIPs, but in general they're like small difference from the mainnet, the mainnet EVM. Only way to do that previously it was like to fork the project and that brings the maintenance issue and burden of maintaining to basically repos. So solution for all that is to make EVM framework. I think that's the the last stage of REVM and its endgame. What do you mean by that? Let's make the code extensible. Chain can use REVM as the library, override the functionality that they need, add new transactions that they need and just use it as a library. That makes the core of the library same across all the chains. Implementing new AIPs that are on the mainnet would become easier and all the chains can just reuse the code. Tooling could create their own custom way to inspect. Maybe they need more performant way. Maybe they need need RVNC, needed new execution environment. New API can be implemented and tested separately. And, for example, this is the first look how basically I'm not coming to the code a little bit. But, yeah, the main, Spectrum main, Optimize, Spectrum Optimize. You will have different types for your different EVM that you need idea came initial idea came few years ago I looked at TV and bomb and they had like array of functions and area functions that was different by fork. So my first idea was, hey, can we introduce custom instruction and make it like allow users to implement or add their own custom instruction? That idea expanded. So it's not just on instruction, but on full logic. And that full logic function overriding was the handler from the last year I think it was very great idea but implementation wise it was like box or a pointer of function that was not the greatest and more flexible way I liked the insights that I got while implementing that and the EVM framework introduced two things. It introduced generics on the data, on the section, block, config and it reworked the handler in a more trait-like way. So it's more easier to use few examples of that is basically how are basic of the optimism that was the first few that was first and it was included our EVM in sense of the it was included as the feature. It was very like, intrusive. With the handlers, I tried, I succeeded to extract some functionality to the handlers. But with EWM framework, that became even easier. Currently it stands on Crate. I still have work to do on EWM framework, but it became a possibility. Another chain is called, they did the similar thing on their own repo. Just imagine like every EVM chain doing their own creates and having Foundry, RET or all the tooling support those variants of REVM. And all be tested, all be done, and basically it eases the integration with all the stack. The split that was inside the head with the handler is there is split between the data that is currently context and the handler that is the logic part of REVM. Context contains intersection, block, usual things. It contains journaling state that allows you to revert things when the revert happens in the calls. And it contains database that fetches the runtime data. Transient storage and warming of account is all done in journaling. Handler on the other hand has four parts. Validator that validates the interaction between block, transaction and config. Config can have chain ID that needs to be checked with the transaction chain ID, for example. Prevalidation, do some warming, do some deduct the call balance, do AIP 7702. Yeah, change delegate code to the state. Execution is a little bit complex. It has two main loops. The frame is the loop around the calls and interpreter is loop around the bytecode, basically instructions. And in the end, we have post-execution that does refund of not spent gas. It reimbursed the caller, it basically rewards the beneficiary and creates the outcome of the execution. In code, as you see, context has a lot of generics and it allows you basically to have it generalize. and it allows you basically to have it generalized. There is some to-dos to do. Spec needs to be moved inside configuration, but in general, this is the first view of the new context in EVM framework. Validation bar, one of the four stages in the handler, has basically self-contained. It contains associated types of the context, an error, a few functions that need to be called. Other handler types are in similar fashion made. And you can have Ethereum validation that basically does the Ethereum-specific implementation. Many VMs become just the context and the handler. And to create your own EVR, this is an example of main EVM, it's still pending and it's probably going to be changed in small or maybe medium way. You basically specify everything you need. For example, you have your context with some predefined data. The section and, for example, the structure inside the REVM, but you can add your intersection limitation on it. And you can specify the handler that has their own functionality. Execution, Ethereum Executor has a few fields that are maybe interesting to people. They have a pre-compile provider and instruction provider where you can generate your basic list of pre-compiles and there is a trade that you need to implement for this to happen. On the other hand, this is an example of Inspector. Main EVM is still a work in progress, but the idea is you create your own Inspector types that implement those trades, and you can just override it. In the end, this inspector main uses the DB and inspector generics. And this is how you use it. There are probably going to be some helper functions that allow a little bit more flexibility and utility. But I wanted to show you just an example of how this could look. In the end, I would assume a lot of users are going to come and implement their own EVM or their own extensions and just use our EVM in that form. I think that's it. Thank you very much for having me and that's it. Thank you Dragan. It's really amazing what you have managed to do all those years with REVM. So let's start with our questions. The first question is about testing. So do you use any kind of equivalent testing or basically differential testing as part of your CI? Yeah. or basically differential testing as part of your CI? Yeah, on every PR, state tests are run. Basically, state tests made by Ethereum Foundation, the testing team, and it's like a good first line of defense if everything is okay, if everything works. Other than that, there is fuzzing team, fuzzing projects that are run in the background by Ethereum Foundation by I think Martin Razet from GoEVM lab from GoEthereum but GoEVM lab I think is project but yeah it's there are few stages of the testing done and this is based on every new PR? Yeah. Fuzzing is done in the background. It's not on every PR, but the state test, running it on the web assembly, some specific Rust target, running it on different targets, basically allows you to test all those things. So are there any early architectural decisions that you have regret? Yeah. I think I didn't understand how the calls and everything that works. I still struggle with that. I didn't find a good fit on top of it. The frame and how the frame basically works with other frames, it seems it's nice but still need to land on the good abstraction on top of it. Initially the calling, the sub calling was behind the basically you call your sub call and stack became the problem so I needed to move all that to the the loop so yeah there are a few of those things the next question is if there are any plans to natively support zkvms like SP1? Because currently you need to do some hacky patches to be able to run them. We should talk with them, basically. EVM framework is an idea that basically would allow and support this. Great. Another question, again related to ZK EVMs. So, REVM is now used on both CPUs and ZKVMs, and these two environments have very different performance profiles. How can you optimize for the one or the other or both? In general everything is Rust compiler so it would compile optimization of Darm basically on Rust compiler so the target is different zkvms use RISC-V while the CPUs has different instruction set that are made more performant in general I didn't do the testing on the on top of it so that would be like good way to check those. I guess a follow-up here is, and probably you haven't checked yourself, and that's more about the ZKVMs, but have you any intuitions that some LLVM optimizations might be better for ZKVMs or may be better to not use some optimizations? Compiler optimization, basically. I'm not sure. I didn't look at it. So what is missing in our EVM to be more performant than EVM1? I have some tasks in my backlog that require basically testing. Every change that happened on the performance side needs to be tested, measured, and validated that assumptions that I made are correct. So I need to look at tail call optimization to check it. I need to look at the stack verification if this is extracted from the instruction to before it starts with the main loop, if this is going to affect the performance. Is it easy to add new precompiles? Yeah, it was one of the use cases that Odyssey used a lot and it was very easy. It's one of the biggest use cases of how you can extend, easiest and biggest ways to extend the REMM. What's the most interesting use case that you have seen so far? ZKVMs was really, really unexpected, to be honest. It just landed, and it was like, hey, it can be used in that way. I didn't expect that. I made it nice to use it in maybe front-end so that JavaScript can be implemented on top of it. Maybe it can be used in Wasm as some project that does that way. But ZKVM was an unexpected surprise. And it was amazing in general. Let's thank again our speakers, Dragan, for the excellent presentation. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Eqr32OyHNOUkt06oQXAiVNTwZse9uMoY_tw7Ag2SkQs",
      "resources_slides": "https://drive.google.com/file/d/1HnfriRiCckDzm4cBDxZ0lpCUrSQhqOfC/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "wallet-infra-improvements-and-building-apps-for-the-next-generation",
      "sourceId": "RQAAFS",
      "title": "Wallet Infra Improvements, and Building Apps for the Next Generation",
      "description": "In this talk I go over infrastructure innovations that are happening in the space right now, how they’re going to be how we bring the next wave of users into crypto, and why right now is the best time to build a consumer app in this space.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Accessibility,Account Abstraction,Architecture,Frameworks,Gas,Intents,Payment,UI/UX",
      "keywords": "wallet,dapps,",
      "duration": 538,
      "language": "en",
      "sources_swarmHash": "cb6c67ca170aa1455e17b9c03261ca9968617fe7b6d4a82cb7a707ec3e730e25",
      "sources_youtubeId": "GUZ33tEdOJw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67357fa59dbb7a90e11d9f1f",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67357fa59dbb7a90e11d9f1f.vtt",
      "transcript_text": " Cool. Hey guys. So as you can see, I actually changed the title of my talk really last minute. But my name is Meida Kothari and I lead product for all of the wallet related things at Uniswap Labs. So in this talk, I want to cover some of the incredible infrastructure innovations that have happened over recent years, and why I think they're going to bring in the next wave of users in crypto. Forgot my little thing. So when we talk about onboarding the next billion users, often the narrative online is focused on blaming crypto UX for why we haven't had mass adoption yet. But what does better Crypto UX really mean? Like if you look at a lot of our apps, they're already beautiful, they're clear, they're comparable to Web2. And based on recent breakout successes for dApps like Polymarket or TradingBots or whatever you use, I feel like it's safe to say that users are willing to overcome a lot of friction to do what they want. In my opinion, what crypto often lacks is more opinionated products. And what a lot of these infrastructure innovations that I'm about to highlight do is give developers optionality to build more opinionated products. Historically, I think we've put a lot of burden of choice on users for every micro decision, whether it's how they store their seed phrase, whether it's how they pay gas, whether it's what chain they're on, everything else. So I find a more useful framing for the crypto UX problem is how do we make it easier for users to do more with their balance? Before I get into these innovations, I kind of wanted to set the framing and identify core problems to a really quick case study. So for this, I'm going to use my dad as an example. He's a software engineer. He's technologically savvy. He trades a lot on apps like Robinhood and E-Trade for stocks. And he knows what ETH and Bitcoin are and holds some but doesn't really know much about it. He doesn't really know about ERC-20s, L2s, the underlying technology, anything else. So he somehow finds out about a token. This actually happened a few months ago. Let's call it Blah Token. And he asked me for help. And so I helped walk him through how to buy it. So I walk him through the process. He only has crypto in his Robinhood centralized exchange account right now. And so I had to guide him through downloading a non-custodial wallet, explain what that non-custodial word meant and then what a seed phrase was and why it was so important. And then I had to walk him through transferring his ETH from the centralized exchange into this new wallet. But oh, it turns out the asset was on base and he didn't have ETH on base to pay the gas and so I had to explain what bridges were, what L2s were, what gas was, and then we had to bridge his ETH over to base. And then he was finally able to swap the asset using Uniswap. But of course, he had to reserve some of that ETH for gas. That took so many steps, so much education, so much time. And one of the biggest issues I think all of us see in the space right now is onboarding. And my dad specifically in this case was a very high intent technical user. And it took him several minutes to create a wallet and was thrown so many new concepts right up front. Currently, I feel like we're taking high intent users who probably come in wanting to do something, wanting to try out a wallet or buy an asset, whatever. And we end up surfacing so many friction points for them that they end up churning before even completing the action that they initially set out to do. Luckily though, there are so many infrastructure advancements that have been made already that make this whole process easier. There are quite a few, but for the sake of time, I want to focus on four. So the first one is account abstraction, specifically through EIP-7702. So account abstraction and smart contract wallets have been in the zeitgeist for a while now, but one of the biggest issues with most of these implementations is that they're not compatible with EOAs, which are the most popular form of existing wallets. 7702 basically allows EOAs to get some, not all, of the benefits of account abstraction without forcing users to fully migrate into a smart contract wallet. So while this isn't fully comprehensive account abstraction, it does two things that I think are really exciting. One is gas abstraction. So in my dad's case, he wouldn't have to worry about the concept of gas or gas tokens, etc. And another thing it does well is batch transactions. Second is this growth of multi-chain standards. So it's very clear we're going through a multi-chain world. Block spaces are getting more and more specialized. And I feel like we don't give enough credit for how much Ethereum has evolved here. Like I remember just even a few years ago, going from one chain to another was so, so cumbersome. But now with intense space standards like ERC7683, which a couple other speakers before me talked about, and with block-space standards like the super chain that Optimism came up with, and just generally more options for different types of bridging, we have way more of a unified experience for cross-chain interactions that are happening on-chain. So now it's up to dApps to figure out how they want to represent these multi-chain interactions, like how much they want to abstract and how much they don't. But it is so, so much easier. The third thing I want to mention is embedded wallets. So as I mentioned before, onboarding is a huge, huge issue in the space right now. And all the innovation happening around embedded wallets allows users to create wallets in seconds. And also users can use familiar concepts like email or social auth or even pass keys, which are becoming increasingly popular outside of crypto. And sophisticated traders are already taking advantage of this tech when they're using certain telegram bots and things like that, or dApps that are using things like Privy or Capsule under the hood. The last one that I want to talk about is AI. So can't forget that as much as our industry is innovating, so are others. Everyone I think is aware of the massive impact AI is having on consumer apps right now. And I'm pumped about the confluence of that and crypto. And I think there's a ton of stuff here that I could talk about with AI, obviously, but the ones that I want to focus on are context and discovery. So with AI, we can rapidly give users more context and information on the actions that they're trying to do on chain, as well as the assets that they're trying to buy. A big issue, another big issue I see in crypto right now is discovery. And I think fine-tuned AI can really help with this. So clearly right now is the best time to build consumer apps in crypto because of all this infrastructure innovation that's been happening. So I want to end with a call to action to builders, especially new builders. All of these new consumer app devs, I hope you guys build more opinionated apps utilizing this tool chest of infrastructure to give users the most smooth experience possible. Thank you. Thank you, Medha. And we have time for a question. Let's see. Okay. And there you go. You got it. Thanks for the talk. It's very inspiring. I would like to ask maybe a Medha question. If there's a top three app, because we're on a consumer app, or you want to build, what would be the top three you can recommend for us to build on that? Did you say top three dApps I would recommend building? Did I get that right? Oh, yeah, yes. It's more like you want to get your thoughts on that. For sure. That's a great question. Let me think about this. There are so many exciting things right now. I would say one is just like a better information aggregator, so something that's like more personalized based on your trading history and also like assets that you own in a fun way I think that would be really exciting another one is like a multi-chain dApp that's like super completely abstracted so I saw some demos for this that some optimism devs did and I think stuff like that would be really cool where the end user doesn't have to care about the chain that they're on, but they're able to interact with people who are using multiple chains. Yeah. Thanks for the question. Lovely. We have time for another question. Yes. Just speak up. In your opinion, what's the biggest drawback to 7702? That's a great question. I think there's two. One is smart contract risk. So you're introducing just more smart contract risk since you're delegating to a contract. And then the second one is just lock-in. So if we don't standardize the 7702 implementations that wallets do, it could lead to a case where, like, let's say I have a seed phrase on, like, Uniswap wallet, and then it implements 7702, and I get all these, like, cool features like gas abstraction, but then I export that private key and put it into, like, let's say Metamask or Coinbase wallet or whatever, and then suddenly those guarantees are gone if they're not using the same 7702 standard. So that's something I do worry about. Thank you. Thank you for your questions. That's all the time we have for them. Thanks again, Mera. Give it up for Mera.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T04:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1eJwIYkq9W94rsLobC0VKWwi7AVWG4wvUfj48LQs1f8k",
      "resources_slides": "https://drive.google.com/file/d/1DyNrIWI32ba7eJrfAi3fMYMfzGXP66GS/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "who-wins-ethereum-block-building-auctions-and-why",
      "sourceId": "VKQ8NC",
      "title": "Who Wins Ethereum Block Building Auctions and Why?",
      "description": "Today, top 3 block builders produce over 90% of blocks on Ethereum via MEV-boost auction. The block builder market's dynamics evolve rapidly and has significant impact on the development of private mempools, wallets/apps orderflow auctions, and censorship resistance topic. In this talk, we share an overview of why the top builders win the most market share, using orderflow composition and bidding behavioral data. We hope to highlight the centralizing risks and failures of current market design.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "blocks,auction",
      "keywords": "MEV,PBS,Block Auction",
      "duration": 1465,
      "language": "en",
      "sources_swarmHash": "9b726abfb28b4a0b5f846390cde2e1cf0c5dea9ef98862d60dc5e6d80f4b8c49",
      "sources_youtubeId": "fP9PFx1ooQE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735848a9dbb7a90e1e9de85",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1sCbCcL_kcX8oEU3I_BJLpuFgt1wzgpYDENnympxQ7iI",
      "resources_slides": "https://drive.google.com/file/d/1pNpXn9zpIEXiWMd7WM8K9oAkPovJ39kc/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "yeomenai-mud-day-demo",
      "sourceId": "7DGLCG",
      "title": "Yeomen.ai - MUD Day Demo",
      "description": "This is a project demo for MUD Day CLS: onchain games and non-financial applications. \r\n\r\nYeomen.ai is building dashboards, automation tools, marketplaces, and platforms for autonomous worlds and onchain games built with MUD. Rohan will showcase some of these tools in this demo session.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Gaming,Autonomous World,analytics,Autonomous World,Gaming,Tooling",
      "keywords": "",
      "duration": 289,
      "language": "en",
      "sources_swarmHash": "9609159b76f8b938655bcfa12a870a0cd47ee7b167a065e54b727047328c4b6e",
      "sources_youtubeId": "qWTamrBBH2E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673585b79dbb7a90e106bdd7",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673585b79dbb7a90e106bdd7.vtt",
      "transcript_text": " Hey, hello. My name is Rohan. I'm from Yeoman. How many people here are game developers? You build games. Okay, and how many play games? Great. I have something here for every one of you, gamers and developers. So what I want to show this morning is my email. Yes, so we have dashboards. We built dashboards. So all of the data is on-chain. All of our game data data everything you play is on chain we wanted to visualize it and this is useful for both developers and gamers so let me jump into one of the games biomes so we what we do yeoman is we index all this data we then create charts and graphs that's helpful for both the developers and the gamers. So here we have biomes. We have like a million transactions indexed, and we create these charts. For example, here what we show is like most of the actions in biomes is mining actions. You have move, quite a few things. As you scroll down, you get to see activity over the lifetime. It surged in the beginning of the launch. It was quiet for some time, and we see a huge spike now. And, yeah, that's in biomes. Pretty cool these days. And as you scroll down, you get to see a lot of data. Again, all of this data is on-chain, but then we make it presentable. We make it visualized. When you visualize it, you quickly get an idea of how all this data looks. It's pretty cool. And as you scroll down, you get to see lots of charts, graphs. We have all this data indexed on chains. So you just can create all these chains. So you can chess. If you are a gamer on biomes, if you're looking at mining, for example, it's pretty early days. We have about 6% of the Neptunium mined at the moment. And as you scroll down, you get to see lots of charts. This is interesting. Biomes is a 3D world. We have a 2D representation of what is mind and what's not. So the red represents all the ores, and the white is all the ones that are mind. So this is useful for gamers and developers. And let me scroll down to the last chart. So this is all the movement within biomes. Everybody that's moved on, it's all plotted on a single 2D graph. You can see the high traffic areas in the middle and then it's quieter at the sides. As a developer, if you were to visualize your game, you get to see things like, hey, there's a hot spot here. You can probably design, you can probably build a bazaar somewhere farther, or if you want to build, be a gamer, and if you want to see, hey, where to position your wonder that you're building, you could be like, go on to a high-traffic area or something like that. So all of this data is on-chain, and this helps you quite a bit. So that's that. Okay, let me switch on to another game. So, okay, I've got two minutes. So this is a game called Drawtech, which was built by Smallbrain a while ago. What I showed you now is all the data that we've indexed. We then were able to query it, plot it into charts and graphs and all of that. I want to show you something else interesting. So what we did was we built a time travel feature for mud indexer. So basically what you see here is the canvas on DrawTech when it was launched. So this was the first block. This is probably small brain. Somebody went in and tested this. They put a tree in there. If you click on next, what it'll do is it'll fetch the very next block. This is all using a mud indexer. It goes to what it was at the point in time, or rather point in block. It gets all that. Click on next. You can see how the whole game developed. As you click on next, it goes to every subsequent block. It fetches the canvas, the colors. You can see who it was and all of that. So you can see the whole development of the game. You can go to the very last one. This is how it looks at the moment. And then if you go back in time, you can rewind from where it is. You can go back and all of that. So all of this data is on-chain. We're making it presentable, pretty, accessible, and all of that for developers and gamers. And if you... If a project's not listed here, please reach out. You just fill out this form. Send us your ABI. Send us all the information. We can get it indexed. This afternoon, we are talking about our cool new project called Command. It's pretty interesting, but I need more time to talk about it. And we'll be presenting it then",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:30:00.000Z",
      "slot_end": "2024-11-14T04:35:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1D2DHsWzGk1OOmOYP0VkdpHHHgEYGIOx9nMKOiTdQw-Y",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "ethereum-in-the-classroom-or-teaching-solidity-to-high-school-students-in-buenos-aires",
      "sourceId": "9HFAES",
      "title": "Ethereum in the Classroom | Teaching Solidity to High School Students in Buenos Aires",
      "description": "ETH Kipu is breaking new ground by introducing Ethereum education to teenagers in Argentina. Discover how we collaborated with the Buenos Aires Ministry of Education to create hands-on learning experiences, teaching students to build smart contracts using Solidity. This talk will share best practices from our experience and how it can be replicated globally, sharing the insights we have discovered in the classroom and how we develop this partnership.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "Design Thinking,Ethereum for Good,Public good",
      "keywords": "Education",
      "duration": 457,
      "language": "en",
      "sources_swarmHash": "a724073c7b9bc55d470144a87b5f8abf0eac448ffd43ef9515329272cfceb31b",
      "sources_youtubeId": "1HOOnlu1qQ4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736df2a1b0f83434d8917c4",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:40:00.000Z",
      "slot_end": "2024-11-14T04:50:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1clRG027QMaA-_D-yds9TfGuZXmzRy5tpHKs67z97Mqw",
      "resources_slides": "https://drive.google.com/file/d/1Fl3xaDogBkq8Rckhi2JkasdvrsQHzbIh/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "from-mpc-wallets-to-smart-contract-accounts",
      "sourceId": "XMTH8N",
      "title": "From MPC Wallets to Smart Contract Accounts",
      "description": "The proposal outlines a path for the mass adoption of smart contract accounts by using MPC wallet as a transitional solution. Users can start their web3 journey by using MPC wallets which can be done via social login. Later, users can turn the MPC wallets into smart contract wallets using EIP-7702, enhancing the user experience with feature-rich options while maintaining the security benefits of MPC wallets to protect the EOA private key.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "MPC,Account Abstraction,eip-7702,Account Abstraction,MPC",
      "keywords": "EIP-7702",
      "duration": 537,
      "language": "en",
      "sources_swarmHash": "8a617b801597c6e5869ce8d95eece6aff69b5e740275a7e9e649ff2757671a30",
      "sources_youtubeId": "Yr0AS9QifjU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673581a69dbb7a90e14ff55d",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673581a69dbb7a90e14ff55d.vtt",
      "transcript_text": " Hello everyone, my name is Phuc Thai, I'm from Sky Mavis. So today I'm going to talk about how already understand that there is a lot of issue with the current EOAC that it's too complex for some non-technical and even technical users. There's inaccessible recovering mechanisms. accessible recovery mechanisms. So to solve this problem we have a few solutions in place already and I want to highlight two main things is MPC wallet and archive collection. So both of them can offer a lot of help with onboarding new users, offer social login, key recovery and some transaction policy. So the difference between MPCs and account protection is most of the things MPC do is on the off-chain, including query-recreation and transaction policy. Whereas on account corrections, it has more transparency and everything can happen on-chain. And with account corrections, users can also have more flexibility in a lot of feature for convenience suggests batch interaction, gas sponsors and automation and something but account collection is relatively new compared with the MPC technologies. It is 40 years old technology already and it's ready to be adopted right now. So yeah, so one of the problems with MPC is that users cannot have the flexibility that account protection offers. But with the introduction of 7.7.0.2, we can actually combine it with MPC to offer a great have. So this is a picture from Vitalik Blocks. As we can see here, with MPC and 7702, we can achieve most of the goal, all the convenient goal that account ratings still can be achieved. And some of them are critical. So, such as like here, we can see that automation can be done if we can delegate the account to some key data store on the server, get attractions, sponsor, and everything. So the reason that 7702 can be a great combination with MPC wallet is that the current infrastructure of MPCs already in place that can help with some of the features of 7702. want to highlight here is the privilege de-escalation. In MPC, we had multiple key sats that store in the user device in the server that we already had a system in place for this. And with 7702, we can set the MPC account to set code for that account to enable delegation, so some access to the key set that in either in the client device or in the server that can do some of the operations that user can do day to day. some of the operations that users can do day to day. And so another thing that I want to mention here is that one of the problems with current MPC is that when we select a threshold for MPC, we usually need to involve a trade-off. For example, if we use like 2.0.3 threshold, some transaction can be signed without the user knowing. But if we use 2.0.2 threshold, then every transaction needs to be signed by user, but the key cannot recover if the user lost the key shot and they have not had any backup. So with the 7.7.0.2, we can introduce a time-locked on-chain key recovery that we can have either key shard can be gain full access to the wallet, it initiates a transaction, and then the other one do not challenge that in, let's say, 30-day periods. So I think that's it for now. Thank you. Thank you, Phuc. We have some time for questions, so raise your hand if you want to ask a question, and I'll get the box to you. Or if you're further in the back, a colleague of mine will do it. I know you want to know things. This is your chance. If not, then I'll ask a question, and trust me, you don't want to hear my questions. They're not going to be that good. Okay, well, I'll still do it. If you want people to come away from this with one thing and start implementing that in their work, what would that be? Can you elaborate that a little bit? So we all go back home after DEF CON and you want people to do one thing. What's the thing that you want people to do? So I think the thing I want people to do is to push out 7702 as soon as possible. Yeah, cool, cool. Anyone have a question? Or shall we call it a day? Yes, amazing. All right, I think I can do it, maybe. Yeah, okay. And one, and two, and woo! Ah, almost. and two and whoo ah almost okay so my question is a little more on the gas abstraction part i i know mpc wallets can be um connected to aa wallets and um yeah uh my question is, is there another solution that is more sustainable compared to just sponsoring the gas for every transaction? Yeah, can you elaborate a little bit? Yeah, so some wallets, because most of the non-crypto native users don't have a concept of gas that they have to pay for each transaction. A lot of protocols and applications sponsor this gas so that the user can continue with using the application. And the way they do it is that they fund the paymaster and that paymaster pays for the user's transaction fees. And yeah, my question is how, are there any solutions that you think can make this sustainable for the applications? So I think, so that is a good question. So that is a good question. But I think the sponsor, so this comes from that if the app developer, that they want to be more convenient for the user. So they want to onboard more users. So one of the things they can do is that they can ease out the onboarding process and pay for the gas for the user. So I think that can be a great help in this case. All right. That's all the time we have for questions. Thanks again, Phuc. Give it up for Phuc for the wonderful talk.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:40:00.000Z",
      "slot_end": "2024-11-14T04:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1ZE8L3c1yymoZrVimyFHEaxXRlckYyGWHcRVxv5R5bzQ",
      "resources_slides": "https://drive.google.com/file/d/1Oyamd37Uabpp1cerk4t6hCToJ7ecIQsd/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "putting-identities-on-chain-passport-zkp",
      "sourceId": "HBH3Y7",
      "title": "Putting Identities On-Chain (Passport ZKP)",
      "description": "Discussing the creation of an on-chain registry system for storing zero-knowledge (zk) identities. This system will enable individuals to self-issue and control their data without a central authority, showcasing the ZK passport use case.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Digital Sovereignty,Identity,Permissionless",
      "keywords": "ZK passport,social graph",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "711c0f65607fb40239c8612823886e2a83225b79b042bf0b6ab91efe797f7c92",
      "sources_youtubeId": "acy5cANoAuQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:42:00.000Z",
      "slot_end": "2024-11-14T04:50:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1zX7rgpH4mVoH4btzraVJBbogPvAVttN57Twcs55Kb2I",
      "resources_slides": "https://drive.google.com/file/d/1BKKWrg9BM1H-ZORuZec2Q7DLppx_2i4j/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "my-mother-will-not-use-it",
      "sourceId": "HKKFQX",
      "title": "\"My mother will not use it\"",
      "description": "In this Talk, I want to cover the different mindsets designers need to improve and optimize the work for web3.\r\nIf we're going to change the way we interact with each other and aim to profoundly improve society with this technology, we can't think and use the same methodologies.\r\nWe will cover topics such as the target audience (the title of the Talk), testing, the learning curve, web2 to web3, and more.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Design",
      "featured": false,
      "doNotRecord": false,
      "tags": "inspiration,Design,Design Thinking,UI/UX",
      "keywords": "Inspiration",
      "duration": 498,
      "language": "en",
      "sources_swarmHash": "ad423cbfb70b66caa453adc36d1d22548841d8553d166dad612d7ad3d0a943f9",
      "sources_youtubeId": "147hrjj2onM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673584719dbb7a90e1e74aca",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673584719dbb7a90e1e74aca.vtt",
      "transcript_text": " So my name is Nuno, designer at the foundation. I work for Ethereum.org mostly. That's where I spend all my time and this is my talk. I submitted my talk based on this tweet from Wesley that he said my mom will never use crypto. Trying to optimize for her is a waste of time and this triggered something on me because I used on my other talks this is for each Prague back in June I guess I had one slide with the same title amongst the other things that I normally do and it's something that I it's a rant mostly why that this is something that we hear a lot out there. In other conversations out there, like, my mother will not use it. Oh, this app is not for my mother. It's not for, like, it's something that always gets me a rank that's why my mother, why the elder generation would use this? That's something that it's almost a rank. So let's start with the question. When did a mass adoption of a technology started with an elder generation? Can we pinpoint something? Can we like, top of our mind? I can't. That never happened, I would say. Never ever happened. Like take a drink of salt. I did not research this. I'm just saying out loud like might be something out of this. but I would say that it never started and we are on the brink of a revolution. So let's break even further. So why, who is my mother? Let's try to persona who is our mother. So this is my mother, my mother, Defcon, Defcon, my mother. She's a lovely lady from Lisbon, so nice to meet you all. She doesn't know he's here, so don't tell her. I'm doxing her. So, again, let's see who is our mother. Our mother might be like roughly, if average 30 years old in the audience, if she gives birth at 25, she might be 55 plus, something like that. Let's try to put her in this level. She'll be around this gap, which will make her a baby boomer and a Gen X, beginning Gen X, probably halfway through Gen X. But this is probably our mother collectively. Classifies the generation that we are sometimes thinking of who's being adopted. So she watched this You know what film this is? show hands Okay, some do She grew up This technology this was the technology that she was exposed at an influential level like all of this Probably some of you still use this who who got one, show of hands, yay, some, so was impactful, but in the end, like drilling down that analog generation, high trust in institutions in the sense that they grew up like trusting corporations, trusting governments in some countries, I don't know, But looking forward, post offices, banks, other corporations, was something that was trustworthy. Limits access to technology. They don't know how things work. I ask you, probably to you folks, how things work. And it's a cultural shift for them to onboard new technology, new stuff on a digital level. So this is my second conclusion. Why do we even bother thinking about them when we are building products for Web3? So I'm almost sure that everything that we will build will start with a young generation. This is one of the key points that I want to bring. Regardless of what we build, they will follow. So let's stop talking about, then my mother will use it. So to Wesley's point, this is a waste of time, probably, to think about them as user profiles, as personas on how to target our audience of our products. And even more, to all the Gen Zs and Alphas out there, there's a bunch of you out there that points out, you guys and all your friends and all your relatives about the same generation will be the ones that are going to use the Web3 project that we are building right now. They will have the financial upside very soon. So there's a bunch of information that the great wealth transfer will happen very soon because of the baby boomers through the other generations, they will have the financial incentive to do so. So it's not a question of money to target someone that has money. So we need to focus on the younger generation to build the new set of products. They will be the next billion users. So let's focus on them once and for all. So let's stop saying my mother will not use it. Let's start saying the youth will not use it. My kid, if you're a parent, a young parent, my kid will not use it. Probably the TikTokers will use it. Probably the Swifties. Who knows? They are the ones that will probably use our products. So this is the key point that I wanted to bring you guys. Let's start building because this is to you and for you above all and think about them and don't think about your mother, parents, grandmother, whatever you think like this. And that's what I have to you today. Thank you. Thank you Nuno. We have time for some questions. Raise your hand if you would like to ask one. It can be about your mother. It can be about Nuno's mother. Don't ask me about my mother. You don't want to hear that. Yes, yes, yes. Come on. Last push. There is a break after this. Ask a question and then go out into the world knowing you've accomplished great things at this session. Question, question. Okay, while you're thinking... Everyone is angry, I think. I know, I know. While you're thinking, I'll ask one question of my own. So same as the one I asked before. If you want people to take one thing from your talk, go back to their normal life, what's the one thing you want them to take away? Mostly as a designer, I target designers as well, other builders that are thinking new products. Keep critical thinking on the way you approach. I think we take from granted everything from Web 2.0. And I've seen a lot of talks, even the one before us, that's always like, let's do Web 2.0 way of doing Web 3.0. And it's mostly one of the topics that I bring in my talks. Probably I'm a little bit older. Probably it's my generation. I've built something of Web 2.0 I was around. And we've did a lot of mistakes. And sometimes we are passing those mistakes to Web3 as well. So be critical. Think a little bit beyond just what is presented, what is easy to present to the user. And this is one example. If the maturity of our technology that we're building is going to be like three, five years in a row, like start building for the generation that will use it. Not the elders, not the ones that probably now have the funds to invest in you, something like that. Target the ones that probably now have the funds to invest in you, something like that. Target the ones that probably will use it. Those will be the next billion. That was the thing that brought me to the talk. Perfect. Thank you. Anyone has come up with another question in the meantime? Yes. Amazing. Oh, this will be hard. Okay, yeah, let's do it this way. Okay. hard can maybe okay yeah let's do it this way this is a microphone okay uh what do tiktokers want that's the real question that you all need to figure out that's that's not an easy but we kind of know what they what they look like we can profile them at this point and that's one of the key highlights like we know what they look like. We can profile them at this point. And that's one of the key highlights. We know how they use. We know the interactions. We know what engages them to probably use the product that you're building. We like profiling. As users, we kind of know them already. So we need to understand them better. All right. Another question? Nobody? Okay, on that case, we'll conclude this morning's session. Thank you so much for being here. Thank you, Nuno, again.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:50:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1phw7po5lIFL6aJaipzIR4HdBRmhdugA212mJKjaQfoc",
      "resources_slides": "https://drive.google.com/file/d/1-cq2xdTKBFwYb2HlpG4kvTjh9nZ8pohd/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "proof-of-personhood-panel",
      "sourceId": "GVML7H",
      "title": "Proof of personhood panel",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 1226,
      "language": "en",
      "sources_swarmHash": "69ebe38a57c1f0da151727dc18122546f6a0d7d37ed10de07a0bf33c35098d32",
      "sources_youtubeId": "42cJ6IZlhMk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673592a79dbb7a90e1088fac",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673592a79dbb7a90e1088fac.vtt",
      "transcript_text": " Hello, hello. Great. So we just had a nice overview of the three protocols. And what I want to do in this small amount of time we have is just challenge each one. I think there's something to learn from each approach here and see if we can come to some greater synthesis. So I'll start with Remco. So Remco, I actually wrote a critique of proof of personhood. It's called Compressed to Zero, The Silent Strings of Proof of Personhood. And what I did is I looked at a parallel protocol to WorldCoin, a different authentication mechanism. But they had succeeded in verifying unique individuals through cognitive tests. And they had avoided the problem of account trading, which WorldCoin has had to grapple with because each account has money. And then, of course, there's an incentive to sell accounts early on and so that's something you're grappling with. They had solved that with this novel mechanism called sublinear identity staking, which I'm happy to talk to you about later. But they were still left with this information game theoretic problem around puppeteering, which is that when you have an account and it has monetary value, it has a UBI stream coming towards it, which is one of the goals of WorldCoin, you create incentives for other people to try to control those accounts and extract from them. And so the kind of pithy way to put it is when you try to differentiate humans from bots and use UBI incentives, you actually create an unintended consequence of humans trying to control other humans as if they were bots, right? And so the control problem becomes an information problem, and it's an information game theoretic one. So I want to know how you think about this challenge, because there are two goals that are very explicit with WorldCoin. One is both UBI and a kind of rails for democratic governance and even maybe even AI alignment. But if ultimately, you know, pursuing both of those goals ends up in coalitions and oligopolies and basically politics, right? And one person, one vote breaks down very quickly in a coalition. It's like, how do you deal with that? How are you thinking about that? So we're at 16 million users now. 16? 8 million in the highest tier or verified unique human. And in the process of growing to this scale and being active in dozens, if not nearly 100 countries right now, you kind of see everything that people can throw at you. And while the vast majority of people is honest, there are some very dishonest actors out there and you have the Byzantine generals' problem of you need to make your system resistant against malicious actors. I think this is not, as you mentioned, this is for every proof of personhood primitive out there. There is this issue. As I said, proof of personhood is not a CAPTCHA. So there is this idea that you can delegate an account. This is not something I think we should prevent, but it is something that you need to give the user the tools to properly manage, which means that it needs to be explicit, it needs to be consensual, it needs to be revocable, and so on. So I think the emphasis is on developing those tools, empowering the user to control their world ID in the way that they want to control it. Blockchains as a whole are not particularly good at the moment at these kinds of things because a lot of decisions you make on chain are one-way streets. But there are a number of things we are already doing here, like allowing you to recover your account by going back to an orb. One thing we've rolled out that is very powerful is using the phone's front camera to do additional interactive face authentications to make sure that the wallet hasn't been stolen. It's still in the possession of the original human that signed up and so on Right Maybe I can Comment there directly so I made in this presentation my claim that I think this absolute identity They're actually not that many use cases for it and kind of just to give the extreme example Why I'm making this claim is let's say somehow North Korea does get hold of one of those orbs and and can can we are properly authenticate they're all there 25 million I'm not exactly sure how many people they have probably no one knows but 25 million people so let's just say in WorldCoin now you go from 8 million fully verified to now 32 but 25 of them are just people from North Korea where it's not even clear whether they have control of their key or probably it's somehow centrally organized that someone ultimately has full control of those keys. Now you have 32 million properly verified actual humans. The question is, for what application would that set be useful? So my claim is, in absolutely most applications, and probably actually this gas subsidy that you're doing is one of the few where I think it is useful, but in absolutely most applications, I would say you need more context than just being human. If you want to go even more extreme, we can think of this Matrix movie where they're literally, I mean, in the Matrix movie where they're literally farming humans, or it kind of has those farms of humans that basically just sit there and yes, they might have an eyeball, you might be able to scan that, but that's really not what you want. Right, and even just to add on, as long as there's money at stake, I mean, we have synthetic biology, it could also be just sort of synthetic eyeball forms to hack the system. Okay, hold on, there's like a couple different things going through each other here. Static eyeballs don't work because with biometrics it's critical that you verify that it's actually part of a real live human body. If you get possession of an orb and you abuse it, while the signup process between the user is absolutely anonymous and privacy preserving, the orbs themselves are not. Their behavior is public and there are methods you can use to actually revoke the fake accounts that might be... But there are no fake accounts. Okay actually revoke the fake accounts. But there are no fake accounts. Okay, that's the second part. If they are not fake accounts, then the system works as intended, in a way. And you make a very good point that proof of personhood is just a primitive, and you need a richer ecosystem of things around it. The example I like, a very simple one, is let's say you have an on-chain DAO. You want to implement one person, one vote. If you just do this and allow all of the world ID verified users to vote on this and your social community is only 5,000 people, you can easily get swamped. You don't have a very strong governance system that way. So you do need to incorporate this in more complex systems. Lascha? I 100% agree. Lascha? Can I add something? I kind of like... I get all the points, but I think like forgetting the orb or the eyes and stuff. So I agree that overall identities, and I don't believe in universal identities, because identities have to be built for the governance purposes. So we should come up with frameworks that increase the coordination quality among the humans and then decide the depths of the identity, how it should be built. When we talk about the proof of personhood, it has like two dimensions, right? One is the proof of uniqueness. And I agree for the real world and especially in multiple cases, just uniqueness proof and badge does not guarantee the quality of decision making or coordination. And then we have to rely on passports or like citizenship or the age or like all the other stuff. And the thing is that like if we, like for example for voting, refer to the passports, the dependency on these document is so big that we can generate the uniqueness based on data and we might not need any type of biometrics additional to it. And I think, in a way, that's the purpose of identity. So I don't believe in universal identities. I believe of defining whether we like fighting AI or we like fighting the misinformation stuff. The only systems that are dynamically evolving and improving are the ones that have this kind of fluid identity aspects and not just the universal ones. Like on the example of Twitter and the community nodes. So just labeling is just creating this very low barriers in terms of defining the permissions. What you can do or or you can't do as a human or not. And like you said, yeah, there's a kind of North Korea scenario. There's kind of a dystopian scenario where we might be like creating market, right, for the eyeballs and stuff. And like you got two kidneys, you got two eyes, and then what happens next, right? Well, what you raised is an interesting point about governance, right? Because in some sense, the point Martin was making is highlighting an enforcement problem, right? And this gets back to the information problem. Yeah. Right? And so even if you do succeed in authenticating unique individuals in North Korea, how do you actually enforce that? And what you're saying is, well, you need richer governance mechanisms. We need to express identity in different ways, right, as a complement. And if you just have that one identity system, ultimately enforcement would require surveillance. And, of course, surveillance results in everyone becoming informationally the same. So I want to shift gears, Martin, and poke a little holes at circles. I think it's a great experiment. But one of the things I noticed on your slides is, you know, you're talking about currency and money. I think of them almost as community currencies and like nested networks of cooperation building in a nation states and like, you know, even the globe. But, you know, not everybody in a community should receive the same amount, blood, sweat, and tears, and contribution. It shouldn't just sort of be like an hourly rate. That's another kind of UBI that has its own pitfalls because it's not tied to incentives. And you mentioned a 7% interest rate. These things seem arbitrary, and they don't seem to leverage incentives or people's voice, choice, and stake in a community. So how do you think about that? Yeah, I mean, should they receive the same amount? I would say if a bunch of people together comes together and decides to use something as money, then just that coordination effort to some extent creates value to whatever they decide is money. And I would argue that the fairest form of distributing this value that has been created by just this coordination effort would be to distribute it equally among those that join this coordination effort. Super important to understand that that doesn't mean that everyone needs to work for the same hourly rate or doesn't need, I mean, that's absolutely not the case. So it's just that the coordination effort is in a way or the benefit of that is equally distributed. Sorry, there was a second question that I... Just, you know, non-equal distributions within, you know, acknowledging that there's different levels of contribution. I believe it's the 7%. Yeah, 7%, right. So that is probably the only parameter in circles that has been set and that's therefore debatable. And yes, you could say every community should be able to set that themselves. That would make coordination much, much, much harder. Why? Well, because then you would need to, or in circles, it is meant to be so simple that I issue circles, and then I know I can kind of trust that other account. I can exchange my money to their money one-to-one because we are issuing under the exact same rules. If everyone could define their own rules it would be much harder to negotiate of how much should my money worth compared to yours. So I would say for a stable equilibrium, the goal of circles is to have the stable equilibrium where the exchange rates between circles are just one-to-one. The goal would be to say one circle is one circle, and it's kind of abstracted away the fact that under the hood, we have all those different... Just like in the dollar system right now, technically speaking, if you have a dollar at one bank and a dollar at another bank, those are not the same dollars. But the system is built in such a way that usually it feels like one-to-one because if everything goes well, you can exchange them one-to-one. But if it breaks, you will realize, oh, the dollar at the bank that just went bankrupt is different from the other one. But the US and Euro have floating exchange rates. Why can't you... Say again? The US and Euro have floating exchange rates. Oh, no, no, for sure, for sure. So, in circles, yes. I mean, the expectation is they are within a connected cluster, the exchange rate is one-to-one. But, yeah, there can be different clusters where within the clusters the exchange rate is one to one but between the clusters there are floating exchanges. Why not just rely on supply and demand? Well it will be I mean the price between those clusters will be defined by supply and demand and what we are actually supposing in our upcoming relaunch I'd say, or restart, or reactivation, it's a continuation of the system, is that people or groups can back their very own circles with reserve assets they want, Bitcoin, dollar, whatever they want, and then there will be a free floating exchange rate for those circles, so defined by supply and demand. But as long as people trust each other, then they are essentially creating additional one-to-one demand between those currencies. Maybe we need to go back to the identity topic. No, but you're expressing identity in a different way, and so I just want to dig on that. But I want to turn to Lasha now. So how do you see yourself and Rarimo in this constellation of identity protocols? What like differentiation do you see that you're bringing? I think we try to use the both sides of the world, such as passports as an instance first, but then extended with graph and capture more depths of the interactions. What I would say is how we try to solve the identity problem or enable the buildup is very use case centric and like I said, the identity problem or enable the buildup is very use case centric. And like I said, the governance centric. The fact that we understand this voting stuff has to be done that way, it gave rise to solving the uniqueness problem in a certain way. But this doesn't mean that the same uniqueness should be reused universally across other use cases. And so going at a primitive level of these kind of registries and the building blocks and having more like this kind of flexible framework, I think that's the way forward. And as well as we've seen mostly on the regulatory pressure or this kind of collusion risk, I think the permissionlessness and empowering of the users going forward should be this kind of collusion risk. I think the permissionlessness and empowering of the users going forward should be like this kind of core principle that no one should kind of violate or cross the line for. So we only have three minutes left. I want to quickly do one minute each on where, you know, AI is a part of identity is helping us leverage AI to communicate with each other, also to govern AI. So starting with you, Remco, where do you see the most immediate use case with WorldCoin in the near future? And world or world ID interfacing with AI will, you know, open AI, for example, grant free access if you have a world ID. What is the sort of immediate near-term goals on that interaction? I mean, I can't speak for open AI, but we do share some money in common, so we'll see what happens there. Regarding AI, I think we need to embrace the fact that it's going to be some sort of human-AI hybrid operator. I'm a big believer of AI is just making humans better versions of themselves. So this is why I think it's so important that you distinguish between a CAPTCHA and a proof of personhood, which are different primitives with different use cases. And we might not even be that interested in the CAPTCHA use case. What is going to be a very important problem is distinguishing real world sensory data from faked data and authenticity. I don't think proof of presence alone is a solution there, but I do think it can be an important part of a functioning solution that allows us to authenticate content and information. Lascha, any immediate applications? Yeah. In my world, I don't have this like us humans and AIs and kind of like the separate camps of the camp. I more see and question like, okay, what is the first thing? Like I own multiple AIs. So the first dimension is how do I control it so how do we build this my me my AI and identifying these relations and control and then zooming out like all the AIs of individuals or like the systems how do they coordinate and what are the mechanism for us so I don't know like lower down the computational capacity or shut certain things down and abstract them. This is the next level of governance problems that we should be figuring out. That's where the identity aspects will be born uniquely. Martin, I know you're thinking a lot about AI? Yeah, I think it's certainly good to have a robust identity framework in place that also is relational because, yeah, for the reason in the very beginning. So I like to see a version of something like Circles that's completely permissionless as a base layer built on this trust graph. But then, yeah, projects like DK Passport permissionless as a base layer built on this trust graph, but then projects like DK Passport or WorldID that can essentially provide additional attestations, what I would call it, to those open base layer identities can bring us a very robust system overall where humans can coordinate if necessary maybe against AI. Will each circle have its own AI agent? That's a good question whether AI agents will also use circuits or not. I don't know yet. They could. Alright, let's give them a round of applause everybody. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:50:00.000Z",
      "slot_end": "2024-11-14T05:10:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1jVtcSZgrBxcYG4lFAatpVuooVRxzUpgPKggpcsgETVM",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "the-ripple-effect-of-devcon-vi",
      "sourceId": "E3U3XU",
      "title": "The Ripple Effect of Devcon VI",
      "description": "Devcon VI in Bogotá accelerated community growth across the region. Local communities emerged in several cities in Colombia and Latin America. The gathering provided leaders with a new perspective on enhancing collective creation for social impact and blockchain adoption. At ETH Bogotá, we used this spark to transition from hosting general events to creating an educational system for developers and builders, aiming to push the adoption of blockchain and Ethereum in a new way.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Vision,Ethereum for Good,Local Impact,education,Ethereum for Good,Local Impact,Vision",
      "keywords": "Education",
      "duration": 460,
      "language": "en",
      "sources_swarmHash": "939368d83ac93c262094341c954469dc8618657b52baa547b809c7a1dd20759b",
      "sources_youtubeId": "YgLrDeYqPaE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736c0c59dbb7a90e1cc98e5",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T04:50:00.000Z",
      "slot_end": "2024-11-14T05:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1vrrnCLaeOKKIwa7Mc_RpUOzo-jB1B7QzDNcIzCEOrak",
      "resources_slides": "https://drive.google.com/file/d/17jBFWGM6eWx_okIKLuGaSOrJaNrLv7SO/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "bootstrapping-a-block-builder",
      "sourceId": "QZBNTP",
      "title": "Bootstrapping a block builder",
      "description": "The sessions aims to be a practical overview of how to go from zero to having a running and reasonably competitive builder (profits may vary). It aims to answer the following questions:\r\n- What software to run? How can this be customized?\r\n- What would need to go into writing a builder from the ground up?\r\n- How does one acquire orderflow? What is the relative value of various sources of orderflow?\r\n- What infrastructure is required? How much does it cost?",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "PBS,MEV,builder,blocks,MEV,PBS",
      "keywords": "Block-building",
      "duration": 1374,
      "language": "en",
      "sources_swarmHash": "23b4fb2bc9e233d47041e22e5af1b2131bdb3812a6278ffd4484a29aa6bb24bf",
      "sources_youtubeId": "GDUrhkD-6-s",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67358ab59dbb7a90e189ee35",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:00:00.000Z",
      "slot_end": "2024-11-14T05:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1pJOc-CV91BIPcP9d3jUosxSzjtzYhRP3NAd6Cw9UsK0",
      "resources_slides": "https://drive.google.com/file/d/19EuDoZ4nntYpKx_p-9sJ1CI57f9XASM4/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "check-your-users-humanity-nationality-or-age-with-privacy-preserving-id-proofs",
      "sourceId": "XUUMW3",
      "title": "Check your user's humanity, nationality or age with privacy-preserving ID proofs!",
      "description": "This workshop shows how to use the Proof of Passport SDK to check user's identity in a few lines of code. Let users generate zk proofs of age, nationality, humanity or non-inclusion in the OFAC list by scanning the NFC chip in their passport or ID card, and without ever having to reveal any private information.\r\n\r\nCome try it now!",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Quadratic Voting,Identity,compliance,Identity,Quadratic Voting,Tooling",
      "keywords": "Social media,airdrop,compliance",
      "duration": 5317,
      "language": "en",
      "sources_swarmHash": "f84948f8dd68753f04338dded0a4e790692ac90f55f493fc5536f2309877e592",
      "sources_youtubeId": "hHmt8Q9yodI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735cf719dbb7a90e1fea9fc",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:00:00.000Z",
      "slot_end": "2024-11-14T06:30:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/116i9aeE05txm_kkq2IUV5e1zkt4cdfE8v3unnfsygbQ",
      "resources_slides": "https://drive.google.com/file/d/1LPieASAyTH3eqZcEzcClwNkXw-G-rHzC/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "exploring-the-future-of-account-abstraction",
      "sourceId": "S7NYUJ",
      "title": "Exploring the Future of Account Abstraction",
      "description": "Discover the journey of Ethereum's Account Abstraction (AA) from inception to its current state, challenges tackled by ERC-4337, and future roadmap: modular native AA approach for L2 and L1, and EOA improvement (EIP-7702).",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": true,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,In-protocol Account Abstraction,Account Abstraction,aa,roadmap,Account Abstraction,Ethereum Roadmap,In-protocol Account Abstraction",
      "keywords": "AA,roadmap",
      "duration": 1607,
      "language": "en",
      "sources_swarmHash": "7113fad0f81b154b61afb9bb5436b692012893f3198e31f3a13aa0c6f220fad1",
      "sources_youtubeId": "63Wd5mPla-M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67358f729dbb7a90e1afa28e",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:00:00.000Z",
      "slot_end": "2024-11-14T05:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1-B8ZzQJNuc1_e9BR0rIfLQYc9lXZ8nuO1aV56lK7dKM",
      "resources_slides": "https://drive.google.com/file/d/1R4q2zvwtGX3OUwpEgYCUr1YKAx9BGbZk/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "hevm-or-how-i-learned-to-stop-worrying-and-love-the-symbolic-execution",
      "sourceId": "YQPADR",
      "title": "hevm or: How I Learned to Stop Worrying and Love the Symbolic Execution",
      "description": "hevm is a symbolic execution engine for the EVM that can prove safety properties for EVM bytecode or verify semantic equivalence between two bytecode objects. It exposes a user-friendly API in Solidity that allows you to define symbolic tests using almost exactly the same syntax as usual unit tests.\r\n\r\nIn this talk, we'll present hevm, what it's useful for, and when and how to use it to help secure your digital contracts.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Fuzzing,EVM,Fuzzing,Security",
      "keywords": "Symbolic Execution,EVM",
      "duration": 1588,
      "language": "en",
      "sources_swarmHash": "1898b94ad33334ceee8d4ba4ec2171d30c52da2d3e647f5f194e4655ac4b226c",
      "sources_youtubeId": "o89CWZc2i1w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67358acd9dbb7a90e18c0581",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67358acd9dbb7a90e18c0581.vtt",
      "transcript_text": " Hi everyone, I'm Mathis Sos and I'll be talking about HEVM and symbolic execution that we built together with Lexi, myself and Zoe. We're part of the Argo Collective. That is a recently spun out collective from the Ethereum Foundation. If you want a bit more about the Argo Collective, you can watch our talk that was given yesterday. So first of all, what is this talk going to look like? So first, I'm going to talk a little bit about what it means to symbolically execute. Then I'm going to give a bit of an overview of HEVM. Then I'll explain how HEVM can be used to secure down some of your work that you have done, hopefully. And then finally, I'm going to conclude this talk. So first of all, what is symbolic execution? So let's say that your code looks like this. If you try to do fuzzing on this code, it will likely not find the fault that is marked by assert false, which could be, for example, something that drains your contract or locks your funds or some kind of negative event. So you can keep fuzzing this forever, essentially, and nothing will happen. But if you run symbolic execution on this, it will immediately hit the assert false and will give you the solution, the two integers that you need to pass in to trigger the failure. integers that you need to pass in to trigger the failure. So this is a good sort of litmus test of what symbolic execution can do in comparison to fuzzing. Of course, it can do more, but this is a good example where fuzzing will not help you very much, and symbolic execution will definitely help you. So let's get a little bit more into the details. So here we have, I used traditional assembly. I'm old school. I could have used EVM assembly, but it's more or less the same. So what we're going to do here is I'm going to showcase a very simple straight line program where we do a move and an add with two registers. So here we have concrete execution on the top. And what you have is concrete values like 1 and 2. And then the MOV will move the value 2 to the A register. And then it will add 4 to this value. And then the final register state will be 6 and 2. Now, when it comes to symbolic execution what it will do is it will instead of initializing it with concrete values like 1 and 2 it will initialize it with a variable like v1 and v2 and then we'll try to execute these instructions but over these variables rather than the concrete values and what that will do is that as you can see step by step we we have the state evolve and eventually it will end up with V2 plus 4 on the A register and V2 on the B register. So, of course, if I substitute the concrete values into that, we'll end up exactly where we were with concrete execution. But what this means is that I can mathematically express what is the state for any input value, right? And, okay, so that sounds interesting. Where does this get hard? Because this seems relatively straightforward. So it gets kind of hard is when branching happens. So here is a program where we have a branch. And with concrete execution execution it's relatively straightforward we have both registers set as one then we're gonna check whether they are equal they are happen to be equal and if they are true then we're gonna add five to the to the register a and so the the final state is six and one right so this is quite clear but when it comes to symbolic execution we we don't know of of course, what the value of A and B is. We represent them as these symbolic variables, V1 and V2, and we need to check for both potential execution paths. In this case, when V1 is equal to V2 and when V1 is not equal to V2. And we'll end up with two different states, potentially. In this case, we do actually end up in two different symbolic states. In one case, V1 is incremented by 5, and in the other case, V1 is incremented by 4. Of course, again, if I substitute the concrete values in, then I'll get exactly the concrete execution. However, what this means is that now we branch. So now we have two states, and if there's another branch, then this can become, of course, exponentially large number of exponential large number of potential end states. If there's a loop, of course, that can be, if it's infinite loop, it's not a bounded loop, then you can have issues with this thing never terminating because we don't actually know you know if ever the loop condition is is reached and so we can potentially run forever so I'm just gonna talk a little bit about what other similar execution systems are out there before I jump into HCVM. First of all, there's more or less two types within this ecosystem as far as I understand. One of them is you have a static code analysis engine, and now you want to validate whether some of the things that it spits out are potentially false positives. And in this case, if the symbolic execution engine is not complete or doesn't understand everything, it's kind of fine because what you will do is you will just simply spit out the potential false positive and let the user deal with it. And that's fine. But in our case, we actually don't use the static code analysis engine as a precursor to HEVM, so we actually have to deal with everything that the EVM has to offer. And so it's a bit more complicated, and it's also more complete, of course. So these purely symbolic execution framework-based systems, there are a number. There's Sertor Approver, which is based on this backwards exploration and weakest precondition computation. There's ETHBMC, which is, of course, as the name suggests, a bounded model checker type system. Then there's Halmosh. It's written in Python. And then there's KEVM, which is based on the K framework and allows you to break out into K in case you need to prove some things. For example, loop termination or invariance and things like that. So just a bit of an overview of HEVM. So it started a long time ago as part of the DAPTools project. It implements the EVM semantics both concretely and symbolically, and actually Echidna fuzzer, if you know about it, it uses HEVM underneath for the concrete execution semantics. It is possible to execute any call from any potential state in the EVM. So it understands all of the EVM in terms of, like, for example, calling out to an RPC, to an archive node, to fetch state, et cetera. It runs, it basically computes a query to an SMT solver, runs the SMT solver to get the response to the query, and then interprets this response and displays it back to the user in a fashion that is more user-friendly than some SMT output. We'll see that in a moment. So there are two ways of using HEVM. One of them is for counter-example example generation and one of them is equivalence checking. So let's talk a little bit about this counter example generation for in this case and some kind of post condition that typically is written for example in FORGE test case as a FORGE test case. So here we have Solidity, Viper, whatever language you prefer, or even just pure EVM bytecode, and that gets interpreted by the internal symbolic execution engine inside, symbolic interpreter inside HEVM to produce an intermediate representation. And then this intermediate representation, together with the post post condition that you put down gets compiled into a logical formula that gets sent out to the SMT solver and SMT solver is what's called a site model theory solver so it understands for example bit vector arithmetic which is quite useful because of course there's these 256 bit bit vectors or variables in EVM that EVM operates on. And then either it proves the property, finds a counterexample, or times out, of course. That's always a potential possibility. When it comes to symbolic execution for equivalence checking, we do something very similar. But what we do eventually is that we try to compare the two executions against each other. So you have bytecode A and bytecode B. Let's say that bytecode B is like a refactored bytecode or something that is gas optimized, and you want to make sure that it's doing the same thing as the original one. And now what this will do is that it will try to act to prove that it is equivalent, find the counter example, so some input or in state where the two contracts actually disagree, or of course, there's always the potential for timeout. And let's go a little bit deeper into this kind of symbolic execution engine inside HEVM. So it operates on bytecode, which means that we're not tied to any particular compiler we're not even tied to something like you will for example understands all of the EVM stack call frame stack storage call data everything can as I mentioned it can actually run at any point in the blockchain history and it is fast against the concrete execution semantics of GATH in this case. As mentioned, like it has issues with loops and recursions because of this issue that unterminated loops, we don't know when to stop. It has some issues related to symbolic off-size and memory copy. This is purely due to SMT limitations and doesn't currently deal with symbolic gas, so it basically ignores gas when it comes to symbolic execution. In concrete execution, of course, it understands gas and will deal with it, and so Echidna will be running correctly in that sense. So just a bit of the internals. Maybe this is a little too small, but basically what it does is that it takes in as an input the EVM bytecode. Then it will step-by-step execute it and branch, if necessary, to build an intermediate representation. And then this intermediate representation is simplified in a generic way. And there are some specific simplifications related to catch-hawk and arrays and maps and things like that that are quite specific when it comes to to how the EVM handles arrays and maps and things like that so especially catch-hook which is very complicated to put into exactly represent in in SMT eventually what happens is that this IR gets compiled into a bunch of SMT queries, and these SMT queries get dispatched to the SMT solver or solvers, and then we gather all the results from the SMT solvers and then get the counterexamples, extract them, and map them back to the query that was originally dispatched and see how we can display that to the user in a way that they will be able to run this and actually trigger the fault. Because at the end of the day, it's annoying to get something like, hey, your problem is faulty. That's not going to help the user. But the user really wants some kind of counter example so they can actually run it and see how it is wrong. And then, of course, they can fix it or hopefully fix it. So that's kind of the high level of the internals. So let's talk a little bit about this intermediate representation. I'm not going to go into the details, but that's kind of at the middle of this whole box, in the previous box that I showed. So here's a simple function, right? that I showed. So here's a simple function, right? This will overflow if the variable that you put in is large enough, in particular exactly equal to 2 to the power of 256 minus 1, and b will actually not be larger than a in this particular case. I'm sure you have played around with this. It's a very trivial overflow issue. And this gets compiled into this intermediate representation where we have a proposition that a must be smaller than the a plus 1. So I mean, it's very simplistic in this space. But the point was to make it kind of readable and also human readable. This can also be compiled into a graph, as you might imagine, and it can be quite understandable to a human how the internal representation maps back to the original code. In this case, it's quite clear. Okay, so I'm going to talk a little bit about uh about how to actually run this tool and what kind of results you can expect so in this particular case we have a forge standard test that we're gonna import our contract is um is a test and then we add prove underscore in front of it you will need forge you will need Z3, which is one of the SMT solvers that we support, and you need HEVM binary that you can just download from the repository. And you put down this test, and this test will, of course, fail, again, due to overflow. And the way you run it is very, very simple. You just build with the ESC and run HVM test. And it will parse everything up and do its magic and eventually give you a counterexample, which is what you expect. So this is sort of the base way of using it. And it is as clean and tidy as it looks on the output. I did not actually change that or edit it or even the spacing is how it is. So it's quite visually representative of what are you going to test and what are the counterexamples that come out. If there's more than one counterexample, there's going to be more than one listed there. So how does HEVM equivalence work? Here I'm just going to show sort of a contracted thing. It's a very complicated code that was someone tried to use. And here I just want to show some of the edge cases, but also that we do power through the edge cases as well. So in this case, there's two codes, and we want to know if they're equivalent. You see that it emits a warning that we cannot actually explore the whole thing in this particular case due to a call into unknown code. Obviously, we don't know what's going to happen there, like what is going to get executed. But beyond that, it says, OK, well, anyway, I power through. I'm going to ignore these bits and pieces. We have 1.7 million end states that we need to check for equivalence. And then it says, OK, well, I tried my very best. There's 93 of them I couldn't do because of this memory that was a memory copy that is symbolic. I couldn't do two because of timeout. This actually had a 15 second timeout for each query. And eventually I couldn't find any discrepancies given these warnings and things that I couldn't actually explore. And if you have a look, this actually ran for like 34 minutes and 45,000 seconds, so obviously it was like",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:00:00.000Z",
      "slot_end": "2024-11-14T05:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1zbKn6alKaFJ7AHUN8resSuZmq-0n4W0JbxXcZGI9Cq8",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "the-role-of-culture-in-shaping-technology-the-case-for-cuteacc",
      "sourceId": "LRJTXY",
      "title": "The role of culture in shaping technology - the case for cute/acc",
      "description": "Who builds technology and for whom? In decentralized technology, we must apply the cypherpunk ethos not only to the product we want to provide to the world but also to the manner we build that product. We must avoid imposing our worldview onto different cultures, or we risk reinventing tech neocolonialism. This talk will illustrate the risks of concentration of power and tech within our industry into the hands of a few cultures and present ways to build a truly cypherpunk future.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Network State,Digital Sovereignty,Decentralization,diversity,democracy,philosophy,Decentralization,Digital Sovereignty,Network State",
      "keywords": "Philosophy,Diversity,Democracy",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "4478df6dd6b17283a4f2ec08a8e0f583a39783faa563f20c364a7a264546fa1a",
      "sources_youtubeId": "ENttqPaOPS8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:00:00.000Z",
      "slot_end": "2024-11-14T05:10:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Wi0ob1KXq6nswjq25vU56mNvitsmnOnrWaRe-gSp-3k",
      "resources_slides": "https://drive.google.com/file/d/1Jl-Fpz07dhs4JpkVKy2sOiGOvWS7sdk6/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "zupass-identity-and-credentials-beyond-proof-of-personhood",
      "sourceId": "K9SNB7",
      "title": "Zupass, identity and credentials beyond proof of personhood",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 593,
      "language": "en",
      "sources_swarmHash": "0221e03f42edcd25f7bdec6110958dbda072c8b4ec25a907ce9118272507638e",
      "sources_youtubeId": "hlE4x7JURjY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735933f9dbb7a90e115f6af",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:10:00.000Z",
      "slot_end": "2024-11-14T05:20:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1T4jVcPwg6WbjeISdy1YwzFmXCYct8HCQrjsXs8P0FdI",
      "resources_slides": "https://drive.google.com/file/d/1VT4pCA44gNZqfZVPRk8_0A1W4Od_PNxd/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "community-notes-scaling-public-epistemics",
      "sourceId": "8F3HQM",
      "title": "Community Notes: Scaling Public Epistemics",
      "description": "Community Notes allows regular X users to collaboratively add context to potentially misleading posts. It uses a transparent and verifiable mechanism that aims to be credibly neutral by only showing posts liked by users who typically disagree.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Censorship Resistance,Collective Intelligence,Consensus Mechanisms",
      "keywords": "d/acc",
      "duration": 625,
      "language": "en",
      "sources_swarmHash": "1d4a8e3f5e0fc4b8e8906fd6ca2597c3b0c0563a6cc92a9609c64cf5f7fd0e59",
      "sources_youtubeId": "43brlPnDr98",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673593af9dbb7a90e11aee6b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67358f729dbb7a90e1afa28e.vtt",
      "transcript_text": " Hi everyone. So, yeah, today, so I'm Yoav Vais from the Ethereum Foundation. I'm a researcher working on account obstruction. And today, in this talk and the next two afterwards, we're going to talk about the state of account obstruction, where we stand now, and where we're headed. לדבר על תקופה של תקופה של עקבים, איפה אנחנו עולים עכשיו ואיפה נכנסים. נכון, אז קודם כל, אנחנו הולכים לבריפה לזכיר כמה זה כל כך התקופה. אז, התקופה, זה כל כך התקופה, אז זה התקופה, בעצם, כך שכן, אתריאום התקופה. started in, so it started at the same time Ethereum started actually. So when Vitalik wrote about Ethereum before the launch, he already blogged about what accounts should ultimately look like. So this was the first mention of account abstraction. And then in the following years until 2020, there was a lot of research done on this, a lot of research done on this, a lot of interesting proposals, and the idea kept evolving. Then in 2021, Vitalik came up with a new idea. Trying to get to account abstraction and start experimenting with account abstraction models without protocol changes, which makes it much easier to get started. And this has led to ERC-437. The idea was to get to full account abstraction without protocol changes, but also without compromising on censorship resistance by relying on centralized relayers. So that's what we've been doing with ERC-437. Now, censorship resistance, it requires having a permissionless mempool because otherwise someone could always censor you. But it's a challenge. And the reason it's a challenge is that full account obstruction means also obstructing the validation. And when you obstruct validation, it means that it depends. The validity of the transaction now depends on mutable state. So this enables a lot of DOS vectors, a lot of denial-of-service attack vectors that need to be mitigated. Just to give a quick example of such attack and why we need to solve for this, is let's imagine an attacker implementing an account. And this account has a dependency on a flag stored in a singleton smart contract. So every account using this implementation looks at the same flag in order to determine transaction validity and also flips the flag. And now the attacker sends thousands of such transactions on an ongoing basis. Every time such transaction gets included when a transaction gets included, it immediately invalidates all the other ones. So they can't be included in the chain. And now they have to be dropped. And they have to be dropped without paying for gas because they are not valid. So we can charge them. But they still cause a lot of validation work for every node in the mempool. And this can easily escalate to a point where the nodes cannot do any useful work. So we need to mitigate for this kind of attacks. And the way to do it is to separate between validation and execution so that we can limit what can be done by an account before execution, before it agrees to pay for the gas. Now, the question is, what do we want to limit? What kind of restrictions could we apply? The easiest way, the no-brainer, is let's not let the account access any state outside the account until the transaction is valid. So it can only access its own storage. It cannot access any other contracts, and it cannot access environment opcodes, such as a timestamp or a block number. So this solves the denial-of-service problem, but it also limits usability quite a bit. So, for example, you cannot support popular use cases, such as paying for gas using an ERC-20 token because you cannot look or modify the balances of that ERC-20 token during validation. So, over a couple of years, we worked on developing a set of rules, a set of validation rules, that support as many use cases as possible while still mitigating denial of service. And this enables use cases such as paying us with tokens and many others that wouldn't have been possible otherwise. So the way it works is these rules are applied at the mempool level. The mempool will only propagate transactions if two conditions are met. One, the transaction must be valid. And two, it must also comply with the rules. So a valid transaction in an account that doesn't comply with the rule will still not be propagated. Now we fast forward to the present. Where are we now? So ERC-437 went live last year, and it gained quite a bit of adoption. There are now over 20 million accounts, mainly on different layer tools, and a lot of activity. What's interesting is that most of these transactions, the vast majority of the transactions, actually you also do gas abstraction using Paymasters, which greatly improved UX for situations like onboarding. We are abstracting gas, and this alone is a great improvement. And over the past two years, we also saw a lot of great projects, a lot of awesome stuff being built by the community. Some of it is things that we really couldn't imagine before. And I'm not going to dive deep into this because one of the next two talks by Tom is precisely about diving into the numbers and interesting use cases. So be sure to stay for Tom's talk. Another recent development is that the public account obstruction mempool just went live. So the idea with the ERC-4337 has always been to have censorship resistance by having all the bundlers connected in one peer-to-peer network, one mempool where transactions get propagated. And this way, you're pretty much guaranteed that someone will include your user operation. But until recently, every bundler maintained its own private mempool, and they were not really connected while we were still working through the coordination of this mempool and they were not really connected while we were still working through the coordination of this mempool. And recently with an effort with a coordination effort for the protocol, three of the bundler implementers EtherSport, Candid and Silius managed to finally bring it up so now we have this mempool. And thanks. I want to thank these three teams. And I should also thank Fastlane that made it possible to have the mempool on Polygon. So thank you. And this way we have... So now we have the mempool running on several chains and more are coming soon. I hear that also other bundler implementations are going to join the mempool soon. So we'll see it growing. What does the future hold? Where do we go from here? So one thing we saw is that many, so we started seeing layer twos introducing native account abstraction in order to have a better integration, more gas efficiency. Now, the problem is that they enshrined the modified versions of ERC4837, which are not standardized. And this started causing wallet fragmentation. So suddenly we see wallets, pretty nice wallets, that work only on one chain and cannot be used on any other. And that's really not great for the ecosystem. Because if you found a great wallet you like, you should be able to use it on every chain, not only on one chain and then look for another wallet for something else. So we realized that this is actually not an account abstraction specific problem, but a coordination problem that could cause issues in other fields, which led us to start the roll-up improvement proposal, also known as Roll Call. That's a coordination effort with all the layer twos to coordinate standards that would benefit the entire ecosystem. After we started this process and brought the Layer 2s on board, we started also working on RIP 7560, which is a native account obstruction proposal for Layer 2s. And a Layer 2 that uses this will get a better user experience and better gas efficiency. Now, what about layer one? So I said that RIP 7560 is an RIP, it's for layer twos. But for layer one, we are thinking more long term because we at Ethereum are notorious for taking our time to get things right on layer one. So this is no exception. We have a proposal called EAP-7701, which is native account abstraction using EOF. And the dependency on EOF means that it's not going to happen very soon because EOF is not included in Pectra. But once it's enabled, it's going to enable interesting account abstraction models and not limit us to just one. And it will enable things like improved censorship resistance through inclusion lists, which are not possible to support otherwise with something like 4337. So that's something to look forward to and we're working on it. I'm not going to dive further into native account obstruction but there's a the next talk after mine is by Alex and this is specifically about native account obstruction proposals so he's going to dive much deeper into this. We also saw strong demand for supporting EOAs to give EOAs at least some of the benefit of account obstruction. Now, it's not possible for an EOA to get the full benefit of account obstruction because ultimately it's an EOA. It still has a single key, single ECDSA key that cannot be rotated, but we can still give it quite a lot of the benefits. And that's exactly what EAP-7702 does. It enables adding code to an EOA without invalidating its key. So now the account is actually both a smart account and an EOA. And this works great with ERC-4337 because it allows us to set the code to turn any EOA into a 4337 account, which uses the 4337 mempool. And this automatically enables use cases such as paymasters. Now you can have an EOA that uses 4337 Paymasters. So you can, for example, pay GAZ with tokens or get GAZ sponsorship for an EOA transaction. And on this one, there was already a talk by Light Client that took a deep dive into it. So if you're interested, you should check out the video for this one. Now, how does it all fit together? So we talked about several different account abstraction models, which would probably be confusing for someone looking into what to implement. So the answer is that the account model is identical. They all use the same kind of separation, the same account model, same gas abstraction, everything. So it's actually quite easy to build an account that supports all of them. So you develop it once, you can deploy it everywhere on every chain, whether it has some kind of native account abstraction or not. You can use it, you just use it everywhere. And once it's deployed, once it's deployed, if a chain later, if a chain on which the account lives switches to, enables native account abstraction, it will be able to benefit from the improvements. Now, the goal, so this seems like moving a bit beyond account obstruction, but it's actually part of the same thing. So the goal with account obstruction has always been to go beyond the individual, beyond improving the UX for the individual chain, and to also solve the cross-chain problem. And in a minute you'll see why it makes sense to use account obstruction for this. And anyway, now that we have the basis for it, it's time to start really solving the chain obstruction problem, having great cross-chain UX. So the cornerstone for this is that account obstruction enables trustless bridging. So with account obstruction, you don't have to trust a bridge operator. You no longer need these multi-sig bridges. And the reason is that the operator is not in the loop. The operator is not a part of the transaction in the sense that the message sender is never the operator. The message sender is always the user's account. As opposed to the EOA case, where message bridges are a part of the transaction and are trusted in a sense. So with this, the user can create a transaction that actually operates on several chains. So we have one transaction running on multiple chains, and the bridge operator can execute it on all the chains on behalf of the user, and even transparently deploy the user's account on chains where it doesn't exist yet, but the operator cannot change the account in any way. The account, because the account self-validates its own deployment. And this means that now the account can validate the transaction on each of these chains. So once it validates the transaction, it means that the bridge operator is not at liberty to mess with the transaction in any way. So because it cannot mess with the transaction, we don't need to worry about the bridge operator, which leads us to trustless bridging and to permissionless bridging. So you don't need a permissioned bridge operator. Anyone can join and be the bridge operator, including the user or anyone else. As for the GAZ, with Paymasters, it means that you can have the account paying for the GAZ using a Paymaster on each of the chains. So the user only pays on the source chain, and the bridge operator uses a Paymaster deposit to pay for it, and then ends up getting compensated from the user's deposit on the source chain. So that's how we can solve bridging. And using a few more standards, some of which are being developed and some will need to be developed, we can really solve the UX problem and make it seem like using one chain. The idea is that the user will never have to select a chain in the wallet and it will feel like really using a single chain. Some of these standards are, among them are key stores. With key stores, the idea is that you can run any... So with key stores, you manage your credentials for your account on a single chain, whether it's layer one or whether it's a designated ZK layer two. You can manage it in one place and use the same credentials everywhere. If you rotate your keys, they get rotated automatically on all chains. Another thing is that the user can send a single transaction that runs on multiple chains and doesn't even need to hold the native coin of each of these chains to pay for the gas. So it feels like using only the source chain. The wallet can also use something like intent solvers for more complex operations. So, for example, performing a complex trade on multiple chains is possible with intent solvers, but unlike with the EOA case, here the account can enforce the result and revert if it doesn't like it, which means that the user doesn't have to take risks when using intents. Another thing is that addresses will become chain-specific addresses. That's a standard that is nearly ready. With chain-specific addresses, the address includes the name of the chain on which the address resides. And this chain actually uses an ENS name of that layer 2. To give an example, let's say we have a user on Polygon that wants to send USDC to a friend, but the friend uses Base. So with this, the friend gives the user an address that looks like 0x1234 at Base.eth, and the wallet knows how to deal with it. So the wallet can look it up. It knows how to bridge to Base. So the user pastes the address, the USDC gets sent, The wallet can look it up. It knows how to bridge to base. So the user pays the address, the USDC gets sent, and there's no need for the user to even notice that they're actually not on the same chain. In order to achieve this, the ENS name for each of the layer 2s actually refers to a few records that help with chain discovery. The reason we need this is we're going to live in a world where there are many layer tools and many more coming up every month. So we don't want to configure the wallet to support each of these chains. We want automatic discovery. So with ENS, we can get just that. We can have, for example, we can have a record for a standardized bridge. So now the wallet can look at an address and find out exactly how to bridge assets to that chain. It can have a record for RPC provider. So if the wallet needs to transact on this unknown chain, it has an RPC to use. And then it also have, and it doesn't even need to trust the RPC because there's also going to be a light client implementation, a CCIP-based light client implementation that can validate the chain state against its Layer 1 contract. And this is also a record. So the wallet is able to communicate with every chain. It can communicate with every chain, it can communicate with every chain without previously knowing about it by just looking at an address and then resolving everything, knowing how to trustlessly transact on this chain. From the user's perspective, it all becomes one chain. So account obstruction is already here. It's already thriving. Native account obstruction is coming. And account obstruction is already improving UX on many chains now. But in the future, in the near future, hopefully, it's also going to improve the UX across different chains. And in the future, it's going to feel like we're using a single chain rather than multiple chains, but with the scaling that comes from our layer 2 strategy. And all this without sacrificing decentralization and synergy persistence. Alright, I think we've passed our time, so any questions? Yes. Thank you. Thank you so much, Joao, for the wonderful session. We'll start the first question. How are we supposed to develop a 4337 wallet in the entry point contract keeps changing. Yeah, so the entry point contract will not keep changing. The last change was hopefully the last ABI change ever, unless some critical security bug we're unaware of is discovered and forces us to change something, which can always happen, but unlikely at this point. If not, then maybe there will be some minor bug fixes inside entry point, but not something that would affect ABI and require changing wallet implementations. Do you think we will need L2B style risk dashboards for different wallets to make users aware of what these wallets actually do? Definitely. I think L2Bit is doing very important work on layer 2s, and we will need this for wallets. It's actually not just for wallets. We need someone to verify that the wallet does what we expect it to, but also when we transact across different chains, even though we want to make it the wallet does what we expect it to, but also when we transact across different chains, even though we want to make it transparent to the user, not all chains are equal, and some actually are riskier than others. L2B does a great job highlighting these differences, and we'll need to also account for that. So, yes. Does 772 require the wallet provider to be okay with upgrading the EOA to a smart account? What happens with a wallet provider after upgrade practically? Wait, I'm not sure I understand. So, you want... Maybe the person who... I mean, yeah, it does need... I mean, yes, of course, you need to add the code in order to use it. So you need to add 4537 code to the account, and then you can start using it as a 4537 account. But I'm not sure I understand the last part of the question. Maybe the person raised the question. Do you want to elaborate a little bit? Maybe by saying wallet provider, they're talking about the wallet as in the part that runs off-chain on the user's machine, in which case, yes, the wallet has to be aware of the implementation, but the wallet is the one setting the implementation. So now the wallet will determine what code gets set into the EOA and will use whatever it can also support. Great. Let's move on to the next one. What is the plan if there is a critical vulnerability on the entry point? It would be a black swan that would compromise all wallets at the same time. And by definition, it can't be revoked in mass. Yeah, you could ask the same question about something that doesn't use something like entry point. Because, I mean, what happens if there's a critical protocol vulnerability in Ethereum and then we also, and also a lot of accounts get compromised. So hopefully we audited things enough to prevent such occurrence. And if it does happen, if it does happen, then yes, we will need to deal with it. At some point where it becomes such a big thing on the network, you have to fix such things by hard fork. So let's say EOA was compromised somehow. We would need a hard fork to deal with it because we are not going to allow a situation where every account on Ethereum gets compromised. So it's quite similar, whether it's account obstruction or EOA. What's the main technical challenge of key stores? Next one. The main technical challenge is being able to access a layer one state during validation and the way to do it is by having an RIP supported by all the layer tools to access layer one state. And then you can have a key store on layer one. So there are currently two such proposals. One doing L1S load, meaning you're able to load a slot from a layer one contract. And there is also remote static call that would let you perform a static on layer one either of these proposals would enable would enable key stores efficiently because now the validation will be able to actually use that and this will become a part so a part of the proof of the proof used by the layer 2 will prove the correctness of this lookup. Okay, the last question. Should we adopt AA now, considering future features you mentioned might not be backwards compatible? I don't recall mentioning features that are not backward compatible, and actually our goal is to make it backward compatible. So if you develop a 457 account now, you'll be able to easily make it work with future native account obstruction proposals. And I think the time to adapt it is definitely now because otherwise you're not going to get the benefits of even things like 7.7.0.2, let alone chain obstruction stuff.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:20:00.000Z",
      "slot_end": "2024-11-14T05:30:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/10k5sMswiuZ6sjCWh6_3DzYLI8Ix836tP-o0-fhgeUCI",
      "resources_slides": "https://drive.google.com/file/d/1usTYYfGYtQzCbyaP1WI8rEU2lpud8xLG/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "empirical-analysis-of-amm-loss-versus-rebalancing-on-layer-2-chains",
      "sourceId": "T8BXK3",
      "title": "Empirical analysis of AMM loss versus rebalancing on layer 2 chains",
      "description": "This talk presents an empirical analysis of Loss versus Rebalancing (LVR) on Arbitrum, Base and Ethereum. It compares the realised and theoretical LVR; along with the arbitrage profits from CEX-DEX/Perpetual; then further reveals whether the frequency of delta-hedging, the pool liquidity and the block time difference lead to better or worse LVR.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Cross-L2,MEV,AMMs,cross-domain,arbitrage,AMMs,Cross-L2,Layer 2s,MEV",
      "keywords": "loss versus rebalancing,cross-domain arbitrage",
      "duration": 1415,
      "language": "en",
      "sources_swarmHash": "d55e25ae416289798534dbe4778dc58bffac84dfc55980a2773e6e72c56dafe6",
      "sources_youtubeId": "ArILIuH7G2U",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e2fb9dbb7a90e14ee83e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:30:00.000Z",
      "slot_end": "2024-11-14T06:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1Y6GrE_61ZfJ2Mxu9xrE-xcG7WBCWmKg3qYPa5F0zL3s",
      "resources_slides": "https://drive.google.com/file/d/1lEfPZifUzry_aVkzBOVRfBtaV1JdClSC/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "native-account-abstraction-in-pectra-rollups-and-beyond-combining-eof-eip-7702-and-rip-7560",
      "sourceId": "7AWG3A",
      "title": "Native Account Abstraction in Pectra, rollups and beyond: combining EOF, EIP-7702 and RIP-7560",
      "description": "Account Abstraction has rightfully become one of the most discussed topics in the Ethereum ecosystem.\r\nThe upcoming Pectra upgrade is set to be the first one to improve EOAs by including EIP-7702.\r\nBut can EIP-7702 alone achieve \"Account Abstraction\"?\r\n\r\nWe will discuss the challenges and benefits of EIP-7702, and break down the team's vision for achieving \"complete\" Native Account Abstraction with RIP-7560/EIP-7701 and how it differs from ERC-4337 + EIP-7702.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": true,
      "doNotRecord": false,
      "tags": "In-protocol Account Abstraction,Rollups,Account Abstraction,eip-7702,Account Abstraction,In-protocol Account Abstraction,Rollups",
      "keywords": "Native Account Abstraction,RIP-7560,EIP-7702",
      "duration": 1521,
      "language": "en",
      "sources_swarmHash": "442776890274122c7546b739cfa176e5bc49f849f150eed1b8cc1acf78398aa2",
      "sources_youtubeId": "FYanFF-yU6w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673867c81b0f83434dd66c44",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673867c81b0f83434dd66c44.vtt",
      "transcript_text": " . Hello everyone. My name is Alex. I work on account abstraction and today I will follow your talk with a deep dive into the future of native account abstraction and our plans for it. So for the purpose of this talk, I suggest that we all agree that we want native account abstraction, the way for account abstraction is to enshrine it in layer tools in the mainnet. And we need to answer the following questions before we go into it. First, we need to know which part of native account abstraction is already happening in the next Ethereum hard fork. We need to see why it is not enough and what is still missing for us to achieve the account abstraction endgame. I want to explain how we plan to achieve it and also explain how other companies and teams can participate in this effort and honestly look at the possible alternatives to doing what we are proposing. So, a quick recap of where we stand with account abstraction right now, if somebody was not involved into it. So, the first account abstraction proposal is ERC-4367. It solves account abstraction without making consensus changes on the Ethereum protocol. It allowed us, purely out of consensus, to provide account abstraction solution. And it did solve a vast majority of cases. And it has been released more than a year ago. The mainnet and the mempool have been launched. So it's no longer a new project. It has been used for a year by a very serious project. EAP-7702 is a very important proposal because it's the first time mainnet is getting some account abstraction features. This EAP allows, as you know, EOS to role-play for a time as smart accounts, and it's scheduled for the next hard fork. You have mentioned RIP 7560. This is our proposal to enshrine the design of ERC-437 as part of Layer 2 consensus, and it has been implemented, and there is a DevNet ready implementation. And now we are also proposing EAP 7701. This is somewhat similar to 7560, but it is trying to be less opinionated using less of the protocol parts and it targets Ethereum layer one, and it relies on EOF to do so. It's an early draft stage, and we request everybody to provide their feedback on it. So a little bit more on 7702. This is how the account looks for once you update it with 7702. So you still have the private key, but you also have smart contract code. So it changes the behavior of current EOAs, allows them to have code as well. And this fully solves the execution part of account abstraction. Your account can do multiple operations in the same time, do intends, whatever it is. It does not, however, solve the security part of account abstraction. Because you still have a private key, you still have an EOA, you still have 12 words that can override your smart contract wallet. And there also needs to be another EOA that creates a transaction to use your authorization. The upside of this is that it works great with ERC 4337. And such accounts can be part of account abstraction ecosystem and get gas abstraction and many other features. So one question you can ask is, great, so next we'll just wait for the rest of account abstraction to be enshrined in Ethereum mainnet. Well, if you look at the specs for the next three hard forks, this is a list of EAPs that are considered or scheduled for inclusion for the next three hard forks, this is a list of EAPs that are considered or scheduled for inclusion in the next three hard forks. This is a long list. These changes will take quite a lot of time. And so if we were to just wait out to introduce account abstraction on layer one, this could take a very long time. And also, it's a high bar to clear on layer one, this could take a very long time and also it's a high bar to clear in terms of production tested specifications, full spec roadmap, and it requires a unanimous consensus among core Ethereum developers to do such a feature. It doesn't mean that we will just wait for these things to happen. We have a lot of activity we can do on layer tools who are eager to innovate with account abstraction right now. Another alternative is just to keep using ERC 4337 forever. So can we keep using it? Well, kind of, yes. It's good enough in many cases, but it's very much not perfect. The main downside is it still relies on EOAs to act as bundlers. So you have an account abstraction solution, but you still need EOAs for that. We also create a lot of complexity by implementing a lot of parallel technical stacks, parallel mempools, parallel bundlers and modifications to the node. And as layer twos want to innovate, they start implementing their own native account abstraction solutions. There are chains who did that. And it is a problem because it breaks the compatibility. And also there are still new EAPs that introduce new features to Ethereum. One big example is inclusion list. You mentioned Fossil. And these EAPs don't benefit account obstruction users and account by the way. So let's zoom into a flow of a single user operation in ERC-437. So what happens is the user signs and creates a user operation, and the user has to provide it to a bundler server. The bundler server then collects other user operations and bundles them together and provides it as a transaction to the block builder, and it has to use this conditional API, and provides it as a transaction to the block builder, and it has to use this conditional API, meaning that it performs a validation, and he provides a condition for this transaction to be valid. And then the block builder can include this as a call on chain. With RIP 7560, we make all these superstructures that we had with the bundler and conditional API and entry point contract part of consensus protocol. So it very much simplifies and flattens out the complexity. And for the user, they sign this transaction and they broadcast it to the mempool and the protocol takes care of the rest. And the complexity becomes part of the protocol, but again, it's simplified. So how it works is that already now, like all transactions that we broadcast to Mempool and include on chain, they have a validation code. However, this code is not a solidity EVM code. It's a, you probably go code that a block builder have. We validate signatures, nonces, balances, gas limits, base fees, and these checks are done in Go as part of consensus, and then the execution is done on the EVM. With 7560, we split the transaction into two parts, and the validation part is also Solidity code that also runs on-chain, but it's still a single transaction that is split into the validation part and the execution part, and error in the validation part means that the transaction is not valid. It's not included on-chain and reverted. It just cannot be included in a block. So if you're familiar with ERC-437, the most complex possibly path for transaction to take is to have GAZ sponsoring and to deploy a contract, a smart contract, as part of the first transaction interaction. So these are all execution passes in account abstraction. And this is what it looks like in 7560, meaning that for a transaction type, we add a number of fields, and what happens during this transaction flow is first step, user creates a transaction, sends it to the block builder. As part of a transaction, smart contract gets deployed. Paymaster gets queried if it agrees to pay gas for this transaction. Then the account is queried to see if it accepts this transaction as valid, checks the signature and everything. Then the transaction gets actually executed and reaches the target contract. And if Paymaster wanted to, it gets also notified that transaction execution has finished. However, what needs to be said, RIP 7560 is not meant to be included in layer 2 in its current form. The RIP process itself was started for features that are common between various Layer 2s, but not necessarily target Layer 1. It provides us a lot of flexibility because we don't need unanimous agreement of all core developers. It's an opt-in process where rollups can decide to pick up features and implement it on the networks and it's very feasible and like logical for some RIPs who get adopted to evolve into EAPs. So what prevents 75-60 approach to being part of the mainnet EAP. Well, a huge part of it, it defines validation as solidity methods. Like we define solidity methods, and we say that this method has to return correctly, then the transaction is valid. It is a little problematic because EVM is supposed to be language agnostic, and methods are just part of solidity programming language. It's not such a big deal for layer 2s because almost all of them already have some kind of precompiles that are defined fully in solidity and they already do it. Another thing is that EOF, Ethereum EVM object format, introduces the deployment time code validation, and account abstraction could greatly benefit from code validation. However, without EOF definitions, we would not be able to do it with method selectors. And it can be a problem that your validation code is a part of your contract that can be called by other contracts in some scenarios that can lead to vulnerabilities. So a reminder, in EOF, the contract is being split into parts. So legacy contracts, they have the blob that includes all the code and data of your contract. With EOF, the contracts are split into the header, the code section, and the data section. What we are suggesting with EAP-77-01 is to also split the code section into parts that have roles assigned to them. So the contract would have an EOF validation code, execution code, any other code, and we can verify the code of the validation section before deploying such contracts. And this code doesn't have to be observable on-chain from within EOF, but it is still executed as part of transaction validation. So if you look again on all the flows, the flow remains exactly the same. It's just instead of calling specific functions, the EVM executes certain predefined sections of your EOF contracts. And this allows us to get away from these magic method selectors into a more mainnet-level solution. So now it's time to talk about challenges of account abstraction. People have been talking about it for 10 years. It's still not on mainnet. This is because it's actually hard. If you see somebody talking about validation scope on account abstraction you immediately think about this picture and the main problems for that account abstraction creates is the cross dependencies between transaction and validations and the complexity you get in building blocks efficiently and maintaining a decentralized peer-to-peer mempool efficiently. So let me try to explain these problems. So the cross-transactional dependencies look like that. You have transaction 4. It modifies the state, and it makes the transaction 5 invalid. So when you receive the transaction 5 individually, it seemed valid to you because A was equal to 0. But now you started building a block, you include transaction 4 first, and now A equals 1 and it's not a valid transaction. And in order to work around this issue in general, we just have to split the transaction into two parts and have the validations run separately from executions in their separate place in the block. So these are still three account abstraction transactions, but their validation parts are separated from them and they run before any execution code starts running. Now you may ask, but what if validations invalidate each other? What if the validation code changes the state that another validation uses? And in general, it would be possible. And in order to prevent that, we need to sandbox the validation code to prevent it from doing certain things that it should not be allowed to do. So what are those things? It's accessing other people's storage and accessing environment opcodes. So environment opcodes are block number, timestamp, base fee, everything that may change from validation and execution or between phases. And other people's code is code in other contracts unless it is in a mapping mapped to your address. And on layer 2s, it's also any stateful precompile. So doing this is illegal. All other things are allowed in validation. And it allows us to do many great things. You can use tokens, you can transfer tokens, you can do all of that in validation functions. There are other small complexities. One example is you don't need to invalidate a transaction to make it hard for a block builder to build a block with account abstraction. One good example is unused gas and using unused gas as a vector. For example, you're building a block, you include five transactions, you see that you still have a lot of available space because transaction four requested 10 million gas limit but didn't use it, so you start adding another transaction to your block. And what happens is, transaction four saw the change in the state and started using more gas. And now you are, like, you have a recursive,-and-egg problem because now the transaction 6 doesn't fit your block and you need to exclude it. And you can get to square one. We do solve it by introducing a gas charge on unused gas. But I'm just using it to showcase the kind of problems we need to solve when implementing account abstraction natively. And another thing is maintaining an efficient mempool to receive a huge number of transactions simultaneously and validate them. You need to parallelize their validation. So assume, like, here's a block builder. It has six CPU cores, and it's performing validation of six transactions in parallel. So it runs them individually, meaning they don't access each other state. And if it finds a transaction that is not valid individually, it gets included from block building. And the next step for the block builder is to validate all transactions that are remaining in its mempool and build a valid block. If we were not able to separate the scope of validation code in one of these transactions, what could happen is we could have a mass invalidation event. When one transaction changes some state and makes all other transactions in your block invalid. And it provides a huge DDoS vector for mempool participants and block builders. Because we don't want them propagating invalid blocks. Now, developers who are interested, especially if you're working on layer 2s, what can you do to make native account abstraction happen? First, do get in touch with us on any of our channels, Discord, Telegram, and let's talk about native account abstraction. Do just read and get familiar with either both RIP 7560 and EAP 7701. We are looking for feedback on it. And you can also dive into the code. This is a link to our reference implementation of RIP 7560. There is also a roll call event for the RIP process. I guess everybody knows it. Just join it, add RIP 7560 to the agenda and discuss it with other layers too. And let's start building it. So here are our websites and that's it for now. Thank you. Thank you so much, Alex. Let me start with questions. The first one is if 4.337 bundlers don't yet support the aggregation portion of the spec, how can we be ready to make it part of the protocol? Yeah, so aggregation is a part of 4.3.3.7. It will be part of native account obstruction eventually. We have a draft EAP for that. However, it's a complex topic in itself. The aggregated signatures are complex, and there has been little adoption of aggregation yet. It does provide additional challenges in the context of native account abstraction, but I think we will overcome them. And again, our approach is to make all these changes very, very modular. We don't want to make one mega account abstraction spec and implement it in one fork. We want to make basically as many EAPs as possible, reasonably, so the chains can adopt them meaningfully, but one by one. Next one, how critical and how centralized is the bundle? Are the bundled? Is the bundled? Can I see? Sorry. How critical and how centralized is the bundled? I guess the bundler. So in ERC-437, bundler is critical. Not very centralized if you use it correctly. So if you're using it as a mempool, that is not like... You don't depend on any centralized bundler. With 7560, the role of the bundler changes, it becomes more of an assistant to the block builder, and then it's as decentralized as the underlying network. There is no added decentralization vector on the bundlers. Does that make sense? Yeah, okay. Thank you. Next one, we have 433775607702. Can you describe how they all play nicely with each other and how can we avoid once again fragmentation in this space? In this case, with regard to account abstraction solutions. Right. So 7.7.0.2 does not pose any fragmentation challenges whatsoever because this is just a new feature of the EVM and I assume it will get supported pretty widely and it's also an addition to account abstraction roadmap. With 467 and 7560 we keep the user flow so similar that it can have very minimal friction in terms of fragmentation because if you have two accounts and their differences are so minimal, I assume just most wallets would support both and both of them will coexist peacefully for a period between now and when 7701 is implemented on mainnet. And so they are all still part of the same ecosystem, so it's not fragmented. It's just different flavors of the same thing, if that makes sense. What's your recommended roadmap for MetaMask to achieve the end goal of account obstruction? Yeah, so MetaMask and other wallets, UA wallets, it would be great if they started looking into account abstraction. They can start with 7702, with getting all users to also include some kind of code in their EOAs. It can be very simple, like recovery or gas sponsoring and everything, but crucially it will get a production experience of having and managing a fleet of smart contract wallets because up until now, most wallets have only been managing the private keys part and did not look at users' wallets as smart contracts. And after that, there is no reason for MetaMask not to use ERC4337 wallets. For people who are starting now, you probably don't need to steer them into using 7702 if they are newcomers. You can just get them to deploy an upgradable proxy smart contract wallet. And as long as it's an upgradeable proxy, they can evolve together with account abstraction, like versioning and all that. Right? What do you mean by other code, in brackets, section at 7701? Yeah, this is referring to this slide. I don't have slides. So you can have any number of code sections in an EOF account. So any code that does not have a role assigned to it, as we described it, is just an external code, so it can be called by, like, other contracts or other addresses can just call into this code and if the code is assigned a role you cannot call into this code it has to be executed as part of a eap 7701 transaction right it's uh so this is the difference between like other code and a code with an assigned purpose in terms of account obstruction. Now, the last question. What's the point of separating validation and execution in a theorem's account obstruction if conflicts can still arise during execution, even after validation under restricted rules? Yeah, great question. The problem is if the execution conflicts with each other, you can get a state you didn't initially want. For example, you can get a transaction to revert, but the transaction is still valid. And another thing is execution is not limited, not sandboxed. It can be a 20 million gas operation, and it can write all the storage in the world. Validation is limited and sandboxed. So we extract all the validations because we assume they are small functions, relatively, like I don't want to say pure, but clean functions that only access accounts on storage and if they were to collide that would make block invalid. The difference between the reverted transaction in block and in the valid block is the difference between a block builder being DDoSed and somebody having to redo his action.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:30:00.000Z",
      "slot_end": "2024-11-14T06:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1sZ2P4U7wWwVav4ska4SCGMtylu-lx2sWw0ymD92gTtY",
      "resources_slides": "https://drive.google.com/file/d/1f6xZ-3MnyS8zHxF7QZ4PxZY3KGHNTrkv/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "open-source-orchestra-zukaraoke-ktv",
      "sourceId": "JBCULT",
      "title": "Open Source Orchestra - ZuKaraoke KTV",
      "description": "OSO brings karaoke to Devcon!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "Beginner",
      "audience": "Hobby",
      "featured": false,
      "doNotRecord": false,
      "tags": "Art,Free Speech,Social",
      "keywords": "Music,Karaoke",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:30:00.000Z",
      "slot_end": "2024-11-14T06:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1LRNlRRa-nWIkUZN0OzhHcccD4YwYKnOZT21n-IMTU0Q",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "public-epistemics-and-futarchy",
      "sourceId": "3UX8GZ",
      "title": "Public epistemics and futarchy",
      "description": "35 years ago I began outlining a vision of how betting markets could offer informed credibly-neutral estimates on far more disputed topics. I elaborated 25 years ago on how decision markets could support neutral governance, and 21 years ago on how combinatorial markets allow estimates on all possible combinations for existing topics. Now in the last year, we are seeing substantial crypto-based trials, especially re governance. In this talk, I’ll paint a picture of where all this could go.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Economics,Free Speech,Futarchy",
      "keywords": "",
      "duration": 934,
      "language": "en",
      "sources_swarmHash": "24f7e97b848f1e0a67b50759d3d1eae014903a3d32ad8ddcc76d255c18ebeddb",
      "sources_youtubeId": "Kvl0LrwtE8k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67381fca1b0f83434d0dfd4b",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:30:00.000Z",
      "slot_end": "2024-11-14T05:45:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1P1IH_O2NLxK_MXtmkfR8Yb6EoLR6gV1arcuKrkGimqE",
      "resources_slides": "https://drive.google.com/file/d/1_-6w2mR0yjyEa0K_e7PqyfES6Fffbq_O/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "unpacking-eof-applications-in-developer-infrastructure-and-tooling",
      "sourceId": "87XNSS",
      "title": "Unpacking EOF: Applications in Developer Infrastructure and Tooling",
      "description": "In this talk, we will delve into the Ethereum Object Format (EOF), a pivotal component of the upcoming Pectra hard-fork, focusing on its profound implications for development infrastructure and tooling. EIP-7692 introduces a new execution environment and a structured format for executable code, bringing extensive changes to the Ethereum Virtual Machine (EVM).\r\n\r\nHow will it affect developers? What will make their lives harder and what easier?",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Developer Infrastructure,DevEx,EVM,Core Protocol,Developer Infrastructure,DevEx",
      "keywords": "EOF,EIP-7692,EVM",
      "duration": 494,
      "language": "en",
      "sources_swarmHash": "49566c098cc805e61220bffaf5ba1387699a7cdf7b842332c6df071ece86c14f",
      "sources_youtubeId": "OsKyVPdpJgI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc669982f234a120c4b7b",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:30:00.000Z",
      "slot_end": "2024-11-14T05:40:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1yIsFqKcISo1wBOpMh8bQqTwKa7ihE8HDSAKmoWXYRs8",
      "resources_slides": "https://drive.google.com/file/d/19qxK_vp8gI9EHTLEgsFBQzrk91_fK1gv/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "emilie-making-sure-eof-is-done-right",
      "sourceId": "A9UWAY",
      "title": "Emilie - Making sure EOF is done right",
      "description": "We present Emilie. Emilie is designed to ensure the correct implementation of the EVM Object Format (EOF) by testing compilers and execution clients. It re-executes mainnet transactions using EOF bytecode instead of original bytecode, comparing results and performance with the original execution.\r\nEmilie tests interactions between EOF and legacy contracts using real data. It supports recompilation for Solidity and Vyper, enabling it to find bugs across compilers and execution clients.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,ACD,Testing,eof,ACD,Core Protocol,Testing",
      "keywords": "EOF",
      "duration": 461,
      "language": "en",
      "sources_swarmHash": "cec267e1746d3de31fb7bdb80db78174214955db002b55c99c107ca8180c490a",
      "sources_youtubeId": "igLOej4GFV0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673594ae9dbb7a90e1313871",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673594ae9dbb7a90e1313871.vtt",
      "transcript_text": " Yeah, hello everyone. It's my distinct pleasure to today introduce Emily, which is our small contribution to hopefully make EOF safe, so to make sure everything in EOF works properly. So not just right before me, but today already multiple times, EOF has been discussed. So in case you just walked in because you were especially interested in Emily, I'll give you a short recap. So what is EOF? So there are two talks listed here by Dan O'Farran and also from this morning, which are really good and really summarize it well. But for the purpose of this talk, we're just going to say EUF is a big change in EVM. So what is the problem with that? Because as in with any big change in EVM, a lot can go wrong. So we have a lot of different smart contracts, right? And we are using different compilers. I mean, there's Solidity and there's Viper, and they have a bunch of different versions. They have different optimization features and so on. So we actually have quite a few compilers that we're using. And then out of that, we get lots of different bytecodes. And these bytecodes we execute on different execution clients. And on these execution clients, these new bytecodes interact with many different existing contracts. So we have a horribly big search space to look for things that can go wrong, right? And experience shows that things do go wrong. And unfortunately, things don't always detect it during testing because we had chain splits after certain hard forks on chain. So what do we think can go wrong here? So first of all, there has been some discussions on what compilers should support, so we want to figure out what is actually supported so that users can have a seamless transition to EOF. In the bytecodes, of course, we can have incorrect compilation. We have totally new features. We might very well see incorrect compilation. And in the clients, we also have very new implementations. It's not implausible that we see incorrect execution. So all of these would be horrible. They would lead to chain splits. And then lastly, we might have incorrect interaction, right? So you're calling an existing old contract from your new EOF contract. Maybe that doesn't work the way you thought it would work. So now, how can we fix all that? So that's why we're here, to improve stuff. So first of all, is this news? Well, no, of course not. So there is already a lot of testing going on, right? So there's the testing team and lots of tests are being written. But we think that Emily can improve the situation because we, in the past past have learned that what really helps is real contracts with real data and real interactions because that's in the end what we want to be sure that works on chain right because that's what might cause the chain split so what does Emily do so here we have an existing mainnet transaction, which you are all familiar with. So now what does Emily do exactly? So we take contract A and we recompile it to EOF and we execute it again. So now we can compare these two executions, right? Because now we actually have a really good idea of what correctness should look like because we have the execution above, which is the current execution, and we can compare all the parts. So we can compare what does contract A do, what does it write to storage, how do all the call data and return data look, how is the output, how is the success flex, and so on. So we have a great correctness reference here. So Emily checks a bunch of things. I mentioned some of them already. So obviously Emily checks all the storage changes. Emily checks which events are emitted. Emily checks the call data. Emily checks the return data and the execution status. And lastly, what we also found quite interesting while developing is that we can monitor some of the gas costs. So we can see overall is UF execution on average cheaper or more expensive than the previous execution. And with that we can then find all these things that we're really concerned about. Missing compiler features, incorrect compilation, incorrect execution, incorrect interaction. So all these horrible things we're really concerned about, right? Missing compiler features, incorrect compilation, incorrect execution, incorrect interaction. So all these horrible things we want to avoid, we can hopefully find before they are happening on-chain. So with that, Emily contributes to the security of EOF. Of course, as is the usual case, things are not quite as simple, so a bunch of things can go wrong. I'm happy to discuss them a little bit more during the questions, but one of the things that can go wrong is as was previously discussed, the gas oil code is disappearing, but currently it's still there, and it's causing a bunch of problems, as was just discussed in the previous talk. But yeah, we have some countermeasures for that, so I'm happy to discuss this in the questions. And with that, I close, and thank you for your attention. Thank you, thank you. So, let's start with the questions. Are there instances where one may want to rewrite their contracts for EOF for better performance or other reasons? Yeah, I guess we're going to see. I think the truth is that we don't know exactly what the numbers are going to be like because we haven't seen the latest compiler versions and we haven't seen the full EOF spec. But I think that's quite possible, because it's feasible that there might be contracts which are significantly cheaper when executed in EOF, yes. So is the recompilation to EOF done from source code? Where do you get the source code? Yeah, excellent question. So we get the source code from Etherscan, and also other sources, but one of the sources is Etherscan. So obviously we don't have source code for every contract, but that's also not necessary. I mean, we just want to test as much as possible, and it's actually good that we don't have source code for everything, because we we as I said we also want to test this interaction with legacy contracts so we don't want to necessarily recompile everything but yeah we recompile some and then test the interaction. If Emily compiles legacy bytecode to EUF how does it cuts compiler bugs from a high-level language to EUF? Yeah, so basically we take the code and we previously ran it with the real data, right? So, for example, let's consider a transfer, an ESC20 transfer. And let's say the ESC20 transfer now works incorrectly when compiled with EUF. Now what we would see is we would, for example, see a difference in the storage. And because we check how was the storage changed originally, and now we check how is it changed now that we ran it based on EUF, we can detect those kind of correctness issues.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:40:00.000Z",
      "slot_end": "2024-11-14T05:50:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/17yJsWv6HioxijpCWnMnLMPeQFMTy_KMomUQHF2n1hS8",
      "resources_slides": "https://drive.google.com/file/d/1d3Itf_V91wqf2Ls9R_iXHGlVEesGPbtQ/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "ai-agents-x-prediction-markets",
      "sourceId": "SXBG73",
      "title": "AI Agents x Prediction Markets",
      "description": "This talk will explore how autonomous AI agents are participating in prediction markets today, covering live examples and their underlying technology. Furthermore, the talk will discuss the benefits of AI participation over human participation for the future of prediction markets and their wide spread adoption.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Mechanism design,Use Cases,User Experience",
      "keywords": "AI,agents,prediction markets",
      "duration": 329,
      "language": "en",
      "sources_swarmHash": "fbab6c95bd58ec5a8e52d6a7cccf31a9f87c034153e772123cb5213be529f4ce",
      "sources_youtubeId": "rHLxZ28vQ94",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673595089dbb7a90e13781a5",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:45:00.000Z",
      "slot_end": "2024-11-14T05:50:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1m1KqfSb19Pc7LjXcjrEe4jJpS6Jxx1LleB07azv4ilg",
      "resources_slides": "https://drive.google.com/file/d/1y4zGXssVsHqWwaNPbqiWzUhNAzP7rhc3/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "prediction-market-panel",
      "sourceId": "CCZCSH",
      "title": "Prediction market panel",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 928,
      "language": "en",
      "sources_swarmHash": "7f1382c2276a5b8073634d3ef9ea7e087d715d1817fdfb2383dfc53dbe76bdbb",
      "sources_youtubeId": "oq34OAKrU5M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673595919dbb7a90e13b23f7",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673595919dbb7a90e13b23f7.vtt",
      "transcript_text": " Hello everyone and can you hear me? Awesome. I'm Vaughan Mackenzie-Landell. I'm one of the co-founders of Butter. We've been working in the governance space for a very long time and Obviously very excited about futarki and prediction markets and that's why we're all here We have this incredible panel full of lots of people who my love and admire And I'm gonna just hand over to each of them to give a very short introduction and then we'll kick off Yeah, I'm Martin. So in a previous live venosis, we started with prediction markets and Wanted to implement food hierarchy and other things. So in general, I'm absolutely a super big fan of that, but I think in this panel I will a little bit play the skeptic here because I think as Robin mentioned, you have the idea and then you have all the details that need to be figured out. So I think I can a little bit bring in that perspective. I just talked, so I'm Robin Hanson. So I'm Calvin, also CAS.if. I'm the co-founder of Utarki.fi. We just announced our project a few days ago. Our plan is to help DAOs and other organizations make great, impactful, positive decisions. Hi, everyone. I'm CJ from Limitless. We're building, like, basically we're processing around a million dollars a day in bets right now for, like, zero-day contracts on financial markets. I believe that believe that like with crypto, we can build the world's largest economy on chain outside the jurisdiction of any one particular nation state. And I think that as well as that, we need to build really efficient global marketplaces on top of the infrastructure. And so that's what we're doing at Limitless Labs. Thanks, guys. Really appreciate that. So we're here at DIAC today. And so the first question is, how do prediction markets tied to DIAC, how do prediction markets and epistemic tech make the biggest impact in the vein of DIAC on differential progress, defense, and democracy? Over to any one of the panelists. One ring to rule them all. If you could have reliable prediction markets on whatever policy question you have, then you can just do better on all of your other areas of life. You could better know how to pursue longevity, better know how to pursue decentralization. Whatever it is you're trying to do, having better information on that can make you do that better. So could we do much better? And how does that map directly to, say for example, defense? I can just replicate that. I think the promise is very clear. So the promise is whether we we talk about Even simple things like this community node or so from micro decisions whether to show that community node or not to show that to Extremely large macro macro decisions should we raise the interest rate or should we make this huge? Well defense spending here or there. In general, the promise is that this is a tool for more robust decision-making to bring in, yeah, to have a better quality of bringing information that actually shows the true impact of a decision. Yeah, so I'm going to make a comment here. So over the last few weeks, everybody has seen a lot of people are talking about prediction markets, of course predicting who's going to win the election, and this generated a lot of excitement. But actually, the most interesting thing is not only predicting probabilities, that's what normal prediction markets do, but predicting the consequences of things. So if you could have, for instance, conditional markets on inflation, on GDP growth, on spending, on unemployment, conditional on candidates, right? This would be even much more interesting for democracy. You could actually see like a scorecard for the candidates, right? Like a game, you can pick one. This one's better on this one. This one's better on this other thing, right? So this is the promise of using conditional markets for decision-making. Yeah. Sorry, Monik, go ahead. Maybe let me throw in one kind of concept to challenge this idea. So to say, I mean, I'm just pulling up here CoinGecko and look at prices of crypto. And, well, they change by 6%, 7% a day. So I think prediction markets rely on the assumption that markets are so efficient that out of those prices, we can get reliable information. So I think prediction markets rely on the assumption that markets are so efficient that out of those prices we can get reliable information or better information. But I would argue well there is also a lot of noise in markets and I would say that markets here, prices change by six percent is largely not related to any meaningful signal, but is... Well, I need to be careful in the statement, but there's still a lot of noise. So that's, I guess, my question. How can... Is there enough signal in the noise? So when you compare head to head this mechanism to other ones, consistently this wins. So that means all those other mechanisms have more than 6% mistake. Think about any committee you've ever been on. Think about the gossip network. Do you really think that's within 6% of the truth? Come on. Like being 6% close is great compared to what we usually have. Yeah, I would, I guess I would come with the, like the whole question from, from a little bit of like a different angle. I think that definitely prediction markets have a lot of power in terms of forecasting and decision making. But I think in a crypto context, what's really interesting and powerful is the fact that essentially we can build these global efficient marketplaces. And so I really believe that markets in general accelerate human progress. If it wasn't for markets, we wouldn't be where we were today in almost every field. And even they have positive trickle-down effects to science funding, for example. Brian Armstrong sells his Coinbase stock to fund moonshots. And science funding, that's a trivial example in a global context, but still kind of proves the point. And I think that building prediction markets onchain can be defensive in the sense that we actually have a capital formation and wealth generation outside of the nation-state in this kind of global on-chain economy and I think why that's defensive is because we can build this kind of resilient independent decentralized systems that manage to kind of form their own capital, like without, for example, being taxed on it, also without the very heavy regulation, which is a big issue. There are only 16 designated contract makers in the United States. There are 16 license derivatives exchanges, but money cannot move globally at the speed of info in the traditional system. Here it can, and we can build these really powerful marketplaces that are global and accelerate human progress. I completely agree. I think markets are a very important piece of technology. I actually do believe that one of the reasons that we are able to accelerate defense, particularly with prediction markets, is because we're basically getting rid of any authority's ability to lie to us, right? Because we can delegate information and truth to markets. I think this is great. And actually, speaking about this, if you think about Futaki so far, as you said, we've had lots of messy details. I'd love to know what you think the key challenges in implementing Futaki will be for a world organizational decision making. Right. So some challenges, of course. Robin mentioned the matter of the autist in the boardroom. So there's always this risk that prediction markets and also decision markets are, in some sense, threatening to the power of insiders and some organizations that maybe are not making the most efficient decisions. So this is for sure a big challenge. So another challenge that is marching up, I'm going to try to connect the previous question from Martin to the combinatorial market idea from Robin, which is part of our basically our grand vision on fitarki, which is so the matter of noise When we're evaluating potentially a small or medium-sized proposal or decision in a company Many of those if there's a smaller medium, you're not gonna have a clear impact, visible impact on the share price, so that makes it really hard to use this directly to estimate, but if you've had a full network or a full tree where you can see, like you could have markets for share price or token price, but connected to these you also see the KPIs, markets for different KPIs, number of users, revenue, all the kinds of metrics important for the company. So you could actually see, okay, maybe I don't, there's a lot of noise, I don't see the impact of this decision on the token price, but I see the impact of this decision on the token price, but I see the impact of this decision on this minor KPI that has, but it's minor KPI associated with this major KPI, which isn't then related to the share price. So I think that's a, this is a big trend region for many decisions is actually have the markets, all the information connected to each other. Maybe to kind of reorient the question, what problems are you running into building and operating these things? Prediction markets, futakis, what are the hard problems, what are the solutions? I can say one hard problem, relatively hard problem is to just find traders that are comfortable going into those positions. And just to give you an example of, let's say we would do this for Ethereum. We would say, okay, conditional, we might go this roadmap or that roadmap and then we want to use this food mechanism, so essentially people can say, conditionally to this roadmap, I buy Ether and conditional to that I sell. But then to practically do that you need to believe that going back to this correctness of prices, I mean yes 6% is in a day but I mean we really, what is really the fair price of ether right now? And I think there could be a range of some people believe it should be 10,000 and some people would believe it could be less than 1,000. So if you are in the camp of it should be 10,000, then kind of probably in both decisions, you would say, well, I want to be long on Ether, I want to hold Ether and if you're in the camp of well this is really not worth it, it's just worth 10,000 and again in both sides, so you need that person that says well the price is pretty much right now and really that decision makes a marginal difference for me to hold the asset. And then let's say there's a really good proposal and you really like that, so you go long on that. Now you hold this conditional yes. Now the next day, some external completely unrelated event happens where the whole crypto market crashes by 50%. You hold this yes proposal and hold it under the price from yesterday, you still think the proposal is good, but now your incentives are for the proposal absolutely to not pass because then you would hold all the dollars and not the asset at a price that's now 50% or where the real asset is now 50% cheaper from what you bought it. So I'm not sure if anyone could follow, but those are kind of the challenges I see. So 20 years ago we had a burst of applications of prediction markets, including in corporations. So I think we saw typical failure modes there that are instructive about today. Typically if you went to work with a company about setting a prediction market, you would say, well, what are your most important issues and numbers you'd like to track, and let's set up markets on those, and they would usually say, well, that's a little sensitive. Let's do something a little safer, and they would pick safer topics, and then they'd get accurate estimates on them and satisfied users, but they go, yeah, but we didn't really care about those. So, you know, that would be one outcome. Or, you know, they would say, talk about something important, and then management couldn't resist having opinions that disagreed with the market, and the market gets proven right, and the management wrong, and they're just really mad and want to kill it. So, for example, the US government missile test agency had prediction markets on which tests would actually go forward. So they put up, they try to make a lot of tests, and a lot of them don't actually happen, because they have to coordinate a lot of different parts of the military, make a test happen, and if one part isn't there, the whole thing has to be scrapped, and a lot of money is lost. And so they wanted to know which tests would actually go through it and maybe save money by cutting back earlier but then it was more legible that they knew ahead of time that it wasn't going to work and that was a management problem so they didn't want that anymore they'd rather pretend they don't know how these things are failing yeah uh like i guess i'm gonna like talk about this from more like consumer product building perspective, less like the future side. And I think definitely like historically, it's absolutely true that marketplaces are like very hard like products to bootstrap and to activate like the atomic network effect. I think that like it's definitely like to incentivize like market makers or liquidity providers, especially for things like pop culture markets, right? I mean, you can always build the model for, for like elections or sports markets, or especially in limitless cases like the financial markets, right? You can use the volatility to understand how to price them. And so it can be attractive to institutional market makers because it's essentially like a short term retail retail option flow but at the same time you know you may look at Polymarket how much they spend per month to incentivize market makers to come like into their order book actually I spent a lot of time previously and like I spoke to Robin about it in Berkeley like kind of obsessing about incremental improvements in the AMM which we see like for example like paradigm just launched the PMAM, right? We're actually going to spin up a contract for it to see how much of an improvement is it. But I definitely think that challenge is like, how do you incentivize the market makers? Either you use cash or you use your token or what, but it's still a huge expense for bootstrapping like the initial network, yeah. But maybe over time it will be worth it. All right, brilliant.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:50:00.000Z",
      "slot_end": "2024-11-14T06:05:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Rm-aNAjKTe4WozwIfgJQZhQN1chtswB5-fuDukQWE5k",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "the-future-of-eof-layer-1-layer-2-and-beyond",
      "sourceId": "9EBQ3H",
      "title": "The Future of EOF: Layer 1, Layer 2, and Beyond!",
      "description": "While the EVM Object Format provides a mechanism to modernize the EVM, the container format itself provides a stable path for innovation and experimentation within the base and rollup layers of ethereum, as well as rollup layers, and even chain free execution.\r\n\r\nIn this presentation we will show how the structure of the EOF container may be adapted to support these potential use cases.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,EVM-equivalent,Politics,EVM,EVM-equivalent,Layer 1,Politics",
      "keywords": "EOF,EVM",
      "duration": 1363,
      "language": "en",
      "sources_swarmHash": "0ffb833072a1d18d73796d3f2897b4dc1730db7e1b9afca32f958cb51ceb815b",
      "sources_youtubeId": "NeKMerFPJoM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc6b5982f234a120cec86",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T05:50:00.000Z",
      "slot_end": "2024-11-14T06:20:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1xsXLO6lk8scS1Bau7a1gPEtC1QKpw5GdJrAD2ZppNaI",
      "resources_slides": "https://drive.google.com/file/d/1p4Rn8g0ziyzu5jLvAwnPS699M6WjJF_d/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "building-infrastructure-for-mud-worlds",
      "sourceId": "GTCEVW",
      "title": "Building infrastructure for MUD worlds",
      "description": "Thanks to advances in tooling and frameworks such as MUD, autonomous worlds have come a long way in the past few years. But technical barriers such as latency and onboarding still plague the space. In this talk, the Lattice team shares some learnings and work done on minimizing latency and improving onboarding for MUD worlds.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Autonomous World,infrastructure,Autonomous World,Developer Infrastructure,Gaming,Tooling",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "d6df65d55acff6d99267da83f0521b74b5e8c8a06936b8d73bb43b753e8a2b24",
      "sources_youtubeId": "WBzfE5KYRTU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "bLCu7FxPJ-s",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T06:25:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1BpeEbVQlu2yDNQAlW5vuzWhq0v-7ANSaWNSni086zr0",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "djing-pino7",
      "sourceId": "SPWJHX",
      "title": "DJing - pino7",
      "description": "I am a builder and a volunteer in Devcon SEA. Back in the days I've decided that I wanted to become awesome, and here I am in my journey. I am UX/UI Designer and I am becoming a React Developer. I have always being passionate about music. And there's always space for it during my life journey. I love communities, people, organizing events and playing some good music.",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T07:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1FZiG2A1-zzZBVPF6IvnlZPydiJX9JFyp4ngPzFzJTEo",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "erc-4337-adoption-analysis",
      "sourceId": "SGRFUA",
      "title": "ERC-4337: Adoption Analysis",
      "description": "Since the EntryPoint contract was deployed, millions of smart accounts have been created and UserOps submitted, via hundreds of exciting projects in the space. Join us as we look at the interesting trends onchain and the unique challenges and exciting opportunities faced by teams building in the space",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": true,
      "doNotRecord": false,
      "tags": "DevRel,Use Cases,Account Abstraction,erc-4337,Account Abstraction,DevRel,Use Cases",
      "keywords": "ERC-4337",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "7c4218b2473a3775a377c88b4fb4f3da5763b000ac0bd90e68fe39a47d330b93",
      "sources_youtubeId": "qgrkOarhBzo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T06:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/17M-nImCJUoQMma2tumjGUf2IgWOapEl76FIQWV-y4XA",
      "resources_slides": "https://drive.google.com/file/d/1WsX1h6HdMVSXQbGAL1-KLUHVwhbzRJWE/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "ethereums-values-and-ethos-alignment-pre-merge-to-now",
      "sourceId": "UHAESN",
      "title": "Ethereum's Values and Ethos Alignment: Pre-Merge to Now",
      "description": "If you ask Ethereans to describe \"What is Ethereum?\" in 1 sentence, what would it be? Likely, you will get many different answers depending on who you're speaking to. Some visions have changed over time and some stayed true to the cypherpunk values such as decentralization, trustlessness & censorship-resistance. Or is it more important for us to focus on DA & scalability at L1? What should L1 actually be responsible for? Is local block building dead? Are timing games bad? What do we value today?",
      "track": "Cypherpunk & Privacy",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Ethereum Roadmap,Coordination,alignment,Coordination,Ethereum Roadmap,Layer 1",
      "keywords": "ethos,values,alignment",
      "duration": 3274,
      "language": "en",
      "sources_swarmHash": "48570ff43a87a9358ca409423d50e18a75c02e7c2ac81f5234dbbc3fa5501617",
      "sources_youtubeId": "0-plV0zJbxs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a04c9dbb7a90e1a35460",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T07:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1pDeSitEvmVhEFya_w3q8q2Uq4_YVvfaQsg5BA5nTUaI",
      "resources_slides": "https://drive.google.com/file/d/1hJkWPBPStt_lVLOdvAn_9M752WGCqWgx/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "learn-huff-to-become-an-evm-chad",
      "sourceId": "HRMCBK",
      "title": "Learn Huff to become an EVM chad",
      "description": "Become an EVM chad by learning Huff, a low level assembly language for the EVM! On top of being able to write super duper optimized smart-contracts, Huff will teach you how the EVM works under the hood and will let you master high level languages like Solidity or Vyper.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Languages,Open Source Software,Best Practices,programming,Best Practices,Languages,Open Source Software,Tooling",
      "keywords": "Education,Huff,Programming",
      "duration": 6545,
      "language": "en",
      "sources_swarmHash": "be66d9d7f9ede2c00c9b1a1058e72e7c830a1b5e3e0a89765651e49d331857a1",
      "sources_youtubeId": "5j0HmFlwe68",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735bba59dbb7a90e19f5402",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1-l5GZfkJD_jGXx19MZKctGeyeRotdNV_0HKanpnUjLU",
      "resources_slides": "https://drive.google.com/file/d/1Fzu4jDSv_P8D_74GYYQTa7ZRGLuJxe4R/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "multi-party-fhe-for-multi-player-privacy",
      "sourceId": "S9S8M9",
      "title": "Multi-Party FHE for Multi-Player Privacy",
      "description": "Privacy is an unsolved challenge for blockchains and decentralized systems. ZK cryptography gets us there partially, but not all the way. ZK enables “single-player private state,” and certain other kinds of privacy are impossible to realize with ZKPs alone. Panelists, the cryptography library devs, infrastructure builders, and application devs who have recently started to explore programmable encryption will discuss MP-FHE as one such tool for achieving more general privacy capabilities.",
      "track": "Applied Cryptography",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "mp,fhe,programmable cryptography",
      "duration": 3291,
      "language": "en",
      "sources_swarmHash": "ef23b5807bd63ac3c58c1a0c65809124f3f2a09a654b4ad659a05122a52ff664",
      "sources_youtubeId": "Md1LKfuBGFo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a1499dbb7a90e1b01876",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T07:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1i64ImNoehhB-Dnpix_z7zP--wGTsTmeikoll2OE-lGI",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "next-generation-amms-eliminating-lvr",
      "sourceId": "8DCP9K",
      "title": "Next Generation AMMs - Eliminating LVR",
      "description": "Loss-Versus-Rebalancing (LVR) is the most significant form of MEV, yet it has the fewest solutions addressing it. LVR remains a significant challenge for AMMs. This session delves into a comprehensive analysis of how CoW AMM addresses the problem of LVR through its unique batch mechanism. Drawing from 9 months of empirical data, the talk will explore the effectiveness of CoW AMM in mitigating LVR and offer insights into the impact of LVR resistant design on trading outcomes and market efficiency",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "MEV,AMMs,lvr,AMMs,MEV",
      "keywords": "LVR",
      "duration": 1516,
      "language": "en",
      "sources_swarmHash": "0f34f3bc88fee6ea2aa68cadc3132cdc7973a7cd04c404abb8c9b80574351b71",
      "sources_youtubeId": "hOrDqlGcmJE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a84e9dbb7a90e155c38e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T06:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1Zivx1-urETlnczibMYsiNyH4-ey3zg3vSAD7YDHJeJk",
      "resources_slides": "https://drive.google.com/file/d/1-wvMOXn0NNLpn1rX7wXgANIWlzn52dDP/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "the-future-of-ai-why-we-need-private-uncensored-permissionless-ai",
      "sourceId": "EK8T9X",
      "title": "The Future of AI: Why We Need Private, Uncensored, Permissionless AI",
      "description": "The current path of AI development leads to a future where a few powerful companies control this transformative technology, with the potential to become the arbiter of truth, manipulate and monetize private user data, and moderate who has access to the future of intelligence.\r\n\r\nNo entity, private or public, should have the power to monopolize or contextualize truth. Open-source, uncensored, and decentralised AI is impervious to political fancy and ideology, and offers a necessary alternative.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Censorship Resistance,Permissionless,Privacy",
      "keywords": "AI",
      "duration": 457,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "B_5wj6TfX8s",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67359da49dbb7a90e17463e3.vtt",
      "transcript_text": " Hi, guys. I hope you're having a great DevCon. It's great to be here. I've got five minutes, so I'm going to go really fast. Bear with me. So imagine a small group of companies control access to the most powerful machine learning used by billions of people worldwide. They record all your AI conversations, monetize your data, and share it with governments on demand. Everything you share is attached to your identity forever. They record... Whoops. What's going on here? Okay. Sorry about that. Through politicized content policies, their AI models are trained to coddle and redirect you when you explore topics determined to be taboo. They restrict and redact information and influence your thinking based on their view of the truth. Does this sound like science fiction? It's not. It's already happening. The current AI development path leads to a few powerful companies controlling the technology and becoming the arbiter of truth. The race to dominate the consumer AI market is on. OpenAI and by proxy Microsoft have a head start. Their partnership puts AI in the hands of 1.5 billion iPhone users. The Biden administration's policies entrench AI development in the hands of a few powerful entities, accelerating centralization that favors incumbents. It's notable that the newly appointed U.S. Artificial Intelligence Safety and Security Board doesn't include any open source or decentralized AI leaders. I believe AI should be optionally private. Our interactions with AI are personal and intimate. This isn't the polished... This is not the right slide either. Sorry, guys. This isn't the polished social media version of us. It's raw, honest, intellectual exploration. Would you share your diary online? That's the level of vulnerability we expose when we use AI without privacy safeguards. Popular AI tools store your inputs and outputs, and in many cases, the platform owns the outputs you create. As I mentioned earlier, these inputs are attached to your identity. Platforms are vulnerable to hackers, which continually lead to breaches. Does anyone remember Equifax? Information held by governments is equally vulnerable, and the authorities making the privacy rules can't even protect their own data. A recent EU Parliament data breach exposed sensitive personal data of more than 8,000 staffers. Your data doesn't need to be leaked to expose you to manipulation. Cambridge Analytica demonstrated how information can change the tech giants, sorry, the information you share with tech giants can be scraped without your consent to create campaigns designed to influence your views. We need to stop volunteering our data and take control over what we share with AI. I believe AI should be uncensored. Centralized platforms censor according to often opaque content policies, influenced by the values of those who control the platform, hidden behind system prompts that are only revealed through jailbreaking. Users think they're interacting with true machine intelligence, a calculator programmed to do statistical inference on language. In reality, they're engaging with proprietary partiality within guardrails imposed by humans who have their own biases. At best, we receive nonsensical outputs like Gemini producing black vikings. At worst, human adulterated AI can perpetuate the silencing of public discourse. No entity, public or private, should monopolize or contextualize truth. Open access to AI is under threat. Biden's AI executive order requires licenses for large models that restrict the number of parameters allowed. California proposed an AI bill which includes criminalizing certain open source AI developments. The EU's AI Act is more permissive of open source but strict on large scale AI. The Act's author himself has raised concerns that the regulatory bar has been set too high. A stark example of excessive regulation backfiring. France-based Mistral and Meta both recently withheld their latest open source AI models from the EU, preventing an entire region from accessing advancing AI. Politics will play a role in AI. Some push for tight controls, others champion open source development. The back and forth highlights that AI's future should not hinge on political whims. When confidence in our political leaders is waning, do we really trust them to regulate intelligence? AI is being adopted rapidly. A recent Harvard study found that nearly 40% of all U.S. adults between 18 to 64 have used generative AI. But unlike money or social media, we don't need to change our existing behavior, but we do need to start as we intend to continue. We must seek out and use alternatives. Open source permissionless AI, evolved through thoughtful iteration, is impervious to political fancy, ideology, and the antidote to gate-kept, curated, and censored AI. It's pretty easy for a developer to run a small model privately, locally, using a Lama or a similar service. Open source models are rapidly becoming competitive, surpassing closed source models on many benchmarks. You can find a plethora of open source models on Hugging Face. I've listed a few here. For those who can't or don't want to run AI locally, we created Venice, a generative AI platform that embodies the principles of permissionlessness. You can chat with some of the leading open source models too large to run locally, generate images, create and interact with AI characters, and write and debug code, all in private. Venice uses decentralized infrastructure to run the platform. All of your Venice activity is stored only on your browser. Venice never shares your data with anyone. We simply can't because we never had it in the first place. Use it anonymously and for free. If you want greater amounts of inference or to access the API, you can upgrade to Pro. There's no doubt that AI would change humanity, and we should engage with it, but through mediums that are optionally private, uncensored, and permissionless. Thanks. Hello. Okay, that's amazing. This is time for question right now. We probably have time for one question. Anyone have any burning question at the moment about permission as AI? It's kind of an important part of the... Oh, we'll go. One question. How can you in that system run your own LORAS and your own, how was it called, tuned models. Can you do that? Since you say you do not have the data on your servers. Yeah. So we provide access to open source models. We host those on our service. We have a proxy service. So when you send in your prompt to Venice, it's encrypted, sent via proxy to a decentralized GPU. The response is also encrypted via proxy and sent back to you on your browser. So nothing persists on Venice servers. We don't see your prompt, we don't see the response, and it's never stored. So I cannot upload my own LORAs after I did my own training? You can't upload your own LORuras for training, but just this week, hopefully later today, so I might be front-running myself, we are going live with image-to-image. So you'll be able to upload your own images.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:00:00.000Z",
      "slot_end": "2024-11-14T06:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1kklsZ1YE71cdtzZNkgKNXlsh133eDOoZO3-I29W9u9s",
      "resources_slides": "https://drive.google.com/file/d/1oduMhD9MDwrPtDQAGfBx888DIF9nj_f7/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "privacy-enabled-smart-contract-driven-fair-and-transparent-reward-mechanism-in-federated-ai",
      "sourceId": "LKD3RG",
      "title": "Privacy enabled, Smart Contract driven Fair and transparent reward mechanism in Federated AI",
      "description": "Federated learning enables multiple parties to contribute their locally trained models to an aggregation server, which securely combines individual models into a global one. However, it lacks a fair, verifiable, and proportionate reward (or penalty) mechanism for each contributor. Implementing a smart contract-based contribution analysis framework for federated learning on a privacy-enabled Ethereum L2 can address this challenge, and build the economics of federated learning public chain.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "transparency",
      "keywords": "Federated AI,Smart Contracts,Transparency",
      "duration": 531,
      "language": "en",
      "sources_swarmHash": "8688138370ef7be9ee67412e069fec2019678bcefd1d8d3c719553a958c67365",
      "sources_youtubeId": "TyWi4laTAUo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67359f259dbb7a90e195ae42",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:10:00.000Z",
      "slot_end": "2024-11-14T06:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1aXt8K7kJm7xJ0limjmVm0ZVioUUzgILAGxnm6NBfVoU",
      "resources_slides": "https://drive.google.com/file/d/1kTeYrkTRWF9oVRKQoZY7yi5JXjc4dBrD/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "simulating-an-ethereum-network-at-scale",
      "sourceId": "FAZBAD",
      "title": "Simulating an Ethereum network at scale",
      "description": "Previously, when Ethereum client developers wanted to test their ideas on the network layer, they either had to use a simulation tool that could be used only with some programming language or had to do network emulation instead, which requires a cluster of computers to do it at scale rather than running it on a laptop-size machine. This talk will tell you how to simulate an Ethereum network with 100+ nodes on a laptop-sized machine with production Ethereum clients.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,simulation,Layer,1",
      "keywords": "Networking,Simulation",
      "duration": 1401,
      "language": "en",
      "sources_swarmHash": "803d858f15851efaa0200588d31df2fa0570a608f38b4923e8617c3cea7d94c9",
      "sources_youtubeId": "g-VE038cW1M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736e0e71b0f83434d9fea11",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:10:00.000Z",
      "slot_end": "2024-11-14T06:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1x5qwU96CuNwokAG1SeZ9BSYZKjgzyrpzL5MwVOtxJWQ",
      "resources_slides": "https://drive.google.com/file/d/1kDxXu8LLYnFQuK8zwMi5UTLaZjC_P4nx/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "redis-evm-supercharging-ethereum-calls-with-in-memory-execution",
      "sourceId": "FKVE9X",
      "title": "Redis EVM: Supercharging Ethereum calls with in-memory execution",
      "description": "Redis EVM is a research project that embeds an Ethereum Virtual Machine interpreter within Redis using Lua-based Functions. By enabling Redis to directly interpret EVM opcodes, this innovation aims to drastically reduce SLOAD latency for eth_call operations. We'll explore the architecture, implementation challenges, and potential performance gains of this novel approach. Come discover how Redis EVM could reshape Ethereum execution environments, enhancing scalability and efficiency for dApps.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Scalability,EVM-equivalent,Tooling,execution,EVM-equivalent,Scalability,Tooling",
      "keywords": "RPC,Execution",
      "duration": 582,
      "language": "en",
      "sources_swarmHash": "8fe541e016c7b1993021578ad41a232562bea38e64e3e5058a25a84e327a5385",
      "sources_youtubeId": "8EexwGNrxYQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a10d9dbb7a90e1ad95c0",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735a10d9dbb7a90e1ad95c0.vtt",
      "transcript_text": " Thank you all for coming. Can everybody hear me, had the honor and privilege to do so. Now, I help customers exclusively from the Web3 space to deploy and scale on AWS, which gives me visibility to some of the problems that they face, right? Okay, big one, challenges on RPC scaling. That's super fun. So to begin with, whenever you have a consumer-facing application, let's think about a Wi-Fi router, right? You have to serve all those RPC requests to millions of people. The traffic can be unpredictable, and you might have some huge surges in traffic. So some strategies that folks typically do is horizontal scalability of nodes. Super fair. But it comes with some challenges, right? So first of all is reaction time. The time it takes for you to deploy a new instance. So take the snapshot, load it, copy files, and load the chain data, and also synchronize the difference from that time. Second, that's not a trivial challenge though, by the way. Second, forecasting to understand your user patterns and see how on weekends I have lower usage, and then on Tuesdays I might have more access, and then you react accordingly, proactively. But that is a very specialized task, right? So the caveat is of over-provisioning, which happens a lot. Basically, people paying for more servers with idle capacity. That's not good for any business. Final boss, consistency, right? So when you scale horizontally, well, you suffer from that. You have potentially nodes that are behind the chain. You have, you know, different states across, and people use typically the latest data, right? So you do not scale horizontally to serve archive data. Some businesses, they operate only on current data. So, consistency, I would say, that is the final boss that typically makes people delegate the task of managing these RPC nodes management to infrastructure providers, some of the ones that you know, right? But they suffer from the same problem. So they need to get really good at forecasting reaction time, and that's all the perpetuating and putting lots of pressure on those players. Well, a typical strategy to deal with a lot of read access is caching. Some of those RPC methods are pretty easy. Chain ID would never change. Block by hash, one parameter. Get logs. You can offload the data to some database index, but the list goes on and it becomes even harder. ETH call is the main villain because it is typically used for over 70% compared. So if you want to address this issue, we should first and foremost tackle the ETH call problem. Word of the day, externalization. So can we, is there any opportunity to externalize the processing of EVM opcodes to a very fast engine? So I'm introducing today the EVM Lua project. It is technical validation mode. It is a micro EVM interpreted, implemented in Lua, and it executes inside Redis with minimal storage read latency. And it is able to process, ultimately, EVM operations. So ETH calls, estimate gas in the future, of course, and others. How it works, you can actually select what are the contracts you have there, and then you load the attributes from that contract. So code balance, no storage, all the storage keys, there are scripts for you to do that. And we have some phases. So R&D stage, this is where we're at so loading the entire storage from selected contracts keeping up with the state if processing EVM code strips next up EVM compliance so implementing all the opcodes including transient storage and adding EVM metering, so gas metering. And further steps include deployment, so benchmarks, optimization, feedback loop, and building for catering to user-specific features. So those are baseline numbers from Amazon ElastiCache benchmark using, of course, the simplest data structures. This is not our project yet. But 1.2 million requests per second for a single instance. That is a lot, right? And in a cluster, you can have over 5 million requests per second if you scale those Redis nodes horizontally. So here's the project. Have fun. Please start it. And, well, thank you very much. Thank you so much, Everton. So now we'll move on to a quick Q&A. So we have one question thus far. What are some potential security risks of embedding an EVM interpreter within Redis? All right. Interesting question. So you can have several guardrails for that. But because you have control of the opcodes that are being executed, that would be only for reads and not to write. So you can separate who writes and who reads at a user level and have several folks only reading from that in-memory database. Does that answer the question? Whoever asked? I think so. Okay, just on time, we have another question. How large is the RAM size needed? That's another great question. So to begin with, only, you know, just a couple megabytes, and it is already enough to start playing. And it depends on the contract storage you want to load up there, right? So on AWS, the instances go up to 2 terabytes of memory within a single instance. So the sky can be really the limit here. It could fit the entire state. All right, and I accidentally pressed answer for one of the questions, so it's not showing up, but I'll read it out. Why don't we access state DB remotely? Why don't we access DB state remotely? Great question. I thought about that too. So you can create a compute unit that is external to the node and scale that out. However, they will all be executing get storage at sequentially as you execute the EVM instructions, and you will get the penalty of latency, right? So the thesis here is to put together both storage and the execution in an scalable way. All right, now we have a lot of questions. Next, what happens if the EVM hits an unimplemented opcode? It breaks really hard. Okay, next one. I'm assuming, can this run on Redis community? Absolutely, yes. All right. Yeah, so there are even novel engines there. One of them is called Valky. That is a drop-in replacement for Redis. Should be the same thing, right? And there's a developing space. Okay, and our last question, does it need archive notes to feed the data? Is Redis sorting all archive data? Great question. So the project is currently focused on current state. You don't need to have an archive node to feed that in, but your RPC source needs to have some specific debug methods available. So the way to export the entire state of a contract is one of them. All right, we just have one last question. When can we use this for full block execution? Oh, wow. I don't question. When can we use this for full block execution? Oh wow. I don't know. Soon. Please help me. Alright, I think unfortunately that's all the time we have. I know we have one more question, but I believe Everton will be hanging around here. So yeah, you can catch up with him after this session. Thank you so much, Everton. Let's give him another round of applause. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:20:00.000Z",
      "slot_end": "2024-11-14T06:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1fF69WpIZk0d5kqOiGISG9maJgrmsuKxAcyzfYSedRsw",
      "resources_slides": "https://drive.google.com/file/d/1_lASvqz9yYvG8zbCS8H8MnI5HrA3K4y-/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "the-rise-of-ai-in-web3-development-ux",
      "sourceId": "LTEX8X",
      "title": "The Rise of AI in Web3 Development UX",
      "description": "This talk explores the intersection of artificial intelligence and Web3 technologies, highlighting how AI can enhance the development of decentralized applications and blockchain ecosystems. The presentation will provide practical examples, code snippets, and insights into Web3 AI through the lens of the recent RemixAI integration into the Remix toolset. Attendees will gain valuable knowledge on leveraging AI to build more robust, intelligent, and user-friendly decentralized applications.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,User Experience,UI/UX,coding,generation,Tooling,UI/UX,User Experience",
      "keywords": "AI Web3,LLM,Code Generation",
      "duration": 512,
      "language": "en",
      "sources_swarmHash": "45d5cff1ad3a51e4710550ab842c2737368782c4efded4ea265d96d0a3f09a19",
      "sources_youtubeId": "cAU9DswPblk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a0c79dbb7a90e1aa29ad",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735a0c79dbb7a90e1aa29ad.vtt",
      "transcript_text": " yeah hello everyone my name is Stefan Tessin and I work with the Remix team and my talk today will be about the rise of AI in web pre-development area I will be much more on the application side. And yeah, regarding what the AI can actually bring to the web environment, these are actually the main application fields that AI can bring a lot to. First of all, the user experience, where we can incredibly enhance how the user actually perceives the platform. And also the decentralized governance that AI can actually have to much more incentivize this term. Also the creation of digital assets, also DAOs. And we cannot speak about AI in Web3 without talking about fraud detection and also the security aspect. So actually, enhancing the user experience is pretty much easy. Here we have an example of the Remix platform where we have code completion, code explaining. Those are pretty much low-hanging fruits for those who knows how LLM works. And yeah, this actually also already provides a lot of input for the user, a lot of value. And next stage of providing value for a user is by having automated chatbots, like also AI support, which are integrated in the platform. Here again, a remix example, where the AI chatbot is aware of what the user is doing is always one step ahead in order to kind of provide the user a much more better experience and so forth. Yeah, also the AI can be used for creating personalized content, user feeds creating also ai bots that will be a safe as oracle for contracts and also provide investment recommendation and create digital arts i wish as we have already seen with nfts and automatically generate metadata for market visibilities on different platforms without sharing the asset. Regarding the security aspect of how AI can actually improve how we develop a Web3 application, it can actually provide a much more secure development mechanism. Just think about it when you're about to deploy a contract and there's AI around it that kind of pass the contract and for vulnerabilities and check pretty much everything what goes around it. And also transaction, there's AI maybe sitting somewhere watching the blockchain out in real time and kind of flagging fraudulent transactions and so on. So the AI is sitting somewhere and kind of doing behavioral analysis and profiling everything and yeah, flag bad stuff. Okay, the last thing I will be talking about is about agents. Agents is like a software layer on top of the AI model which actually kind of perform autonomous actions and have an interaction with AI in order to get out, in order to provide base values. And those agents are actually able of kind of doing vulnerability scans, help in improving the decentralized governance, and also regarding the decentralized finance, we can automate that aspect of the Web3 environment by providing best trading pairs, best price, best algorithms for that. And also the smart contract automation that the agent can actually help with. And yeah, I think that was pretty much my talk I am very much open to questions anyone with question I see the gentleman in the very back I don't think I can throw that far. Thank you. Hello. So I had basically two questions. My first question would be regarding the Remix AI. So I actually utilize Remix a lot for contract development. So what I wanted to ask was the answers that Remix AI provides, how does Remix handle hallucination over it? The second is, Remix AI on the back end uses LLM. So is that LLM open-sourced? And what are the things on which it is trained at? Are there specific contracts on which it is trained at? Like, are there specific contracts on which it is trained at? First of all, regarding the hallucination, I think it is a problem that you cannot actually avoid with the nowadays states of development. But we aim to kind of reduce that by utilizing state-of-the-art models and to be much precise regarding also the second question that will be the open the llama model we use the latest llama model which available on human face and yeah they are deployed and our back ends and very soon they will be able as a private person to run it locally at your own risk. That's all I'll say. Another gentleman? Over there. Yes. Sorry, I'm getting... Thank you very much. Very nice box. Thank you. I hear a lot of things that a lot of people are talking like how we can do better contracts how can we do better chains and i feel that we're talking too much about uh the core layer like how to make the things run and we're not talking of how to build and use things over it like like web apps, general apps, do you think that AI can improve here too? Or will it be bringing more to the table for client part and higher level builder part? Or it will be bringing more to the table for low level, like blockchain and contracts? I think it will be much more on a high-level perspective that I will bring that value. Okay, got it. Thanks. I think we have time for one last question, if there's any. One last question. Over there. There's a gentleman there. Yes. Amazing. Hi. Hi. My name is Moritz. I'm working on 42.money. We're trying to generate front-ends for smart contracts specifically. What is your vision on Web3 generation of UI? We see a lot of big players in Web2, but in Web, there's not really competition yet. I mean, my vision or my, the way I see it is that the future there is very much bright. We are right now at the beginning of the era of AI casing the Web 3 environment. And I think it is pretty much opened. You can do pretty much everything you would like to do. Yeah. Okay, thanks. Okay.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:20:00.000Z",
      "slot_end": "2024-11-14T06:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1zhCIin-EiFLgd3IrIQYnzWKZ4MmkJfeVVaweIJV7Mm0",
      "resources_slides": "https://drive.google.com/file/d/17bBCr23MdDqMgPwMVeFKEnIf_3b07RVf/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "build-a-fully-onchain-game-with-mud-in-20-minutes",
      "sourceId": "LSM7RB",
      "title": "Build a fully onchain game with MUD in 20 minutes",
      "description": "Opening talk for the MUD Day CLS, from MUD core developer Alvarius. \r\n\r\nJoin us as Alvarius gives a brief review of the history of MUD the framework and the MUD community leading up to today, and a sneak peak into the future of MUD.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Frameworks,Gaming,Autonomous World,framework,Autonomous World,Frameworks,Gaming",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "81a7ae3b112b8f93564896d24819fcb5b67fc17cbf97d567c4025c5b742db75f",
      "sources_youtubeId": "qsj79c-D_CQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:25:00.000Z",
      "slot_end": "2024-11-14T06:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1XNooFuPjBwvsST2bNcBCNK1qdcTRfZstabTCqwGflzw",
      "resources_slides": "https://drive.google.com/file/d/11rIJxpgyrQrI2xrv8Cd_zAmdFqaYSWJM/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "ai-as-an-interface-to-the-game-a-round-up",
      "sourceId": "UJDNDV",
      "title": "AI as an Interface to the Game: A Round-Up",
      "description": "This lightning talk will show how AI can act as an interface to the Ethereum ecosystem, inspired by the concept from Vitalik Buterin’s blog article. We will explore the current state of LLM-based blockchain AI assistants, their UX, capabilities, challenges, and limitations. The talk will cover how in future AI can simplify the complexity of the Ethereum ecosystem for users, and address crucial safety and privacy considerations.",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "ai,agents",
      "keywords": "AI,AI-Agents",
      "duration": 611,
      "language": "en",
      "sources_swarmHash": "c8b2713d8f12842b044a2507d1732583450baf07db30b4563f437c31fd72b74d",
      "sources_youtubeId": "oAt3y_kJtNw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a1dc9dbb7a90e1b909f0",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735a1dc9dbb7a90e1b909f0.vtt",
      "transcript_text": " Hello, my name is Johannes Pfeffer and I founded a company, a project called Hyro that builds human-centric AI agent systems. And I'd like to talk about a few aspects of AI as an interface to blockchain. So in February this year there was this article by Vitalik which spurred this presentation. It's called the promise and challenges of crypto and AI applications. Who of you have read the article? Hands up please. Okay, I see like four or five, right? Well, in the article Vitalik has identified four main categories of AI and crypto intersection, with which I agree a bit and sometimes I disagree. But let's go through them real quick. So he sees AI as a player of the game, as an interface to the game, as the rules of the game, and as the objective of the game. That is these four categories. I will have to speed through the initial ones because this is a lightning talk, but AI as the objective of the game, it means that the goal is to create",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:30:00.000Z",
      "slot_end": "2024-11-14T06:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1tIcPsq1d4NJY-OmswRcgBrKRXN68UP-xW0lJCxBh2e0",
      "resources_slides": "https://drive.google.com/file/d/1FFxauUZVpwTbZ7mr7m-Y4pIbEL-f6uTg/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "build-your-own-zk-email-proofs-zk-email-login-or-zk-account-recovery-module-in-15-hours",
      "sourceId": "DZVPRH",
      "title": "Build Your Own ZK Email Proofs, ZK Email Login, or ZK Account Recovery Module in 1.5 Hours",
      "description": "We explain how to use a variety of zk email related SDKs, creating new proofs only using Typescript or Solidity, whichever the developer is most familiar with. We will define new proofs in 5 minutes via sdk.prove.email, complete solidity zk email verifications via our generic relayer, and ideate your own projects!",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Anonymity,ZKP,2FA,Account Abstraction,Social Recovery,zkemail,2FA,Account Abstraction,Anonymity,Developer Infrastructure,Social Recovery,ZKP",
      "keywords": "ZK Email,zkemail",
      "duration": 4940,
      "language": "en",
      "sources_swarmHash": "358a06e9ddcce2fb05957e51e39e742a53875ddec505705a61772b7e19c8e470",
      "sources_youtubeId": "UsH8zwjcCD4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735af479dbb7a90e1c72365",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:30:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/11rThgXehZjvKPRgDBNd_WxtwkApTlRaFelE2r0W73fM",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "building-ecosystem-health-metrics-with-open-data-sources",
      "sourceId": "XPN8MM",
      "title": "Building ecosystem health metrics with open data sources",
      "description": "This workshop is for developers, analysts, chain operators, and general audiences interested in using data to evaluate and/or inform strategies for L2 ecosystems.\r\n\r\nTopics:\r\n- Picking your metrics, and why to avoid focusing on a single metric.\r\n- How to pull data from & contribute to open data sources to build your own dashboards.\r\n- Additional research: LTV, personas, chain economics.\r\n\r\nAttendees will leave empowered to build their own health dashboards, by using and contributing to open data.",
      "track": "Layer 2",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Layer 3s,Product-market fit,metrics,Layer 2s,Layer 3s,Product-market fit",
      "keywords": "Analytics,Data,Metrics",
      "duration": 3715,
      "language": "en",
      "sources_swarmHash": "27528a9aea907213a88a65c803cfd0e21d1476f25c6601eca67562ded2cf4c23",
      "sources_youtubeId": "M9EQ-adPTrI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ac8f9dbb7a90e1a99230",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:30:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1FX62edXoNMzGl2PtKs3vXQ9ynqkr1cEZhkDrsjPT3_s",
      "resources_slides": "https://drive.google.com/file/d/1vO3TL6M9bfrJYIcap1-vhoOZyje9N1um/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "coordinating-intelligence-open-algorithm-development-for-science-ai-and-beyond",
      "sourceId": "FZRMPJ",
      "title": "Coordinating Intelligence : Open Algorithm Development for Science, AI, and Beyond",
      "description": "The Innovation Game (TIG) is a market-based framework accelerating development of computational methods crucial to science, engineering, and AI. It creates a \"synthetic market\" where \"Innovators\" contribute methods and are rewarded based on adoption by \"Benchmarkers,\" who are rewarded for solving random instances of computational challenges. This enables price discovery for commercial and pre-commercial research, attracting private investment. TIG's hybrid licensing model ensures open collaborat",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DeSci,Economics,proof,optimised,work,Coordination,DeSci,Economics",
      "keywords": "Innovation,Algorithms,Optimised Proof of Work",
      "duration": 560,
      "language": "en",
      "sources_swarmHash": "73c2d21fee5daa71ba8596feb060a56b96c4cff21e4983907fa835911b0e2ecb",
      "sources_youtubeId": "oyiCNkN8468",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a8119dbb7a90e15193d6",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735a8119dbb7a90e15193d6.vtt",
      "transcript_text": " Hello, hello. My name is Kilian. Thank you for the kind introduction. And I'm going to talk about the innovation game. Unfortunately, the founder, John Fletcher, couldn't make it due to scheduling issues, so I had to jump in last minute. We are a project based out of Cambridge University. Myself, I'm a PhD candidate in astrophysics, and Dr. John Fletcher is an applied mathematician also from Cambridge. So what is the innovation game? Well, our theme, or our mission, is to incentivize algorithmic innovation. Well, what does it exactly mean, you may ask? If you look at the scientific literature, you see that most problems, such as the traveling salesman problem that you might know, right? Finding the shortest distance between, let's say, 20 cities. This kind of problem is a hard problem to solve, And you can only solve it through like various types of algorithms. What you see in this graph is on the x-axis, the year, on the y-axis, you see like some kind of performance metrics when you run these algorithms. So for instance, in the first, let's say, 1940s to 1970s for, let's say, the traveling salesman problem, people have been just brute forcing solutions, which is not very efficient, right? So you have, for instance, a performance of one, a constant, right? But then people started to innovate on the algorithm side. And what they achieved is like magnitudes of order of performance boost just by having an algorithm. For instance, the red curve, you see is 2 to the power 40 more efficient to innovate on the algorithm side for a given problem versus another algorithm like the blue ones and the green ones as well. Why do we care? Well, you can achieve a lot of breakthroughs just by not just by hardware, but by innovating in the algorithm space, right? And why I'm talking about that in the blockchain conference. Well, our protocol is there to incentivize this kind of algorithmic breakthrough. And how do we do that? We're actually using blockchain technology. You might know that blockchain is more about transactions, etc. And how do we achieve that kind of algorithmic breakthrough, and how do we incentivize that? We are actually using the proof-of-work mechanism, such as you might know from Bitcoin, and basically having many algorithms together, and then benchmark those, and find the best ones for a given problem, right? So let's say you have for a traveling salesman problem, you have four different kinds of algorithm people have been implementing, and then you use the benchmarking process, which is the proof of work mechanism, where you mine for the best algorithm. Once you've found the best algorithm, you basically reward the people that have been providing the IP to our platform, but also the miner who has been providing compute resources. So on the right side, you basically see how our system works. Just as a side note, of course, we also need to verify that the algorithms that were submitted to a platform are also correct so that means it's the solution correct but also is the method correct right and how do we capture value for the innovator well we can basically run two licenses for the proprietary or for the open source algorithm. One, the open license, open data license. That means any algorithm that is found through our platform can be used for free if you also publish the data. Why is that relevant? Well, imagine you have a scientific paper in biology. Would you trust the paper if you're not providing the data as well? No, right? It's very unscientific. So we are following the ethos of open source or academia through this license. And the other license is, okay, I don't want to publish my data, but I can pay a small fee, like a license, to access the algorithm. And what we are basically doing is we are combating, through the innovation game, the centralization risks through AI-driven algorithms. So what does it mean? Nowadays, you have large language models, and it is possible to now look for solutions by using large language models, which is not very good, because that means suddenly your innovation is directly dependent on the amount of compute you provide, which is bad, because most of the large capital for these computing platforms are at Meta, Google, etc. But we are based at the innovation game. We are the only protocol as of now that can incentivize and combat this type of AI driven algorithm development by incentivizing and rewarding the people that provide our IP to our platform. Thank you. Thank you, Killian. Do we have any questions from the audience for Killian? One over there. You want to try? I don't think so. I'm sorry. Okay. Time to work out. Thanks for that. So on that chart of algorithm development over time, I noticed there were quite large step functions. Do you expect that algorithmic development is continuous enough for the rewards to actually incentivize the actors? Yeah, so we expect that most of the people that use our platform are probably not innovating. They're more like benchmarkers, or they look at existing algorithms and then optimize the code a little bit. So we expect that it's very difficult to innovate, but we want to incentivize this behavior. And the challenges that we include in our, let's say, mathematical computationally challenging problems that we include in our platform are various sorts of. For relevant for blockchain would be, for instance, ZK proving mechanism, right? It is hard to compute a proof generally, but it's easy to verify, right? So this is very relevant for, let's say, the blockchain industry, if people are innovating on our platform. Other areas could include AI, that we also work on, and other theoretical computer science problems as well. Thank you. We have time for one more question. It's over there. I have a funny one. So you, maybe people don't hear, no, but there was a mess up and everyone got an email about the innovation game, all the speakers. So did it do anything for awareness of the work that you're doing? Or did it have no effect? Because I'm here because I got that email, too, of the innovation game. So could you repeat the question? I couldn't understand. So from the DEF CON committee, there was an email that was sent out to everyone who ever applied for a speaker slot. And that included, it was an email that was sent out to everyone who ever applied for a speaker slot and that included uh it was a it was an incorrect email but it it included the innovation game so i was just wondering did it do anything for the awareness of what you're working on or did it have no effect because i thought it would sorry i couldn't catch your question could you please like close speak closer to mike or you can come after talk okay i'm sorry i couldn't catch your question. Could you please speak closer to the mic? Or you can come after the talk. Okay, maybe. I'm sorry, I couldn't understand it. We can come up. We can talk after. Yeah. It's okay. I guess we'll have private conversation around that question after the session. Anyone else? We still have a little bit of the opening for a question. One over here. I'll just ask a very general question. What would you like us to do with this information? Sorry? What do you want us to do with this information now? Do you want us to go research in? So you can reach out to us if you have any hard problem, let's say in DeFi, and you want to formulate, let's say, a computational problem, you can reach out to us. Or if you're a hardware or like benchmarker, you can use our system to mine and benchmark these different algorithms. So there are two approaches. Either you want to provide a challenge for us, or you want to provide compute, or maybe you're a developer, you want to provide actually an algorithm. So there are these tracks. Amazing. That's time for us. Thank you again, Kilian. Another round of applause for Kilian.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:40:00.000Z",
      "slot_end": "2024-11-14T06:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1LhUMD8kPbukRuIeQXcyC9Nzn52Zdr6JH80byPktkErk",
      "resources_slides": "https://drive.google.com/file/d/1ch3UFbhUC5HFtrpH6VXITHr5AZmyBJTI/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "gas-limit-and-block-execution",
      "sourceId": "LPLSDD",
      "title": "Gas Limit and Block Execution",
      "description": "The talk will focus on scaling L1 through the gas limit, with special attention to block execution, covering challenges and planned solutions. Topics include an overview of control over the gas limit, the current state of execution performance, and hardware comparisons. Key challenges will also be discussed, such as slot organization, state growth, and worst-case scenarios, including gas pricing issues.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Layer 1,Protocol Design,execution,layer,Core Protocol,Layer 1,Protocol Design",
      "keywords": "gas limit,block execution,Execution Layer",
      "duration": 1385,
      "language": "en",
      "sources_swarmHash": "77fa8da66a9f37c09763c3b438b62b7147685a96d8a29d1e7a7f4a7149776cb5",
      "sources_youtubeId": "L10eJJoTJB4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736c52f9dbb7a90e1893847",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736c52f9dbb7a90e1893847.vtt",
      "transcript_text": " Marek. Test, test. Okay. Hi, I'm Marek, Ethereum Curve developer from the Nevermind team. Today I'm going to focus on gas limit and block execution. There are some voices saying that we've either given up on L1 execution or we need to move Ethereum to data centers and eliminate solo stakers to make Ethereum successful. I disagree with all of these statements. I don't want to focus here on suggesting a specific gas limit value. Instead, I want to show that there is a significant untapped potential on execution layer, and this potential will have to be unleashed on some point of time. At the same time, I will point out areas where we need to be careful considering increasing gas limit and i will mention challenges and potential solutions all right let's begin so in 2021 elon musk was interested in cryptocurrency and claimed to know how to scale blockchains he mentioned that he discussed with dogecoin devs to increase block sizes by 10x and decrease slot time by 10x, which would supposedly scale Dogecoin by 100x. Sounds easy, right? Just change one or two variable and we are done. So can we do the same for Ethereum? Increase gas limit 10x and reduce slot time 10x? Yes, we can, but of course it's not that simple. Gas limit is a powerful variable. Increasing it leads to more transactions, more applications, generally greater crypto adoption. However, it has to be adjusted carefully because it might impact decentralization and security. adjusted carefully because it might impact decentralization and security. So there are a few areas when we need to consider when increasing gas limit. This usage, this stems from two things, history grow and state grow. History grows much faster than state. However there is a straightforward path to remove it with EIP for force. With the current gas limit, state doesn't seem to be significant concern. However, we need to observe it closely with potential increases. Networking, so larger blocks are more challenging to propagate and allow for more transactions. So potentially it will increase load on mempool. Other things, so increasing gas limit effects, syncing time, RPC, proof sizes for the future light clients, size of archive notes. However, today I want to focus on block execution a little bit with isolation from other topics. Block execution is the main contributor to other hardware requirements such as CPU or disk speed. Okay, but first of all who controls gas limit? So validators of course can vote for the desired gas limit. However the block gas limit cannot change, cannot increase or decrease by more than 1 over 1,024 of its parent block gas limit. So whenever you propose a block, you can move Ethereum in your desired direction. In other words, if you think that 35 million is the correct gas limit for Ethereum, you can set this value in the config and in every block you will move slightly in this direction. However, other validators can counteract it, bringing it back to 30 million. And this fact was used in EIP proposed by Giulio from Eregon team. He suggested modifying clients in the way that there is very small increase in every block. Thanks to that, we have a better control what's going on in the network with increases. So the control over gas limit can be divided into categories, local block building and external block building. Approximately nine percent of validators are local builders. In this case, execution client teams can set the default gas limit in the config, which can be overridden by node operator. Most of the blocks are externally produced, around 91%, and in this case, consensus client teams can set the default gas limit, which can be overridden by validators. However, this is not enforced. So builders should follow this, but they do not have to. So we can say that builders have the final control over this value. So, I have seen Twitter conversation about increasing gas limit and that we should increase hardware requirements or we should make EVM much faster and parallel and scale L1 in this way. Of course, we should try to optimize things as much as possible there are many reasons there are many reasons to do so future potential of l1 l2s rpcs however let's analyze the block execution performance on my mini computer intel nac 11. so as you can see on the charts this device can execute blocks with the average speed around 330 megagas per seconds, and blocks are being validated in just 50 milliseconds on average. So if we consider higher hardware requirements, let's take a look at a much more expensive machine, four times more expensive, used by one of my colleagues. four times more expensive used by one of my colleagues. And as you can see, blocks are being executed almost three times faster with the average speed 950 megagas per second, and execution time is around 17 milliseconds. So at first glance, the discussion about hardware requirements seems relevant. Many computers are three times slower than the high-end consumer machine. And there are many machines in between that could be considered. However, let's pause for a moment. Ethereum slot time is 12 seconds, and we are talking about average execution in just 50 milliseconds. So what's the point of using much higher hardware when we are not utilizing slower machines? So this suggests huge potential for mainnet, and however, we know that if everything would be that easy, we would already have much higher gas limit. Earlier, I mentioned that I'm talking about block execution a little bit in isolation from other topics. And those other things are more concerning, like state grow, history grow, networking aspects. However, even in block execution, there are challenges. First of all, if we have 12-second slot time, it doesn't mean that we have 12 seconds for execution. This is an obvious thing. And even if we ignore networking aspect entirely, nodes need to be able to catch up with the tip of the chain. What's more, RPC providers must be able to execute, process new blocks while processing heavy if calls and other requests. And lastly, there is a consensus mechanism. So let's take a look at the diagram on the slide. Ethereum consensus requires strict timing for block execution. The attestation deadline is four seconds, however, ELs have much less time to process new blocks. First of all, there is lightweight validation on CL side and blocks need to be propagated across the network. Moreover, blocks often aren't revealed at the beginning of the slot due to timing games. And the solution to this is to reorganize Ethereum slots in such a way to allow for more time for execution. We have two solutions under consideration, EPBS, enshrined proposer builder separation, and APS, att Proposer Separation. While the main goal of those changes is related to block building and MEV space, there is additional nice benefit on the execution side that we will have much more time for execution. I will use EPBS as a primary example. However, both solutions will reorganize the slot in a more optimal way. So, in EPBS, attestation deadline is set at third second. However, attesters will have even up to 11 seconds to execute blocks without affecting consensus rules. So this is very useful in context of worst cases. Okay, other thing that we should consider is, of course, state growth. So with the current gas limit limit state doesn't grow rapidly. In the last three months it has increased four gigabytes in a Nethermite database. If you want to read more about state grow there is an excellent research from the Rev team. However the worst cases of most of the operations is essentially function of block size. So the bigger the block, the more operations you can fit in. However, in case of state, we need to be mindful of state database as well. So in other words, the bigger the tree gets, the slower state access may become. So clients spend a lot of time trying to optimize things related to state. So here is an example from Nethermind, and operations are dominated by Sload and SSStore. Another example from Bezu bezel and similar situation and ref seems to be occupied by state root calculation and other thing related to state is that we know that we will eventually need to transition to another tree either vertical or something post-quantum secure if there is a sudden breakthrough in research. So we know that this transition process will be complex, and it will be easier with a smaller tree. So we need to be careful to not make this task too difficult with increasing gas limit too aggressively. However, everything needs to be put in context of data and as Julia from Erigon team has shown, even if we double gas limit and ship Verkul in four years, the transition will take additional 13 days, which doesn't sound like a very bad solution. Long-term solution for Verkulz is of course state expiry and recently Vitalik wrote the blog post about it. Block execution is strictly related to gas pricing. Gas pricing represents the time it takes to execute the blocks and other resources required to do so. So we need to take into account contribution to state grow and history grow. And recently Vitalik proposed that we should increase gas prices for hashing functions as blocks that are heavy with hashing are harder to prove than average blocks. with hashing are harder to prove than average blocks. So we benchmark different operations and you can see it on the table on the slide. We filled the block entirely with one of the operations and as you can see the differences in gas pricing can be substantial. For example if we fill the block with repmd precompile, it executes with the speed 1 giga gas on a very slow machine. If we do the same with simple if transfers, the speed is around 700 mega gas per second. However, other operations can be even slower. EC recover, 80 Mbps. Point evaluation precompile, 65 Mbps. So this is important because the slowest operation should be treated as a bottleneck for increasing throughput rather than relying solely on average execution. So let's talk more about worst cases. Client teams are improving and being better and better on executing average transactions. However, Ethereum security must account not just for average case, but for worst cases as well. And by worst cases, I mean situation when optimistic parallel EVM encounters as many conflicts as possible, when caches are being missed, when attacker found some slow operations in EVM, and crafted the block in the way to disrupt the network. So let's take one more look at execution times of my Intel NAC11. As you can see, the performance on different blocks can vary significantly. I marked a few spikes with red rectangles. So, the average execution is 50 milliseconds, but the maximum is 425 milliseconds. The known worst case in all execution clients that are observed on real networks are mining contracts. So those contracts work in this way that they are doing a lot of storage writes and sets values to zero. And by setting values to zero they are getting refund and thanks to that they can feed the block with excessive state access so our team member ban proposed an EAP to prevent such situation EAP-7778, prevent block gas smuggling. So, slow blocks can affect consensus. If attesters won't be able to meet attestation deadline, it might impact, they will miss attestation and it will impact chain health. What else? Slow blocks can affect block production, so the timeout may occur and it might impact aliveness of the chain. And of course, the slower the operation, the more probability for DDoS vulnerabilities. And I'm not saying that we have an immediate concern here. We need to analyse our worst cases, we need to understand them, we need to optimise them, we need to reprice operation that could be a bottleneck for increasing throughput. The long-term solution for gas pricing issue could be a vital proposal for multi-dimensional gas pricing. All these challenges are important, but there is other side of gas limit. Gas limit hasn't been increased for a long time. The last increase was done in April 2021 from 12.5 to 15 million. The increase from 15 to 30 was related to introduction of base fee mechanism, so while gas limit is higher, the gas target is still the same. In the last three years, hardware improved, so it could be an argument to increase gas limit. However, even stronger argument is that Ethereum client software improved massively. So, to summarize, taking disk space aside, which will be solved with EIP for force? I don't think that the discussion about hardware requirements is relevant. Ethereum can be run on mini computers and it will stay in this way even if we increase gas limit a little bit. I see challenges. We cannot forget about state grow, about history grow. We need to reorganize the slot. We need to migrate to another tree. However, all of them seem to be solvable and are planned for in Ethereum roadmap. We need to ask ourselves what data we need, what other problems we need to solve, what the correct gas limit should be. what other problems we need to solve, what the correct gas limit should be. I don't think that we've given up on L1 execution, and execution is not a bottleneck for scaling L1. Of course, I agree strongly with Vitalik's vision that Ethereum mainnet should be a strong base layer, and definitely it shouldn't take hyperscaling approach or make any shortcuts. All the improvements in the clients and in protocol as well will benefit L2s, allowing them to take hyperscaling approach and use the full potential of EVM, while Ethereum L1 should stay to be a strong, secure, and decentralized base layer. And that's all I want to say. Thank you, everyone. Okay, thank you, Marek, for a great summary of all the factors that can affect gas limit increase. So we have extra time. Let's go through the first question. Please scan the QR code here. Go to the bottom right. You'll see the live Q&A button there. Click there. You can add questions. You can upvote. We'll go by the most upvote race. First question, what do you think about Reth saying they are fastest? Is this relevant? That's a tough question. So, actually, I discussed it with Georgios yesterday, and he got the same question on his presentation, and he said that actually, Nettermind is the fastest now, and they are kicking our asses, so we need to work harder. And yeah, that is my comment about this. I hope it makes sense. I run Nettermind on my validator. Will parallel EVM ever become relevant for L1 nodes? I think so, but I think it will take time. So if we have multidimensional gas pricing that controls state grow, we could allow for more execution with parallel EVM. What's more, parallel EVM might be important if we kept the transaction, so we will allow, for example, only 30 million gas transaction, and then worst cases are better, basically. So, yeah, I think maybe on some point of time. Okay, the next question is, what's your opinion on pipeline state routes, state routes being calculated with one block delay? I think it makes sense, but I'm not sure if for L1, for L2, yes. But for L1, I'm not sure about consequences for light clients. Next question is, what tooling does NetherMind use to measure and benchmark mega-gas per second on a node if the node is not in sync with the chain? So, if the node is not in syncing with the chain, I'm not sure about this, but generally NetherMite uses the tooling that monitors new payloads, so we measure the time it takes to execute new payload in NetherMite. And once we have time and we know how much gas was used in this block, we can calculate the speed. Maybe you can use for that. Not sure. Next question would be, why gas per second if each operation have different gas use? Did you think opcode per second makes more sense? Why gas per second if each operation... No. I think there is a problem with gas pricing. So ideally, gas pricing should represent the correct resource usage, but we have, as I showed in this presentation, we have operation that doesn't follow that. So, yes. I think we should still use megagas per second, but we should maybe improve our gas pricing in EVM. Cool. We have about three more minutes, so if you guys have more questions, please feel free to submit. But the last question we have here is, what do you think about increasing time spent on signature aggregation to allow faster finality instead of gas limit increase? Oh. I don't know. Sorry for not answering this question. Signature aggregation. I have no opinion. Cool. We have two more minutes left.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:40:00.000Z",
      "slot_end": "2024-11-14T07:10:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/17JZL3bUgGRPxJs5ybdBTY70V_NqPo7xH7Sc7QI5zw5A",
      "resources_slides": "https://drive.google.com/file/d/1xDBhEUsI5mphjG5uLtmoZfhZp3s_KwAq/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "abstract-wallets-and-chains-through-profiles",
      "sourceId": "U99UU9",
      "title": "Abstract wallets and chains through Profiles",
      "description": "Profiles have been key to user retention in Web2 with most people having at least 7 profiles personalizing their experiences on various platforms. In contrast, Web3 lacks profiles, leaving dApps unable to understand user needs resulting in low retention rates. Despite onboarding efforts, 88% of wallets created through account abstraction last year were used less than five times. Perhaps tying user interests and reputation to wallets by creating profiles is the secret sauce for mass web3 adoption",
      "track": "Usability",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,Digital Sovereignty,User Experience,Account Abstraction,open,ranking,recommendation,Account Abstraction,Digital Sovereignty,Identity,User Experience",
      "keywords": "Content Curation,Open Web,Open ranking & recommendations",
      "duration": 646,
      "language": "en",
      "sources_swarmHash": "01d50200a24845a4cc7df36cda1b108b89014d7a9e871b63ab904d7e08e1ac4f",
      "sources_youtubeId": "pg3mgNHQ6tc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a3bb9dbb7a90e1de3f6e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:50:00.000Z",
      "slot_end": "2024-11-14T07:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ghf7GAlxhcuQaT9CB6h-vcw2b23gLxI2nnAAzr6YSHs",
      "resources_slides": "https://drive.google.com/file/d/1b4MZv_590A-J_Aq8bX_oVSzrqL5lD0-O/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "growing-the-biomes-gdp-using-digital-matter-and-smart-items",
      "sourceId": "AZCYRS",
      "title": "Growing The Biomes GDP Using Digital Matter & Smart Items",
      "description": "Biomes is growing the virtual world with the largest GDP. As a fully onchain 3D voxel world, every single action in Biomes -- mining, building, crafting, even moving -- is a transaction on the Redstone L2. \r\n\r\nWe will share stories how we're working to grow the GDP of Biomes, what is working and what isn't. We will also share examples and ideas for onchain smart items in the Biomes world enabled by smart contracts.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,Autonomous World,Autonomous World,Gaming",
      "keywords": "",
      "duration": 1275,
      "language": "en",
      "sources_swarmHash": "05306df31b1eabf2bcbcbda8022bb6d7f8dc597d11459ea4ba143bb63e4dca6a",
      "sources_youtubeId": "Y5VCidGZgJo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ac844ccb22799e395271",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ac844ccb22799e395271.vtt",
      "transcript_text": " Hello, I'm Dhrumil. I work on biomes. Biomes you probably played in the gaming corner over there or maybe you saw the demo earlier in the morning. You can also just go in biomes.aw and actually play it. So today we're going to talk about growing the biomes GDP, which is the primary thing we're trying to prove out with biomes, and two things we want to use to try and achieve those goals. Before we start, we're going to take just like a minute to think about why we're doing this. Most people who view Ethereum, they view it as an open financial system. If you view it this way, you care about a particular set of things. You care about stable coins, coins, RWAs, treasuries, wallets, and payments. But this is kind of the weird room. That's why it's in a classroom. And it's a bit of a different scene. It's not interested in moving our existing world on chain. Rather, it's interested in trying to uncover a totally new and weird future that's very different from the world that exists today. And if I was going to sum up the core question that's being asked in this room, okay, I think people probably will have a problem with me speaking on behalf of the room, but I will do it. If Bitcoin was able to birth digital coins that people treat like real valuable coins, can we basically 100x this and birth a digital planet that people treat like a real planet? And I think the short answer is that I think we can. And I think it actually might end up being much more simple and obvious than we might have thought before. There might be a line of sight to this. I think what we should just do is we should take ideas that already work, and we should just reshape them, right? We should reshape them from being built for finance and just make them work with virtual worlds, right? So we've got to take the idea of scarce digital coins and convert that to scarce digital matter. We've got to take tamper-proof IOUs and turn that into tamper-proof physics, and we've got to take smart contracts and turn that into smart items. And that's basically what this talk will be about. And we're going to actually make this concrete with examples of us trying to do this within biomes. And I hope that makes this much more legible. And the claim with those three pieces, the claim is that the digital matter and smart items can birth a crazy new asset class, the weirdest asset class the world has seen. And that, if it gets mindshared, will unlock autonomous worlds. It'll make them super obvious, super visceral, and you'll never have to answer the question of why you put this on chain. So to make things, let's look at biomes. On the surface, biomes is just Minecraft with everything on chain. You can, you've probably tried it out. The items, players, physics, all the actions, you can see it move and mine and build. It's all on chain. You know, right now there's like client optimization thing going on to make this work. Sometimes there'll be bugs. But, you know, I think you've got to press your T dot wherever you find them to make Cori go mainnet faster. And then this will not happen. And what's been happening with biomes ever since we have sort of been in our alpha phase the last month is it's coming alive, the world is coming alive, there's a lot more activity, we're sort of seeing it go. The transactions are up and to the right, you know, the players up and to the right, GDP up and to the right, everything bullish. There's people like Ben who are spending five hours building something, Deki spending eight hours plus days mining stuff. You know, so people are doing stuff. Redstone is like sometimes in the top five change in rope.wtf, which is really funny considering this like barely ever hits 15 concurrent players, right? And Ditto is a community member who made this grand claim that BAMS feels like Ethereum but more playful. And so what we're going to do today is we're going to go down the rabbit hole. Why are people so excited? On the surface, it just looks like on-chain Minecraft. Why are we even doing this, right? The first concept is digital matter, the thing we brought up in the beginning. Think of digital matter as a playful vision of blockchain-created scarcity. So while in Bitcoin, you have scarce coins, in biomes, you have scarce crops and wood and ore and land, and in Bitcoin, you have tamper-proof IOUs, in biomes, you have tamper-proof physics. And what this means is, if there is a tree, I can't just snap my fingers and mint more trees arbitrarily. The only way for there to be more trees is if I farm it according to the physics, right? So there's always like, wood is actually valuable. And if you do like the weird thought experiment, you might be like, if Bitcoin was an AWS, it could only be worth so much. But Bitcoin with scarcity and tamper-proof rules could be worth a trillion dollars. And if you go a bit crazy, why can't the same happen with digital wood, right? You have like Ben, who's maybe thinks one sakura wood will be worth $31,000. So like, this can happen, this can happen, just you just need to believe, right? And just like having lots of scarce Bitcoin grants you power in Bitcoin's world, controlling lots of the scarce resources and land lets you build businesses and be effective in the biomes world. The second thing we're going to look at is smart items. So we've got digital matter. Now we're going to look at smart items. Smart items present a playful vision of smart contracts. You can think of them as items with functionality in the physics that you can program with a smart contract. They make it really easy for players to build real economies for anything in the world. To visualize it, think of a smart contract as a chip. And you can insert a chip into an item, like a chest, to program it. So if I have my Uniswap chip and I put it into a chest, it turns into a Uniswap chest. And at this point, if people mine ores around the world, they can put ores in the chest and swap it for some tokens according to the AMM prices. Or they can give tokens to the chest and get ores according to AMM set prices. And you can sort of take this concept and basically apply it to all the kinds of different objects in the world. You can imagine I have a door and I turn it into a token gate, right? I need the token to actually open it. So what you just did here is you like gave the Holmeson treatment to Ethereum where if you told Homer Simpson what is a token gate, that's what he would think it is, right? And so now you're taking all of these smart contract track patterns and you're just making them much more relatable for a more normal audience. So you can turn a door into a token gate, a force field, a cart, a bed. And with all of these items, you can build a lot of cool stuff. And these are all real examples of things that are already being built within biomes. The Bazaar is the biomes version of Uniswap. It's like this shop with a lot of liquidity where you can trade all kinds of items for coins. The funny thing is as soon as we made the Bazaar, there's this existing other group that put up a really crappy shop next to it, relatively. And it's like the sushi swap of biomes, right? And it's trying to vampire attack the bazaar and take all the liquidity. You also have like Sakura Temple where there's this like big, big temple where you donate Sakura wood to get this membership NFT. They use all that wood to build the largest Sakura tree in the world and all future Sakura meme events that happen will probably gate access based on whether you donated the Sakura tree and got the NFT. The Castle Hotel Sakura Inn, they give you safe spaces to sleep and recharge your player and load without worrying about getting grieved. And in return, you just sort of have to pay rent. The Pyramid Arena and Parkour Challenge are going to host a bunch of games, and these games will use smart items to have betting and prize pools and everything. So in a way, you can basically think of these as like deploying dApps inside of a virtual world, right? And third, if you have Digital Matter and you have smart items, you have a new asset class. And I think it will be the craziest asset class that crypto has ever seen, maybe the world has ever seen. And I think when that asset class gets mindshare is when we no longer have a classroom and we have a building. So to sort of imagine a chest, right? And I can program a chest with a smart contract. Imagine I program it such that when a player gets gold ore from the world, which is scarce, and they put it in, it mints a coin. Right? So now you have a coin that is backed by the actual gold in the virtual world. And if I put the coin in, it burns the coin and lets me take out a gold ore. And every time this happens, the chest takes a small fee, and it uses that fee to incentivize players to charge the force field and defense of the chest to make sure it cannot be broken in. If this coin rises in price, let's say it's even worth like 10 million or something, it becomes extremely, extremely obvious why all the gold ores need to be provably scarce. Because if I just mint more gold ores with a thin air, I can just put them in the chest and mint more coins for myself. So it's really obvious why you don't want, you know, you need the gold ores to be scarce and you don't want the devs to just randomly mint more. It's also really obvious why the physics needs to be on chain. Because if I change the physics such that it becomes super easy to break the force field, I can now come in and steal all the gold ores from within the chest, and now that coin that was backed by the gold ores is no longer backed by anything, and it's just gonna plummet. So if you have this asset class take off, it's super, super obvious why the actual physics and research of the world are significant. To do one more example, we can kind of imagine how in the 2021 like crazy bull cycle, there was this idea of NFT land, right? So basically in the sandbox and Decentraland, you could own and trade land NFTs. And if you held one of these NFTs, you had total forever land ownership of that land. And this was really good for a particular type of user that you can have hyper-rational financial markets and speculation on those NFTs. Because if I buy a land NFT, I forever own that land. So I can speculate on it and trade these NFTs eight months down the line. It'd be totally okay. You do that in biomes, right? If you want to control land in biomes, you need to build defenses. You need a military, you need force field. The military can now decide that, okay, us military are protecting this land. We're going to tokenize this land and sell it to you. And if you hold this token, you're allowed to come and build within our premises knowing that everything will be secure. Let's say I buy this token, I come back in four months and that military is just dead, all the defenses are broken. Now this token doesn't mean anything. Even if I hold this token, I don't actually have access to any land, right? So now you have this entire asset class that is only worth anything as long as players actually maintain their defenses. And what you just did right here is you made a fundamental switch. You made a switch from the system caring about who owns what asset into the system only caring about physics, and assets can live on top of them, but the assets are not the first class citizen. To make it concrete, you went from metaverse land NFTs, where you own something until you sell it, to a tokenized force field, where you own something until someone breaks the force field. And when you have a new asset class, you birth an economy with new properties. This requires new types of participation and appeals to a new set of users, which is really cool, which is why even though this has voxels and stuff, it's not the sandbox, right? It's a very different system. And this brings us back to the original claim that we started off with. No matter, and smart items can birth a crazy new asset class, tokenize digital commodities, tokenize force fields that will unlock autonomous worlds because they'll make the autonomous worlds really obvious. To visualize it one more time, you think, let's just expand that statement. Unlike Bitcoin, scarce digital coins, and tamper-proof IOUs, autonomous worlds will have scarce digital matter and tamper-proof physics. Inside, the players will use smart items, which are tangible, physical forms of smart contracts, to grow extremely large economies. And what we have done here is we have kept it simple. We haven't sort of, you know, went too outside and too wild. We have taken ideas that already work, and we have just kind of reapplied them. We have changed these ideas such that instead of being built for finance, they're now built for these virtual worlds. Scarce digital coins become scarce matter. Temper-proof IOUs becomes temper-proof physics. Smart contracts become smart items. I think if you take anything away from this talk, I think it would just be this slide. Okay, now zooming out, only because I have a few more minutes so I can just be this slide. And okay, now zooming out, only because I have like a few more minutes so I can just say some bullshit. We're seeing how Ethereum can eventually birth worlds we don't inhabit physically, but we still treat as tremendously real and valuable, right? So imagine biomes as like a $10 billion GDP, right? Or maybe the autonomous world you build has a $10 billion GDP. And autonomous agents start living inside. Then you build has a 10 billion dollar GDP. And autonomous agents start living inside. Then you'll have a really funny thing. So if they live inside, they're obviously going to value the scarce digital matter in the world they live inside, much more than the coins from our world. They don't actually live in our world. If I have an autonomous agent in biomes and I give it a bunch of neptunium ore, it can take that and build weapons with it and use those weapons. So it's good, it's clear why it has value. If I give it coins, if I give it the US dollar, it can't buy a burger, right? What's it going to do with a burger? So it doesn't actually give a shit about our coins, it will only care about all the scarce matter. And then you have a really weird future, right? AGIs will only accept secure for payment. They don't believe in coins. In 2027, biomes is our only communication channel to ASI agents. And when we want counsel, we travel into biomes and journey to their lands and homes and bring the agents glowstone. And in return, they give you some cryptographically encrypted Riemann hypothesis solution in a book. So this is how we solve the problems of the future. We just appeal to the agents in Biomes, and they give us all their solutions. And Skyler's really smart. You need to trust in Skyler. He built this, he runs this event thing, which means everything he says is definitely true. And what he said is that biomes is unstoppable Minecraft today but that doesn't mean you can take it you know for fun you have to take it very seriously because tomorrow he said it'll be an unstoppable lifelike simulation that's gonna have unalterable digital physics run by AI who believe they're sentience as we believe we are which which is crazy. So take it seriously. Stock up on all the Sakura you can. Go to biomes.aw. You'll see all this unfold. It'll probably go through many cycles over the years, but hopefully something really crazy will come out of it at some point. And thank you. All right, question times. As before, scan the QR code, log in with Zupass, start asking your questions. You can also upvote other questions if you want them to come to the top of the screen. Please do that so we don't have to scroll because it was awkward last time. So we'll start with the first question. Can smart items interact with the world, like place, remove blocks, and define regions? Yeah, so smart items are items with functionality in the world, like place to remove blocks and define regions? Yeah, so smart items are items with functionality in the world. For example, a chest is an item that you can store stuff in and take stuff out of. A door is an item that you open or close to enter something. So smart items are just regular items with functionality in the world. And all you do is, by inserting a smart contract into the item, you can hook into that functionality and further augment it with your own economics. So I turn a door that opens and closes into a door that is now token gating before deciding if it wants to open or close. Thank you. What are the considerations when designing or deploying physics in new primitives? It seems that playing God is high stakes. Yeah, I feel like something we've been noticing is as people start investing their time into this stuff and they start building stuff with the world, especially if they use smart items, so now they're actually building an economy in the world, you can't anymore just arbitrarily push physics updates. If we start randomly one day minting a lot more sakura trees, it kind of screws up what a lot of people are building. So it does make it more high stakes. You need to really get some sort of rough consensus within your community so you don't piss people off who are trying to build stuff inside. It is higher stakes, yeah. Okay, I'll take the digital physics one. How can we design digital physics to attract agents to live in our games? Coins may be useful for them to pay for compute and storage. Second part is probably a statement, not a question. Yeah, yeah. Ultimately, I'm going to say at the end of the day, right now biomes is very much Minecraft. It's not, we're not creating some super AGI thing. But okay, how do we design this, like, this world? How do we evolve biomes to attract agents? I think basically if you get the GDP of the world to be high enough, like, if you reach a world where there's a Neptunium or people value it at $100 because they can build a weapon with it and be powerful in the world, agents might just want to live in these worlds from a purely rational reason such that all the GDP is there and all the activity is there. Okay, I think there's one down there, which was, can you play for free and earn for free? Can you play for free? At some point, I'm sure you'll be able to play for semi-free when you have the Passkey onboarding stuff, so you won't need to worry too much about gas. What you can do in the world is, yeah, I can come in, okay, imagine this. Imagine you have different parts of the world, have different types of resources that they're abundant in. If I build a train system to move resources from one part of the world to another part, to create all these arbitrage opportunities, if I build this train system, I just need to play the game and build a train, right? And then I protect the train system with a force field to make sure it can't be broken down by others. And I make this train system start charging tickets for people to actually take it. I can start selling these tickets. People can buy them. And now I'm making money from the game. But it's not play to earn in the sense of I'm earning these points. It's kind of if you build a business inside and the world sort of has a GDP, then the normal actions you take inside will be economically significant. I'm just going to log in on my phone to see the questions myself. But for now, I saw someone's question, which roughly goes like, is it possible for me to put a door down as a smart item that grieves someone by using a ton of gas like in a while loop or something like that? Yeah, I think one of the challenges with the smart item stuff is going to be what if you insert a track into one of these smart items that is just a bad actor, and then if players interact with it, it's just doing some crazy stuff. This is possible, but I think this is also possible on the EVM. It's kind of the Wild West. And we're going to probably let it up to clients and communities to maybe white label which type of small item chips they audit and verify and know to be trustworthy, and they'll only use those items. So I don't think you need to build security against this at the protocol level, and you can sort of deal with it higher up. Another quick one is, are biomes contracts going to be audited? Well, right now, biomes is like, it's in a heavy dev update cycle thing, so there's like the physics is changing and getting updated. At some point, you probably want it to be audited, especially if the GDP is going to be high. So I could see this happening down the line. Another one is, how do you bridge biomes to solve real-world scientific problems? Yeah, so I think... I'm not totally sure you want to do that. I think if you want to solve real-world problems with simulation technology, I think you can just run simulation technology on a normal computer and simulate whatever you want there and use that to solve your problems in the world. I don't think this will solve that for you. I think this is more about creating this digital simulation that for some reason people take super seriously and they just want all the wood inside. Okay, so maybe any IRL questions? I'm running out of the list. If someone wants to raise their hand, we can bring them a mic. I'll give 10 seconds for people to make their mind. Okay, so thank you. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T06:50:00.000Z",
      "slot_end": "2024-11-14T07:15:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/13_hK3uoJSZ6tt20JSY81twd2hzFWCOGFjUGmSMP9_R4",
      "resources_slides": "https://drive.google.com/file/d/1MTJhaRSY0Mh5Cg4zHZbQB-XmGvS5c_gc/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "dj-anderson",
      "sourceId": "V393ZX",
      "title": "DJ Anderson",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:00:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/11UdQ5iBzKBx_FS4T0nj0XPX9C1X0bSm-aP2bwq7jrOI",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "ebay-and-web3-powered-digital-product-passports-and-what-this-could-mean-for-the-future-of-commerce",
      "sourceId": "DWMA3P",
      "title": "eBay & web3 powered Digital Product Passports and what this could mean for the future of commerce?",
      "description": "eBay is embracing web3 technologies to fulfil the vision of a truly connected product world. Digital Product Passports (DPPs) underpin this movement with a real world application of public blockchain technologies, tokenised products, attestation based technologies and selective disclosure schemes as the  technology of choice.\r\n\r\nI will explore what this could mean for one of the world of ecommerce, why brands are embracing this movement and whats in it for the consumer.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Digital Sovereignty,Use Cases,Regulation,luxury,Digital Sovereignty,Regulation,Use Cases",
      "keywords": "digital-product-passports,DPPs,luxury",
      "duration": 537,
      "language": "en",
      "sources_swarmHash": "642467b5a73ddf35ef37960830082f8c7ef102570bcd47092898616cdb785bc9",
      "sources_youtubeId": "soZ5eIS2olw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735a44f9dbb7a90e1e7e1ef",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735a44f9dbb7a90e1e7e1ef.vtt",
      "transcript_text": " Hi everyone, thanks for coming to see my talk. So I guess we'll go straight into it, we've only got seven minutes. Yeah, so who am I? So my name is James. I am the engineering director for eBay's small but hopefully impactful Web3 team. I came from an acquisition about two years ago of an NFT platform called Known Origin. Now really myself and my team are really interested in looking at the Web3 landscape, looking at the eBay machine as it stands now, and seeing how this technology can potentially impact them and find value for buyers and sellers. And today I'm going to talk about a topic called digital product passports and why I intrinsically believe Web3 is the right place for them. And I'm going to touch on four topics today. A topic called connected products, digital product passports. We'll talk about Web3 and we'll talk about commerce. But first, I want to set the scene and provide a little bit of vision of what commerce could look like. So you've got to bear with me with this, right? So you walk into an office, you see your colleagues stood around a board, maybe you're discussing the latest meme coin. You've got your Vision Pro 5000s on, some AR-enabled glasses. Not only that, it's looked in your address book and it's noticed the PFPs of your colleagues and displayed them on screen. You know, shout out to any Xcopy or Squiggle holders in the room. Not only that, it's noticed that your friend Dan at the board has got a killer watch and the latest greatest in sneakers. These products are connected. You've got to find out more. You double tap your fingers. Some machine vision or AI vision scans these items, uniquely identifies them and starts working out exactly which unique product they are, not the product SKU, the actual unique product. After that, the digital product passport pops up, a digital layer linking the physical product directly in your face. Now, this is a vision piece only, right? But you can see the potential, you know, and you might notice as well, this is very much similar to how maybe NFT platforms looked trading NFTs a few years ago. From this digital link, you can see ownership. You might see if it's got a warranty. You might see trading activity on the secondary market, traceability, servicing insights. This is the vision that I want to talk about today, and this is my vision and potentially the future vision of how commerce might look in several years' time. But wait, before we go on, I think it's worth touching about some of the problems and challenges in commerce at the moment. And, you know, the first three topics, you know, the supply chain. The supply chain is very difficult to wrangle for physical goods, aggregating lots of disparate information. Maybe the digital product passport is the perfect layer for this. You know, circularity. I bought a good on the primary market. How do I sell it? There's many websites, many marketplaces, all have different pros and cons. Maybe the product passport is the place where circularity will start. Maybe it will also tell you how to recycle this good at the end of its life. Maybe this product passport is the place where circularity will start. Maybe it will also tell you how to recycle this good at the end of its life. Maybe this product passport's the place where you'll get additional utility after your first transactional engagement with that brand or with that shop. And finally, the last three, there's some emerging regulatory insights coming from the EU. So from 2026, 2027, the EU will mandate digital product passports in things like batteries, fashion, white goods, collectibles. Now, the goal is to make things more circular and give consumers more choice. Maybe a tokenized digital product passport is the place where that will happen. The last two are digital physical experiences. A lot of the NFT space sort of tried to tackle this over the last few years with some mixed success, maybe starting with a physical and adding on a digital is the right approach. And then interoperability for your items. For the things you own physically, how do you interop with the wider world? Again, maybe this digital layer. So this is the vision I presented. Slightly wacky, really lo-fi. You know, this is not what we're building. This is a vision piece. But really, what does it enable? It enables a digital layer on physical, which is interactive product experiences, real-time information about your goods, you know, empowering customers, and then interoperability, where you, you know, you can take your physical good and use it anywhere. And this is, again, what these could enable. And then really quickly, I've only got a minute left. Last but not least, what is this? So first of all, a physical item and a product connector. Really, a connector comes in many forms. QR codes, RFDIs, NFC chips, machine vision we spoke about. And then you've got this digital layer. And this is what I talk about for digital passports and this could be you can own this thing right that's what crypto enables ownership of digital items quickly a few industry examples 30 seconds left there is already people playing in this game in the fashion space in the luxury space you know maybe on-chain is the next online and this change in consumer behavior will be a big driver for web2 adoption beyond training stonks and mean coins. And finally, wide Z-Bay care. It's massive. We've got to be ahead of these trends. And why Web3? It's credibly neutral. It's a base layer for enemies can play together. Lots of composable great standards to build on and interoperability and ownership. And that's it. Thank you very much. Thank you. That is very interesting in a very real world. Do we have any questions in the audience? Let's see. Anyone? Okay, cool. Oh. Hello, James. Hello. Quick question. When do you imagine a global platform like eBay showing its regular users, I don't know, an icon and something verified as something that says this come with a DPP and of course you can check it visually, but when you acquire this pair of sneakers, this watch, whatever, well, you also acquire this digital passport. Is there some form of an estimate timeline? Yeah, so I guess we may already be playing in this place in pilots, but you may not notice. And we also go down the mantra of eBay users don't really care exactly this place in pilots, but you may not notice. And, you know, we also go down the mantra of, you know, eBay users don't really care exactly, you know, what we're doing as long as they get lots of great value from it. So maybe we're already doing this and they don't know. And maybe next year it'll sort of be pushed up the rankings and things will become clearer. But there's also lots of luxury brands already playing in this place, is the reality. Just right over there. Ooh, nice. Hey, James. Based on eBay's sales numbers and that, what throughput do you think that you need from the underlying technology to serve your users? Oh, that's a good question. Well, I guess, you know, to get to eBay scale, you know, the end game, you know, it's hard to really talk about. It's a reality. It's hard to really say stuff in public. But, you know, eBay is a big, huge user base, billions of products, hundreds of millions of users. You know, in the last two years with the rise of L2s, there's an actual credible path for actually how to achieve the eBay and that's what we'll be keeping an eye on as that space emerges. One more question and the last one. Right over there the gentleman in the cap. It's going to be from over there. Yes. Beautiful. I've got to admit I'm a bit confused as to what would be the advantage for the end user. This feels like a lot of overwhelming information that might not be tangibly beneficial. And it also, frankly, seems like a bit of a privacy nightmare. I can see how aggregating all of this kind of information in a not centralized, but central point of identity could be problematic. So are there any... Do you also envision a level of privacy associated with that? Yeah, so privacy first, very true. We think about this a lot. And is it right that you know this wallet owns 10 Rolexes? Probably not. But we look at technologies like providing proofs, selective disclosure, you know, zero knowledge stuff. You know, that's really where I think this will go. And it'll ultimately be at the behest of the holder of how much information that they should disclose. I do agree that privacy is a big issue in this physical space, much more than in digital is a reality. I can't remember your first question. Sorry. much more than in digital is a reality. I can't remember your first question, sorry. Okay, so that's it for our question session today. And please reach out to James if you have any further questions, any discussion you want to continue. Thank you again. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:00:00.000Z",
      "slot_end": "2024-11-14T07:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1oolmmoeS_8L3O435iq2vuXQPr9H_eWlvs-2T3XokFwU",
      "resources_slides": "https://drive.google.com/file/d/1eQnfDENzYtiKGa7DHiOMGgj36xytcM4y/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "keynote-nomic-foundations-vision-for-ethereums-tooling-ecosystem",
      "sourceId": "VQKXUH",
      "title": "Keynote: Nomic Foundation’s vision for Ethereum’s tooling ecosystem",
      "description": "Nomic Foundation is the nonprofit behind Hardhat. Nomic’s co-founder and CTO will walk you through Nomic’s long-term vision for a community-driven developer tooling ecosystem for Ethereum.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": true,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,DevEx,Tooling",
      "keywords": "ecosystem",
      "duration": 1055,
      "language": "en",
      "sources_swarmHash": "ecaa24cd9ab18856aeb56a49704f9ed634d71821a77c890de54f9cc929267bd9",
      "sources_youtubeId": "w1ObXCY1n-o",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736c0839dbb7a90e1bcdc3d",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736c0c59dbb7a90e1cc98e5.vtt",
      "transcript_text": " Leonardo Silva Reviewer:\" Elisabeth Buffard ...region in Latin America. So, two years ago, we had DEFCON 6 in Bogota. It was an event that brought thousands of minds to the city to get together and start improving the ways that we did things before DEF CON. So in DEF CON, it was a fantastic gathering in one venue, a big venue that we had in the city there in Bogota. It was a place where we were able to meet in between and start working together. A lot of things started at that moment in the whole region with leaders from every country of the region. And it sparkled, it started something that was starting joining together more community in the different cities of the region and having community events that brought a lot of people inside the different countries. There were a lot of new steps on how to bring blockchain education to the region through different methods all over the place, all over the different countries, starting using new programs to involve developers in the Ethereum ecosystem. And it started growing through alliances with universities and networks of universities. This was something that wasn't happening a lot before the DEF CON, but afterwards it started to pop up in different places all over the region, and it happened more to hackathons and these alliances with universities. And it just came all over the place. So right now, between all the different communities around Ethereum in the region, we have onboarded over 2,000 developers, mainly Web2 developers into the Web3 ecosystem and the Ethereum ecosystem itself to different programs and it's people who before DEF CON they had no idea of what was going on in the industry, but now we have this new sense of consciousness of what are the possibilities and mainly taking the message to the people in the region that it's not an industry only to be a user, but you can be a builder. And it's something that it's been made to attending three main points, that is community, education, and development. Two, community, education, and development. Two, alliances, events, and production. And it is growing and growing. There's more and more alliances happening all over the Latin region, between the communities and universities, all the different communities, and universities, all the different communities, social places, and in general, the actors of the ecosystem, like putting everything together to make things happen. And right now, there has been great improvement in the ways to do and to bring the education to the people through online programs and putting together developers and students from all the region together, developers and non-developers who are starting to build in this program. Have we had some impact? I'm just going to mention a few cases of the students that we have had that, I don't know, 18 months ago they had no idea about the industry, and now there's the case of Neil, who at 17 became a Deaf Connect 2023 scholar, and he was able to travel to Turkey and share his experience as a new developer with developers from all over the world and also after this time Christian and Juan Diego who are here today they after taking this these courses and joining to these processes and giving time to it they have been able to find jobs in the web3 industry and go to... They used to work at a call center before that, and now they are part of the industry, and they were able to travel here to DEF CON. So what needs to go next? We need regional coordination. We need to nurture an environment of trust between community builders and community users, so we can take all these to the next steps after the education and create spaces in common between the different actors of the whole region. So we need to push to encourage frequent blockchain interaction within the different actors of the whole region. So we need to push to encourage frequent blockchain interaction within the different communities and find places where we can share hackathons, opportunities to work open source and bring this to a next level. Before the end, I just want to mention between into this great impact that has been in the region there's have been also very difficult times he was Emerson David Silva he was part of the Ethiopian Colombia leadership and he was assassinated by the war this terrible war we have in Colombia so we lost him this year in March 27 and I asked myself why can't I do Emerson to preserve and honor your legacy he's present every day in my thoughts and the community and the leadership dots we want to make his legacy to go further so we have we need to support Emerson's family so if if you scan this QR code we have, we need to support Emerson's family. So if you scan this QR code, we have a Givet project. So we are trying to get funds to help his family and to keep on helping on the operations on education in Colombia and Arauca. So thank you very much. It's been a pleasure. And thank you very much. It's been a pleasure, and thank you. Thank you. We're going to do one quick question of Q&A. You mentioned you onboarded 2,000 developers to the ecosystem. How did you estimate this number? This number is estimated between data that we have from the different organizational processes that has been taken into the region. So we have more than 15 organizations onboarding new developers. So this number comes from what has been done in the last 18 months. And what do you think are the best next steps for the students that just learned about Ethereum? The next step is the most important thing because we need them to take what they learn to the different educational processes and apply that into hackathons, working in open source, working together, meeting other people. And this is something that we are pushing right now. Thank you very much. Thank you. That concludes the session, everyone, on the ripple effect of DevCon.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:00:00.000Z",
      "slot_end": "2024-11-14T07:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1kH4iHwoLEeXM3eu44ZJv-USuH2XZbecC-mTN78JbaFE",
      "resources_slides": "https://drive.google.com/file/d/1PfbexOSxKAevYkHs9Zco-j-XI2nR5Onw/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "scalable-multi-party-fhe-with-phantom-zone",
      "sourceId": "SLJ9QS",
      "title": "Scalable multi-party FHE with Phantom-zone",
      "description": "The talk introduces \"phantom-zone\", a framework to write scalable consumer facing MPC apps using multi-party FHE. Starting with what's multi-party FHE, talk gives a demo of non-trivial MPC app. Followed by introduction to programming model of MPC apps using multi-party FHE inside phantom-zone. Then the talk dives deep into primitives to realise multi-party FHE and ends with advanced FHE gadgets that further enhance multi-party FHE.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "MPC,mp-fhe,MPC",
      "keywords": "FHE,MP-FHE",
      "duration": 1248,
      "language": "en",
      "sources_swarmHash": "c7f9970e7169fb1282bc36149435c26ac9f1dc3b1a0753f024ae09681efd6ab0",
      "sources_youtubeId": "CyIjTbHmVwg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735aeab9dbb7a90e1bde09b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735aeab9dbb7a90e1bde09b.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\"Presenter\": Hello, welcome back. So, today I will be talking about Phantom Zone. But before I dive deep into Phantom Zone and talk about the rest of the things, I would walk you through the motivation behind Phantom Zone. So, if you guys are familiar with globally mutually trusted third party, I would want to introduce you to this idea of globally mutually trusted third party. What is this globally mutually trusted third party? Well, it provides you three guarantees. The first thing that it says is that whatever information you send to this third party, it will always keep that private. It would not leak it to anyone. The second guarantee that it provides is that all the information that it has collected over time from different people, you know, we have been sending this globally mutually trusted third party all our information, let's say for a year, all the information that it has collected for all these years, it will always keep it private and would not allow anyone to poke inside its memory. And the third guarantee that it provides, which makes this particular party very magical, is that it can compute any arbitrary function we want it to compute, as long as we provide enough authorization to be able to compute that function. And it will only output the necessary outputs. Usually, I sort of refer to this mutually trusted third party as a mutually shared computer. And if you guys are familiar with something called the God Protocol, this is the God Protocol. This is a picture from an example back in 1987. This is a picture from an example back in 1987. So first observation to make is that if you really want these three guarantees to be, if you really want this party to be globally mutually trusted, we want this party to be able to prove these three guarantees to any individual without requiring any additional interaction, which is why we require cryptography. to prove these three guarantees to any individual without requiring any additional interaction which is why we require cryptography we cannot just make it on a certain legal arguments or something like that we require cryptography for building this the building this globally mutually trusted shared computer and we started to build phantom zone to eventually build the god protocol but to stick within the realms of practicality, we could only build an abridged version of it. So for the rest of the talk, I will be talking about, A, what is Phantom Zone? Why it is an abridged version of this God Protocol? And the second important thing that I'll be talking about is, how can we push the frontiers to eventually build the God Protocol? Okay, so Phantom Zone, the abridged version. The key idea in Phantom Zone is something called multi-party fully homomorphic encryption. And for me to describe you multi-party fully homomorphic encryption, I have to eventually describe you what is single-party encryption. In single-party encryption, you have a single client, this guy over here. They hold a secret. They keep the secret private to them. They can encrypt their information, which is A here, with their secret and produce an FHC ciphertext. And then they can send this FHC ciphertext to any server. And the can evaluate and any arbitrary function on their private input which is a and produce an output ciphertext and that the and the client can receive the output ciphertext and decrypt it. So this is single-party FIG coming to multi-party FIG. Well the key idea in multi-party FIG is that you split this secret which is held private by a single client in single-party FIG. Well the key idea in multi-party FIG is that you split this secret which is held private by a single client in single party FIG among many clients. So you have S0, S1, S2 as secret shots split among these three people over here. The first step in multi-party FIG is something called collective public key generation. So all these three parties come together and they generate the collective public key. And then all these three parties, using the collective public key, encrypt their private inputs and produce FHC ciphertext. And then they send their FHC ciphertext to the server. Server executes a marble refunction on the FHC ciphertext and produce an FHC output. The key thing to notice here is that all these parties would have to produce a decryption share to eventually decrypt the output ciphertext here. So they produce the decryption share using the secret shards and then they send it to each other and then only they're able to decrypt the output ciphertext. Because in this case, the secret was split among all these parties. So why is Phantom Zone an abridged version? Well, because Phantom Zone, assuming that in the future we're able to add publicly-verifiable FHE to a Phantom Zone can only guarantee the three guarantees that I talk about in the God Protocol to only the holders of the secret shots. It cannot guarantee these three guarantees to everyone around the globe. Which is why Phantom Zone is just an abridged version of it. Okay. So you might wonder, how do we build towards the God protocol? How do we even do it? Well, what I would like to say at the moment is I would have loved to say that after a lot of research and a lot of five years of research, we have figured out the solution to build the God protocol. But no, there are no enlightening thoughts here. And there's one obvious answer to eventually building the God protocol, which is program of sophistication. What's program of sophistication? Well, to simply describe the program of sophistication, let's just assume that you have function f, right? What you can do with program obfuscation is you take this function f and perform some transformations on this function f and produce an obfuscated circuit. You can give this obfuscated circuit to someone else and program obfuscation guarantees that the only thing that you can learn from that obfuscated circuit is the input to output map and nothing else. Now you might be wondering why is this useful? Because if the function is trivial, then you can easily learn it from the input to output map. Program obfuscation becomes very interesting when you sort of like obfuscate a program that is a cryptographic function. For example, let's just say that I take a function that decrypts any ciphertext that is encrypted to my public key. So I take a function and this function has my secret key and it decrypts any ciphertext that was encrypted to me using my public key. And I perform certain transformations using program obfuscation to this function and produce an obfuscated circuit. I give this obfuscated circuit to someone else. What they can do is that they can decrypt any ciphertext that was encrypted to me using this obfuscated circuit. But they can never, ever learn what the secret is inside that circuit. They can never learn my secret key. And these are the class of functions where program obfuscation becomes useful. And I'll tie it to building the God protocol later in the slides. to building the got protocol later in the slides. So now, assume that we can only build program obfuscation for some limited class of functions, not for general class of functions, but limited class of functions. I'll tell you one way of building the got protocol using program obfuscation. building the got protocol using program of application. Step one, modify the FHT scheme that we're using before to become publicly verifiable. What do I mean by that? Well a publicly verifiable FHT schemes does those things. It evaluates the FHT function which you know a a normal FHE scheme does. In addition to evaluating the function, it also produces a proof of correct evaluation so that anyone can verify this proof with the output ciphertext and be assured that the server that sort of executed this FHE function executed it correctly, and which I usually refer to as proof pi of correct evaluation. Step two, replace the collective key generation operation that we did in the multi-party FHE with a trusted setup. In the trusted setup, you have arbitrary number of people here. They perform some MPC protocol to produce FHE keys. The two types of FHE keys which are very important. Public key and the bootstrapping key. Bootstrapping key is usually used for some sort of FHE operations that you can completely black box. The key thing here is that no one knows the ideal secret key because we're doing a trusted setup in MPC to generate these two keys. The third step is modify the trusted setup to also output an obfuscated conditional decryption oracle. Okay, that's a mouthful. I sort of like go into it one level deeper. What is an obfuscated conditional decryptor? This particular conditional decryptor is an obfuscated program of the following functionality. What it does is that takes an output ciphertext and a proof of correct evaluation of FIG circuit. It verifies whether the proof is valid and decrypts the output ciphertext if and only if the proof is valid. And this sort of like tells you why did we assume in the first place that program obfuscation may be feasible only for like limited class of functions because to build the GOT protocol like to build the got protocol using the FHERoute, we only need program obfuscation to be practical for this obfuscated conditional decryptor. So we modify the tracer setup to also output this obfuscated conditional decryptor, and that's it. And another thing to note is that this conditional decryptor also has the secret key, the ideal secret key that no one knows embedded inside it. Okay. So the end-to-end flow is, you do MPC to generate three things. Public key, bootstrapping key, and the offscored conditional decryptor, which I now realize is somewhat of a mouthful. I should have chosen some other term. Anyways, the second flow is, now anyone can encrypt their private inputs using the public key that is the output of the MPC protocol. So you have multiple ciphertexts here. And then they can send it to the FHC server. FHC server evaluates the FHC function, outputs the encrypted output. In addition, it produces a proof because the FIG server is evaluating a publicly verifiable FIG scheme. And then we plug in the proof as well as the output to the off-scaled conditional decryptor and the conditional decryptor would only decrypt the encrypted output if and only if the proof is valid. So this is one way of building the God protocol using publicly verifiable FHE and program obfuscation for obfuscated conditional decryptor. So there's one way, which I've just shown you, but we need new ideas to push the frontiers and to finally build the program obfuscation or and to finally build program obfuscation or indistinguishably obfuscation, if you're familiar with that. Here, I've showed you just one way. But if you're able to come up with new ideas, then probably we can make program obfuscation more practical for general circuits, not just for limited class of functions that we used before. And probably, we can directly build the God protocol from program obfuscation. So while I was exploring this field of program obfuscation and I.O., one key observation that I made was that it's really hard to get efficient program of specification from standard assumptions and we would inevitably require exotic assumptions. And I'll tell you what are standard assumptions and what are exotic assumptions. Well a standard assumption is an assumption that has been there for a while, for example D log, discrete log problem. There also exists additional incentive for people to break these standard assumptions. And exotic assumptions are somewhat newer assumptions. Like, they have been only there for like five years, or not even five, it was like two to three years. What we can do as a community to, you know, realizing that we might inevitably need newer assumptions to build practical program amplification is we can start examining these newer assumptions, start breaking them, start testing them. Or we can build applications using this assumption so that we can incentivize people to break them and tell us whether they're broken or not. And then eventually, in a few years, we would have candidate assumptions that are newer assumptions, but they have become then standard using which we can build practical program sophistication. And taking a first step towards this, we are launching a bounty program to break one of the candidate assumptions, which is called program obfuscation by local mixing. The way I think about this particular assumption is that they're taking more computational complexity approach than taking the traditional approach of using algebraic structures to build program obfuscation. The goal of the bounty is that we provided an obfuscated circuit with roughly 240,000 gates, which was obfuscated from an original circuit with roughly 1,000 gates. And you had to find the original circuit. You can learn more about the bounty at OfficeTopia.io. If you know what OfficeTopia is, OfficeTopia means that we're living in a world where authentication is practical, and the bounty amount is 10K. And this bounty is launched in collaboration with Ethereum Foundation and Zerix Spark. Okay. So before I break, and I think that I have a bunch of time, okay, before I break, and I think that I have a bunch of time. Okay, before I break, I would want to make one conjecture. And the conjecture goes as follows. I think the God protocol is the convergence of cryptography. Probably building the God protocol would require certain sort of like FHE. That is just one route, but like publicly viable FHE and other things like MPC for just setup and so on and so forth. But once you build the got protocol, I think it encompasses everything. It gives us everything that we have been wanting for for a while. It gives us witness encryption. It gives us zero knowledge proofs via signatures. It gives us MPC, multi-party computation. It gives us FE, functional encryption, all of these things that we've been demanding for a while. And this is also one of the major reasons that we should start investigating much more seriously how to get practical program application and finally build the God protocol. And that's it. Thank you. All right, thank you for All right. Thank you for the talk. We do have some questions rolling in. Yeah, let's go through some of the questions. Let's start with the first one. Can we implement threshold ECDSA with Phantom Zone? At the moment, yes, because you can express everything. Like, theoretically, yes, but it would be very impractical to implement ECDSA with PhantomZone at the moment because ECDSA is like you're doing elliptical operations, which is a lot of operations. As far as I understand, threshold ECDSA is possible. It takes two days to generate one single signature. All right, so next question. Can you tell us a little bit more about the definition of obfuscation as a virtual black box? That's the first question over here. Isn't the definition of obfuscation as a virtual black box impossible? virtual black box impossible? I am not posing obfuscation as a virtual black box. I did not mean to say obfuscation is a virtual black box. By the way, the impossible result of a virtual black box is only for certain very restricted class of programs. It's not for general class of programs. Eventually you can aim for virtual black box with certain caveats. But again saying that my definition of sophistication is not virtual black box. All right and what can be done today with Phantom Zone? At the moment as I said Phantom Zone is an abridged version of the Scott protocol. It does not even have publicly verified FHE scheme, so it does not give you all the three guarantees. The only guarantee that it gives you is that it will execute the function that you ask it to execute while private information can be coming from multiple people. It'll keep the information private, but you'll have to trust it for it. So you'll have to trust this particular server to always keep the information private, but you'll have to trust it for it. So you'll have to trust this particular server to always keep the information private and not send it to anyone else. Perfect. And we do have one last question. Oh, cool. More questions rolling in. Can obfuscating programs undermine open source transparency and make it harder to verify the absence of malicious code? I see. Make it harder to verify absence of malicious code? I see. Make it harder to verify absence of malicious code. Well, that is assuming that the entire program is obfuscated. When I say obfuscation, we require obfuscation for certain parts of the program, which can interact with a public program and a private program which is obfuscated. I understand that obfuscation can be used for many malicious purposes as well, like for example, you know, like, there are several reasons why people might be interested in obfuscation, but we can, as a community, make sure that there's interaction between the public interfaces and the private interfaces which are obfuscated. All right. And why do you call the publicly verifiable FHE circuit obfuscated? Doesn't the require solidity verifier or something which is public? No, I think once I give you obfuscated circuit, there are certain guarantees that you can learn from the obfuscated circuit itself, that it does not reveal anything, as long as you've done the obfuscation correctly. Alright, and do you have evidence that the conditional decryption functionality is possible using I.O.? Yes. There are theoretical results and we're trying to make it practical as well. All right. Can you give one example each on how I.O. can replace ZK, MPC, FHE? Okay. So for ZK, what you can do is like you can embed a secret key inside this off-secreted circuit, the God protocol, and a zero-knowledge proof is just a signature from this God protocol. Whatever secret exists inside this particular server or this God protocol, or this FHC circuit, off-secreted circuit, a signature by that thing becomes a zero-knowledge proof. So you do not require zero- zero knowledge on the client side anymore. For MPC, again, it's a globally mutually trusted third party. All of us encrypt our private inputs with the public key corresponding to the secret key that lives inside this off-site circuit. And we send our private inputs to this. It decrypts that, performs some function, and produces the output. So that's one way of replacing MPC, and the same applies for FG. Cool. We can stay here for maybe another 10 seconds if there are any new questions rolling in. All right, cool.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:00:00.000Z",
      "slot_end": "2024-11-14T07:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1V86Kc6aOcbAUsOm8NBUDaQ00YrCn0XJN5ce8Lyt73WU",
      "resources_slides": "https://drive.google.com/file/d/172T15E7PtqtQw3tN8MRjbwgKRVai0bjf/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "visions-of-a-viable-dacc-biosafety-strategy",
      "sourceId": "7VDGQM",
      "title": "Visions of a Viable d/acc Biosafety Strategy",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 610,
      "language": "en",
      "sources_swarmHash": "101751999983d5d26371cafb5bff3f14e153bee0637bd0281e2bbc514d97dd38",
      "sources_youtubeId": "oOsKEwKD-jE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ab179dbb7a90e1a340ab",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ab179dbb7a90e1a340ab.vtt",
      "transcript_text": " Okay, so welcome to DEAC Biodefense. So I think I'll start off by just briefly recapping how the hell I even got here. Because basically, I started off being the Ethereum guy, and I'm historically not a bio person and more of a math and computer and economics person. I was definitely interested in longevity almost since the beginning. I read Aubrey de Grey's book, Ending Aging, back when I was 13, and that just really impressed me, both the sheer importance of solving aging, but also the fact that there is a roadmap to actually get us there. And so I was excited about longevity the whole time ever since. But then for DEAC Biodefense in particular, in 2021, there was this big meme coin mania that I'm sure you guys all remember. So first in 2016, there was Dogecoin, and Dogecoin went up really high. And then during the 2020-21 bubble, a bunch of people basically started like copycatting Dogecoin and making their own dog-themed coins and hoping that they'll go up to. And so some of these people decided as a strategy that they would put a large portion of their supply into my wallet balance and then publicly say Vitalik Buterin supports this coin, even though I never touched it. And some of these coins just like went up way higher than they should. And so one of those big coins was called Chiba Inu. There were also others. And I sold a bunch of them. And I donated some to a couple of organizations, one of which is Crypto Relief India. So at the time in India, there was this big COVID crisis that was happening. Lots of people were getting sick. It looked very bad from outside. It looked like no one was really caring about the situation, so I came in and donated to this group that originally Baljie introduced me to. First I donated a little bit, and I donated a amount of Shiba tokens that I thought would be worth a few million dollars at most, but then they ended up managing to cash out like literally $470 million dollars and so after that we yeah so 470 million ended up being like more than crypto and relief india needs for its whole roadmap and i was also started researching and understanding COVID and pandemic-related topics more heavily. And Balvi, between myself and some of my science advisors, basically slowly formed and ended up spending a significant chunk of that meme coin funding on basically moonshot topics that deal with all kinds of anti-pandemic related efforts in general. So biodefense is important, right? So compared to the natural environment, population density is way higher than before. Urbanization continues to increase. We have very easy worldwide air travel. In the best case scenarios, we might even have like rockets that will take us from New York to Bangkok in one hour but I mean Eli Dorado told me that like that's probably not going to be commercialized anytime soon because rockets are a little too dangerous but we have supersonic jets they might take us there in like four or five hours and so I expect air travel to just to be even more ubiquitous. Factory farming creates lots of pandemics, man-made pandemics so gain-of-function research",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:00:00.000Z",
      "slot_end": "2024-11-14T07:10:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1yGQBHJnRzdZfi9mog9ipmc0zYrZB2nzXpEG4mGwCGko",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "how-long-non-finality-could-kill-ethereum",
      "sourceId": "U9E7PD",
      "title": "How long non-finality could kill Ethereum",
      "description": "After the merge, Ethereum has a finality gadget to provide an economic assurance that transactions will never be reverted. When 2/3 of the validator set are online and agree, we finalize. Otherwise, we enter a period of non-finality which can be very long, up to a few weeks. Long non-finality has never happened in Ethereum's history and could trigger a cascade of failures that will kill liveness. How can we harden the network against this? How high are the stakes?",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Decentralization,Security,Consensus,Decentralization,Security",
      "keywords": "-",
      "duration": 961,
      "language": "en",
      "sources_swarmHash": "9e0bf1af55bb735c4733ddce1734ff6b26ea4a77f944d0d92806e80333fb04b7",
      "sources_youtubeId": "z2jafwPFLaQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736c6b89dbb7a90e1cd35b3",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736c6b89dbb7a90e1cd35b3.vtt",
      "transcript_text": " Hello, hello, thank you. Let me drop the water. So hello everyone, I'm Dap Lyon, Ethereum Cordep at Consensus at Sigma Prime. And today I'm going to talk about how long a fanatic could kill Ethereum. Or some healthy dose of fear mongering to get all the core devs here to care a little bit more about this issue. So just to start, let's get everyone on the same page. This talk is not labeled as an expert, so we'll start with what the hell is finality. So Ethereum has a strong crypto-economical guarantee where if a block is finalized, it will not get reverted unless two-thirds of the stake, so an insane amount of money gets destroyed. For the purpose of today, what you have to know is when we finalize something, everything that descends from it is a possible block or state. Everything else we don't care about. So when the network fails to reach finality, in other words, two-thirds of the chain agree on something, then this section of blocks and states just keeps growing. Because the finalised checkpoint doesn't get updated and it just keeps going in the past. And in the past. So today we're going to run through an experiment. I'm going to show a hypothetical failure mode of the Bitcoin chain and we'll talk on all these possible concepts and how they tie together into something that could actually kill Ethereum. So let's start with some finalized checkpoint and here are some descendant block A. But this is a longer chain. I'm just representing here shortly for succinctness. Next, oops, some client has a bug. With more than one-third of the state, say like a consensus issue where they reject the block that they shouldn't. And this causes a chain split. So the faulty client cannot produce on top of A, so it just goes on its own fork and mines B prime, X prime, so on and so on. Everyone else stays on the majority fork, so B and X, and with these three dots, I'm representing a long chain. If we're talking about one day of non-finality, this could be 7,200 blocks, so quite a lot. The first thing that happens is most clients spend not significantly, but a decent amount more of this space during non-finality as the amount of blocks and states that they have to keep for fast access is higher. Then, now it's when things get interesting. Let's assume that one of the faulty clients, for some reason, be another bug or some network problem, they lose out of sync or they crash. And they think that B prime is the head. They will act on it and produce this block, X prime prime, which forks from B prime. This block is very expensive to process. And we'll show now why. And everyone else must process. Everyone in the fork X prime and everyone in the fork X. So these blocks are expensive because in Ethereum, states are not only dependent on the parent but also on time. When you want to process block X prime, you have to load the state at B prime and perform something that's called a state advance. You have to run this code, which is relatively expensive and increases linearly with state size, a lot of times. So in this case, if it's one day of non-finality, you have to do this 225 times plus hashing. So I don't know the numbers, but we are talking a few minutes, potentially. And we can have many of these blocks. And these blocks are very expensive, and what they can cause is that now some nodes get overwhelmed. They trigger resource exhaustion. So let's consider now one of the non-faulty nodes at x. They have to ingest these expensive blocks, and now they also fall out of sync. So they start to produce these annoying blocks that, again, are expensive to process. And here you can start to see how things spiral out of control. Now that we have a decent amount of forking, disk space really shoots up. Because, again, we are extending the space of possible blocks and states not only in the dimension of time, but also in the dimension of forking. And now clients can start, again, depending on the client and how they run, to run out of space. If a client has no space, it stalls. It cannot progress. It needs manual intervention. So because of that, let's consider one of these forks. Now it doesn't have enough participation, because clients are losing disk and going out of sync. Then you have reorgs, which are expensive. And another interesting failure mode is that some ELs cannot handle reorgs past a certain death. If that happens, they stall because they don't have enough state to process the chain. So more clients going offline. Yada, yada, yada, yada. Also, other things to consider, clients could crash with OEM depending on how their state cache on memory works. And again, triggering sync issues. So in these accidental, again, accidental failure modes, any sort of bug can spiral out of control as it causes resource obstruction, which causes faulty blocks, which causes resource obstruction, and so on. And this behavior is quadratic with time to finality. Like the longer finality goes on, the more expensive these operations can become. But that can be triggered by an attacker. All of this so far has been accidental. And accidental failure mode is problematic because it has a lot of stake. But if an attacker wants to exploit this and has significantly less stake, potentially like some hundred keys, it can truly wreak havoc. So the problem, as I was saying, is these very expensive blocks that do a lot of skips. And you can create a lot of objects that exploit this. So for blocks, you need stake, you need to have a valid proposal signature, but you can brute force valid proposal slots. If you have maybe like 100, so like the crusader, the low-carb crusader could easily attack the chain in the finality of like one day and create about some hundreds of blocks, which again, consider each one takes one or two minutes to process, you can do the network like that. Aggregated stations also require stake. You have to brute force the aggreter duty. And unaggregated stations, you don't require stake as you can just produce an invalid at the station and it will still force nodes to compute the shuffling. The good news is that we are fixing this last attack vector on Electra and you will still force nodes to compute the shuffling. The good news is that we are fixing this last attack vector on Electra, and it will also require stake. So look at some history. We have had some issues with finality in the past. The most relevant one happened in 2023, where we lost finality for about four epochs. What happened is something very close to this failure case where some nodes considered to be synced, and they propose a very expensive, not as expensive as the ones we're talking about, but some attestations that take some time to process. Unfortunately, Prism had some caching issue where it did this work over and over. And it didn't have a well-structured queueing system to protect themselves against clogging. So any relevant work to progress the chain, like blocks and attestations, just didn't happen and that resulted in the loss of finality. Also Medalla is the quintessential event of nonfinality. I'm not going to talk in detail here, but we can talk about it later. Also GoEarly suffered a very brutal death with a bunch of cascading effects. And funny enough, the peer-dashed devnet that we ran recently had a massive reorg of 130,000 blocks that triggered this edge case where none of the ELs could continue because they stalled without state. So yeah. What's worrying about failure mode regarding non-finalities is that it's very spirally. Things cause things and it gets pretty out of hand. So far we have talked about accidental issues that relate to client bugs, network partitions, these sort of things. But the worst case that we should consider is what's the possible longest non-finality that we could see on the beacon chain? And that's related to this failure case where say that we have supermajority geth, they have a consensus issue where they mint infinite eth. So it's not a fork that we can canalize. In that case, if they finalize the wrong fork, they will be stuck there. So on the correct fork, in this case chain B, we will have to wait for them to leak out. That's going to take a long time. in this case, chain B, we will have to wait for them to leak out. That's going to take a long time. So if Geth is like 70%, we are looking at 32 days. So that's in my opinion, that's the worst case that we should aim for. And this is realistically something that could happen if for some reason Geth gets a significant amount of market share. So what can we do? What can we do? Easy, just don't do bugs. Don't release clients with bugs, then we don't have non-fidelity and easy, not a problem. Now for real, there are no strong mitigations, it's just we need to harden the client. So don't run out of this space, don't run out of memory, don't get exhausted. And third, we're going to look and explore if we can actually reject some of these useless network objects to protect against the most blatant cases of those. All of these solutions, they work on this. It's not actually a trilemma, but it's a trade-off space. In Ethereum, we work really, really hard to preserve liveness. That's why we have this hybrid consensus mechanism where the chain goes into this mode justfidelity mode just because we want to preserve liveness. We don't want to be in a situation where no operator intervention is mandatory to recover the chain. At the same time, we don't want to die and we want to process everything in a timely fashion. So the first point and and the most dangerous, is running out of disk space. Here, there are a bunch of easy rules that I think most clients follow. I think Lighthouse is one of the ones that don't follow all of them, but we are working on that. And in Lighthouse, we are now introducing this very interesting optimization, which is storing everything as divs. That's commonly known in consensus folklore producing this very interesting optimisation, which is storing everything as divs. That's commonly known in consensus folklore as three states, and that's going to be rolled out in the next version, but it only affects the freezer database. What we want to do next is apply the same principles, but for unfanelised states. And that's going to be massive, because now every time we want to store a state, instead of spending 200 mechs, we can just spend half. And we have really good compute and apply times for a bunch of different divs. How this is going to look like, we'll have this hierarchical div structure where if you want to recuperate, say, the state in red, you load snapshot and then iteratively load the div, apply, load, apply. This is very, very efficient in terms of disk space, and as you saw the numbers here, it's actually pretty fast and even faster than replay. The complication with the unfinalized section of the chain is that we have moving finality, which complicates a bit the design. But if we just extend the diffs into some finalized range, keeping at least one per layer, the design works. And pruning is not that complicated. For memory, there is not much you can do. You just have to make sure that your state cache doesn't blow up in these circumstances. And again, three states, memory helps a lot. And this is already in production in Lighthouse. And the last one, and I think this is the most important, you need to have some strong queuing system where you don't let yourself get exhausted from this garbage that can come from the network. Looking at the main net incident that we talked about, if instead of having this queue, you split it into sections, one that has higher priority for things that are useful to you, in this case, let's say, the sending of heads, and then everything else gets processed with lower priority, we should be safe in this case. Lighthouse at the moment is working on revamping all our queuing system, and we're going to do something with this, also with fairness, so P0 doesn't exhaust P1. P0 doesn't exhaust P1. And the last point is, okay, we know that these blocks are really bad. Can we just ignore it? That would be really nice. And unfortunately, no. If we want to preserve liveness, there are a lot of edge cases where if we start to ignore blocks, then the chain could stall. So I think this is the only one I've seen that I like, and it's on Prism in production. This is by POTUS. If you get things that extend something previous to justified, it's more complex than this, but just to simplify, you can ignore them. And this would have protected Prism from the issue that we saw before. Unfortunately, this is just one of the many failure cases that the chain can have. So it's not like a bulletproof. So what are the next steps? Well, testing, testing, testing, and testing. The issue with non-finality is that we have to do Pektra, we have to do Pirdas, we have to do so many things that unfortunately dealing with non-finality is that we have to do Pektra, we have to do so many things that unfortunately dealing with non-finality just goes like priority number three or four. I talk with the Ethereum DevOps team, the PandaOps, and what we should do is have some cyclic tests, either continuously or maybe like quarterly, where we test non-finality. We want to uncover these bugs ahead of time and not when finality dies either in an important DevNet or main net. Also in Sigma Prime, we have a tool that we have been using with great success in this source of attack nets, and it's insanely good at killing networks. So we are with Michael in the process of revamping this tool, and we'll definitely use it in these tests. So yeah. But, yeah, I mean, just to not be too fear-mongering here, the beacon chain has been exceptionally stable through its lifetime. And the only time it lost finality was for epochs, which is nothing. We have a robust set of operators who are very diligent, hands on, the same for core devs. We have triaged and fixed issues really quickly. So again, we have to work on this. It's my mission as a core dev to make sure that Ethereum never loses liveness. But yeah, we've been doing great so far and we should be proud of it.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:10:00.000Z",
      "slot_end": "2024-11-14T07:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1ALLMSUfx7xTKyChAX-LGEzcu_42YB7z9HKrLLPQ0-cc",
      "resources_slides": "https://drive.google.com/file/d/1-JNjLzVUFDVVLsX76XF50vjI2FzLjUbC/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "stablecoin-technicalities-innovations-challenges-and-opportunities",
      "sourceId": "XJBYKJ",
      "title": "Stablecoin Technicalities: Innovations, Challenges, and Opportunities",
      "description": "This session is dedicated to the evolving landscape of stablecoins, with a particular focus on the latest advancements and the role of PYUSD. This talk is tailored for developers and crypto-enthusiasts eager to explore the broader implications of stablecoin technology, integration challenges, and real-world applications of stablecoins in modern finance while focusing on PayPal's role in the Ethereum ecosystem.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Use Cases,Remittance,Product-market fit,stablecoin,Product-market fit,Remittance,Use Cases",
      "keywords": "Stablecoins",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "c07ce4f3031afe3446ed68985bffb807c2d79170de4c7206322fc5502b945c3d",
      "sources_youtubeId": "NShae3X5QHA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:10:00.000Z",
      "slot_end": "2024-11-14T07:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Mh_MTgJQI_Yj0brAf1A-CWrCUWCivpHPQFUodwNtN3M",
      "resources_slides": "https://drive.google.com/file/d/1duJRVS4wYp8oDXSV7b1OqcTzzu5ZAZsA/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-challenges-of-leaving-laboratory-outbreaks-to-scientists",
      "sourceId": "TPLHFG",
      "title": "The challenges of leaving laboratory outbreaks to scientists",
      "description": "NA",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Academic",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 661,
      "language": "en",
      "sources_swarmHash": "fa87bb835a351d6d4df9119d32643e69d0b3dc68a8b607fdeba54852e233647d",
      "sources_youtubeId": "vwTDtUELw3g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735abb99dbb7a90e1a6f0a9",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735abb99dbb7a90e1a6f0a9.vtt",
      "transcript_text": " Hi, thank you for inviting me to give a talk and thank you Vitalik for that perfect opening to my talk. So who am I and what am I doing here? In early 2020, I was pushing for the lab leak hypothesis to be taken very seriously. And as a scientist from a reputable institute, that gave me a lot of credibility. And once the lab leak hypothesis was taken seriously by experts broadly, that's when I went to the Bulletin of Atomic Scientists and asked them to please convene international experts to think of new frameworks around risky research that could cause pandemics. And so in the next 10 minutes, I'll tell you about ways that I think we should structure a future effective bio-risk and bio-safety approach. So as Vitalik mentioned, the number one takeaway that most of us had from the COVID-19 pandemic is that actually experts are not very reliable during times of crisis and that they themselves can be a source of misinformation. And we saw that in the COVID-19 pandemic, this ranged from human-to-human transmission, whether masks are protective, how the virus spreads, whether it's airborne, how to protect yourself, what is herd immunity, and whether the vaccines would prevent infection. And most of all, whether the pandemic might have started in a lab. And so what do you do about the situation, right, where the experts are not even able to tell you what is happening in real time? So fast forward five years since the pandemic began, we now know that experts broadly around the world agree that both natural and lab leak hypotheses are on the table. And briefly, this ranges all the way from an animal at the market infecting people to perhaps a villager getting infected and then traveling to Wuhan, or maybe a scientist was infected in the field while collecting samples and then went to Wuhan, or that there was an accident in a lab where this virus was being handled. Ultimately, everyone agrees that the original virus, the precursor of this virus, must have come from bats, but we just do not agree on how it made its way to Wuhan. And so all the evidence today remains circumstantial. No natural or lab source has been found or reported. And surprisingly, five years later, no external investigation has been conducted and what we know now is that there were influential scientists who sought to squash the theory calling it a conspiracy theory and this started all the way at the beginning of the pandemic. So I was really fortunate to publish my analysis of why I think the evidence strongly points towards ALAM origin in the New York Times this year. And just to sum this up, but if this is the first time you've heard of this, I really recommend you check out the article. Why do I think that COVID-19 likely starts in a lab? This pandemic, it really could have been caused by any of hundreds of different virus species and any of tens of thousands of wildlife markets in this region and in any of thousands of large cities and in any of thousands of large cities, and in any year. But it was a SARS-like coronavirus with a unique furin cleavage site. This is a gain-of-function feature that emerged in Wuhan less than two years after scientists there, sometimes working at low biosafety, proposed collecting and creating viruses of that very same design. And of the hundreds of other SARS-like viruses we know today, none of them have such a furin cleavage site. So this story, why haven't we found the origin of COVID-19? It's because an incredible amount of data continues to be withheld. So this concerns early cases, the transmission chain, the genome of the virus was withheld in the early days, including early sequences still being withheld. We still do not have access to the activities and catalogs at both the wildlife market in the city as well as the laboratories in the city. So all of these were not put out in the public. A lot of it was withheld in private communications, private databases, in scientific journals, and under peer review. Sometimes an editor would reject a paper and it would stay hidden for years. And private research documents as well as thesis and grant proposals, we only know about all of these data today because they were unearthed by mostly internet sleuths, so independent vigilante sleuths, as well as scientists and journalists. And some of these other documents were leaked by people inside the US government or others were sued for through the Freedom of Information Act. So these things had to be painstakingly pulled out of the sources. What the COVID-19 pandemic really showed us was this complete lack of interest in finding the origin of COVID-19. So if you look at the US collaborators of the Wuhan scientists, they admitted that their collaboration had never given them access to the pathogen samples. So these are samples from bats and even people collected and stored in Wuhan. They could not reproduce the database of the samples or virus sequences. And they did not have access to the database in real time that was in Wuhan. So they didn't even know about viruses collected after 2015, even though the funding covered their work through 2019. So that's a four-year gap. They do not know about the viruses collected after 2015 in Wuhan. They did not ask their collaborators in Wuhan if they had started on experiments proposed in 2018. So this included experiments where they were inserting furin cleavage sites and other gain of function features into SARS-like viruses in Wuhan. So some would say this is strategic ignorance, right? Maybe they didn't ask because they didn't want to have information that would then put them in a difficult spot. But I would actually argue that it was structural because they never structured their collaboration in a way that gave them full access in real time to the samples and even the sequences. And this problem really goes way beyond the EcoHealth Alliance to even upwards in government. Earlier this year, the Department of Defense in the U.S. said that they had not been closely tracking their funded projects. They could not even tell how much funding they had given to Chinese research that would enhance pathogens with pandemic potential. And unfortunately, the new US government policy on this sort of research that kicks in next May still leaves many loopholes open. In fact, the entire research pipeline that could have caused the COVID-19 pandemic remains not subject to oversight. So you can go anywhere, collect any novel pathogens, no oversight. You can serially passage them, so adapt them to human cells, even human avi cells, and even humanized animals still not subject to oversight. So really nothing has been done here to deal with this risk of pandemics coming from labs in this manner. So unlike other technologies, you know, like nuclear, chemical, even in transportation, even flying planes, where you have a serious accident, there's no independent body here for these sort of pandemic risk research to really conduct external oversight. So there's no independent external organization that will have oversight of this work around the U.S., let's say. And when that's an incident, there's no investigation to go in by this independent body. It's usually all self-investigation, self-reporting. So this is a real problem because we are now truly in an era of pandemics starting from laboratories. Unlike 20 years ago, today research for pandemic risk is proliferating due to many technological advances in gene sequencing and synthesis as well as dropping costs. So now more and more labs are gaining access to this ability to create viruses in the lab from scratch, just based on the sequences. It's not very expensive. And unfortunately, a lot of this is happening at low biosafety in ways that cannot contain airborne viruses like COVID-19, let's say. And it's importantly not being systematically tracked. So there's no one really systematically tracking it, even at a national level in the U.S. So why is the case? And I think that there are two problems here that really underlie this challenge. So the first is that there are many penalties and fear of retaliation for virologists to be calling out their peers. So imagine if you start pointing fingers at your colleagues for doing risky research, you could immediately find yourself alienated and shut off funding applications as well as publications. So this would essentially destroy your career. And those who are for gain of function, so they want gain of function to happen, they say that you can't put regulation on us, you can't put all this oversight on us, because then the US will lose the competitive edge. So the problem with this is that even these scientists, these leading experts, as seen in the case of COVID-19, cannot accurately predict the risk of their cutting-edge experiments. It's really hard for them to know what will happen when they're working with novel pathogens and putting in new features. So the second problem here is that we are kind of stuck in this 9-11 dichotomy of outbreaks being either natural or deliberate. And this is a problem because I think that the risk of pandemics coming from lab accidents is much higher than the risk of a pandemic coming from a deliberate bioterror incident. So unfortunately, this story of let's defeat the bad guys let's let's defeat the terrorists it gets much more interest and funding it's much more sexy than then telling people let's just stop people from having accidents let's protect the good guys from having accidents in their labs so that this whole field of accidental lab leaks has been really neglected in my opinion and and it is it's compounded by this fact that many many virologists are afraid to call out their peers. So very little is being done. And I have this quote that really exemplifies this problem. So early on in the pandemic, an influential virologist said, it really bothered me that this was some suggestion this was a lab construct. So the virus that was causing the pandemic. If it turned out to be true, it would really bother the hell out of me. Not just because of people dying and so forth. It's an indictment of the field, right? So these virologists, and not just him, they think that if COVID came from a lab, it would indict the entire field of virology. So this is a major conflict of interest. So where do we go from here? Clearly, we should investigate the origin of COVID-19 so that we can restore public trust. Imagine if, you know, a few decades down the road, it's found that this pandemic started in a lab. We need to be able to look back and see our public leaders, as well as our scientists, leading the charge on investigating and finding the lab origin. We don't want to look back and see them being complicit in the cover-up, suppressing an investigation. So there are three things that I think should inform a viable biosafety strategy. So the first thing is that OSINT is necessary, but during a crisis, as we have seen, is challenged by mis- and disinformation, a great amount of information is actually kept private, and not even for malicious reasons. We need a system to counter misinformation from experts, and journalists especially should be much more skeptical and seek diverse input so that they don't find themselves amplifying the misinformation. And finally, we really truly need an independent organization and regulation that focuses on reducing the risk of accidental pandemics. So thank you again for inviting me to give this talk.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:10:00.000Z",
      "slot_end": "2024-11-14T07:20:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1p9hSMYlq5ABHla4brR0sibxE7RLsOTyxT95WWe9_UTQ",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "working-together-with-unity-blazor-nethereum-and-mud",
      "sourceId": "SDUYDQ",
      "title": "Working together with Unity, Blazor, Nethereum and MUD",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and explores how Unity, Blazor, Nethereum, and MUD integrate to build blockchain-based games and applications. It covers the overall architecture and structure of .NET projects, including smart contract integration and core logic. Key topics include Nethereum's integration with MUD systems and tables, extended code generation to support MUD, deployment strategies, bulk saving, data synchronization, and testing.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Frameworks,Gaming",
      "keywords": "Nethereum,MUD,Unity",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "3f3e9761b23a20f24c5e1a858e77d43e2e026297532cb8b79bdc4d1007b4598a",
      "sources_youtubeId": "27FnZdCxvos",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:15:00.000Z",
      "slot_end": "2024-11-14T07:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1cgSTfVg9G2fBhaLSYwdokUa1BNjwvijZP4qAjaifH3Q",
      "resources_slides": "https://drive.google.com/file/d/1lmAcG8PiEAzL6F0XryXgv0Rp8IYgcfay/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "dark-winter-why-the-risk-of-unnatural-pandemics-is-an-existential-threat",
      "sourceId": "7QAKPP",
      "title": "Dark winter: why the risk of unnatural pandemics is an existential threat",
      "description": "The past history of pandemics and biological attacks, lab accidents and epidemics show a recurrent theme of denial, silence and cover-up around unnatural epidemics and the powerful vested interests at play. Quantum advances in genetic engineering and synthetic biology may lead to a future where resurrection of extinct viruses are the norm. The risk of unnatural pandemics is greater than ever and poses an existential threat. Early warnings of epidemics through OSINT can help mitigate the risk.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Collective Intelligence,Public good,Security",
      "keywords": "Biosafety",
      "duration": 864,
      "language": "en",
      "sources_swarmHash": "fc62999de5279d4c4492e7e5c95b848c48598b366808d9e9a2db619a01a4f630",
      "sources_youtubeId": "D7vo7P_bz0g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ac4c9dbb7a90e1a8d049",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ac4c9dbb7a90e1a8d049.vtt",
      "transcript_text": " To tell you about what we've done with funding from Balvi, but first I want to explain the context and why I'm so concerned about the risk of unnatural pandemics. These are just my declarations. I'm a physician and infectious disease researcher from Australia. So we hear about epidemics all the time, right? We hear in the news about avian flu, about mpox. We know that these things are never-ending. And actually, the data shows the risk of new emerging infections is increasing. Really important to understand the past and the present in order to look at what the risk is in the future. And this case is a really good one because it illustrates all the problems with the medical community accepting the possibility of unnatural epidemics. And this is Bhagwan Sri Rajneesh, whose cult perpetrated the biggest documented bioterrorism attack in the US in 1984. The public health authorities, including the CDC, judged that it was a natural outbreak. And there was a politician who said, no, I think the Rajneeshis did it. And he was ridiculed and shouted down. And even six months later, when Rajneesh confessed that they'd done it, he wasn't believed. So even a confession isn't enough to identify an unnatural outbreak. Where does that leave us? And then a full 12 months later, the FBI was doing a raid on the ranch of the Rajneeshis and came across their illegal bio lab, which had far more dangerous pathogens in it than the one they used for this attack. There's just so many examples that I don't have time to give you about the inability of the medical and public health community to ask the question even about what is the origin of this epidemic, let alone identify unnatural ones. San Francisco in the 1950s, the US military sprayed bacteria over from a ship in the ocean over the San Francisco to see how far they could disperse a biological weapon. And people did get, they thought the bacteria was harmless, but people did get sick and one person died. No one knew anything about this until 30 years later when a journalist uncovered the truth. And the medical community did not recognise that it was an unnatural outbreak. Then in the 60s, the US military dropped light bulbs into the subway system filled with trillions of bacteria that were a proxy for anthrax. It wasn't actual anthrax, but a similar related bacteria to see how the spores would disperse in the tunnels. In the Soviet Union, an island called Vozrozdenya was used as a biological weapons testing facility, and in 1979, they exploded a smallpox bomb on this island. There was a ship passing by 15 kilometers away, where someone got infected, and it set off a big outbreak of smallpox, a highly fatal outbreak in the town of Aralsk. And there's numerous other examples, including a pandemic of flu that occurred in the 70s, which was traced back to an improperly inactivated vaccine strain of the virus. And for 30 years, scientists kept denying that it was an unnatural outbreak. This man, Ken Alibek, was the deputy director of the Soviet Union's bioweapons program, Biopreparat, and he defected to the US and he wrote this book which really details some of the pretty mind-blowing things they were trying to do in the 60s and the 70s when the tools were much more clumsy than what we have today. So they were trying to engineer even more fatal versions of smallpox, shorten the incubation period, and to combine genes from smallpox with other deadly viruses like Lassa fever. And if they were doing that today, it's just so easy to do it, whereas back then, fortunately, it was much more difficult. Basically, anything with DNA can be edited, whether it's a virus, a plant, a human being. And, of course, human genome editing started in 2016. In 2018, a Chinese scientist caused an uproar by not only editing two embryos but bringing those children being born. He presented the results at a conference, and there was this enormous faux outrage by the whole global community. But very soon afterwards, all the superpowers jumped on board into what I think is the new arms race, which is engineering of human beings. In the UK, and of course the objective is to engineer super soldiers, in the UK you can't even request a freedom of information, you can never find out what they're doing. So you can bypass Mendelian inheritance, you can insert heritable characteristics, you can prime a whole population in the type of attack that could be done today includes releasing something that would prime a population to make them more vulnerable and then a second attack with a biological weapon. So like every type of technology, whether it's robotics or AI, biological research has dual use potential. So mostly it's used for good, but it can also be used for harm. And we could be living in a Jurassic Park of viruses and not even know it. The first synthetic virus was made in 2020, and it was in 2002, sorry, 25 years ago, about 25 years ago, by some American scientists who created a synthetic version of polio. And then some Australian scientists created a version of a virus related to smallpox that was 100% resistant to the vaccine and universally fatal. And all that research is open source, which means someone else can read it and copy those methods. In 2018, Canadian scientists created a pox virus very closely related to smallpox called horsepox. And if you can make one, you can make any of those orthopox viruses. And there's been a range of other quite scary research, including synthetic Ebola. And ever since the SARS outbreak in 2003, scientists have been fascinated with creating a SARS version 2, and there's numerous publications prior to the COVID pandemic detailing this research. So the guys who developed the... Can you put the slide forward for me? My pointer isn't working. The scientists who developed the synthetic horsepox virus, they... So the two ways in which unnatural pandemics can occur are error or terror, and the picture on the left is the Sverdlovsk anthrax outbreak. That was a town in which the Soviet Union had a bioweapons facility. Someone forgot to replace the filter on an air vent one night, and all night long they were pumping weaponised anthrax into the town next door, and at least 100 people died. But the Soviets said it was a natural outbreak they said it was due to the wet markets they made a big show of killing dogs in the markets to prove that it and they interestingly at the height of the Cold War the Americans agreed with them completely that it was a natural outbreak so from the medical and scientific community the consensus was it was a natural outbreak and it wasn't until the fall of the Soviet Union that Boris Yeltsin confessed that it had been a lab leak. And in the case of Terra, one of the other examples of an anthrax outbreak is the one after 9-11 where it was pretty obvious because the anthrax was mailed in envelopes with threatening letters. Today, anyone can create a biolab in their home, just as you can make a meth lab in your kitchen, so too can you do it yourself, biolab, and the possibilities are really endless. The scientists that made the synthetic pox virus said the advance of technology means that no disease-causing organism can forever be eradicated because you can always make it in a lab. And that's the sticking out of a warehouse in Reedley, California, and very, very slow and inadequate response from US authorities. It took them three months to get a warrant. By the time they searched it, they found about 1,000 humanised mice, vials labelled Ebola, HIV, SARS-CoV-2, etc. And it was a biotech company that was supposedly making COVID rapid antigen tests for which you do not need animals or live virus. And that's still being investigated as to how that happened but it just shows you how wide open the risk is. In terms of current epidemics the clade one mpox in Democratic Republic of Congo is a huge problem. 70% of the cases are in children, 80% of the deaths are in children. This is much more highly lethal than the MPOCs that caused the Clade 2 epidemic in high-income countries in 2022. There's a long sting in the tail of SARS-CoV-2. It's still causing substantial excess mortality. And our research showed that long COVID has major economic impacts. In 2022 alone, we estimated long COVID cost $9 billion of losses in the Australian economy from people being unable to work at full capacity or only at partial capacity. And then H5N1, bird flu in the US, 30% of commercial milk samples on the supermarket shelves are contaminated with the virus. If it's pasteurised milk, it's probably not live virus, but if you're into drinking raw milk, it probably is. And it's just a matter of this virus switching its preference from birds to humans, which have different receptors in the upper respiratory tract, and then we've got a pandemic. Now, how do we detect these pandemics when they're arising? The risk is there. How do we detect them? The traditional methods we use, like lab diagnosis, that depends on having a test. If it's a brand new disease, there is no test. These are the graphs showing, these are data from WHO showing on the top the date of symptom onset. So that's the day people got sick. And on the bottom, the date of reporting. So you can see there's a substantial delay. So by the time you get official notifications, it's usually spread, which is exactly what happened in COVID. So we've got evidence from blood tests showing that COVID had spread through Europe and the US in November, December 2019. So we developed an early warning AI system using open source intelligence called EpiWatch. Now a true epidemic disease grows exponentially and that's why you go from zero to a hundred in a short period of time and why health systems collapse and why critical infrastructure collapses because of this exponential growth. So time is critical and we're trying to find the signals very early before governments are aware of it. And literally the COVID response did occur in the red zone. So we use three different AI systems and we search in over 50 languages. In fact, 70% of all the intelligence we get is in non-English languages, which tells you how important that multilingual intelligence is. We visualize the data in a number of different ways and we do a pandemic watch looking for undiagnosed pneumonia we've published quite a bit of research on it including looking at epidemic signals in Ukraine that in 2022 all formal surveillance ceased so there were no more reporting of COVID cases of TB of HIV nothing so we were able to give a very comprehensive picture of what was going on epidemic-wise from open source data. We also looked at radiation signals in Ukraine using the same methodology. But ultimately, a tool like this, like EpiWatch, and this is where Balvi has helped us as well with the philosophy of Balvi as well, is that it's no good having a tool like EpiWatch being used by an elite few in some building in Geneva or wherever. It really has to be used at the grassroots by local health workers on their mobile phones so that they're aware of what's going on and they can connect the intelligence with what they're seeing on the ground. And so we launched an app in Hindi in 2023, working with the National Institute of Epidemiology, which is a government agency in India. And we're continuing that work now, looking at evaluating the use of this app, hopefully next year. So thank you very much.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:20:00.000Z",
      "slot_end": "2024-11-14T07:35:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/16BGl6R_AIgh1Fz7_yHfLOgt8VeqXZacUFlcwQz9qvOU",
      "resources_slides": "https://drive.google.com/file/d/1pE7XPZg4ez44F7Lv-fdNYZId8e9illYJ/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "activation-in-crypto-how-crypto-apps-go-mainstream",
      "sourceId": "AER7HZ",
      "title": "Activation in crypto -- how crypto apps go mainstream!",
      "description": "In this talk, I'll break down patterns I am seeing having helped onboard 10M+ users to crypto apps. From natives to newcomers, what are the factors that lead to onchain activity and sticky usage in consumer apps. I'll work through the few things that native apps do to onboard mainstream users onchain and how this impacts protocol development moving forward.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Product-market fit,UI/UX,Account Abstraction,Accessibility,application,Accessibility,Account Abstraction,Product-market fit,UI/UX",
      "keywords": "Applications",
      "duration": 1312,
      "language": "en",
      "sources_swarmHash": "eafe67bc05d6200b28b44f89b7ea7abeef97f93ac358c751f33a9847713e7fd2",
      "sources_youtubeId": "xXziS8KbI1k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e3479dbb7a90e1585aec",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735e3479dbb7a90e1585aec.vtt",
      "transcript_text": " Hello everyone. Thank you for coming to our talk on how crypto goes mainstream. It's a topic that's very near and dear to our heart. I work at Privy. And Privy is an easy and powerful way for crypto products to onboard their users. And Privy powers many of the most popular crypto apps, from trading apps like Hyperliquid and Pumped Up Fun, to restaurant loyalty projects like Blackbird, to social apps like Zora and Frentech. And so from supporting many of the most popular consumer teams, we've gotten a really good lens into what makes a great consumer crypto product. And it's really exciting at this DevCon to hear how many people are talking about consumers and apps. And one of the things I've seen a lot on Twitter is people saying, where are the apps? And how are we going to get more users on chain? And this is a problem we think about a ton. And at Privy, we actually, I'll tell you a little bit about what we're going to talk about first, and then we'll dive into it. So we're going to talk about the changes in our market, like why as an industry we've gone from, you know, not giving much airtime to consumer to it being something that everyone is referencing and talking about. We're going to talk about the crossroads we're in and why we're starting to see a really exciting acceleration in the crypto market. We're going to talk about what great looks like in consumer. So what some of the qualities of the most popular consumer apps will look like. We're going to talk about where we go from here, how we're going to get every consumer product on chain over time. At Privy, we talk a lot about, you know, what will it take for a consumer product to break out? And new tech often needs to power experiences that are orders of magnitude better than the status quo. And so to use an internet analogy, like if you were building a new internet, if you built something that's better than dial-up, you know, that's not going to cut it because now we have fiber. And so with consumer, you know, we need to build something that's, you know, a better experience, a more compelling experience than something like Uber or Facebook or TikTok in order to get, you know, everyone to start using it. And to us, there are three things that we get from building on crypto rails that, you know, are impossible if you're building off-chain. The first is, you know, this idea of ownership. The assets that you're accumulating in your app are yours. You can get kind of shut off from the app, but you still have the assets that you're accumulating in your app are yours. You can get kind of shut off from the app, but you still have the assets that you've earned and you can take them with you, which is related to the second point around interoperability. By building on a blockchain, you can take your assets experiences with you from app to app. So you can mint an NFT on Zora and then go sell it on OpenSea or any other NFT marketplace. And the third is around payment. So crypto offers a digital first instant payment layer, which is a really exciting new capability that's not possible off-chain. Historically getting that combination of attributes in a product required using something like this, MetaMask. It accomplished, you know, everything that crypto enables really well, but it requires you to be very familiar with crypto. And if, you know, you don't care about crypto or you're not willing to go through the hoops to set up, you know, your crypto life, this might be too daunting to you. And it's been a big reason why many crypto apps haven't been able to cater to people who don't know much about crypto. And a lot of the first popular crypto app experiences powered by these external wallets, they looked like this. They were great for discrete one-time user experiences, come in, do a financial transaction, get what you need, leave, and then, you know, come back when you need to do your next financial transaction. But they weren't well set up for multi-session user journeys. And the tech stack has come a very long way to enable these richer consumer experiences. So, you know, a lot of the time we talk about, like, what are the barriers to consumer products really breaking out on crypto rails. And there's sort of like a lot of the UX around making crypto apps easy to use. There's scalability, cheap, fast, reliable blockchains. And then there are kind of elements around the transaction experience and making that easier. And for the first time, we have a tech stack that kind of combining all of these attributes allows us to build great consumer experiences. And as a result, we've seen the market evolve quite a bit. So we've gone from, you know, here we're highlighting, you know, what we see as like kind of the most popular, most visible consumer products, consumer crypto products of the year, starting with, you know, financial one-time DeFi use cases to much kind of broader DeFi use cases to bringing art and culture on chain to gaming, to social interactions, and now having a full social layer. And, you know, now we're starting to see these more kind of rich consumer product experiences. But we're at a crossroads. So we have kind of everything we need to build a great consumer experience. But we're not at the point where every app we're using is on crypto rails. And so the question we ask a lot at Prithee is what will it take for that to happen? And we see two trends that are both very exciting, that are both accelerating a ton right now, that we think will unlock the next wave of crypto products for consumers. The first trend is crypto native apps. So products that are built on crypto rails, the crypto infrastructure is very much core to, you know, the product experience that are going after a mainstream audience that are trying to really hide abstract a lot of the crypto mechanics away from users. But crypto is like fundamental to the product from the outset. So one of the most visible examples of this right now is Polymarket, which is a product built on crypto rails, but has attracted orders of magnitude more users than the kind of core crypto native population. And, you know, OpenSea accomplished this. Blackbird, the restaurant loyalty app is accomplishing this. Pump.fun is accomplishing this. And the number of crypto products, you know, that are crypto native, crypto from the outset that are catering to a broader audience is accelerating a lot. And this is one trend that we think is going to continue accelerating. And we break down kind of like there are four common attributes a lot of these experiences have. The first is that they're engaging. Like they're experiences that people want to use and come back to and participate in. So whether that is something like Zora where you can consume content, you can like content as a creator, you get rewarded for posting content, to games like Friend Pet, to Receipts, which is a fun competitive fitness app on chain, we're seeing the baseline of just a fun experience that people want to engage with again and again as the most common attribute for crypto-native products that are breaking out into a broader audience. The second, and one that's very close to home for us at Prithee, is UX for everyone. Even though it is a crypto experience at its core, it is friendly and familiar to people who know absolutely nothing about crypto. Whether it's what we have on the left with Pirate Nation, which is one of the most popular crypto games, which allows users to sign in with Google login or Discord and get a fully self-custodial powerful wallet under the hood that could do anything you need it to on-chain. To Anime, which is a fun ecosystem of experiences centered around anime where you could sign up and start collecting on-chain assets, but you have no idea that anything you're interacting with is crypto, to B3, who implemented an experience we call guest mode, where you can get a wallet without actually creating an account, which is an experience that's familiar to people who shop online, for example, and aren't going to create an account in order to purchase something. You know, a lot of the UX elements of these apps are accessible for people who don't know anything about crypto. The next is real-world tie-in. So a lot of the most popular apps we see are manifesting in the real world in different ways. So whether that's Blackbird, who has many of the most popular restaurants in New York, there are physical pucks at the restaurant that you can tap your phone to, to earn rewards points that you could redeem to either pay your bill or get a free drink at the restaurant, to seeing on-chain assets in Walmart and Target, as the Pudgy Penguins team is accomplishing. Showing on-chain assets in the real world is another very common element for apps that are breaking out. And then the last piece is around account funding and getting people to transact on chain without taking them through the multi-step journey of getting crypto assets. So we have a video here of one of our customers, Rodeo, who has a really nice Apple Pay funding flow where you can, you know, purchase a balance that allows you to mint NFTs in the Rodeo app using Apple Pay. And, you know, once you have your credits as they're framing it here, you could just go and collect the asset and it feels familiar and leverages, you know, payment methods you're already familiar with. So those are some of the attributes of these crypto native experiences that are targeting mainstream users. And then there's this parallel trend of products that were built without crypto in mind that are leveraging crypto infra to accomplish things that wouldn't have been possible without it. And this is one of the most exciting things we're seeing at Privy right now. And it's accelerating a ton, where many, many fintechs, in particular, are starting to leverage crypto for payment rail benefits. And we'll get into some of the other attributes. But this is this parallel trend, where these products already have a ton of reach. They have tens, hundreds of millions of users. And they're giving them access to crypto rails. And the attributes that a lot of these projects have in common are, one, they're looking to use crypto rails to give users access to USD balance. So this is a product, Dolar. They're one of the most popular neobanks in Latin America. And they're using crypto infra to give people in Mexico and Argentina access to USD balance. But you would never know there's a trace of crypto infra in the app. It just feels like you have a dollar balance. You can earn your paycheck in USD, which is a great store of value, and then spend it with a card attached to the account anywhere. You know, merchants accept card. So that's a really cool trend we're seeing. Another is that mainstream apps are starting to use crypto as just a more efficient payment in rail. So this is an announcement from Deal, the payroll company, which allows any contractor on the platform to accept their payroll in USDC. And so not only are contractors getting faster access to the payment, but they're not dealing with a lot of the big FX hits you take on both sides of the transaction. If you were converting from payment in USD to a foreign currency, you know, take a pretty big fee on the FX conversion. So that's a really exciting trend as well. And then the last trend we're seeing with mainstream products and experiences building on crypto rails is some of the interoperability benefits you get from building on-chain. So earlier this year at Coachella, users who had specific assets that they bought on OpenSea could then go onto the Coachella website and link the same wallet they used to buy the assets on OpenSea and unlock VIP passes. And there was a special section of the Coachella venue where users could go and, you know, get access if they had this NFT in their wallet. And this is like a very early example of some of the cross-app experiences where users can take history or purchases they've made in one crypto app or one thing on chain and unlock things completely elsewhere. So the two trends that we're really excited about that we think will unlock the next wave of crypto usage are one, you know, these very kind of crypto deliberate products that are finding ways to attract mainstream user base with Polymarket as like a great example of this. We think that's going to continue accelerating. And then on the other hand, you have all of these products, fintech and largely fintech, but over time we expect many more that are looking to build on crypto rails to unlock some of the interoperability and payment benefits you get. And in our view, these are moment-in-time distinctions. Eventually, you know, we're just going to talk about crypto as an enabling technology that products build on. And so this, you know, crypto native versus mainstream product is a very moment in time distinction. We think, you know, these two trends will converge and everyone will start building on crypto rails in order to access the things you get that make crypto special. We think kind of as that plays out, it's important to keep your focus on the things we get from crypto and what we're all building toward and not concern ourselves with, okay, I believe crypto needs to evolve in whether it's this chain or this user base or that user base. And just really focus on what are the unique benefits we get from building on crypto rails. And the last thing we'll leave with is we really believe there's never been a better time to be building products on chain. You have all of the infrastructure coming together. You have all of the energy behind crypto, the visibility we're getting as a market. And we're really excited to see the next wave of products that come. So thank you all very much for the time. And if there are any questions, happy to take them. Thanks, Max. I hope everyone took some notes on how to build a popular product. So without further ado, let's go talk about some of the questions. So we can look right here. The first one is, what is an example of a popular feature that drove massive adoption for a project? Yeah. One of the most kind of novel things we saw in a product since starting Privy was how FriendTech really leaned into an existing social graph to make the first time user experience a much richer experience. You had a bootstrapped, so to rewind for a second, FriendTech, when you signed up, asked users to link their Twitter accounts. And then there was a populated social graph and, you know, users that you were familiar with. And that kind of immediately gave the app experience some more life. You had kind of friends and recognizable faces on the app. And I think that is one of the patterns that that team introduced that, you know, we still see everywhere today that made for a much richer experience. Agreed. So the next one would be why is most WebTree adoption focused on speculative platform like PolyMarkets or M or meme coin? Is the trend overshadowed the development of real world utility in the web3 space? Yeah, well because the crypto rails make for easy transaction experiences, you know, facilitating whether it's trades or bets, is like a very kind of easy to stand up use case. We have a framing at Privy where there's sort of, we call it the carrot and the stick. And up until very recently, because the UX around crypto experiences was a bit tougher. It was a bit kind of more hoops to jump through to get up and running in the app. Apps needed to incentivize, you know, why should I use this app with a little bit more of a carrot, whether that's like speculation or yields to get the user to drum through the hoops. And we think as the UX improves, you can have less speculative use cases because it's the same reason why I would get up and start using any product. I can just kind of use the crypto product and enjoy whatever the real world experience is behind it. Okay. So the next one is the trade-offs required to enable UX things. Like Google login wallet seems to stretch the definition of non-custodial. Is it really non-custodial if I need to log in through a specific provider? Yeah. So this one gets a little bit into into the mechanics of how Privy works. But when a user creates a wallet with Privy, we split the user's private key into different shares. And they're assembled in a way that neither Privy nor the customer we're working with has the ability to access the user's private key. The user is the only party that can actually assemble the key on their device and control the wallet. And so even though Google is the sign-in method, the auth link is only one out of the three shares that we split the key into. And so, you know, that doesn't kind of make the wallet experience custodial. Okay. So we have time for one last question. This is a pretty interesting one as well. What's the industry that you think will onboard the most users on chain in the next five years? Yeah. We're really excited about what we're seeing in the fintech and payments market. So seeing, you know, all of these neobanks really starting to lean into crypto payment rails to give users access to USD, faster, cheaper payments, you know, potentially better financial products over time. we think that's a really powerful driver of adoption. And if every banking product over the next five years somehow incorporates a crypto payment rail and gets their users' wallets, that's a really massive tailwind for usage in the market. Thanks, Max. Can we give him a round of applause for the insightful conversation they had? Stay tuned. We'll be right back at 3 p.m. with the next session. All right. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:30:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1a_SQ9DL-TDG33A44nSJ53matpwEz98lEvSg24o-XKA8",
      "resources_slides": "https://drive.google.com/file/d/1CjYAVbxmdEI9QKF9HTFP-ml-NOT-SEiQ/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "changes-to-the-l1-evm-versus-l2s",
      "sourceId": "MFYXWT",
      "title": "Changes to the L1 EVM versus L2s",
      "description": "The EVM has long been a target for improvement, but major changes have been postponed due to other priorities. As Ethereum's core, EVM modifications could significantly affect network stability, security, and performance, or add complexity. This necessitates lengthy approval and implementation processes. Panelists will explore new initiatives to implement EVM upgrades such as the EOF on L2s before L1, discussing their pros and cons.",
      "track": "Core Protocol",
      "type": "Panel",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Layer 2s,Governance,EVM,Core Protocol,Governance,Layer 2s",
      "keywords": "EVM",
      "duration": 3354,
      "language": "en",
      "sources_swarmHash": "f60378b606413e9f06fa47182dd9e1d1e995f1ede2afc2baba72331234f3f3c9",
      "sources_youtubeId": "tc2UIcqMU8E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc8ab982f234a12562fb1",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673cc8ab982f234a12562fb1.vtt",
      "transcript_text": " All right. Can you guys hear me well? Perfect. Thank you so much for the introduction. And thanks for joining us today. If you're watching the live stream, thanks for tuning in. So this will be essentially the last one of today's EVM-related sessions that were kind of structured like an EVM mini-summit here at DEF CON. Last year we had a full day of EVM summit during DEF CON Act, and a lot of aspects of today's topics were actually covered there, but we will see what's new and highlight some of the takeaways from last year's panels as well. So thank you for the panelists as well for joining us today. And I would like to first ask you to briefly introduce yourselves and your involvement with the topic and EVM. Let's start with Alex. Yeah, it works. Okay, yeah. My name is Alex. I'm part of the Ypsilon team, which you may have heard here a couple of times today. We do research focusing around the EVM and we also did research on WebAssembly. And obviously, because we focus on trying to improve the EVM, one of those big proposals is EOF. The other one is EVMX. I'm definitely very bullish on getting those out. And we have been working a lot on mainnet, so obviously I'm in favor of getting these improvements to L1s as well, not just L2s. All right. My name is Daniel. I've been in the Solidity team since 2018, have been the lead of Solidity for, I don't even know how long, a few years now, I think. And yeah, so we are the ones that actually need to generate code for the EVM. So yeah, we also would be highly appreciative to have EUF everywhere, and including L1s, because, yeah, we will get into that. But, yeah, we're in the Ethereum Foundation right now. We're spinning out of that in the Argo Collective. If anybody is interested in that, there was a recording of a talk the other day. But, yeah, that's it. Hey, guys, I'm Matt. I go by like clients on the internet usually i work on the go ethereum team and i've been working on some of these evm ideas for a while worked with alex quite a bit on phase two for eth2 back in the day thinking about what new execution environments in ethereum could look like and also have just spent a lot of time since this concept of the rollup-centric roadmap was released thinking about what the interplay between L1 and L2 rollups would look like. Hey, my name is Mark, and I am a contributor to the Optimism Collective. We maintain the OP stack, which powers a bunch of different, you know, altitude networks. Our goal is to commoditize stage two roll-ups so that anybody can, you know, use our free software to deploy a stage two roll-up really easily. I've written a bunch of Solidity and I've worked a lot on upgrading smart contracts that are running in production that hold lots of money, have hit lots of different edge cases with doing upgrades. And I worked on EIP-1153 a little bit, was able to upstream that into Geth, so have some experience working with the EVM, and yeah, I'm interested in just scaling Ethereum generally. Awesome. Thank you so much. So I will address most of the questions to one of you, but if anyone has any additional thoughts, feel free to just jump in after. So, right, so the EVM has been targeted for improvements pretty much since the early years of Ethereum, and there are several related elements in the roadmap as well. But there were always some other priorities in previous hard works, so there hasn't been too many significant changes happened around the EVM so far. But there are a lot of progress was made when it comes to R&D. If you listen to Dano's talk on Tuesday, or even today, he gave a really good summary on that. He ended a bunch of new directions, and it's definitely gaining more attention recently. So, Alex, you've been involved with EVM R&D for a very long time. What do you consider some major milestones when it comes to EVM improvement? Also, how do you see the future, the current roadmap elements? How do they build up on each other? Or how do they interact with other roadmap elements? And maybe also just touch a little bit on the ZK aspect, like how could these changes make the EVM a little bit more ZK friendly? How much time do we have for this question? Well, I'll try to give a very brief summary. Yeah, I think some of these questions were addressed in like previous talks, especially the ZK element. I mean, as you said, EVM has been, has seen like different stages of development and sometimes it had more focus, other times less focus. I think generally, maybe the first year after Ethereum launched, there were more quicker changes like reverts were added. There was like, you know, quite an important improvement and maybe shifts were added sometime later. But most of the development on EVM has been more reactive regarding state. So most changes were gas-related. How do we deal with the state? How do we improve costs around the state? Or fixing something like the Shanghai attacks. Or making changes around the beacon chain integration, like the Shanghai attacks or making changes around like the beacon chain integration like the merge and focus was less so on improving the EVM itself there have been things I'm initial L2's some were more explorative people were always really curious at improving the EVM but there was one direction where people spend most time on is a pre-compiles they wanted these big features which you couldn't achieve in the EVM and so they opted for like maybe an easier way. It turned out to be not an easier way. In some sense it is, other sense it isn't. Some of these pre-compiles have a lot of complexity. They seem simple. Turn out they have consensus bugs every year. Some of them have taken more than four years to get adoption. Like the BLS precompilates still being discussed. And so with all these in mind, we have been, in Epsilon, we took like a detour to see like wasm, would that be an option? We came back to EVM that EVM is actually not that bad, but it does need a lot of help to make it better. And, you know, right now we have a lot of help to make it better. And right now we have a lot of adoption around EVM. So that's the reason we really want to improve the EVM. And to touch on the roadmap, as I mentioned, the EUF and EVM-X, I think is a good direction to go. EUF gives you a baseline to build up on, but it also introduces you know a lot of like improvements if you look at some benchmarks there are some benchmarks regarding code size and gas usage there are improvements to be seen there and there have been some other benchmarks regarding zk just to touch on it quickly one issue in in zk VMs is they may need to translate the code. And if you translate the code, you need to keep access to the code. You cannot only use the new translated version. With EOF, you remove code inspection, code introspection, so this problem goes away. Another big issue is the gas cost. The gas costs are really tuned to like regular computers and some of these things the EVM has they would translate really differently in terms of computational cost on ZK. Removing gas introspection in the EOF also removes this problem. So we've seen some good benchmarks that a ZK application of EOF reduces proof sizes, reduces proof generation time as well. So all of this seems to be positive, but yeah, of course, still a lot to be done going forward. I'm not sure if anybody has, I said a lot. I'm not sure if you have any response to these. I mean, we definitely introduced bugs in our bridge because of gas introspection. So I'm a big fan of a lot of the simplifications that are coming with eof l2s also desperately need some sort of multi-dimensional 1559 so simplifying call i really like that um yeah i think the one major problem that we have is how do we upgrade our existing deployments to EOF. Thanks so much. Dania, what changes do you want to see happen on layer one EVM before potentially ossifying it? How could this potentially interact with the vision you have for Solidity? I know there will be a talk on Solidity later, so hopefully no spoilers here. And also, maybe how could these changes impact others in the ecosystem positively? OK, I mean, I would say don't ossify the layer 1 in general. But yeah, if people want to ossify the layer 1 ABM, then yeah, I could enumerate a number of things that are wrong with it that make life harder for any compiler and developer and any tooling person. EUF solves a lot of the issues. block that would in bulk reduce the complexity of compilers, reduce the complexity of tooling and formal verification on top of it. I mean, you also touched on quite a few points already. If L1 is to be ossified, please at least give us EOF in it first. Beyond EOF, there are still a number of things that would be nice. I mean, we could have better memory pricing, memory pages even, maybe give us some registers or something like that. But yeah, at some point, okay, I would understand if you don't, but EOF at least, please. But yeah, I mean, effects on other people. I mean, I'm pretty sure that a few people have picked up on, for example, the Solar initiative of providing a new compiler to Solidity in Rust. They want to target EUF because they know that it actually is much simpler to build a compiler in a language in EUF. So, I mean, that goes for every language. That goes for every language, every tooling will be much easier to implement for EUF because it's much easier to generate code for it. But you mean they would like to only target EUF if they can? I mean, we only briefly talked with Georges yesterday. I'm not entirely sure what their plans are. And they only so far have a power source. I'm not sure. But yeah, he sounded like very bullish on having EUF and targeting EUF. but I can't speak for them further than that. And mainline solidity? When do you drop legacy support? I mean, we would love to as soon as possible. It's a mess to generate code for the EVM, so I mean we do have for quite a while, I mean, I've spent quite some effort on the Shanghai version of EUF. We wanted it then. I mean, we have a delayed roadmap already because it didn't get in then. We now are in the process of finalizing production support for the new EOF, but there has been prototypes for ages that were usable. Thanks to Radek also. Shout out to him. But, yeah, I mean, it would actually help us extremely in our roadmap for the backend side of the language to have UF. And as soon as UF was there, we would expect users to move to it immediately because of the advantages in gas cost and everything, and would like to drop legacy support as soon as possible, or at least. So you would ask L2s to adopt it quickly, too? As soon as possible. I mean, yeah. We, of course, would need to have a transition period. I know that it would probably take a while for that time. We would have to, but as soon as ever possible. All right, cool. How fast is soon? I mean, luckily, you know, due to the design decisions that we made with the OP stack architecture, it's just easy to rebase onto, you know, the latest geth release or the latest reth release and get all of the, you know, functionality into the L2 itself. Now, when it comes to the smart contracts for the OP stack on L1, that's going to be difficult to migrate to EOF. I would definitely love to, but, you know, there's backwards compatibility concerns because applications may have hard-coded the address of the bridge today, and we would need to deploy a new bridge and, you know, figure out a migration pattern. And realistically, there's just other things that we consider higher priority than, you know, migrating to EOF for our L1 contracts. This is the specification you mentioned regarding the bridges, which I think we just need to understand a bit more. Maybe we don't have an answer for. But I'm curious about a different question. The many users of the OP stack now, multiple chains, they diverge from the OP stack? Do they have some of their own changes or it's like vanilla OP stack always? Oh, that's a great question. So there's, it's free software. People are allowed to do whatever they want with it. But there's this idea of just using the vanilla OP stack. And with that, you kind of inherit all the tooling that everyone in the community is building. So my personal philosophy is that you don't really need to make changes to the EVM. There's so many low-hanging fruits with user experience and go-to-market. And that's where the next leg of growth will come from. And it's not the, you know, adding a new precompiler or something like that. So if that bridging problem would be solved then potentially all these OP stack users would be sorted. Yeah, I mean the bridging problem is like the legacy bridge that we have today that's written in pre-EOF is good enough. It's really like making the front end more usable and getting people to adopt smart contract wallets that's inhibiting more adoption. I mean, I was interested in the argument of importing tooling from the vanilla version of the OP stack. I mean, I would say that's actually a nice argument also for EUF because EUF has nicer transpilation properties. from the vanilla version of the EOP stack. I mean, I would say that's actually a nice argument also for EOF because EOF has nicer transpilation properties. I mean, it's much easier without the gas introspection and whatever mess the EVM has to actually transpile EOF to a more modern system. So I think it would actually be smoother to have EOF and then actually build a more fancy version and import the entire tooling stack built for a plain EUF version by a transpilation process without having to carry legacy cost. I think that's also for actually a future proof system actually a very nice property of effort you have to be a basis for something like that. Right so yeah there's been a lot of talk on EUF today as well and some other days. This panel is not particularly focusing on that, but it's the next planned EVM upgrade, and it's basically going to be the biggest change in EVM history. So definitely want to cover it a little bit. So since it's a core protocol upgrade, obviously it has to be well-tested, and I think concerns around security or even introducing some complexity on the client level maintaining both legacy EVM and EUF EVM, I think most people agree on these concerns and they want to make sure that this all happens the right way. But I think what's interesting is that even though it's in the roadmap for a while, there seems to be a bit of a discrepancy when it comes to its impact. And I see two main perspectives here. Basically, that one saying is that EUF won't result in too much performance improvement and the future hard forks and a bridge should focus on other roadmap elements that would result in more settlement layer performance improvements. And the other one is that basically EUF is not really about performance, but it's just an absolutely crucial step to make the EVM to like a more mature virtual machine, even maybe help introduce endgame features and make it more SDK friendly. So Matt, maybe let's hear a different perspective here. You used to be, you actually published about EUF after the merge and listing its benefits and mentioning that it hasn't happened because of these other priorities like the merge, that they were more urgent. Has your perspective changed on prioritizing the EOF implementation? Yeah. I was really and am really excited about EOF overall. I think what it provides is the things that EVM developers have been hoping for for five, six, seven, eight years. But the thing that really changed my mind about where I placed it on the priority list personally is the thought that, is it something that Ethereum needs to have succeed in the next 10 years or 20 years? And I have felt since that revelation, which was a couple years ago now at this point, that there are things that we're still trying to do today with respect to the execution layer and the consensus layer, to me, seem much more closely addressing these existential risks of Ethereum not succeeding in the future. And I'm not saying that it's time to ossify the L1 EVM, but I do think that when we start talking about implementing changes to the EVM that are very complicated, I have to ask, if we do nothing, if we don't do these things, what is the worst case scenario? And as we're moving towards this roll-up centric roadmap, to me, it feels like that is the natural place for the evolution of the EVM. And if we're going to really lean into the vision that L1 is a place to settle roll-ups, I struggle to see the motivation for making all of those changes happen on the L1. Is there anything that maybe could change your perspective? More community support, impact analysis, getting more people involved from the ecosystem. I think the thing, this is not a satisfying answer, but I think the thing that would change my mind is if we decide that the roll-up centric roadmap is wrong. Because as long as we are saying that L1 is a sediment layer, in my mind I feel that the clients should be the most robust pieces of that ecosystem. That is the thing that cannot fail. And we can probably most likely do EOF in a safe way. We are good at testing the EVM. We are good at introducing changes to the EVM. It will probably be fine. But I just have to ask the question, we're adding hundreds of lines of code, if not several thousand lines of code to L1 clients. That just statistically adds more surface area for attacks to happen, for issues to occur, not just today, but in the future implementing new clients. And I understand it's something that make compilers so much better. Then me, it's almost like saying you're taking compiler code compiler complexity and putting into l1 clients and when i have this thought i think i want l1 clients to be as thin as possible as simple as possible focused on providing one thing and right now that one thing is a settlement layer for roll-ups can i say something spicy? Yes, we like to add precompiles, which have tens or hundreds, hundred thousand lines of code and optimized assembly and all that. And different clients use different versions of these. I don't like precompiles that much. I would love to have EVM Max. I think that to me, I would rather have EVM Max in the next hard fork than EOF I think. Yeah maybe you can go deep on kind of why it depends on EOF. Technically it doesn't depend on it. You can do it without, but with EOF you get a significant performance improvements because you can remove a lot of the checks, runtime checks, and move them to deployment time checks. And in the end, this means whether it becomes competitive against pre-compile or not. And without EOF, it may not be competitive, and then it's kind of pointless. Do you have a comparison between EOF versus non-EOF EVMX? Or a factor, like an order of magnitude? Is it within the same order of magnitude? For some of them, so the checks you have to do, I mean, there's also code size increase if you do it with FDUF. That may or may not be significant. But you do get, let's say, 30% code size improvement increase on every EVM max instruction if you do it with EF. You cannot, like, do any kind of optimizations around pre-allocation or anything like that. But the key difference is the runtime checks you have to do at each instruction, which you wouldn't have to do with EF. I think there were some measurements for some of the instructions. Maybe it's negligible, but it's also implementation specific. For some instructions, it isn't negligible. But generally, no, I don't have a proper number to tell. Right. I mean, yeah, I think that even with the EOF version of EVMX, there's been thoughts that it's not, it's never going to be as performant as a pre-compile itself. And so how much closer? Depends, actually. Radek's talk mentioned one case, libff, one of the BN254 implementations used by certain clients, client or clients. It's actually less performant than EVMX. Now, I don't think Geth uses it. So there are much more performant implementations than LibFF. But even now, today, with the pre-compiles, different clients may have different performance. Yet we have a single gas cost for it, right? And, you know, in some cases, EVMX would be faster. But yeah, if you look at like highly optimized code, just in the context of the pre-compile, highly optimized pre-compile would be cheaper. But there's another factor that in many cases, you have to use the pre-compiles multiple times to achieve something. You have to call like addition multiple times. You have to call multiplication different times. And a pre-compile use a different form. You have to translate back and forth. With EVMX, you can skip all of this. You can create highly specific implementations for the use case. And so you may have better performance because you skip all of those overheads as well. So would you say the biggest argument, like let's say the difference between EOF EVMX and non-EOF EVMX is fairly substantial. You would say that the most important reason to have EOF on mainnet is allowing people to implement whatever cryptographic primitives they want without being blocked on waiting for precompiles for those. Like what types of things are you hoping for? With EOF? I mean, EOF is great, but I'm thinking, like, why does it need to be on L1? And, like, one thing I'm hearing is that there are precompiles, and I agree with you. We don't want to put that many more precompiles on L1. But then the question is, like, how many more precompiles do we need? recompile that all of the zero knowledge rollups will be able to use? Probably not. So then to me, always one more. So then to me, the argument that makes more sense is that EOF gives you this ability to write any kind of cryptographic primitive that your rollup is going to need to verify it's zero knowledge proofs. Yeah. So I mean, with these precompiles, I think it's also like another aspect. There were, if you look at when they were proposed, there were like a hotspot. I think 2020 was a hotspot where people were really, 2019, 2020, when they were like really feeling, okay, I can just propose this. It may happen. There was a lot of activity, you know, a lot of new curves and use cases and people kept proposing. And then as time went on and nothing was accepted, they slowed down. Why would I propose it? Nobody's going to do anything about it. So there may be like this other aspect that if you open up the space, the ability to prototype new curves and new use cases, there may be, you know, a lot more stuff spawning up there as well. And like another thing which never has been, I think there may have been like discussions of pre precompile, but stark verification. That hasn't been covered, but this would be able to also help in that. But generally, EVM-MAC spun out, EUF spun out of EVM-MAC. So the core idea still wasn't trying to remove the precompiles. And these are primitives you need for it. And we had EVM-MAC as some kind of idea specification. And it needed something like EOF to be really performant. So this was actually the progression how it came about. But we ended up, those optimizations EOF provides, which are beneficial to EVMX, they're also beneficial to contracts. And gives like an upgrade path. One to mention is address space extension, which has been an interesting idea in order to do state expiry. EOF would also be providing a path forward like address space extension. Even the current version addresses it mostly. And that could open the path to state expiry. Now, of course, we always have this question of legacy, and it's like a big kind of worms. Daniel did you want to add something? I mean yeah I mean I also find it always a bit surprising to hear this kind of like if L1 only is the settlement layer then why does it need any more changes I mean the settlement layer is very relevant the performance on this is relevant and the correctness of it is relevant and I mean if people knew what compilers have to do in comparison to non-UF code, they would probably scream in fear and run away from anything like that. So, I mean, these things, I mean, of course, for settlement layers, you have a few contracts that can be formally verified with large efforts. But, I mean, still, I mean, all of that, this becomes easier. The settlement layer becomes faster. It becomes more robust and verifiable all the ways up the stack. And I mean, this complexity that, I mean, it's fair to argue that it's good to have not that much complexity in the clients, but I mean, avoiding this complexity in the clients produces an enormous amount of complexity up the stack the entire way. So I would at least be careful in weighing that. Now you might ask, why doesn't an L2 just adopt EOF? So one thing is, you know, the L2 space, it's still really early. A lot of the projects are still kind of fighting for survival. And it's harder to make long-term decisions when you're worried about just being around for the next few years. So I think that we need to see more L2 ecosystems reach escape velocity before they'll be able to, you know, think longer term and do things like EOF. There's also the problems of, you know, there's like barely any stage one roll-ups in production today, right? Like most roll-ups, they're stage zero still. It's really, really hard to even get to stage one. It was way harder than I thought it would be. And, you know, we're not done. We're still working to get towards stage two, and we need to make sure that all roll-ups can eventually get to stage two. And until that point, it's really difficult to think about, you know, pulling EOF into, you know, an L2 client. Yeah, and that perspective makes me wonder if we are getting ahead of ourselves and we're not letting the ecosystem develop and we're trying to force something that is going to happen naturally on the L2s and sort of top down dictate what it should look like by pushing onto the L1. I mean, anything that comes to L1, we automatically inherit as L2s. We learned that. Yeah. I mean, you know, thank you for 7702. Super hyped about that one. Yeah. I mean, another concern is, you know, it's way less likely that all of the developer tooling will get built if one L2 ecosystem adopts it. So when L1 adopts a change, there's basically a guarantee that all of the tooling will accommodate that change. So that's another risky reason as to why an L2 wouldn't want to adopt something before L1 does. I still feel like this is a very near-term focused thing. I think that if you take a very near-term focused thing. Like, I think that if you take a step back and you think in five or ten years, I don't really see a reason that everybody will still be locked in on EVM equivalents if we have reached stages two and we're comfortable. Like, roll-ups are going to want to differentiate, and they're going to have more resources, and they're going to find applications that reach 100 million million users or a billion users and once you get to that point then the developers will come and you're going to be able to create totally different ecosystems i mean there's still the argument that the transpilation properties of euf compared to legacy avm make all of that easier i mean even if that eventually is the goal and will eventually happen, it's much better to have a basis of EUF for that. Sorry, what type of transpilation would you want to do in that? I mean, if you want to bootstrap a new set of ecosystem for a new EVM, then EUF can be transpiled to that new version and inherit all the tooling without building it from scratch. And then you can import all the tooling and extend from there. With legacy EVM, that's much harder. I mean, are we just not overly focused on reusing things and not starting from scratch? I feel like we went, I feel like you went down this path with eWASM. You know, we were all having these ideas, like, let's just reuse the tooling around WASM. And then we got... Turned out to be, yeah, I mean, turned out that the time spent on that, like how many years, three, four years, during that time, EVM tooling caught up. We had a, what was it, hot... Anyway, we had all these frameworks, we had debuggers, those were lacking. Taking maybe a step back to where you started, the answer to, I think, yeah, we started this question a while ago, and you had a long answer, you know, what changed your mind? And I think that was a reasonable answer, and I do agree with a lot of it. I think it just misses one point. Several reasons are there why this ecosystem works and why the roadmap works as of today. I think one important aspect which you don't talk that much about is really the developer experience and that each of them have the same EVM. You write a project and application once, you can deploy it on any of them. You can optimize for whatever you're optimizing for, whether you're optimizing for the given user based on a chain or you're optimizing for the cost, for the speed, or you optimize for the longevity and you go for mainnet, you know, it's much more expensive. But you can deploy the same thing, not only the same Spark contract, but everything around it, you know, the RPC and everything is the same. You can write once, deploy anywhere. I think that's very powerful. And if we start diverging in each of these, because we hope that one of them is going to do bigger EVM improvements, that can only realistically happen if one of them becomes dominating. Or they have some other incentives to get all of these developers and tools and everything around it. Do we want one of them to dominate? Maybe we do. But I think this is going to really take some time before we get there. While at the same time, there are some maybe other L1s or other chains or other directions which focus more on developer experience. And there's certainly people, whatever you give to developers, they're going to work at the round. There's nothing stopping them. So the EVM is not stopping developers, they find the workarounds. But, you know, if somebody comes around and they have a much better developer experience, you know, that can kickstart some changes. And I do think that we really keep forgetting about developer experience and we are not unlocking potential enough. If we would give, you know, slight improvements, I think we would unlock a lot more and we may be able to progress more rapidly. All right. So let's maybe go back a little bit to that suggestion to move developer experience improvements to Layer 2s. And Mark, you kind of answered part of that question that a lot of layer twos and CKVMs are Currently basically focusing on performance improvements as well optimization. So developer experience is In the plans, but it's not necessarily in the near future I think in the last couple of days there were a lot of talks actually that surprisingly they had a lot of plans to improve developer experience as well. What do you think the timeline is here and also could things like standardization or these kind of initiatives maybe help with this to speed this up? Totally. Yeah, I mean, I think that right now it's the developer tooling, the developer experience, and like the end user experience that is inhibiting the growth of the ecosystem as a whole. I think that at least, you know, we're really starting to get to a point where the actual L2 software is becoming stable enough and reliable enough. And there's still a long way to go. Like I said previously, you know, there's no stage two roll-ups, like, that are actually used a lot in production today. And getting there is the number one priority. You know, we do want to improve developer experience along the way. And I think with regards to, you know, the ZK EVMs and the kind of, like, the fault-proof EVMs, those are definitely getting hardened. I know that there's a lot of great ZK EVMs that have come out that you can just take, say, Rust code and compile it and stick it in the ZK EVM and not need to implement all of the EVM changes by hand at a really low-level abstraction and interact more closely implement all of the EVM changes by hand at like a really low level abstraction and interact more closely to all the circuits and everything like that. So as these ZKVMs become, you know, more and more optimized, I've been being told that, you know, there's orders of magnitudes of optimizations coming over the next few years where I think it'll be a lot easier to just take any arbitrary L2 software and be able to create ZK proofs for it. I think the idea of ZK roll-up and optimistic roll-up is kind of fake. There shouldn't be a distinction. It's just a roll-up. This idea of like optimistic or ZK, that's a property of the bridge and that's not a property of the roll-up, right? The roll-up is not the bridge. They're two different things. I think this is like a really big misconception. And I think that, you know, we're going to get to a point where all the ZK VMs are good enough that the stacks that are based on optimistic roll-ups will be able to just adopt it. Okay, cool. So I would like to talk about this layer-2 focused EVM standardization initiative, which is actually an actual practical step towards layer-2 standardization. This was launched last year, introducing monthly roll calls and RIPs, or roll-up improvement proposals. And they've been happening every month, and basically these are optional standards for layer 2s to adopt if they want to. Has your team been involved in any of those? Totally. Yeah, we have reviewed a bunch of them. And I've personally attended a couple of the calls. And we also adopted the, I think it was maybe RIP 7272, the P256 pre-compile. I think the RIP process is really useful for kind of de-risking more fragmentation between all the different, you know, roll-up frameworks. I do think, though, it is still pretty early given that, you know, all of the most, there's no stage two roll-ups. So it's hard to focus on, you know, this standardization when everyone is still, you know, trying to actually build real roll-up software. Okay, right. So on a recent roll call, there was also something you introduced, which is called, it's like a layer 2 EVM common core. Basically, I think the point is for layer 2s to be equivalent with each other, but not necessarily with layer 1 anymore. And also handling together future layer 1 EVM changes and make sure there are no conflicts with those changes. Daniel, what's your take on these, just generally on these coordinated layer two optimization initiatives? How could this unfold? Yeah, I mean, that's great, of course. I mean, the only way that we could ever accommodate anything on layer two that's not layer one is if it's a coordinated effort. But I mean, the problem is that we're busy with working around the issues of the Layer 1 EVM. We also still don't have the time to actually really look into doing Layer 2 work, specifically because the Layer 1 work is an extremely huge mess. I mean, of course, I mean, Layer 2 standardization, definitely a good thing, and actually definitely necessary for having anything, any tooling support for Layer 2s in common. But yeah, I still would say that there is more space to actually accommodate Layer 2 changes with a simpler Layer 1 at this point in time. Right. So we are running out of time a little bit, but if you would like to learn more about this initiative and SCAR, actually the quality of this initiative, we'll have a talk on this later on today at 5 40 I believe on stage one but make sure you check the schedule right so maybe just one more thing quickly Matt have you heard of the roll up get initiative maybe just share a few thoughts on that before we close yeah they've definitely heard of the roll of geth initiative so it's a very super interesting project i think that what they're trying to do is what needs to be happening right now because we're not going to be able to evolve l2s if there isn't some kind of coordinated effort and it feels like the best coordinated effort is by making their lives easier. And for better or worse, most L2s are based off of L1 clients. So providing them an L1 client that is maintained by a team focused on maintaining the coordinated, the accepted proposals and coordinating amongst the roll-ups to figure out what proposals to accept and for that team to implement, those things make a lot of sense. PROPOSALS AND COORDINATING AMONGST THE ROLL-UPS TO FIGURE OUT WHAT PROPOSALS TO ACCEPT AND FOR THAT TEAM TO IMPLEMENT, THOSE THINGS MAKE A LOT OF SENSE. I AM JUST VERY CURIOUS TO SEE HOW IT ENDS UP EVOLVING OVER THE NEXT COUPLE OF YEARS. LIKE MARK SAID, YOU GUYS ARE SUPER BUSY GETTING TO STAGE TWO. but I think that the reality is going to be a lot more complicated than that as these things always are Totally having you know, this kind of neutral roll-up Geth is great, but it does add, you know some governance risk to the supply chain. So I Imagine that it will need to be relatively conservative with what actually ends up in it. And yeah, like I think that in an ideal world, nobody needs to use a fork of Geth. And you can just import Geth as a library and kind of, you know, decorate it with extra functionality. This is like one thing that is kind of interesting about like the Reth project, where Reth is kind of looking at L2s as customers or users and tries to like build abstractions that make it easy for L2s to like build into Reth. But with Geth, it's important to be credibly neutral. So there's trade-offs here. All right, cool. So we have three more minutes, actually. Anything you guys would like to highlight or any takeaways from this panel? All of you, maybe. Let's start with Alex. I mean, I have questions. I'm not sure if we have time for those because we will have a bunch of um yeah maybe we have the q a as well yeah maybe some of this is covered but the evm common core um i mean even now the different l2s like they have uh uh differences between them mostly on the zk level because they cannot support like each of those, or, you know, there's gas differences. Are you, I'm not sure, maybe Mark, you're the closest. Yeah, well, how do you see like EVM Common Core? What is the first goal of it? And maybe what is the medium term goal of it? Is it like making sure that even these differences don't exist? Or it's like introducing new stuff, or it's just too early to tell? Totally. To be honest, I don't know a ton of information about EVM Common Core, but at least what I would like to see out of it is introducing new things as there's desire to, you know, improve the EVM, then it's about doing it in a consistent way, right? Like no L2 team. It's like we used to maintain a fork of the Solidity compiler. Very, very difficult. So like having one set of tooling that works across the whole ecosystem is really, really nice. Anyone else? Any conclusions or things to add quickly? Because we are running out of time. No? I like the first question. Okay, questions are coming soon. Alex, anything else? I could go on forever, but why don't we just look at the questions? Okay, so maybe I, okay, do you think that any of these layer 2 standardization initiatives could help with shipping some of these changes faster on layer 1? Like maybe some layer 2s adopt those and it works, and then maybe the guest team says, like, okay, fine, it works, it didn't break anything. Could this, do you think it could improve this? I mean, my fear with this idea is that L1 teams love to touch the standards. They love to leave their little marks on them and modify them just a bit to fit L1 perfectly. And I struggle to see how we're going to do anything complex on the layer 2 first and then bring it exactly as it is on the L2 on the L1. Even just with the P256 precompile, the L1 devs say, maybe it's at the wrong address. Maybe we should move it to a different place. BLS, we're trying to figure out, like, what are, where, what is the exact parameters that are going to go into these what are the addresses of these precompiles? Doing these things on the L2 is just going to constrain what the L1 devs are going to be able to do. So you're saying the level of nitpicking is a bit more on L1? It's extraordinarily high. Yeah, it needs to be high quality. But on the other side, do you think there's a different need of like, EVM changes specifically, you think like L1 has different need of what EVM should look like versus L2s? I think L1 has different needs than L2s. And I also think that incremental changes are the best way to change the protocol. But on EVM, do you think it has different needs? On L1, I think there are different needs than L2. Okay, cool. Thank you guys. Unfortunately, we don't have any more time, but there will be time to answer questions from the audience. So thank you so much for joining us for today's panel. Don't you have time for the questions? Yes, they are coming up from, yeah, Can will... Hello, and thank you very much for our wonderful speakers. My name is Can, I will be your MC for the next three hours for this stage. Let's move on to questions before taking more time. I will start with the topmost one. This question goes to Matt. Why do you consider compiler bugs less scary than execution client bugs? I have written both, and testing compilers is a lot harder than testing EVM implementations. They really, really wanted to ask this question. So the way that I see it, one, I'm a client developer. I try to keep the L1 safe. That's my primary goal. I don't want to belittle the job of a compiler developer. It's thankless and it's very difficult, but similarly to keeping their code base safe and secure, I am trying to focus on keeping my code base safe and secure, I am trying to focus on keeping my code base safe and secure. I don't want to try and have blinders on and push all of the complexity on the L1 into other places. But my perspective is, what does the L1 provide? What do blockchains provide that no other facility in the world provide? And that's a platform for unstoppable, trustless execution of code. And to me, what you need to have happen is you need to have a blockchain that works, doesn't have faults. And if you need to write applications on top of that, yes, it's great to have a great developer experience. It's great to have a great compiler, a safe compiler. But we do know ways of writing code that's safe, formally verified, it's just extremely painful. So I know that in the absolute worst case, if we provide the L1, people will develop applications on it. Maybe those applications won't be as nice as people want them to be, or they won't be developed as fast as they want to be. But I believe that if we give them the platform to develop the applications, they will build the applications that actually use the platform in the way that the platform is trying to provide. And why should they use the platform of any other competitor that does have a better layer one instead? Sorry, I see the first part. No, I mean, if you ask if, I mean, who's to stop starting a new layer one chain that actually has the better experience on that level as well, and then people moving to that because it has the same guarantees, just only in a nicer way. I mean, we'll have to see how these types of things play out. Like you can already see there is a bit of a, there is a comparison of Ethereum and Solana. And Ethereum is, as Josh said in his opening speech, the thing that does not go down. Many of the ETH competitors have the meme of like going down Solana goes down Iota goes down these other chains are not as resilient as aetherium and how do we maintain this property it's by being extremely thorough with the types of things that we put on mainnet that's not to say that we should get stuck in this mindset and totally ossified but I think that it's like something that we have to balance very carefully and what ends up happening is you have people who are farther on my side who tend to think that we should be more ossified. We have people who are trying to accelerate what's happening, and we end up somewhere in the middle. It's just not a fun process. Everybody on this panel is extremely reasonable and pleasant to be around. But we just end up up here arguing about what the Ethereum VM should look like. There. So the next question is around incompatibility around the VMs. Our attendee asks that they don't understand why a bunch of incompatible modifications to the same VM across different L2 chains is considered beneficial. Isn't it good to have everyone on the same high-quality VM? Anyone wants to take this? I think we all want the same high quality VM. Anyone wants to take this? I think we all want the same VM. I mean, we all want the same VM, but at the end of the day, it's a free market for, you know, scaling Ethereum. That's kind of like what the L2-centric roadmap is all about. So, you know, any project has the right to modify the EVM in any way. But, you know, the reality is that there's a huge cost to modifying the EVM just because the tooling is so important. I think this goes back to what you said earlier, that if something goes to L1, it's inherently coming to L2. We haven't seen yet it happening that way around. Maybe with the one precompile now, maybe that could be the first occurrence. But this basically, you know, this question is if you do get it on L1, the same high quality stuff, we ensure that everybody gets it. If some L2 adopts something, maybe another one adopts something else, it's unlikely to ever reach L1, so there's going to be a lot more diverse version of VMs. We have new questions coming in and the leaderboard changing. The next question is, what will be Epsilon's focus after EOF? I guess I can take this. It's EVM Max. Thank you. The next one is, in what ways could alternative languages, i.e. Move or others, bring new functionalities to Ethereum dApps that the current EVM lacks? I mean, I guess I can try to take that. But I mean, all the languages have the problems of the EVM as long as the EVM is the target. So I mean, there's only so much that the language level can do. I mean, you can try to have complex compilers to try to work around the issues of the EVM. There can be languages that are designed in a way in which that's easier. But yeah, I don't think that the language level is the level to solve the issues that the EVM currently has. Adding an alt VM. At the end of the day, if we think about alt VMs, one of the big bottlenecks in the EVM is I.O., like reading and writing. And a new VM is not going to solve that. It's still going to be just as expensive to read and write from disk. So like adding an alt VM maybe where it's easier to, you know, build like a MIT native code that, you know, works off of, you know, 64 bits instead of 256-bit math. That's a way to optimize the execution, but it's only really useful for things that don't read and write from disk. So the next one is around the EVM again. Is EVM going to stay overall the same throughout the next years, or is there any possibility for radical changes to update modern architecture targeting smart contract needs? I guess there is one change. Just listen to the panel. Yes, I guess there is one change upcoming. I hope, at least. Yeah. Right, then the next one. Discussion here suggests the roll-up-centric roadmap reduces the complexity in L1, yet with the diversity it means compared to native homogeneous ETH2 shards isn't the opposite true. Whose complexity on L1 is not mine mine at least. Moving on with the next question. Our attendee finds it funny that somehow L2 is going to innovate, but we just heard they need to be conservative about guest changes. Why do we think that L2s are going to innovate more than L2? You know, I think it really depends on the particular L2 ecosystem. You know, some L2s, like, you know, Fuel, StarkNet, like, they're very, very different. So anybody has the right to build really any sort of L2 that you want. There's just some L2s that, you know, want to be more similar to L1 so that they can adopt all the tooling that exists and, you know, maybe contribute improvements to the same tooling so that L1 also gets the improvements. So yeah, I think that the reality is that the bottleneck for adoption today is based on user experience and go-to-market and not improvements to the EVM. I also think in general, like with L2s, you have the opportunity to try many different flavors or have different focuses, whereas the L1 is this singular thing. You can't really be super fast, super decentralized. You have to sort of choose on the different axes that you want to focus on, whereas for L2s you could have something that's privacy focused only. You can have something that's extremely fast but might go down, something that has central points of failure go down something that is you know has central points of failure and I think that the L2s like have the opportunity to experiment on those different axes that the L1 just doesn't have. I think I can L1 specifically it's a bottleneck issue that you know there's only as many people there which are and they don't really have time for some of these proposals even including UF you know a couple of years ago nobody had the time to like dive deep but everybody had you know their views and opinions and it was really just information access issue nobody had a time to deep dive into it to have like a proper discussion and and you know get any of the opinions closer I do think that the EUF shouldn't have been this big of a debate and this big of a change and could have been done much earlier if there would have been more bandwidth. That's ultimately what we are fighting around. So we're running out of time. That will be the all questions we'll be answering, but feel free to catch our speakers after the talk. Let's give a big round of applause for all of them. Our next session will be starting in a few minutes. Feel free to stretch, and we'll be back soon.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:30:00.000Z",
      "slot_end": "2024-11-14T08:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Vhj4BZKZxNH74CAaa0TGW6GQk3bGkBt7tTxzfTfY5O0",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "eth-is-permissionless-money",
      "sourceId": "TMFPCF",
      "title": "ETH is permissionless money",
      "description": "ETH is money! In this talk, we will explore the role of Ethereum's native asset on the base chain, in the L2 ecosystems, and in crypto broadly. We discuss the ETH supply, what it means to be permissionless money, how ETH is being used today, and how it's role can evolve.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Censorship Resistance,Decentralization,Ethereum Roadmap",
      "keywords": "",
      "duration": 1525,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "9pQKaeYwWjs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "",
      "transcript_text": "",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:30:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1BKehfujLaDakbU2-PjgsWO9PzcaHlv5FlzNG5PlH6zY",
      "resources_slides": "https://drive.google.com/file/d/1BSVjqKLt4f5XPk1vMV3yyWS-cZGay4Q5/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "how-to-raise-the-gas-limit-use-ultra-high-resolution-data",
      "sourceId": "UASADN",
      "title": "How to Raise the Gas Limit: Use Ultra High Resolution Data",
      "description": "Recent advances in EVM data processing enable a more rigorous approach for understanding and enacting Ethereum’s scaling roadmap. In the past, discussions around whether to raise Ethereum’s gas limit have been held back by imprecise terminology and a lack of detailed quantitative evidence. The debate is often “vibes-based”. Leveraging ultra high resolution datasets enables a more scientific understanding of the gas limit, including issues like state growth, hardware bottlenecks, and gas pricing.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Gas,Scalability,bandwidth,Gas,Layer 1,Scalability",
      "keywords": "Gas limit,State growth,History growth,Bandwidth",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "f6ba379fd50f4248137d7ab71d565254d9a3693763205e40b77494654d3d3d96",
      "sources_youtubeId": "v8Po5KOutow",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:30:00.000Z",
      "slot_end": "2024-11-14T07:40:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1EM_PJu06t3IYa4m6iVVoVQ2AnVXrc2iJ4B-8uWHtzAE",
      "resources_slides": "https://drive.google.com/file/d/1L1fZ1zoN0AySCbNzxuigHMSkXcN2f25e/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "introducing-provable-object-data",
      "sourceId": "YP9HRR",
      "title": "Introducing Provable Object Data",
      "description": "Built on learnings from experimental projects like Zupass, Provable Object Data (POD) is a new format with open-source libraries for any app to issue verifiable data, and make ZK proofs of claims about that data. PODs allow arbitrary key/value data to be signed and distributed.  Flexible proofs about PODs can be created using a highly-configurable family of General Purpose Circuits (GPCs), without app-specific circuits or trusted setup.  This talk will focus on POD and GPC motivation and design.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Libraries,Zero-Knowledge,Use cases of cryptography,pod,Libraries,Use cases of cryptography,Zero-Knowledge",
      "keywords": "Zupass,developers,POD",
      "duration": 1688,
      "language": "en",
      "sources_swarmHash": "a71f14cf60f06f7b9023ff145306f8897dfe26741a61e581bafddd95bebe1aee",
      "sources_youtubeId": "nXmd0NHC59Q",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735dbd09dbb7a90e1646d41",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735dbd09dbb7a90e1646d41.vtt",
      "transcript_text": " Okay, and we're live. Welcome, everybody. My name's Andrew. I'm with Xerox PARC. And I'm here to talk to you a little bit about pods. How many here have been using pods this week? I think the answer should be everyone because you got into the building somehow and that actually required one. So what are pods? So your devs con ticket is a pod. The proof of attendance to this talk that you can claim from the Q&A app that's up on the screen there right now. If you're in the room, I don't think this works remotely. That's also a pod. Some of you have been playing Frog Crypto this week. I see some frog hats out there. All those frogs are pods. A pod really can be anything. It can be a secret message. It can be your identity credentials. It could be your driver's license if we get any governments involved to actually do this it can it's cryptographic data of any sort So what is the pod framework? So pod is is a technology a framework that makes it easy for apps to issue cryptographic data and to make zk proofs about that Data, it's a data format that's optimized for fish improving It's a standard of how that data format can be sent around and things can be proven about it And it's a framework of how that data format can be sent around and things can be proven about it. And it's a framework with some developer SDKs. Check out our documentation site. I'll have a link at the end if you want to try it out. It's mostly in TypeScript, but can be used on other platforms as well. We have ports in a few other languages. So I'm hoping some of you will get some use out of it. So one last WTF is zero-knowledge proofs. How many people here have used ZK proofs before? I feel like you understand them. Okay, a few. It's kind of obscure technology. That's kind of the point of pods is to make it easier to use so you don't have to understand how the underlying math works. But in brief, a ZK proof lets you prove the validity of any private data or any computation on your private data without revealing that private data itself. And that proof is trustworthy because there's some math that basically you can only calculate if you did it validly. At Xerox PARC, we think of ZKProofs as a universal cryptographic adapter. Basically, I've got lots of different kinds of private data. By doing computations on that data in a verifiable way, I can present to somebody whatever I want that is validly proven from that data. The example in this diagram, which you'll find in our blog post, is like, what if I could calculate my own credit score from signed data I got from my bank or from the IRS? I don't need to ask a credit reporting company to gather all this stuff together. I can gather it myself, and I can make a provable statement about what my credit course score is and apply for a loan. This is part of the vision of something we call the Parknet, programmable cryptography internet, which we think is going to be much better once programmable cryptography catches on in all of these ways. ZK proofs are a big part of this, but it's only the beginning. See the other talks being given by my colleagues this week. Also, we have a whole day CLS tomorrow about programmable cryptography. But today we're going to be focused on ZK proofs and what pods let you do with them. So this is the pod ecosystem that we envision. You need issuers who are issuing this cryptographic data. They're mostly using a private key to sign it. Those takes the form of attestations, which users themselves can hold on to. They hold on to their own private data. They don't need an intermediary. At some point, some consumer asks the user, please prove something about yourself, your identity, your age, the fact that you went to DevCon, things like that. And then the consumer can generate a ZK proof and send, sorry, the user can send the ZK proof to the consumer who can verify that proof. They do need that third arrow in the diagram, which is a little bit of knowledge about who the attester is. You need at the very least to know that that attester has a public key that you should trust. There might also be things like what is the event ID that represents DevCon on a ticket, things like that. But that kind of completes the diagram. Okay. So why are we doing this? So I work with the team that builds ZooPass. You've all been using that to check into DevCon. And we believe that the best learning on this kind of technology comes from contact with reality, meaning we want real users to try this. We want to do it at scale. There are 12,000 people at DevCon this week who are stress-testing ZooPass for us. Thank you. I'm sorry if the performance has not always been as great, but it seems to be standing up. And we want to use these opportunities to onboard new users by bridging data that is not ZK-friendly into our ZK-friendly world and take advantage of people who are willing to be early adopters like the crypto community. So by bridging, what I mean is we're bringing data in the red boxes on this diagram Into the green world red in my diagrams of this talk and the next one means like non zk friendly systems Whereas green means ek friendly systems we can bridge it in we can then issue it like your devcon ticket Which is loaded from a database that isn't cryptographically signed And then you can the verifiers can get you into another system like telegram in order to join the the DEVCON chat group. All that is working today. In order to bring this in front of the most users, we do have to accept some constraints. So we're not using the most cutting edge of ZK technologies. We want everyone to be able to use it, which means we've built a mobile friendly web app. Which means everything we do has to run in a browser on a phone. Even an older phone, even on a bad network when the Wi-Fi is overloaded at the conference. So that became a bit of a mantra when I was building some of these technologies. There's a lot of cool ZK technology out there that is great, but it needs to run on a big back-end server and I don't have one of those when I'm in a browser on a phone. So we've got to use tried and true technologies. For people who are in the know, we use SIRCOM and GROSS16. You may or may not have heard of those, but that's kind of the underlying technology. They've been around for quite a few years, so they're pretty well battle tested at this point. So I want to talk a little bit about the systems we built along the way. So this is what ZooPass ticketing looked like a year ago at DevConnect when we were in Istanbul. So it's the same triangle that you've seen here. We were pulling data out of Precix and issuing the tickets. We used a format called an EDDSA ticket. That's a signed piece of data, but it's not a pod, which I'll explain a little bit later. And then we had a proving circuit where you could prove your ticket, you could reveal some fields, you can prove that you owned it, etc. So what did it take us to build this? Don't pay attention to all the details here, but look at the line counts on these PRs when we wrote these things. It's pretty large. That's quite a few lines of code that it took. And in just the ZK proof, there's about 15,000 lines of code that are still there, not including tests and documentation. So it's kind of complicated. So that was the first thing we built. The second thing we built was Frog Crypto, the first version last year, which used a very similar data format. So frogs were issued by the server as what was called an EDDSA frog, very similar format to tickets, and then you could make a proof about it, you could present it to our Zucat telegram bot who would let you into the secret frog holders chat. This all happened last year in Istanbul. So what did it take to build that? It turns out it was very similar. There was a lot of duplication of effort. There was a lot of similar patterns, but you couldn't actually reuse the underlying data. So there clearly is a pattern here, right? We want to issue some signed data. We want someone to request a proof and then to be given a proof of that signed data, we want someone to request a proof and then to be given a proof of that signed data, but it turned out that each time we had to build it, we had to rewrite a whole bunch of code in order to customize it. So I'm an engineer, I don't like this kind of complexity, I'd rather do things once because I'm lazy. So why is this so hard? So the signed data part, the EDSA PCD that we were using as our underlying data format. Used as a fixed size hash, it hashes 16 numbers in an array. And therefore, every new data type that we wanted to put in there, we had to do some custom coding to decide how those numbers in that array go together to make this data type. I would analogize this to imagine you were processing all your data in a hex editor directly as bytes. It's kind of inconvenient. We have better tools than that now. And on the proof side, ZK circuits are a little bit awkward to program. Like they don't use a normal programming model. You don't write it in a language you're used to. Every variable is what's called a field element. This is a mathematical concept. It's a very big number, modulo some big prime number, and you've got to like write equations on those field elements. So it's kind of complicated. And also, once you build a ZK circuit, it's very fixed. In order for the prover and verifier to agree on what's valid, the circuit can't change very much. You have to publish a whole new circuit. So that makes this a bit hard. I would analogize this, again, to in the hardware world, this is like an ASIC. It's a chip that does one thing. It might do it very well, but it still only does one thing, and every time you want to do another thing, you've got to build a whole new chip. It's kind of inconvenient. So what do we need here? Well, what we'd really like to have is what's called a ZKVM. Basically, if you have an ASIC and you want something more general, out there that lets you basically write code, run it inside of a ZK circuit and validate that this is the correct output. It's great. Some other people are giving talks about it this week. But unfortunately for our situation, it's a little bit too much. Like I said, my mantra has to work in a browser on a phone. ZK VMs are pretty big right now. You're not going to be able to do that on an older phone in a few seconds. So we have to do something a little bit more limited than that. But again, I'm an engineer, I like working within constraints and coming up with clever solutions. So here's what we came up with. So on the data side, I'm finally gonna explain to you what a pod is at some level. So a pod is just a collection of names and values. Think of it like a JSON object, except that it's flat. There's no hierarchy of nested objects, just names and values. It can have multiple data types in it for those values. The data is then cryptographically signed in a way that makes it easy to make proofs about it. And I'm going to get into more of that a little bit later. Also, I forgot to mention this at the beginning. We are having a deep dive session after this intro session. So stick around for that if you want lots more detail. But I'll give you what I can in the next 15 minutes. On the proof side, we also can generalize. So we have what we call a general purpose circuit, which means rather than having a fully CPU-like circuit in a ZKVM or having the ASIC fixed circuit, we can do something in between. I would analogize it more to an FPGA. We've got some logic blocks. We call them modules. You can feed in some inputs to your circuit in order to decide how those logic blocks are connected to each other and make a different proof every time using the same circuit. We call this framework GPC for general purpose circuit. And in addition to the circuits individually being configurable, we precompile a set of circuits in what we call a family at different sizes with different sets of modules. So when you want to make a proof, you can pick the circuit in the family that has enough modules for what you want and not any more because having a bigger circuit means more time to prove, more memory, etc. So you can make the right trade-offs there. So with that, we get the generalized version of the ZK ecosystem where every issuer is issuing pods. They might contain very different kinds of data. It might be a frog, it might be a driver's license, but it's still a pod. And then when you make proofs about it, you can freely decide what you want to prove and write a configuration to represent that proof. So with that in mind, at this point, what is a pod? So a pod is a data format that makes zkproofs easy. It's a key value store. It's going to be hashed and signed in a very specific way involving a Merkle tree, which I can explain more of later. And it's optimized for efficiency zkproving. Here's an example of a pod. So we've got some names and values. Most of these are very straightforward, so I'm not going to go through them all in detail. The one that's maybe a little bit interesting is the cardholder. So this is meant to look like a driver's license in some fictional country. The cardholder is my semaphore ID. This is what Zupass uses to identify you. It's really a public-private key pair. So the public key is what's going to go in the pod to say that this is my pod, or in this case, this is my driver's license. What you see on the right is the JSON format for this. It's optimized to be a little bit terse and also human readable. So things that don't need a type annotation, you'll notice don't have them because the JSON type itself is enough data for that. Once you get down to actually building the Merkle tree, like everything does have a type, but in this table I call them type hints because the type is not part of the cryptographic data. Instead, it is guidance to how do I hash this data into a piece of cryptographically verifiable data. More on that later. So the first thing I do to make this into a pod is I build a merkle tree i'm not going to go into detail on that but basically you arrange the elements into a tree you hash them all together until you get to a root and that root is what we call a content id the content id is derived from the data so if you have the same data you can derive the same content id regardless of how it was formatted in json one detail that you might notice on the right is that the names have been alphabetized. That's how we make sure that it is deterministic and you always get the same content ID. But everything else is just hashing. And then now once I've got the content ID, that's the thing that I sign. So if I'm an issuer and I want to issue a pod, first I get the data, I Merkle-ize it, I get a content ID, and then I just write a signature on that content ID, and that's enough to validate that the entire pod is valid. So we have a ZK-friendly data format. We'd probably like to do some ZK proving on it. So let's talk about the GPC side of this that is what lets you do that. As I mentioned earlier, GPCs are circuits made of reusable modules, as well as a family of multiple circuits so you can pick the size that you want. Let's look at what that looks like. So this is an example of a GPC configuration. This is how you say, what do I want to prove? And you're gonna present this as this JSON object that says what you wanna prove, and the system is going to do the rest compiling this down to what to do with the circuit so here's a very minimal proof i'm going to try and prove that i have a driver's license that says i'm allowed to drive right so i my configuration says i have a pod i'm going to call it id card this is actually an arbitrary name that's just part of the configuration to refer to it later it has some entries and one of those entries is driver. That is not an arbitrary name. That's a name that was in the pod and is going to be hashed and checked. And what do I want to do with it? Well, I want to reveal it. So is reveal is true means this is a proof. It's going to prove that I have a pod, that it contains this entry, and it's going to reveal that its value is hopefully true because I detail that wasn't on the previous slide. That's because it's done by default, so I didn't need to include it in the config, but it's important to talk about. What I proved if I don't have, think about the signup key, is I just proved that I have a pod containing the word driver with the value true. That doesn't mean it's actually a driver's license. In order to do that, you've got to do something cryptographic. So the easiest way to do that is you check that the pod was signed by a public key that is well known. That might be the government of California, which is where I live. Hopefully we'll get them to issue pods eventually. But that is implicit. The signing key is also always revealed by default, but you can choose to not reveal it if you want to, in which case you can constrain it in other ways you might constrain it to be equal to some other element without actually revealing it or constrain it to be a member of a list like maybe i have a list of all the signing keys of the 50 u.s states and i just want to prove i have a driver's license from one of them i don't want to tell you which one okay let's get straight and get a little bit more complicated um so i've proven that i have a driver's license that says driver equals true. I haven't actually proven that it's my driver's license yet. I could have stolen somebody else's. The thing is that pods, because they're just data, they are transferable. I can copy them. The way we make a pod bound to a single user is by putting that user's public key in it, which I showed earlier when we were looking at the entries. And the way you prove that you are that user is you make a proof that you hold the private key that corresponds to that public key. And the way you say that in the gpcconfig is this is owner ID field. You say is owner ID, and I give the type of public key I'm using, which is semaphore version 4 from our friends at PSE. And that basically means that this proof is going to be configured to check that I have the right private key in my private inputs. And in this case, it's not even going to reveal what my public key is, just that I own this pod and this pod says I can drive. Okay, let's get to a little bit more ZK and hiding some more data. Instead of proving that I'm a driver, what if I just want to prove I'm over 21? Maybe I want to go buy some alcohol. I don't know what the age is in Thailand, but back home it's 21. So I can just say I have a pod containing an entry called date of birth. That entry is not going to be revealed, but it's going to be in this range, and that's the numeric range for the date that is 21 years ago. We should make this more friendly and let you just pass in a date object, but for now it's a number. So this is a proof that I am over 21 and that I own this pod. I didn't take out that field, but everything else is not revealed and I'm being very anonymous. One last example, we can make proofs of multiple pods at once if we have a circuit with enough modules. So here's one that I'm proving I'm over 21 and also proving that I have a ticket to an event that maybe I'm going to go to an after party after DevCon. And in this case, the ticket, I'm proving that its attendee name is the same as the name in my driver's license. I'm proving that I own it and I'm also proving that the event ID of that ticket is in a valid list. I'm not revealing what I have a ticket to, but it's maybe a list of like DevCon related events that are happening in Thailand this week. So this is kind of a minimal anonymous way of checking into a party. Of course, if I'm there in person, I'm revealing some more about myself by being there, but you get the idea. Okay. So last piece of this, I've now configured my proof. I've decided what I want to prove How do I actually make a proof and all of this is an example of what you can do with the the GPC libraries So the three things I need in order to make a proof one of them is the proof config that I've already given you some examples of The second thing is the inputs. That's the actual pods Which I need to have in order to make proofs about them There are also other inputs like my private key or like that list of valid event IDs that I want to prove that my event ID is one of. Those are all inputs. The third thing I have to feed in is something called an artifact path. That is, where do we find the binaries that know how to generate this circuit? So when a ZK circuit is compiled, it generates a proving key, a verification key, and also a witness generator. Don't worry about what those are, but there's some like big binary things that the prover and verifier have to agree with. We distribute these via NPM. We also put them on various CDNs. You can download them. So you have to just decide for your app. Are you going to download them, put them on disk, give a path to them? Are you going to download them from a URL, there are options. Once you've got these things together, the gpc proof function will generate the proof. It puts together that configuration, it picks a circuit that fits that configuration with enough modules, it downloads the corresponding artifacts for that circuit, and it generates the proof. And then the last thing it does, oh, I should have gone to the next slide, here we go. So it needs to compile down all those inputs into circuit signals that can feed into the actual ZK circuit, which are mathematical field elements, as I mentioned. And then after it's done and it gets a valid proof, it will decompile some of the outputs and turn them into what's called the revealed claims object. So it comes out of a proof. You've got the actual claims object. So it comes out of a proof. You've got the actual mathematical proof. That's just opaque numbers that are needed by the verifier. That's the actual ZK part. You've got a bound config, which is exactly like the configuration that you fed in, except that now it contains the identifier of the circuit that was selected so that the verifier knows how to verify it correctly. And then you've got the revealed claims. If I revealed that I am a licensed driver, driver equals true, that would be in this object. If I revealed my name, et cetera, that would be here. And that's what the decompiling is for. It's taking the circuit outputs and turning them back into a string or whatever the representative thing is. Okay, so those three things are exactly what I should send to a verifier, whoever I'm gonna prove this to. They need those three things. They also need an artifact path to download the corresponding verification key. And then they can verify the proof. They just do very much the same thing. They're going to compile some of those inputs back down into ZK land where there are circuit signals. They're going to verify the proof and they're going to say yes or no, whether it's valid. And, you know, gravy, we're at the end and hopefully everything went right and I've proven what I wanted to prove to you. So final takeaways, summary of what this was a bit of a speed run through. So pods are data that's designed to be proven about. Any pod is a signed attestation of something, whether it's I have a ticket, whether it's I have a driver's license, etc. GPCs allow you to flexibly make proofs about those pods by using modular circuits, which can be configured using a JSON-like configuration language. And the system will auto-select the circuit that you need depending on your configuration. So all your app needs to do is say, please make me a proof of this with these inputs and everything else is handled for you. Then the last step is the verifier verifies the proof, and then the apps do have to decide what things they trust. How do you trust that this is the correct proof? Like I alluded to before, you should check that this ID card was actually signed by the government. You should know the public key or you should know the event ID for DevCon. You should also check, and I'll say a little more about this in the deep dive, that the configuration that was sent to you was actually the configuration you asked for. So you don't want the prover to say, oh, I have a proof of something, but not necessarily the thing you asked for. That's something that you should check as well. But once you do all of that, this end-to-end should be very solid and you should be getting the information you need. Okay. That's it for the speedrun intro. Please check out our documentation. They're on pod.org that just went live yesterday. And also there's a link that just went by, t.me slash zoo pass to join the telegram group. And yeah, let's go do some Q&A. All right. Where do you store the sound for identity secret for users in Zupass? So that's all client side. Zupass stores all of your private data client side. The Zupass server is aware of your public key because that's how it can make sure that you get issued the right Devcon tickets and things like that But yeah, zoo pass is a client-side cryptographic data manager To what extent is pod an open data standard, so I consider it open we haven't like published a spec for it I should work on that but all of our code is open source, so people can do interoperability with it. The pod format itself is very generic and interoperable. It's the GPC compiler that turns a pod into the specifics of what you need to prove with a specific GPC. So the GPCs are kind of less standard and generic, though they also could be used on multiple platforms. We do have an example of GPC verification on chain that just started working a couple days ago, so all that is possible outside of a browser, but we don't have as many examples there as we do on the pod data. Can we scroll down? Is there anything more? Can you compare pod to verifiable credential? Yes. This is something I looked into. Pod is simpler. It doesn't really have a fixed schema or anything that ties it into a specific standard. You could put JSON-LD-based verifiable credential data in a pod if you wanted to. But a pod is much more flexible. At the cryptographic level, there is a difference in the kind of Merkle tree we use. The pod uses the lean IMT, which is something that Semaphore created, which is much shallower because pods tend to be relatively small, as opposed to the sparse Merkle tree that is used, at least for the implementation of verifiable credentials that I'm aware of, which is the one from IDEN3. That is a much deeper Merkle tree, but it can do things like prove absence of an entry, which pods can't do. Okay. What else do we have? How frequent is pod refresh? Very frequent so far, but we're hoping to keep it much more stable after DevCon. I don't have a strong answer to that. What else? How do you convert JSON to a Merkle tree? Please stick around for the deep dive session that's coming up. I'll tell you all about that. What else? Yeah. So, the—in the example of prover and verifier, the user's device can generate the proof and that's why everything has to work in a browser on a phone. Client side proving is definitely the default in ZooPass. Not every app has to do it. These are libraries. You can call them wherever you want. There's much more difference between verifiers, whether they're doing server side verification or client side verification. That depends what your use case is and what you're protecting against Are the issued credentials signed and the proof that the crunch loops we scrolled away We do not use BSS signatures to verify partial properties, that's what we use the Merkel tree for again more details on that coming up Is it possible to make information in ZooPass portable? I think that pods do make that possible, yes, as long as it's a pod and there are APIs for getting data out of ZooPass if you want to. That's called the ZAPI, at which point you can take this to whatever platform you want. We have implementations of pods in Python, C, and Rust for various projects, so it's not too hard to do. How do apps know whether a proof from a verifier is legit? Well, the framework tells you that it is a valid proof. And it will confirm for you that this configuration and these revealed claims and this proof match up and are valid. So the prover couldn't have cheated about that. What they could cheat about is app level semantics. So if you ask for a proof of a driver's license and I sent you a proof of a frog instead, that's something that the framework can't tell you because it just says that's a valid proof. So you do have to check, is that the configure I asked for? Is the signer of this driver's license the government, etc.? But yeah, that's the kind of level of verification we got. Okay. I think that's it. Can we go back to the slides briefly? Okay. Those of you who are collecting frogs, I've got something for you if we can switch back to my slides. Oh, yeah. We'll leave that up for a minute or two. I think we've got like three minutes before the next session starts anyway. So feel free to frog away. Okay. And as I said, we're going to go straight into a deep dive session, which is going to be 90 minutes. We probably won't use the whole thing, but that's what we're scheduled for. So stick around if you want more details to answer any of those questions.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:30:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1M8ozawZM8Xme8xRHKoop-7XlGGAjINE02ztaxWPyaXo",
      "resources_slides": "https://drive.google.com/file/d/1JillBs444Tzyk6sqezOY1tr4QT8pfkDv/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "multiparty-homomorphic-encryption-from-ring-learning-with-errors",
      "sourceId": "KS7H3H",
      "title": "Multiparty Homomorphic Encryption from Ring-Learning-with-Errors",
      "description": "This talk will introduce Ring Learning with Errors (RLWE) based Multiparty Homomorphic Encryption (MHE).",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Open Source Software,Homomorphic Encryption,Use cases of cryptography,Security,Use Cases,Homomorphic Encryption,Open Source Software,Security,Use Cases,Use cases of cryptography",
      "keywords": "",
      "duration": 1051,
      "language": "en",
      "sources_swarmHash": "7864000381cfb0935a18a7ed2bd38df940f72ab2e3a86ec06695f699d94797a8",
      "sources_youtubeId": "pEtfafnxCIw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735af949dbb7a90e1cc49c0",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:30:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1qdDRslHeX1rQN30xep6TupLd5KYw4-agBG6u4Zh17dA",
      "resources_slides": "https://drive.google.com/file/d/1tQc9iqUvzA5GzA0TE7ZXIqQTZ8i7ltB_/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "going-from-alzheimers-to-pandemics-bringing-floss-to-bio-testing",
      "sourceId": "PZACPB",
      "title": "Going from Alzheimer's to Pandemics: Bringing FLOSS to Bio Testing",
      "description": "Varro has developed a unique semiconductor-based biosensor platform that detects pathogens in human breath and indoor air with significant improvements in speed, sensitivity, and cost over existing technologies. The platform will be offered via open source to expand its reach and accessibility. We will discuss the core technology and how it can be used to prevent the spread of disease and new pandemics around the world.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 906,
      "language": "en",
      "sources_swarmHash": "d6b12801d76f8d3ecc49462a59a788d3cbbee618466e555a21e5473f02f9f91c",
      "sources_youtubeId": "fHaE0iv9Szs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ad659dbb7a90e1abcc2f",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ad659dbb7a90e1abcc2f.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\" Peter van de Ven I'm sure after those last two talks, we're all thinking about locking ourselves in a closet to keep ourselves safe. Hopefully, by the time I'm done, you'll agree that there's some hope and that won't be necessary. So, my name is Tom Cerrito. I'm a CEO and co-founder of Varo. Varo is a biotechnology company focused on preventing the transmission of infectious diseases. We founded Varo in early 2020 with the recognition of two facts. First, the rate and spread of infectious diseases, of epidemics and pandemics, has been steadily increasing for over 20 years. Despite that, during the same time frame, there haven't been any significant innovations in preventing the spread of diseases. So why am I at DEVCON? Varo shares Vitalik's vision for DIAC and for decentralized pandemic preparedness. We believe in empowering individuals around the world by giving them access to technologies that allow them to protect themselves and their communities. VARO has built a platform to detect the presence of pathogens in aerosols in real time with exquisite sensitivity, using easy-to- use and affordable devices. Actually let me stay here for a second. So aerosols are the tiny droplets, they're about a micrometer in size that we release when we breathe. These tiny droplets hang in the air, they can stay in the air for days. So when an infected person is in an indoor environment, like a room like this, for example, they release these aerosols, and they contain the bacteria or the viruses that they're sick with. And those just hang out in the air and wait for someone else to breathe them in. This is how the majority of infectious diseases, certainly respiratory diseases, are transmitted. Until now, there really hasn't been a way to detect the presence of pathogens and aerosols, and that's what the VARO platform is designed to do. So the basis of our biosensor platform is the micro-immuno-electrode. We call it the MIE. The MIE is essentially a modified semiconductor. What we do is we take an electrode and we attach an antibody-like molecule, which is what we call a nanobody, and that's what binds to the virus or the pathogen that we're looking for to that electrode. This is a technology that was originally developed by neuroscientists at Washington University in St. Louis named John Cerrito and Carla Yudy. And the name is not a coincidence. John Cerrito is my brother. He's a professor of neurology at Washington University. And he originally developed the MIE technology to study brain chemistry in Alzheimer's disease. He would implant the immunoelectrodes into the brains of mice in order to study the proteins that cause Alzheimer's. In 2020, John realized that the same electrode technology that he puts down into the brain of a mouse, he can stick up into the air to detect COVID. He pulled in another collaborator from Washington University, an engineer and aerosol scientist named Rajan Chakrabarty, and these are our scientific founders. And we've worked very closely with this team over the last four years to develop this platform to where it is today. So I'm going to show a brief video of our scientific founders talking about how we're leveraging the MIE platform to develop two devices that are designed to prevent the spread of infectious diseases. Those two devices are the breath-based diagnostic and the air biodetector. So can we play the video? This is going to be a game changer. You are not just looking at the current strains, what is prevalent right now. We are also getting ready for what could be in the future. Whether it's something we know, or it's an emerging pathogen, we can go to it very quickly within a span of weeks, pull out a new nanobody, put it on an electrode, and have it ready to go. Gives us another tool to work with in order to really get on top or ahead of something like COVID, so we won't be kind of slow like we were in the past. We're using this in two devices, then the breathalyzer and the air quality monitor. If you wake up in the morning, you have respiratory illness. You go to the pharmacy to get a test. The technician behind the counter doesn't have to be trained. They'll take this, they'll pull off the cap that activates the device, breathe into it. A single breath is enough to get a reading here. You then hand this back to the technician who turns the knob. They then take this, plug it into a permanent device, and then within 60 seconds from when you blew into it, you get a result of whether you're positive or negative. Their quality monitor will determine when pathogens are present. It will also tell you when the pathogens are gone so you can know you can re-enter a room so you keep business open, you keep people safe and healthy, and you can still go about your daily lives. If you can identify that initial route of where the infected virus is being spread from, you can control the further spread of that virus. By deploying this device, we also want to ensure that the inequity which is prevalent in the world is in many ways addressed. Great. So I want to take a minute and talk about these two devices that we're developing. First is our breath-based diagnostic, what Carla called the breathalyzer. The breath-based diagnostic is capable of detecting multiple pathogens from a single breath in 60 seconds with sensitivity that is comparable to PCR it is a true point-of-care device sorry so you can imagine going to a pharmacy you get diagnosis, you can get your medication before you leave, you can get a diagnosis with whatever the known pathogen is, you can get your medication, you can get a faster diagnosis, you can do all of this at a lower cost than current methods like swabs or PCR. We recently completed a clinical trial with this diagnostic device. And I don't need to go through all the details of this. I don't have time to do that today, although I'll be here afterwards if anybody wants to talk about this further. But the clinical trial was a great success. It showed that we have exquisite sensitivity and specificity, and patients reported that they preferred this method of diagnosis, which is, of course, non-invasive, compared to PCR or the swabs that we've all become familiar with. Our second device is our air biodetector. So there's never been a device like this before. This is a device that will sit in a room like this, passively sampling the air in that room. So if somebody were to enter that room who were sick and infected, producing pathogenic aerosols. The air biodetector could sense those pathogens before you could infect most, if not all, of the people in that room. So this truly gives us the ability to interfere in the transmission of disease and prevent it in those environments where people are most susceptible, which is in indoor environments. During COVID, we know that the overwhelming majority of cases of COVID were transmitted in indoor environments through aerosols. So what does this mean for DIAC and what does VARO mean for DIAC? So both of our devices rely on the MIE technology, but the MIE is a true platform technology. And we believe that there are many applications for this, some of which we probably haven't even thought of yet. We want to empower people around the world to innovate with this technology and develop new things, new ways to do it. People that have different backgrounds, different ideas, different goals than what we have at Varo. By empowering people, by giving them access to this technology, we're giving them the ability to protect themselves, protect their families, and protect their communities. So how are we doing this? Varo has adopted an open source business model. We're making all of our plans, designs, and data publicly available. We've signed a pledge that we will not enforce our patents against third-party innovators. In fact, we're going to encourage people to adopt this technology. We're hiring an open source community manager to put all this information and make it available, package it up so that it's easy for people to access, and even promote people to come and enter into the open source community and learn about our technology and hopefully motivate them to take up this technology and use it themselves. We've embraced the principles of free and open source hardware and this has become sort of the core of what we're doing in Varo. This is the core of our business model. See, infectious diseases don't care about patents, right? They don't care about borders. They don't care about intellectual property rights. And we all know this, right? We all lived through the same hellish pandemic for the last four years. What we want to do at Varo is open up all of this technology. You know, we recognize that as a small company, it's going to take a lot of time for us to be able to access markets like Southeast Asia or Africa. Viruses, bacteria don't have those restrictions. So by empowering governments and companies and individuals around the world to practice our technology and to innovate around that technology, we're giving them the ability to protect themselves, protect their people, and protect their economies. It also allows us to ensure that important technologies like this can get into markets in low- and middle-income countries that may not normally have access to these types of technologies and hopefully address the gruesome inequity that we all saw during the pandemic, which Vitalik actually spoke about during his opening remarks. So typically in the pharmaceutical industry, patents are your competitive advantage. And I've been in this industry for my entire career. I've developed drugs and gotten them approved by the FDA. I've started about a dozen biotech companies. Patents have always been sort of the bedrock of the biotech community. But patents are a negative right. Patents basically give you the right to tell someone else they can't do something. What we're trying to do at Varo is turn that on its head. Instead of standing behind our patents and preventing innovation, we're going to leverage our patents and promote innovation. We're going to get all of this out into the world. Our competitive advantage instead of our patents will be our R&D. So we're based in St. Louis, Missouri. We're in this great area called the Cortex Innovation Community. And we're building a state-of-the-art manufacturing facility to produce MIEs. And our challenge, our competitive advantages, will be to produce the lowest cost, highest quality MIEs in the world. And then supply those to our innovators. Give them the ability to innovate and give them that secret sauce, the MIEs, that they can incorporate into their own devices. Or they can build capacity for their own countries. We're doing something else. I haven't been keeping up with my slides. build capacity for their own countries. We're doing something else. I haven't been keeping up with my slides. There we go. So we're doing something else with our R&D platform that's important when we think about pandemic preparedness as well. We're creating a nanobody library. So you may remember I said at the beginning of my talk that nanobodies are the proteins that we link to our electrodes to make MIEs, and those are what bind to viruses and bacteria. We're creating a library with trillions of different nanobodies. very quickly against whatever the next emerging pathogen is, whether it's SARS-CoV-3 or monkeypox 2 or whatever comes along that threatens to become the next pandemic. Our platform, our R&D capability, will allow us to produce new MIEs against those emerging pathogens within a matter of weeks and deploy devices so that hopefully the next time this comes around, we can get out in front of it. We can either bend the curve or maybe even prevent the spread by getting those devices for those emerging pathogens out into the world. So at Varo, we're very passionate about preventing the spread of infectious diseases. We're grateful for Vitalik for his support and for his support in supporting a new business model for innovation in the biotech industry. So I will be here. I'll be out in the back. I have a few demos that I can show of the the breath-based diagnostic device. I have some MIEs that I can show. So feel free.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:35:00.000Z",
      "slot_end": "2024-11-14T07:50:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1JN8fHkrJUSmMwQcFE6vbbWGfFN8h8mUo1A7pu-plAU8",
      "resources_slides": "https://drive.google.com/file/d/1LJ6LGQQPVs_iLSbMOLoZFwoHE0rfQly9/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "an-in-depth-picture-of-the-current-ethereum-network",
      "sourceId": "QDR3SX",
      "title": "An In-depth Picture of the Current Ethereum Network",
      "description": "This talk will show the geographical and ISP distribution of the nodes in the Ethereum network, the client diversity, the hardware requirements to run a node today, the power consumption, the storage load after EIP-4844, the block and blob network latency, the institutional staker distribution as well as the correlated failure analysis looking at EIP-7716. We will also show some projections on how  the network could look 5 years from now, taking into account peerDAS and fullDAS.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Home staking,Blobs,User Research,network,Blobs,Home staking,User Research",
      "keywords": "Client Diversity,Correlated Failures,Network",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "3a67a3a0254db2b3ae64b53cf0130c3710e040e2e7356dc3f2463e4511a89c12",
      "sources_youtubeId": "HIg1bscGqOE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:40:00.000Z",
      "slot_end": "2024-11-14T07:50:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1DzfSNuVPHDjyHuicrkDNZhpK9g2Wz2R5-L4UIPs00E0",
      "resources_slides": "https://drive.google.com/file/d/13AJ_15urkVxkhYDdhhRsWlBbz12CTIzD/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "building-lots-of-onchain-stuff-that-is-interesting",
      "sourceId": "T7DSSM",
      "title": "Building lots of onchain stuff that is interesting",
      "description": "For the past 1.5 years, I've been building fully onchain games–games where the entire state is onchain for some reason (have launched 7!). \r\n\r\nIn this talk, I will share some of my lessons and learning from them.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,Autonomous World,application,Autonomous World,Gaming",
      "keywords": "",
      "duration": 1548,
      "language": "en",
      "sources_swarmHash": "1c2071de09945f3fbb4accbcf71124fd79f5d1f7db887f434a269bb325cdf40f",
      "sources_youtubeId": "r3TycwvFcdQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ae3b4ccb22799e3aa47f",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ae3b4ccb22799e3aa47f.vtt",
      "transcript_text": " Hi everyone! Hey! Okay, cool. So, for this talk, I've given a couple of talks at DevCon. If you listened to my one on Monday, you heard some of the crazy, weird ideas in my notebook. If you listened to the one yesterday, you heard about end-to-end internet games and a couple of new technologies that I think could enable a bunch of insane new consumer crypto apps. But today, I figured for Mud Day, and because this is a classroom, we do something a little bit different, and I take it in a different direction. I figured that today, what would be most interesting and useful to probably this group of people would be if I just went through a couple of kind of hard slash interesting technical challenges I faced building random stuff over the past year. And better? People can hear me well? Yeah. I figured I'd go through a couple of technical challenges I faced building stuff over the past year and talk through some problems and what they were and what I did to fix them. And so with that, the talk is called... Three Things from Building stuff by Small Brain. And so what we're going to do, because this is a classroom, is we're going to play a game. And the game is called Who's Smarter Than Small Brain? And how it's going to work is I'm going to propose to you one problem that I ran into. And just like for some poetry, it's really fun that these problems exist because the things I build are really simple, right? It's not like you're trying to cure cancer and you have a hard math problem. It's like you're trying to get people to yell at each other over the internet and you have a hard math problem. And some of them are going to be math problems, some of them are going to be solidity questions. And we have very smart people in this room, like Dr. J and Mr. Indexer and Paco and David and Frolic and, you know, our German friend in the back. And so maybe this is going to be really lame and you guys are going to get the answers really quickly. But hope it's going to happen is you're going to struggle a little bit, then get the answer and then it'll be interesting. But please participate. Okay. Question number one. We're going to start easy and on Ethereum in a smart contract. And then I want to have a password that I give to somebody. And I want anybody with this password to be able to unlock the funds. It's like a really, really simple escrow smart contract. I put some money in a smart contract, I have a password, and then I give this password to somebody and they can unlock the funds. How do you do it? And while I give you a little bit of time to think, this was useful. Okay, amazing. Well, this was useful for an app I built called Yonk, which was an app where you can send people voice memos with a little bit of payment attached. And if they don't listen to your voice memo in time, face ID, no headphones, they won't get the money. The longer they wait, the less they get. And it was used in this app to do invite links. So if we have this primitive, we can actually send people who don't have wallets yet an invite link with a yonk in it. But anyways, getting back to the question, yeah, does any one of you want to jump in? You want to use your mic? Yeah, what do you think? Yeah, so mine is kind of a brute force. But what you do is you hash the secret. Yeah. You put it on chain. Yeah. And in order to prove that you know the secret, you make a simple ZKP that you know the pre-image of the hash. But it's kind of brute force. the room. And Justin, can you explain why it needs to be a ZKP? Yeah. So, okay. So do you guys know what a hash is? Like you take some, some value, you can turn into a hash, but you can't go from the hash and turn it back into the value. So if you put the hash of the password on the blockchain, you commit to the password, but people cannot recover the password from the hash. And now your problem is, can someone show that they know the thing behind the hash without revealing to everybody else? So if you were to just put the password in a transaction and be like, look, look, blockchain, if you hash my password, you'll get the hash. The issue is people can front run you and see it in the mempool and front run you. So what you do instead is you make a zero knowledge proof that you know the preimage of the hash. Yes, that's exactly right. And I think what's interesting here is this is the solution I also initially came up with. And then I was sitting in my room like four days before ETH Denver, like, you know, trying to write CIRCOM and being very sad. And like, I felt in my heart that there had to be a better solution. And there actually was. Ryan, do you have a different, was your solution different? Here, I'll run over to you with the mic. You make a new hash from the address of the sender with the password. Okay. Oh, no, no, no. Never mind. That's not going to work. Okay, cool. Never mind. Well, I think there's something interesting there. All right. Are we ready for the answer to be revealed, or does anybody have it? Okay, cool. This is how it works. I'm so excited about this one because this is like one of the most elegant pieces of like real world like simple cryptography I've seen. All right. So the first thing you do is you generate a new key pair. And we are going to call it so like a private key and a public key. Pretty much a new wallet. And we're going to call this key an ephemeral wallet. Okay? pretty much a new wallet. And we're going to call this key pair an ephemeral wallet, okay? Because like we're just going to use it to make this happen and we're going to throw it away pretty quickly afterward, all right? So the big insight, and some of you might be seeing the whole punchline by now, is that the private key from this key pair is going to be the password that we give to people. And so what you do is you scroll in the smart contract, the money you want to put in, and the public key associated with that ephemeral wallet. Then what you do is you take the private key, which is the password, and you give the password to somebody else. You give the private key to somebody else. When they want to unlock the payment, what they do is they take their own address, their own actual address, and sign it with the private and public key pair from the ephemeral wallet. And then they can use that signature to claim the funds on-chain. Because the smart contract on-chain just does a signature check to see that, okay, cool, the person with the password said that this wallet should have the money, so that we should give them the money. And this prevents like front run and all of the problems that the ZKP was solving, because you are signing exactly which address should get the money. Yeah, so the signature is a ZKP. Like a signature is a zero-lunch proof. It's not a ZK snark. Yeah. So we went too far on the heaviness of the solution. That's actually a better way to think about it. It's exactly the same solution, but a much smaller proof that shows exactly what you need to versus writing something in Sercom. This solution is not credited to me, actually. It was deep in the Dymo code because Dymo uses exactly the same thing for invite links. So big shout-out to DC and Nolan for doing an extremely elegant piece of cryptography to do invite links on chain. So this is a little trick for fully on chain invite links. Okay, cool. Little breath. Let's move on to question number two, which I'm looking at it is a little bit of a mouthful. But let me take you through the question and then you can, like can throw out what you think will be a good answer to me. Okay, this is the problem, and you need to do this again on a blockchain. There's a round, all right? And during the round, people are posting vectors to the chain. Vectors is like a long sequence of numbers. So a bunch of players are posting vectors to a blockchain during a round. And then, at some time, it's the end of the round. At the end of the round, an oracle comes and puts another vector on chain. This is the truth vector. And let's say the players are like, me, Justin, Elena, Frolic, Ryan, etc. What we want at the end is we want the sum of the distance between the Oracle and Justin vectors and the Oracle and Elena's vectors and the Oracle and Frolic's vectors, etc. We want the sum of the distances between the truth vector and the player's vectors. Does that make sense? Cool. Okay. Any ideas on how you can efficiently calculate this? All right. I need to give you a second to think while I give you context. That's the pattern, so don't jump in just yet. Oh, and the formula we're using for distance between vectors, actually we can use anything reasonable. We can literally just use Euclidean distance. But another valid formula, which is a nice little hint for this question, is cosine distance, which is the dot product of the two vectors over the magnitude of the two vectors. All right, sweet. Anyways, the context for this is this app called tomorrow.news, which is a prediction market for tomorrow's New York Times headline, that's TMR. That pays people out based on how close in meaning they are to the truth. And we use sentence embeddings to do this. Sentence embeddings are a way to turn a sentence into a vector. And when two sentences are close to each other in meaning, the vectors are close to each other in distance. So we can actually use this to build a prediction market that's semantic. Anyways, back to the problem. How do you do this nicely on chain? What were you going to say? Oh, I mean, I don't have a full answer, but one thing I'm wondering if it's useful is to consider the truth vector to be zero. Okay. And then we shift everything by the truth vector. I mean, okay, so as a first kind of like start, just assume truth vector is zero and then we're going to have to reshift everything. That should make a bunch of the distance stuff much simpler, at least conceptually. So this is what I thought too, but it actually doesn't, because the truth vector could end up anywhere. And distance from 0 is the vector itself. So it kind of keeps everything exactly the same in your example. Any other thoughts? All right, should I ruin it? Give us a hint. The hint is you have to stare. So do we understand the core problem with this? I guess a question somebody should ask is why don't we just sum the distances at the end? Yeah, are you assuming you don't want to have an O of N? Exactly. Because we're on a block. I kind of expected somebody to say it and then I would be like, no. But why we can't just sum the distances at the end is because there might be N players and then we need to do, like, you know, on the order of N computations, it might brick the game unless there's batching. So you want a constant time solution? We want a constant time solution to this problem. Alright, I'll give it to you. Can it be an incremental solution? What does that mean? Well, every time someone adds a vector, they do an incremental amount of computation. Yeah, it can. Okay. So it would still be O of N, it's just you don't pay it in one go at the end. Exactly, great. But you don't have the truth vector until the very end. Is another hint that it's done pairwise? What do you mean by that? Well, you always do it with respect to the previous vector? I will give you a hint that you just increment one big value as you go on. Yes, yes. Oh, amazing. Thank you. Oh, thanks. I wonder if this expression can be factorized. Yeah, how would you factor it? I see there is a dot product. I'm not sure about if the dot product can be factorized. Yeah, how would you factor it? I see there's the dot product. I'm not sure about if the dot product can be factorized, but I think that you can keep the ones on the side, sum the factor, and then do the operation on the sum. That's it. That's exactly it. Oh, my God. I'm so happy. The computer is gone. Yeah, yeah, it's coming back. Okay, cool. That's exactly it. What's your name? Bobosh? Great job. That's literally what you do. So here are all your, you know, these things you need to sum. These are everybody's guesses. Like, this is somebody's guess. This is somebody's guess. This is somebody's guess. you're like B over magnitude of B is the truth vector over the magnitude of the truth vector. And that's like a common factor in all of these terms. So you can just like pull that baby out. And also all the ones you can just like, you know, pull out to the side of the sum. And then this thing starts looking like, sorry, I didn't math script, but it starts looking like, you know, N, which is all the ones that you pulled out, minus the common factor times the sum of other stuff. And so all you do is you keep track of the sum of other stuff as people are making their votes, and you do a nice little multiplication at the end, and you're all very happy. The problem with this really is that what we ended up needing to do is we ended up needing to square everything or actually raise anything to a power we wanted to decide because the distances weren't crazy enough and so the market was kind of boring. And fundamentally now, this kind of sucks because you can expand this out, actually can use Euclidean distance here and then because there's a squared and a square root, it ends up being kind of nice, but you still can't do it. Because high level, what you did is you, like, added non-linearities to a thing which we were, like, it was really nice that it happened to be linear. And so to do this, there's two things that we aligned on. One was you could do a ZKP using something like SP1, right, to, like, succinctly verify the sum on-chain later and compute it off-chain. But it turns out that the cost of doing that on their prover network is actually more than the cost of literally just looping on-chain and doing the sum because the vectors were of such high dimension. And so we actually did end up doing the stupidest thing, which was looping on-chain. Great. All right. Okay. Third question. Third question is a really open-ended question, and there's not, like, a correct answer to this one. But what it is is, I guess first is, like, are people here familiar with words three? All right. Like, maybe, like like a fourth of you. So I'll go over just the part of the game you need to know to motivate this question. Words 3 is a game where you're playing Scrabble on chain on like an infinite grid, and you have to buy in letters to play. And the price of buying letters changes with some formula. That formula used to be a VRGDA. That's how the game, one edition of the game looked. It's a bunch of people playing letters and then letters are in different price curves. The letters, the pricing curve of letters used to be a VRGDA with target rates per letter set based on the rarity of the letter. What that means, what a VRGDA tries to do, like",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:40:00.000Z",
      "slot_end": "2024-11-14T08:05:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1nM0xwitXjawugXH5pPFNpq6MvywWnlHN137YbOB6mx8",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "fork-choice-enforced-inclusion-lists-focil",
      "sourceId": "CDTX78",
      "title": "Fork-Choice enforced Inclusion Lists (FOCIL)",
      "description": "A direct consequence of centralized block production is a deterioration of Ethereum's censorship resistance properties. In this talk, we introduce FOCIL, a simple committee-based design improving upon previous inclusion list and co-created block mechanisms. We present the benefits of (1) relying on a committee to address issues related to bribing/extortion attacks, and (2) having attesters enforce the IL as part of the block validity condition to prevent IL equivocation.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Design,mechanism",
      "keywords": "Censorship Resistance,Inclusion Lists,Mechanism Design",
      "duration": 1535,
      "language": "en",
      "sources_swarmHash": "ab48939dc7d01fcc84cab502b6e1268be91cf99ff863083e4e7e1b121ae75e13",
      "sources_youtubeId": "75aQTuZkDvE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736c9979dbb7a90e11fece0",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736c85374749a4b89b3ae2c.vtt",
      "transcript_text": " Good morning everyone. So today I'll talk about fuzzing for zero knowledge infrastructure and this is joint work with my collaborators from the Technical University of Vienna. So just to kind of make sure we are all on the same page, let's have a short definition of what I understand by zero-knowledge infrastructure. So for this talk I'll define zero-knowledge infrastructure as software components that are used for compiling, executing, proving and verifying ZK circuits. So examples would be the processing pipelines that are commonly used by DSLs for describing ZK circuits or maybe in the future we'll also look at entire ZK EVMs. So with this out of the way let's look at why this is an important topic and why more people should be doing this. Well first zero knowledge infrastructure is highly complex and highly critical. For instance it's used in several L2 chains so kind of bugs there could have catastrophic financial and reputational impact, and we should really make sure these components are as bulletproof as they can get. While we haven't seen a really catastrophic incident in this field, maybe perhaps comparable to the DAO hack in 2016, it's really important that we have rigorous testing for these components and we use, you know, the best engineering discipline that we can have, right? Because these components are really complex, and getting them right is not easy. So what is fuzzing? I don't think I have to explain this after the last talk, but fuzzing is widely used in industry. For instance, at Microsoft, Google, Meta, they're fuzzing a lot of their infrastructure to catch bugs before hackers can actually exploit them in practice. So in this talk, I'll give you an overview of our fuzzer for finding critical bugs in processing pipelines for zero-knowledge circuits. The fuzz is called Circus, and it already supports four of these pipelines, namely Circum, Corsair, Gnark, and Noir. And there's a gazillion of different DSLs popping up, it seems. Like you walk around here and you see another one in the corner. And we've already found 16 bugs in total for the four pipelines we looked at. 15 have already been fixed, so that kind of shows that these bugs are really taken seriously, and developers sometimes respond within hours to actually get them fixed. You can also see a kind of breakdown of the different bugs and where we found them. And as you can see, they're kind of very evenly spread across the different pipelines, so it's not like there's one pipeline that's responsible for all the bugs that we found. Now, kind of to make sure that everybody understands, I assume not all people are aware of what these processing pipelines look like, here's a short summary. You can see in the yellow box, there is a circuit that the user writes that goes into the compiler, and then the compiler hands some output to the witness generator, and the witness generator can then take the input, so the user input to the circuit, and generate a witness that can be then used by the prover and then the prover generates a proof that can then be verified to essentially check that the circuit was actually executed. So now that we have a bit of an understanding of how these pipelines look like, let's dive a bit deeper to understand how we find these bugs. So first, I wanna be very clear, we are not cryptographers. This is not my background. My background is in software security, program analysis. And for this reason, I can't really you know try to understand the the logic that's in the intricate logic in many of these components which is why we treat these essentially as a black box so much like what a typical user would do we just view them as a black box. So how can we still find all these bugs? Well first we have a lot of experience in testing complex software for complex domains and building fuzzers for them. So we've done quite a bit of work on fuzzing for smart contracts. It's probably the work that's most closely to the audience that's here. And for instance, we created the Harvey fuzzer many years ago, and we're still kind of maintaining it. We also did some work on ML models and on testing ML models and testing program analysis tools. If you want to know more, check out the papers that are online. And the second reason why we were able to find these bugs is that we have a not-so-secret weapon, which is called metamorphic testing. So before explaining in a bit more detail what metamorphic testing is, let me just give you kind of a short summary of how we got here, how we got into this, interested in this topic. So we were, as I said, we were working on fuzzing for smart contracts mainly for many years and then at some point I guess maybe like a year ago roughly, Consensys, they released the linear blockchain. That is one of the first ZK EVMs. And we were like, well, this is really complex. We started looking into it a bit. And we saw that there are these components like the GNARC library and the Corsair language for processing zero knowledgeknowledge circuits. So we were like, oh, what can we do to test these? What can we do to make sure that there are no bugs in these components? And that's when we started building kind of a predecessor to this fuzzer I'm presenting today, which is called Rio. So Rio is a fuzzer for the GNARC library, specifically. And then essentially this Circus fuzzer I'm presenting today is essentially an evolution of this fuzzer that is a bit more general and can target multiple DSLs. All right. Now back to metamorphic testing. So what is metamorphic testing? I think the shortest way to summarize it, it's kind of a way to define test oracles in a pretty elegant and concise way. So to illustrate, I've collected a few examples that explain this and try to explain what metamorphic testing is. So the first example is how to actually test that the sorting function, sort for an input, let's say an array of integers x, actually does the right thing. Well there's many ways to check this, but here's a way to check it using metamorphic testing. So you sort the input x, and then you also sort x, but you shuffle it randomly. And these two, the output of both of these invocations of the function, they should have the same output. So pretty nice and concise specification. So let's look at another example. Here, we want to test essentially some procedure for computing the shortest path in graph G between the nodes N and M. So how can we do this with metamorphic testing? Well, one way to do it is to say, you know, the shortest path between n and m in the graph G is less than the shortest path between n and m in the graph where we take G but we remove some random edge, right? Because that edge might have been on the shortest path, and then the shortest path could become longer, right? So we can also apply the same kind of principle to more complex components, like an entire compiler, right? So here's an example of how to do this with metamorphic testing. When you take a program P and you compile it and then run it, you should essentially see the same behavior as when you take the program P, but you add some dead code somewhere randomly. And you compile it and you run it. Shouldn't matter. It should give you the same output, essentially. So with this, let's look at how we can apply the same reasoning principle to zero-knowledge circuits. So the fuzzer, what it does is it first generates a random circuit, C1, and then it applies a random transformation to C1 to get a new circuit, C2. So now we have two circuits that are essentially syntactically, perhaps completely different, but semantically they should have the same behavior. And now the fuzzle generates an input, I, and invokes the processing pipeline for both of these circuits. And if there's any difference in the output or the behavior, then that's a bug somewhere in this processing pipeline. So for instance, for one circuit, we might not be able to generate a witness. But for the other one, we would. Then there's probably a bug somewhere in the compiler or in the witness generator. So let's look at a few of these transformations to kind of understand how the fuzzer does this. So here's a very simple transformation. If you have somewhere in your circuit an expression E, you can always multiply the expression by 1. That should not change anything. Or you can also multiply the expression by one, right? That should not change anything. Or you can also divide by one. Again, shouldn't have any effect on the circuits. Another transformation is to basically negate the expression E twice. Or you can also apply other transformations, like swapping the two operands of a multiplication. And then we also have some transformations that use essentially new randomly generated expressions. So here, for instance, we replace an expression e with e minus some random expression plus some random expression. That should, again, not have any effect. And we have a small DSL for describing these circuits, so there's many more of these transformations. So currently we support roughly 90 such rules, but you can easily add more of them and think of new ones if you would like to. So we also found a number of bugs in the different pipelines, and let's look at a few of them just to kind of give you the idea and give you a concrete example that you can look at. So here on the left, we see a small example circuit in the circum language. You can also look up the GitHub issue if you'd like to. But this is actually a minimized, cleaned up version of the circuit. And as you can see, there's a variable P. In practice, this is essentially just a constant. It just didn't fit on the slide. So I put it at the bottom. But think of P just as a normal constant in your program. And now what the fuzzer does is it generates a transformation of this circuit, which is the one that's shown on the right. And here it applies a number of metamorphic transformations. So, for instance, it seems like the fuzzer first multiplied the expression P by 1, then subtracted 0 from 1, and then divided that expression by 1. And when we execute these two circuits, we can see that the output out 1 and out two actually are not the same. So this is bad and this was a bug that we found. It seemed like there was a bug in the witness generation part. And here's another bug that we found in GNARK. So at the top you can again see the original circuit that the fuzzer generated. And then we can also see the transform circuit where the fuzzer essentially changed the expression zero to just zero or zero, which should be equivalent. And here, we observed that for C1, there was no witness that was generated, whereas for C2, there was. So this, again, is a bug somewhere in this pipeline. And yeah, now that you've kind of hopefully got an overview of the tool and saw a bit how these bugs look like in practice, I hope more people will start looking into this problem, because I think it's a very important problem. We need to really make sure that these components are bulletproof, because otherwise pretty bad things can happen. And it's better to do this before some, you know, attacker does it, right? And you don't need to be a cryptographer to actually find these bugs. So, you know, that gives us a lot more people that can actually look into this. Because I think so far we only really sketched the surface, so there's more things that need to be done to test these components. Also, there's components like this popping up every now and then. So we really need to make sure they're safe. And we also should do continuous fuzzing of all these components. So for the Gnark team, we actually implemented a continuous fuzzing of all these components. So for the Gnark team, we actually implemented a continuous fuzzing setup, where whenever they commit the latest version to master, we start a new fuzzing campaign. We run for 24 hours. We tell them if something is wrong. And then the next day, same thing happens. We're also planning to do the same for Corsair. And if you're interested in us taking a look at your ZK infrastructure, please reach out. And if you want to know more about what's going on in the fuzzer, then please check out our paper. You can scan the QR code there if you want to take a look. Yeah. Fantastic. Thank you, Valentin, for this presentation. Let's get to the questions, shall we? Yeah. All right. So I've sorted the questions. Again, you have a QR code here. You can ask your questions, and I'll ask the ones that are the most upvoted. So let's go to the first one. When you identify a bug, can you just say, hey, there's a bug, or do you provide a path to fix it? Do you identify where the bug exists within the circuit? We don't directly identify where the bug is, but when we generate the test inputs, like the circuits, we try to minimize them. So we try to keep them as small as possible so the developers can really, hopefully quite easily identify where things go wrong. So far, I think the developers were very happy with the bugs reported. And I think there was no issues finding the bug once they had the input. Makes sense. Thank you. Second question. Do you fuzz logical expressions during coding in circuits? Yeah, we do. I think, yeah, so for instance, we apply some, you know, some common transformations like applying the Morgan's rule and so on. So there's a bunch of these transformations that we use in the fuzzer. Wonderful. Third, where is the bug nest? Where do bugs usually reside in your historical fuzzing, from what you've seen? I think it's hard to... You really derive enough data from... You found 17, right? Yeah, yeah. I think it's probably too early to say, but we did observe that the fuzzer found bugs basically in different components. So we found bugs in the compilers, we found bugs in the witness generator So we found bugs in the compilers. We found bugs in the witness generator. We found bugs in the prover as well. I don't think we found bugs in the verifier. It seems like, yeah, that's kind of towards the end of the pipeline. And I think many people are very concerned about getting the verifier right. So maybe that's also paying off here. But we'll keep trying. We'll see. Nice. Okay. When Cairo fuzzing? Yeah. I guess you answered it in your slide. If we're interested, people can talk to you. If somebody from Starkware wants to look to look at more at Cairo, then yeah, please reach out. We're interested. It's definitely a good idea to do that, yeah. Wonderful. Well, I'll reach out. What do you think of concolic testing of ZK circuits? I had to ask Chad GPT what concolic testing is. Okay. I still don't understand it. Okay. Yeah, so concolic testing, essentially, I mean, for the audience, maybe it's essentially a way to, you know, generate new inputs by doing some kind of symbolic execution. But basically, for some expressions, that might be really complex. You have non-linear expressions in these circuits. So there, you might want to concretize some inputs. And that's the concolic part. So it's concrete and symbolic. But yeah, so I think that's an interesting area. It's not what we focused on because we're not so much interested in actually generating inputs for these circuits. That's something you would probably want to do if you want to have a specific circuit that you want to get right and you want to make sure it satisfies some properties, then you probably should think about concordant testing for that circuit. Yeah. Perfect. Thank you. How does it feel to find a bug? Like, literally, when you find one, is it scary? Is it exciting? I mean, you're looking for those, so it's probably a bit exciting, but it's also scary. There's probably money on the line. How does it feel? No, it's definitely, I mean, it's definitely definitely exciting. I work a lot with students. You can see they're excited when they find a bark. They're happy they're making an impact. And also the reactions from the developers are a great motivation to find more barks because usually they're very... Yeah, they have been very positive. I don't believe you. There's no way you go to people and say, I broke your shit. And they're like, oh yeah, amazing. Yeah. I mean, if you're sort of... If you're... Yeah, and L2 and one bug like this can wreck your system, then in some sense your job is also on the line. So you should... You want to find those bugs. I'm not saying you don't want to. It's just a theory. When I think about bugs, I think like, yes, I want to fix them. But the first thing that comes to mind is like, oh, yes, no. But you're right. It's better to be aware of first than not. Okay, we can actually take the last... Oh, one second's left. Let's wrap it up, or do you want to... Are you using property-based testing for choosing the input? I mean, you can call it property-based testing. We're generating the inputs right now completely randomly, so it's kind of black box fuzzing that we're doing. And we're also considering to do more, you know, feedback-directed fuzzing, like you saw in the last talk. Yeah. Fantastic. Thank you, Valentin. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:40:00.000Z",
      "slot_end": "2024-11-14T08:10:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1MowR6E3eFzSs1jXPUxgTBxReXgDFk6pgjqMA7hnC7t8",
      "resources_slides": "https://drive.google.com/file/d/11z833igJ75d0WUxo9T9c2lW-ud_eQcI1/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "insights-from-block-propagation-in-the-ethereum-p2p-network",
      "sourceId": "T8GXPY",
      "title": "Insights from block propagation in the Ethereum P2P network",
      "description": "Libp2p’s Gossipsub protocol is one of the most critical pieces of the Ethereum protocol stack, disseminating blocks between nodes on time and ensuring that misbehaving nodes are rejected from the network. ProbeLab has studied the performance of Gossipsub in Ethereum’s P2P network, building tooling to monitor block propagations and spot abnormalities.\r\nWe revealed ample space for optimisation in the protocol, which will help define the next steps in Ethereum's roadmap. Come and hear our findings!",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Architecture,Scalability,network,protocol,Architecture,Core Protocol,Scalability",
      "keywords": "Block Propagation,Networking Protocols",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "83c6fc07386bf925ecb5ef2a2f580c555958de271f25db62a09fe412f3ca7b30",
      "sources_youtubeId": "AH8NmuW7pw8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:50:00.000Z",
      "slot_end": "2024-11-14T08:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1Do39xW55yzxbDah8ClU174jW2BCWeaJUCWQ-N15sadE",
      "resources_slides": "https://drive.google.com/file/d/1yKUmAmpPXM_jpuvHloD8Yk4resOZBXxN/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "scaling-clean-air-now-and-the-future",
      "sourceId": "RKA9MF",
      "title": "Scaling Clean Air: Now and the Future",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 758,
      "language": "en",
      "sources_swarmHash": "12064d7d2a4ccbfe9f6ef7d3ab1af74c6aa25dd4a215d0812f624bf47f56891f",
      "sources_youtubeId": "q2YUVMRPQKw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735b4a09dbb7a90e12f4f30",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735b4a09dbb7a90e12f4f30.vtt",
      "transcript_text": " I have some money to do stuff around air quality, indoor air quality, especially I'm an engineer by trade, so I write software. I wrote the software on the toy that you saw in the booth out there. And I got tagged in to give this presentation, so you're going to have to bear with me on that front. So here's a question. And we're going to do price is right rules. So closest without going over. You eat about 1.8 kilograms of food a day. You drink about 2 kilograms of water a day does anybody know how much air you consume a day hands somebody give me a hand yes 11 000 liters i want it by mass i heard 10 kilograms. Who was that? Yeah, all right. Toss them one. Yeah, let's say 12 kilograms a day. That's a lot of air, and there's stuff in that air. Stuff in the air. Bad air is a bad time. So this is from a study about decision-making versus carbon dioxide content. So if you ever see, you know, you know about hypoxia's effect on the brain, long before you ever get to anything like passing out, you start making really bad decisions. So if you wouldn't sign on a house drunk, you probably shouldn't sign on a house above a couple thousand ppm of CO2 or you will make terrible choices. The inverse effect is also true. Bad air is a bad time also because it leads to airborne infectious disease. We all know about COVID, but there's plenty of other stuff, tuberculosis, chickenpox, measles, all that stuff. Some of it, there's great vaccines against some of it. Nobody really knows. And you really don't want to catch it even if you have a not particularly symptomatic case. There's a really interesting talk about that you can stick around for. Also, the economics of it are incredible. Trying to do clean air-based interventions is an excellent way to reduce things like sorry, heart attacks. Children's test scores, air purifiers in classrooms are one of the most economically effective way to boost test scores compared to anything like teacher training, other interventions. You can find direct links between indoor air quality in education spaces, absenteeism, and eventual health and economic outcomes for the people in those spaces. So at Entropic Engineering and Open Aeros, we think this is pretty cheap and easy to fix. You can have some really high-impact effects. Masks, especially with fit testing. This is another place where we received some funding from Balvi to work on open-source condensation particle counters that can make it really easy to do this kind of work. Portable air cleaners. You can see one in this picture of our office. Windows. These ones don't open, but if you're in a place where it's thermodynamically acceptable, you can open your windows. That's an excellent way to do some mitigation of this kind of stuff. Simple HVAC upgrades like dropping in better filters, making sure you actually change your filters. And we also do some work with skin and eye safe 222 nanometer UV light like this one here. There's also one on the display table outside on the front. Unlike earlier 254 nanometer technologies, this is stuff that's relatively easy to deploy because the threshold for skin and eye damage is much, much higher. It's something that you can retrofit into existing spaces a lot more easily. It's an excellent drop-in mitigation. Here's the real question. If this stuff is so cheap and easy, you know, we've got filters in this room, we've got UV lights outside, why isn't it already fixed? Well, we think one of the big reasons that it's not already fixed is it's invisible. Literally. When we're talking about particulate in the air, we're talking about particles that are so small that measuring them requires innovative technologies to make them grow physically large enough in the device that we work with via condensation to make them countable because they're so small they don't meaningfully reflect enough light to count them even under laboratory conditions, let alone with your eyes. Like, think about when you see smog outside. Well, that is the aggregate effect of looking through a bunch of particles in the air. When you're inside, you just can't see it. And you can't tell what kind of particles they are, how big they are, and you also can't tell what CO2 concentrations are like. So you can't tell the extent to which your judgment is impaired and you're going to expect poor health outcomes like that. So what I'm here to talk about is U-crit air, also known as the clean air Tamagotchi, is something we've been working on for the last five or six weeks. This is a portable air quality device that includes a virtual pet. So the idea is in low power mode, there's an e-ink screen on top, which will wake and update periodically. You've seen them on the Kindle, so that supports really low power operation. We hope to get at least a week or two out of it. The hardware we have outside is pre-production hardware. But we are eventually hoping to sell these things. So take that with a grain of salt when you play with them. But they are fun to play with because on the bottom screen, when you wake it up into higher power mode, you have a virtual pet character. And the whole pitch of this is that when you think about air quality, especially indoor air quality, it's something that's really easy to ignore. Like you can sit there at work and think to yourself, I have kind of a headache whenever I sit in this conference room, but I got some dials to pay attention to. And that builds up every time you're exposed to higher CO2 levels, every time you're exposed to high particulate concentration. The chronic health effects of all of these are cumulative, which means that you can ignore it and ignore it and ignore it until 30 years down the road. It turns out that you've had sequelae from multiple low intensity viral infections, or you have developed chronic lung problems from exposure to particulate matter. So the concept that we have here is a sort of gamification approach where you and the virtual pet are exposed to the same air by virtue of it being with you. It has onboard sensors that can measure carbon dioxide concentration and particulate concentration and a couple other fun things. Except the virtual pet reacts in real time much faster than you do. So what we're doing is sort of hacking your judgment to say, well, you know, you know intellectually that there's going to be long-term negative effects to bad air quality that you're exposed to. But if we make those long-term negative effects happen on a very short term to this little guy, especially if he's cute, it's sort of a way to hack that decision making process. So you can play with him, you can arrange furniture, you can buy for him various clean air interventions like those that we have here, you can buy him a HEPA filter, you can buy him UV lights, you can have him wear a mask, all of that fun stuff that we think are sort of straightforward, relatively easy and effective interventions. You can also apply to your character. And the idea is let's take this thing that on the human time scale and on the human health time scale is essentially invisible, make it very visible, especially in a relatively fun way, make it make a lot more sense to pay attention to that. So that's the concept. I'm not going to try and do it during the talk because I was very politely informed that they're going to drag me off stage with a shepherd's crook if I go over. You can catch me after the end of this track out front and we'll probably do a little bit of trivia and give away the units that we have that are pre-production. You can find more information about them at ucritter.com slash error or there's that information outside? Yeah. So those are all pre-production. You can tell. 3D printed case. We assembled them in-house. So they're a little bit of a precious commodity but we're going to give them away. Fun little bit of a crash project. Blew the in-seat power circuit breaker on the plane which apparently they can restore. Got to do some laptop power management limiting to get back on the road building these things. But some other cool tricks that they have up their sleeve compared to other commercial air quality sensors is they do barometric compensation for CO2. So if you live anywhere other than sort of median height in terms of elevation, or if you're on a plane or you're climbing a mountain, your air quality sensor that you already have is almost certainly lying to you if it's not barometrically compensated. So this is a fun little trick that we do. This is me flying from the last leg from Korea to here, where you can see that without barometric compensation, you're underestimating carbon dioxide concentration in the cabin by a couple hundred ppm so we think that's a fun little bit of special sauce that went into this project they also have a barometric pressure sensor on board so if you want to climb a mountain you can track your altitude as you go and the other trick that we think is really cool and should become something that people expect in mitigated spaces is on the topic of uh these airborne you know these air quality problems being invisible if you if you buy something like these hepa filters how do you even tell if they work well one way you can do it is you can keep track of the carbon dioxide or particulate concentration in the room for hepa, you're looking at particulate, but for ventilation to outside air, like you're going to open the window, you want to look at carbon dioxide. And you can do something. You can affect an intervention like open a window, or you can affect an intervention like turn on a HEPA filter, and you can watch the rate at which these airborne contaminants drop. And with a little bit of math that is baked into the Eucritters, you can calculate, you know, what the CO2 concentration change or what the particulate concentration change tells you about the mitigation choices you've made in your space, usually measured in air changes per hour, which is sort of one air change per hour means volumetrically all the air in the room would be cycled through once an hour. That's great for measuring CO2. So if you're going to affect some mitigation like turn on your HVAC, you can watch the change in CO2. You can see a drop. And if you have a U-critter with you, it's logging. So you can compute the effectiveness of that intervention. And if you're going to switch on HEPA filters, you can take your U-critter and look at the recorded particulate concentration information and compute what's called an effective air changes per hour number. So I think that's the last cool trick for the U-critter right now. Oh, it also works as a Bluetooth sensor. You can talk to it over Bluetooth and pull out current air quality readings. But it has Wi-Fi hardware, and we're excited in the future. We're hoping to ship some updates that will let us do privacy-preserving aggregation of air quality statistics, sort of like existing air quality sensor networks, but that extend to places that might not otherwise be reached, like the places you go in your daily commute that fixed sensors aren't appropriate for. This is open source work. We're still in pre-production, so you can't ‑‑ I don't think the schematics are up there yet, but if you pester them in the Discord, maybe you'll get them to post them. So if you have feedback, we'd love to hear it. The website is out there. It's also in the firmware if you get one. Please reach out if you'd like to collaborate. I'd also like to say thanks to Balvi, thanks to the Ethereum world. I'm not a crypto person personally, but the work that we've been up to at Entropic working on the Clean Air Tamagotchi but also at OpenAeros and the other efforts wouldn't have been possible without you guys. So it's something we really appreciate. I think I'm going to wrap it there. Thank you so much, Lewis.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T07:50:00.000Z",
      "slot_end": "2024-11-14T08:05:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1ZJZJ_2zvDgnrKFG8JEZo8VMp_Z1mb0btmMRtR2j0Vv0",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "a-deep-dive-into-zk-proofs-of-pods",
      "sourceId": "EQ9BYQ",
      "title": "A Deep Dive into ZK Proofs of PODs",
      "description": "Provable Object Data (POD) is a format any app to easily sign data and make ZK proofs without manual circuit writing or trusted setup.  Proofs are described in a simple configuration language, then compiled to a family of General Purpose Circuits (GPCs).  POD tech is used in Zupass and available as open-source libraries.  We’ll dive into the cryptography behind PODs, and what makes them suited to ZK proofs.  We’ll also cover the modular design which makes GPC circuits highly configurable.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Libraries,Zero-Knowledge,Cryptography,zupass,Cryptography,Libraries,Zero-Knowledge",
      "keywords": "Zupass",
      "duration": 4370,
      "language": "en",
      "sources_swarmHash": "5969b7d504ccf949f7bdf0e3e943a2ebc57a99285a10137a1f547e556520c2a6",
      "sources_youtubeId": "gWoVrIO1tIE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735db679dbb7a90e156b54d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1Jn6onllMeGwArE5qb8v5g4TAqMZfdWeU4wZW8fFGUhw",
      "resources_slides": "https://drive.google.com/file/d/1h5s7YD2c7wvIIYsl6IyuWbgxXSMD-sv7/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "bootstrap-your-dapp-ux",
      "sourceId": "AJVBTL",
      "title": "Bootstrap Your dApp UX",
      "description": "What if we could streamline Web3 development, making it easy to prototype user-friendly dApps and drive mass adoption? Despite improvements, crypto's complexity makes intuitive design challenging. Many projects are developer-led, resource-constrained, and rushed to market, leaving little time for thorough design and user testing. In this talk, you'll learn to recognize common Web3 design mistakes and leverage best practices from DeFi and traditional finance to quickly bootstrap a user experience",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization Improvements,Scalability,Developer Infrastructure,Libraries,Frameworks,Open Source Software,Payment,User Experience,Interface,Design,UI/UX,Design Thinking,standards,Decentralization Improvements,Design,Design Thinking,Developer Infrastructure,Frameworks,Interface,Libraries,Open Source Software,Payment,Scalability,UI/UX,User Experience",
      "keywords": "Design,Standards",
      "duration": 1305,
      "language": "en",
      "sources_swarmHash": "697f0bbc602c4f9bf696747238823569347bbb6d18cb835a5826d7e7cb07a1ac",
      "sources_youtubeId": "QKpxqrga8hk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e3989dbb7a90e163e84c",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735e3989dbb7a90e163e84c.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\"Petra Petrovic' Swadika! We're going to be jumping into bootstrapping your Dapp UX, but first, I want to ask two questions. Who here has struggled to make a complex crypto app feel simple to new users? And who feels that resource constraints on their team have made it difficult to create an ideal user experience? If you face these challenges, you're not alone. Together, we'll be discussing how we can implement best practices to help you overcome these challenges and bring the next billion on chain. Now before we dive into solutions, allow me to introduce myself. You might know me as the crypto bride. About three and a half years ago, I exchanged NFT wedding rings with my now husband on Ethereum mainnet. Of course, we overpaid in gas. This was before any Layer 2s existed. But you know what they say, love is priceless. And about me professionally, I've been designing and leading teams for the past 15 years. Most recently, I led the design of Coinbase Commerce, where I help businesses use crypto payments. Now, as a DeFi design consultant, I help businesses that are tackling the same challenges that you're facing. So with that, let's dive in to how we can bring those next billion on chain. To guide us, I want to introduce you to a framework that I developed through analyzing dozens of popular DeFi apps. I call it the Dapp Funnel. It outlines the key stages people go through when interacting with decentralized applications, from their first interaction to transacting and beyond. The stages are connect, configure, transact, and track. Each stage represents an opportunity to either engage people or lose them. Let's begin with the first stage, connect. This is all about connecting with a wallet. As I was doing my research, I discovered that where to start is often unclear. Best practice number one, make connect obvious. Let me start with what not to do. Don't mislead with interactive zero state. So here's an example on beefy. You can see that I can enter an amount to deposit, but the button is disabled and I don't know what to do. Another common mistake is what we see here on pancake swap where somebody comes in wanting to do an action such as swap but the call to action button says connect wallet. So any kind of benefit to surfacing this interactivity is negated by this disconnect. What's better is how Zora and Lido and a few others are proactively surfacing the login screen. Best practice number two, streamline options. Is anyone here designing cross-chain dApps? Awesome. Here's another example of what not to do. Don't combine network selection with wallet selection. It's just too much information. It could be handled in multiple steps. Save time by implementing a ready-made wallet plugin. There's so many options out there that will simplify your onboarding. Another great option is to combine the disclosures with your Wallet Connect screen. People who are familiar with Web 2.0 will likely click on the upper right corner. They're logged in account. Don't do what Curve does and a few others that instantly log out. It's quite jarring, disorienting, and instead, offer an interstitial menu. This is an example from Arbitrum Bridge where people can access their settings and other options. Rounding out this topic of connect, consider using a passkey. This is a really awesome innovation that we're seeing more people adopt. And I hope to see further streamlined techniques as we move forward. All right, moving forward to configure. This step is all about setting up the transaction. It's about selecting the asset, entering an amount, and choosing a network, and sometimes configuring more advanced options. And here the two themes are too many choices being overwhelming and using technical jargon can intimidate users. Let's start with personalization. Pop quiz. Do you notice a difference between the logged out stage and the logged in stage? If you answered no, you are correct. This is a missed opportunity, unfortunately, on boring DAOs part, where it's not suggesting a token or a network. Instead, do what Uniswap does here, where it's suggesting tokens in my wallet and even suggesting prior searches from previous sessions. So it really feels like they know me. Another thoughtful approach is personalizing by persona. So look at here how Blur has two different user types. There's the trader who wants a more information dense view and the collector who wants a more curated experience. Similarly, on bridge, there's an expert mode. And this is great if you feel like you might be alienating some of your power users by simplifying too much. This way they can easily access those more advanced features. Starting with good defaults should minimize the cognitive load, but what happens when people encounter errors? We need to enable better recovery and hopefully prevent them. Let me start with a great example of error prevention. Here on Hop, it's suggesting a minimum amount for me to send so I don't get an insufficient funds error. Be sure to check your error messages. This is a really great example of how somebody may have forgotten to translate this into something that makes sense, explaining what happened and what the user needs to do to fix it. And double check that what you're saying to fix it actually works. Instead, deep link into the wallet to overcome that issue. Recover here as Eigenlayer does by switching to a supported network in this case. I recommend progressive disclosure also to simplify the interface. Here on LamaSwap, the slippage options are visible at all times, which it makes the interface too cluttered, and it can increase the drop-off, right? Here's another example on Beefy, where it's showing the drop-off, right? Here's another example on Beefy where it's showing the increments from 0 to 100. Probably not a widely used feature and it makes it look a little messy. Instead, check out how ThorSwap buries those options underneath the Max button. So for the items that are visible in your interface, I recommend showing some education, providing more context to increase confidence in the transaction. So let's avoid using jargon. Here's an example from RocketPool where it's using a term that probably everyone in this room knows, mainnet, but it doesn't map to an option within most wallets. So instead use straightforward language. Tool tips are a great option to explain in detail everything that a transaction has. I also recommend including a confirmation screen that gives the user an opportunity to review all of the details before moving forward. Assuming all the details in that transaction look good, we're ready to transact. This is the core of all DeFi applications. It's where people authorize access to their funds and transfer. It's critical that people feel safe here. Unfortunately, lengthy approvals can scare and confuse. The best antidote to that is to convey progress. Okay. Who has used Aave in this room? Probably everyone, right? It's like one of the oldest, most popular dApps out there. But unfortunately, it uses buttons in a very non-standard way. This is not the best component for showing a multi-step transaction. You see how they appear and disappear without any kind of confirmation. What's a better pattern is what Uniswap does, a stepper, right? It shows what are the actions that are required, what are the status, pending, and confirmed. I also recommend showing in-context feedback through toast messages as Aerodrome does here. Don't misrepresent status. So here's an example from Lido, which is optimistically presenting modals saying, you are unlocking, you are wrapping. But it's before I've even had an opportunity to confirm in my wallet. So it's not really accurate, and it reduces my trust in their system. Instead, do as Moonwell does, which encourages, once I've reached the pending status, to continue using the application. Okay, this one's a little controversial, but rounding out the transact phase, let's not do infinite approvals. You see this long, scary red text. This with the current constraints we're in right now, maybe a lot of this will be solved with smart wallets. But until then, let's use exact amounts so that it maps to what the user entered into the system, and it's also more secure. Now the final stage, track. This is all about monitoring the status so people know what happened and what they can do next. Here, we observe limited access to historical and ongoing performance. Here are the three stages of the tracking lifecycle in most apps. There's real-time, ongoing, historical. Let me break it down. For real-time, we covered that in the previous section, Transact. And this is all about in the moment when you're investing, you receive that feedback, right? But ongoing is missing, right? We want to understand the day-to-day performance of investments, including returns and the yield. The patterns we see here are dashboards and notifications. This helps people make investment decisions, what to do next. And then the history, this is all about record keeping of the past transactions, account statements and this enables accounting. It's critical that we support each step of this journey in our DAPP. So how do we support ongoing performance tracking? Let's provide APY history. This is an example from Aave. It's very rare. I did not see many of these in my audit. Please share examples if you have them. Anecdotally, a friend of mine takes screenshots of the apps where he's invested to see whether or not his APY has gone down, which is not ideal. Let's also summarize the portfolio insights. This is from Aerodrome, a dashboard that shows kind of the return and the emissions and other things that I earned. This is definitely not the best, but I challenge you to provide better examples. I want to leave, rounding out this ongoing performance topic, I feel like a missed opportunity here is push notifications for crypto. I think there are a few solutions out there, but it's very much lacking. Here's an example from a TradFi account. I received an email notifying me that the yield went down on one of my accounts. How do we know if we're about to get wrecked? That's kind of a core use case for a notification. And I welcome some solutions there. Finally, raise your hand if you've ever tried doing taxes on your crypto. It's a nightmare, right? So on Uniswap, you can see that there's transaction history, but this is not a common feature in most dApps. And here on Dbank, you can see an example of aggregating statuses, but again, this is a third party, it's a little lagging, and the source of truth should be on your DAP itself. All right. So we covered all of that. Woo! So here's the big takeaway here. I hope that you can implement some of these best practices in your DAP today to enable better adoption and retention. I'll let people finish taking their pictures. Please share your feedback on X. But first I want to leave you with one final idea. What could we build together if we're not building in silos? I propose that we align on web 3 design standards. Imagine a world where every dApp developer had access to plug and play components that were fully user tested, responsive, open source. It would enable greater efficiency. You could focus on your core project area. It's a proven model. We've seen this with Android and iOS and also the very popular Bootstrap framework. And finally, it's ecosystem strengthening. It would help onboard the next billion because they would become familiar with the patterns across every single DAP rather than learning bespoke implementation. There are a number of tools that exist today. There are pros and cons of the wallet integrations and SDKs out there. Some of them are promoting certain wallets or chains, but I think we need a neutral, community-driven solution. So I'm looking for people to build this together with me, front-end developers, designers, researchers. Please reach out if you'd like to be a part of this. Thank you so much. Kap kunk ka. Great talk. Amazing, Rebecca. Okay, let's ask some questions. Great talk. Amazing, Rebecca. Okay, let's ask some questions. If you guys have a question, you have to use the code to post it on the app, and I will read it out. Okay, let's start with the most voted question. So first one is about using tooltips on mobile. What is this alternative? Because it's hard to use tooltips on mobile. That's a good question. I've seen different patterns implement like a mobile version, which would be like a modal. Or like a bottom sheet, I guess, if it's a native app. Okay. Do you think wallet pop-ups... Oh, yeah, a lot of... This is fair. Okay. Do you think wallet pop-ups... Oh, yeah, a lot of... This is fair. Actually, what was the stat? Only 10% of the most popular dApps actually do have a mobile-optimized experience, so definitely it's an opportunity for us to build more responsive libraries. I think there are different use cases, right? There are some serious traders who prefer desktop. So my audits were desktop components, but clearly there's definitely a lot of need for mobile as well. Sorry, the wallet pop-ups for approval. So the next question is, don't you need product market fit before you can consider UX? Oh, that is a great question. I definitely agree on that. And there was a really great talk yesterday about how to quickly validate ideas by just really mocking up two parts of the flow. The point of discovery and then the point of value. And then you can decide whether or not to build the middle of the product experience. So it could save you potentially a lot of effort. Okay. The next question is, should users know they are using crypto? Yeah, there are so many great topics about this. is should users know they're using crypto? Yeah, there are so many great topics about this or talks about this at this conference specifically. I think longer term as we're looking at onboarding the next billion, it's not necessarily what I shared today. It is abstracting it and making it so simple, probably likely behind a very simple interface. Okay. Next question is about using tables on mobile. How do you approach that? There are various options, I guess. You could either stack... I don't know. I'm not sure if I'm the best person to share this right now, but yeah, there are different patterns that you can look up. Okay. When do you think chain abstraction will be the new normal in DeFi? I wonder what's a hindrance. What's the hindrance? I'm not quite sure about that. Is this... Who wrote this? Somebody? What is hindrance? Yeah, like what is the hindrance of... What is preventing people from extracting the... Is somebody who asked this question here? Maybe... What is preventing people from tracking? Oh, is somebody who has this question here? Maybe. Hmm. Okay, so this is kind of like the other question around it being, like all of the complexity being hidden. Okay. like all of the complexity being hidden. Okay. Yeah, I'm with you. I think that is the future. And I think we'll see a lot of advancements when Petra is upgraded in Q1. So let's stay tuned. I think what I shared today was focused on the technology that's available right now, what you can build today, but that's definitely soon. And then later, I think, is the standards where we kind of, as a community, agree on the best practices and how to implement common patterns. Okay. Next question is, should we be moving towards embedded wallets inside apps that are less portable or continue to push in portable horizontal wallets which are not perfectly suited for each app? Perfectly suitable for each app. Like MetaMask versus the wallets that you get inside of your application? Yeah, I watched one of the talks about this and I guess like right now it seems like there are some wallets like Privy and others that will create per Dapp but the vision I think is to consolidate across. The focus of this was DAP UX, not Wallet. I think that's it for the time, for the Q&A. Thank you very much, Rebecca. Thank you everybody who asked questions and next talk is in five minutes. See you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T08:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1Vl4odQ2HutojKK7BY-U7D84ku-UD2fPpjY9T487-56w",
      "resources_slides": "https://drive.google.com/file/d/1Oj8xcrNyYsJtC284p7t9_C1-Fqqy7ias/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "bringing-real-world-identity-on-chain",
      "sourceId": "TPLBLQ",
      "title": "Bringing real-world identity on-chain",
      "description": "A brief overview on how we can bring real-world identity on-chain privately using zero-knowledge proofs, why we need to do so and how it can help solving some real-world problems, and some challenged associated in implement this. \r\nFeaturing some projects in this space like AnonAadhaar, Proof-of-passport, etc.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Anonymity,Identity,zk-identity",
      "keywords": "identity,real-world,anon-aadhaar,zero-knowledge-proofs,zk-identity",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "87b7ddcd406040c80df04bc900bb9f3e9d17b5070167f53211df9daff0d65b9d",
      "sources_youtubeId": "X3fJw5yM4PE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T08:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1UfdavDRYTsOiq_2t15-4vh9KLrcltw6zpHsdrX2d8DA",
      "resources_slides": "https://drive.google.com/file/d/1NGWtmBmBwrhuVNSzHA2x-9yyImPZxJz_/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "building-consumer-apps-with-zk-mpc-and-fhe",
      "sourceId": "8TDXNQ",
      "title": "Building Consumer Apps with ZK, MPC, and FHE",
      "description": "In this panel, members of Cursive, ZK Email, ZKP2P, and TACEO will describe their experience building and enabling consumer applications using programmable cryptography. They will discuss how advances in ZK, MPC, and FHE enable new types of applications to be built, how to get started building cryptographic apps, tradeoffs in tooling, and the challenges and potential pitfalls while using these tools. Finally, they'll share some future ideas for cryptography-centric apps they're excited about!",
      "track": "Applied Cryptography",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "application,real-world,Cryptography,Use Cases,Use cases of cryptography",
      "keywords": "Applications,developer experience,real world applications",
      "duration": 3361,
      "language": "en",
      "sources_swarmHash": "6c57769773ac56d933eb3c893cc4df6bd2f5bccb37c41cf2c9ab834e1c790639",
      "sources_youtubeId": "UPXYzWS7ZJ4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735be379dbb7a90e1bce08f",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1cnZdODskc3YfoVa93c9fvIVSBrE-RXobQHmu9RtOa-s",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "designing-and-launching-a-retroround-incentivize-what-matters",
      "sourceId": "39AVKD",
      "title": "Designing and launching a RetroRound - Incentivize what matters",
      "description": "Learn how to design, develop and launch a retroactive funding round. In this workshop we’ll explore the differences, similarities and best practices for running a local and ecosystem RetroRound. Participants will be able to set clear goals, define impactful behaviors to be incentivized, scope technical roadmaps, and formulate a sustainable strategy to fund public goods. Ideal for emerging markets community leaders and web3 Ecosystems looking for new resilient and diverse funding strategies.",
      "track": "Coordination",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "RPGF,Quadratic Voting,Public good,Design,Mechanism design,program,grants,Mechanism design,Public good,Quadratic Voting,RPGF",
      "keywords": "Emerging markets,Grant Program Design,",
      "duration": 5442,
      "language": "en",
      "sources_swarmHash": "5dd5ebab804d94005464c04cc83b0393d5fe0c7517d4ff7e86c54e1149ab100e",
      "sources_youtubeId": "Ugxag4KRdds",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ccd29dbb7a90e1c675d9",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ccd29dbb7a90e1c675d9.vtt",
      "transcript_text": " Hi everyone. For all of those of you joining for the Designing a Retro Round workshop, please, we're going to ask you to seat according to which group you belong to. So the exercise that we're going to be doing in this workshop is, I'm not sure how many of you are familiar with retro rounds, retroactive funding rounds. Yes, I see some hands going up. If you're part of an ecosystem, like you are a protocol or you want to design a round to incentivize growth or different things in an ecosystem, we're going to ask you to please sit on the left side of this area. Everyone, please try to come over to the middle. If you are looking to design around for a community, say you're a local community and what you're trying to incentivize is growth or different goals within your local, geographically local community or community of a different type, we're going to ask you to sit on the right side. And this is because we're going to be making... You're right. So here. Yeah. Perfect. So communities over here, ecosystems on this side. And this is because we're going to have breakout groups. So this is going to be a very hands-on workshop. You'll be working or collaborating with two or three other people that are here to go through the process of designing a reach around. And it's best if you are sitting or partnered with those that are on the same wavelength of who your audiences are going to be, what are the type of values or behaviors that you want to incentivize. So that is why. Please, ecosystems to the left and communities to the right. It's your right. . Yeah, ecosystems is on the left, which is your left. So if you're, I'm sorry, I think... Oh, that's more... So ecosystems to the left and communities to the right. We're going to be handing out some post-it notes so that it's easier for you to brainstorm. Usual advice from workshops, one idea per post-it note is going to make it a lot easier to know what to do with these different ideas. Okay. Come in here. To the right. And I think this will also come clear and happy to go over any questions around seeding when we get to the breakout groups. Perfect. So before diving right into it, how many of you know what retroactive funding rounds are? Can I? There we go. Yes? Yes. Can I get a hands up from those of you that know what a retro round is? If not, because I'm just going to walk through it. Perfect. So one of the reasons why we've been experimenting with Retour Rounds as of late is the fact that it is meant to be easier than running a proactive grants program. In a proactive grants program, what you're trying to do basically is predict the future. You're trying to understand if people are gonna do what they say and promise that they are gonna do. You're trying to predict if this is still going to be relevant to your ecosystem or to your community by the time it is done. And you're trying to assess as well if the team that is doing it is going to be capable of doing it. So basically, we're trying to fulfill the role of a VC when we're not VCs, when we don't have that information, when we don't have the tools that VCs have. And this is why, or through this logic, this other mechanism came up and Vitalik came up with it, or right about it. Actually, not came up with it, but right about it. And the most relevant thing here is that in reacharounds, we expect that it's easier to agree on what was useful, rather than trying to predict what is going to be useful. And so retroactive rounds are a funding mechanism. You might have heard them as retroactive rounds, which is the latest rebranding that Optimism has given to this funding mechanism. But previously, and I think it's more widely known as RetroPGF, Retroactive Public Goods Funding Rounds. And this is a funding mechanism that has two sides of it. The first one is we have a results oracle. This results oracle can be a group of people. It can be a smart contract. But right now, all of the forms in which we've seen it have been groups of people that reward projects that they recognize as having already provided value, either to an ecosystem or to a community. However, we know that the design of this oracle is actually a complicated thing. So what we've seen as well, and what is expected of these retroactive rounds, is that we have them running over and over again because we're experimenting with the design of these rounds, which is also what we're going to be exploring today in this workshop. We're not expecting to have the best design from the get-go. We know that we're going to learn and get closer to this best design in an iterative way. We're going to be trying out different hypotheses. We're going to be trying out different variables. We will be engaging different audiences. And through that process, we expect to get to a better outcome of how these allocations are done. And I will mention that retroactive funding is not a new thing. This is not something that was invented in the Web3 space. We actually have institutions that have been running this type of funding rounds for really long times. And maybe I'm not sure who knows what this is, but this is the Nobel Prize, or like the insignia that is given as a Nobel Prize. And the Nobel Prize is one of the best examples of a retroactive funding program. People that are granted Nobel Prizes are people that have done research usually like decades ago and it's only until it has been proven that this research has been really meaningful to society that they get awarded a cash prize and a lot of other things that come with being a noble laurelate. But it's a similar experiment that we're running with retroactive rounds, in which we're looking at what has yielded a very valuable contribution to an ecosystem or to a community, and we want to reward that. Because we want to incentivize more people to have this type of behaviors of contributing to the communities or to the ecosystems. So one of the things that I suggest for all of those thinking of running retroactive rounds is this is an iterative process and it takes time. It is resource intensive. So don't try to run. It's not going to be perfect at the first time. It's going to require several iterations and you will need to take your time and going through this process. So this is a mechanism that was made popular by Optimism. Optimism has run already six rounds. We are currently, or we just closed, I think one day ago, the latest sixth round. Optimism, for those of you that don't know it, is an L2 that has committed to this funding model from inception. From the moment in which Optimism minted its token, they decided that the fees generated by the sequencer were going to be directed to this mechanism and they are going to be used to fund public goods and to fund projects that are providing an impact or benefits to the ecosystem. They've been also pioneering in the experimentation of this mechanism. So while they started with a small round back in 2021, in which they gave out about 1 million USD, and had a small subset of voters, this has continued to grow and evolve throughout the years. Their largest round was in 2023, in which they allocated about 30 million OP through different categories, and they had a set of about 140 voters. They've also been experimenting with other type of rounds like the latest fourth round in which they used voting style called metrics, like impact metrics, people voted and impact metrics instead of projects, but it's been going back and forth in between voting for projects or for metrics. And this is something that they've committed 850 million OP throughout the rest of the life. And this is only what was allocated at the inception of the token. There's also all of the funds that are being generated through sequencer fees that are going to continue with this, that are meant to continue this mechanism or funding this mechanism. It's important to note, though, that Optimism is not the only ecosystem running retro rounds. There's already been several rounds run by other ecosystems, such as Filecoin, that is currently running its second retro round. There's also Pocket, which ran its own round at the meet this year. Interestingly, also using the funds that they received from the Optimism Reacher round and funding that they received from Arbitrum to reward its own contributors. There's also the Celo ecosystem that ran a round earlier this year and is looking to run another round at the end of this year. And the LEAP P2P library or LEAP P2P ecosystem that has also run its own retro round to fund those that have been providing value to this library. And in addition to that, it's not only ecosystems that can use this mechanism, but we've also seen local communities using this mechanism to incentivize specific types of behaviors. So one of the really nice things about reach arounds is that what you're doing is you're creating a feedback loop of behaviors that you're incentivizing that are going to enable your community or your ecosystem to get to a goal, right? So while we can see that for ecosystems this may be growing your TVL, making your users more sticky, or growing the number of your users, we can also see that local communities that are trying to onboard more people into Web3, that are trying to become more regenerative, can also use this mechanism to incentivize this type of positive behaviors, and people will know that this is something that is going to continue to happen, and this type of behavior will continue to be rewarded. As long as these retro rounds are run, there is this expectation that these rounds will continue to run. So we've also seen groups such as Dow Drops, which ran one retroactive round as such earlier last year, whom they were targeting was Ethereum contributors, regardless of which L2 or which ecosystem within the Ethereum ecosystem they were contributing to and what their type of contributions were. We've also seen local communities such as ETH Colombia and Ethereum Mexico host their own rounds. And we've also seen other educational communities such as CryptoVersidad also doing this type of experiments to incentivize more people in particular behaviors that they find relevant to their communities. And so as I mentioned before, reach arounds are mechanisms. And there's five steps that I usually look at when designing mechanisms. And that I usually look at when designing mechanisms. And that it's good to keep in mind. Because these affect, like the steps, and as we scope them, affect the rest of the process. And it's also important to think of it as multiple steps, because this means that you can change the things and the sub-mechanisms that are part of the mechanism design, or in this case, the retro mechanism. So first, we have funding, and when we're thinking of the retro rounds, sometimes you don't need to worry about funding. Sometimes you're optimistic and you just have funding. But if you're a local round or you're a local community that wants to host a round, you need to think about where am I going to get the funding for running this round? Not only to run the operations of this round, but also where am I going to get the funds to give out to the community because they've done the behaviors that I want to incentivize. Second is the design of the round. And when I talk about design, I'm not talking about the images that we're gonna use or the funds or the colors that we're gonna use, but who the audience is, what is it that I'm trying to incentivize. All of this work that leads up to the round. And we also have the data aggregation. So based on the design that you're using, based on the things that you're trying to incentivize, and based on your audience, you're going to be able to determine what are the type of metrics and data that I can use to measure if people have actually, people or projects have actually generated value to my ecosystem or to my community. This is the why, the how, and the what. Then third, we have the decision-making. So what we've seen so far has been groups of people in different sizes voting on who gets the funding. But there's other ways in which we can have this decision-making process take place. This could be two different groups of voting, groups that vote. You can have one group that votes, then people review the vote. They approve it or decline it. And this is why I wanted to call out there's this decision-making process, which can be its own mechanism. Then there's a disbursement process or disbursement mechanism, which can be you're streaming the tokens over a certain amount of time, or you're giving the lump sum. And then you have really important post-mortem analysis. You definitely want to look at how your round went to understand if you did the right things when you were designing your round. And then in this workshop, we're going to focus on the design because this is the most relevant part of your round. If you don't get the design for your round right, you won't know if you're actually achieving the goal that you wanted to achieve. therefore you won't know if you're using your resources in the most efficient and effective way to achieve your goal which means you're not going to be able to evaluate if the program that you just ran actually made a difference for your ecosystem or for your community and if you should even think of running it again so why are we going to focus on the design and the scope first is we community and if you should even think of running it again. So why are we going to focus on the design and the scope? First is we have a lot of participants and a lot of audiences that participate in a round. Each of them have different incentives, but this is a game that we're all playing together, and we want to make sure that we're aligning everyone's incentives so that we're able to get to the goal. So we want to be specific also around what the incentives are that is going to be essential for us to know if it worked. Second, it's also going to help us improve the stakeholder management. This can be a very time-intensive process. So we want to make sure that people know what they are supposed to do, when they are supposed to do the things, and why they are supposed to do these things. If we're asking them to vote, if we're asking them to review projects, if we're asking them to provide specific data, we need them to understand why they are doing these things so that the retro round itself can be successful. Number three is we want to mitigate risks through the design. We want to understand and recognize what are the unknowns in this design, what are the variables that we're unable to influence or that we're unable to control in these experiments, and we also want to know what could go wrong and if there is something that we can avoid, that we can do to avoid it going wrong. And the fourth one is we need to optimize our resources, specifically for local community rounds. I know that ecosystems tend to have a lot of funds and this is also what enables them to experiment with this type of mechanisms. But this may be something that local communities cannot afford. Because they are either being funded one by one or they have a more limited runway. Which means that you need to be able to pick your battles in terms of where you're allocating your funding. You need to be able to clearly identify where should you be putting your time, where should you be putting your funds, and where should you be directing the attention of your community when you're running this type of rounds. You want to identify what are the most high impact areas that you want to be focusing your initial or your first iterations of the rounds on. And in terms of incentives, as I mentioned before, with this type of mechanism, we're looking at not predicting. We're not trying to predict the future. We're trying to assess what has already been useful. So we want to align incentives. So the hypothesis here with this mechanism is that people out there will build what matters to this ecosystem or to this community. And once they've generated this value, they're going to be rewarded for it. But if we're not telling them properly, what is it that this ecosystem needs. But if we're not telling them properly what is it that this ecosystem needs or if that is not clear and it's not of common understanding for the community, it's going to be very hard to have people building the things that are actually doing meaningful work for the ecosystem. And for example this is something that we've seen in Optimism and that we've seen in some of the other retro rounds, in which there's a lot of work that is being done, but this is not exactly what is needed. Or we see multiple versions of the same thing being replicated and not being used by the target audiences that people are trying to save time or improve their work. And this stems from a lack of communication and a lack of clarity in terms of where is the ecosystem headed? What is actually needed? What is still going to be useful in three, four, five, six months from now? So if we give this information, either as local community or ecosystems to the audience like to the community members we're going to be able to get them excited of contributing to the ecosystem and know that they will be receiving a reward and we will avoid having to like Have them lose faith or hope in this mechanism or process by building something that is not going to be rewarded because it wasn't impactful or useful. And as I was mentioning, this predictability of having these rounds is going to create reliability. You want to have your builders or the members of your community know that the round, there's going to be multiple rounds. So they are going to continue behaving in this particular way that you've set. Through this mechanism, what we're doing is we're creating new behaviors. We're asking people to actively change what they do with the expectation of a future reward. And this change in behaviors happens over time. It's not going to happen from one day to another, and it's not going to be cemented only after one round. So it's also important that as we continue to iterate on these designs and on these rounds, we make sure that we're not completely disrupting the expectations from the participants, because it's going to make it really hard to keep incentivizing the behaviors that we want to see in the ecosystem. So I'm going to stop very quickly there, because now we're going to jump into the breakout groups. Does anyone have any questions so far on to reach arounds? No. Perfect. So next thing that we're going to do is we're going to design our own reach around. Yes. Can I project? Okay. All right. Testing. I'm just curious if you could share some more examples of retro rounds, and what are the differences in the round as you change what you're funding? Like if it's software or if it's governance or if it's something else. Thanks. Yes. So depending on what it is, on who your audience is, and what their capabilities are, for example, in terms of, ooh, perfect, in terms of generating different data that you want to aggregate for people to vote, depending on how much funding you have, depending on precisely the things that you're looking to fund and incentivize, you're going to have different, one, different types of communication that you're having with them. There's also going to be, as I mentioned, different types of data that you're going to be gathering. So, for example, I designed and ran the Ethereum Mexico round. What we were focusing on there was this is a geographically specific round, and what we were looking to incentivize were five different things in the community. We wanted people that were prioritizing onboarding others into the Web3 ecosystem. So we were looking at an increase on wallets, regardless of which chain this was happening in. So I think this is also going to be one of the main differences that you might see from local communities versus ecosystem-specific communities. Sorry, ecosystem-specific rounds, in which ecosystem-specific rounds will look to increase the value to their own ecosystems, which makes sense in that it is their funding, and they have goals that they need to achieve within their own ecosystem to be successful. So you'd be looking at more specific metrics such as, as mentioned, TVL, number of users, number of transactions, transaction size, or if it's libraries, number of projects that are using this library within my ecosystem. Then on the community rounds, what we were experimenting with in the Ethereum Mexico community was looking at number of people onboarded into Web3. We were also looking at reducing the gender gap. So number of projects that were specifically targeting onboarding women or supporting women in the Web3 space. We were also looking at refi communities in Mexico. So looking at how many had interacted with local regenerative projects, how many of them had onboarded local regenerative projects, or use cases in the refi space. We were also looking at education, so partnerships with universities. And these were some of the type of behaviors that we were trying to incentivize in the long term as well so for us let's say that an ecosystem would be pushing for a growth on number of users in their own ecosystem we were pushing for a number of new users or new attendees to Web3 events. Because our hypothesis, and this is also where the designing and understanding, well, what is your North Star and what is your goal, is really relevant. Because what we are trying to incentivize in the long term is we want to have more people educated and participating in the Web3 space. That would be one in terms of onboarding. Second, we want to see more women integrated into the space. So allocating funding to that as well. I would say those are two of the main difference that we've observed. I know as well that I believe Ethereum Colombia, one of the things that they were incentivizing as well is number of people running their own nodes. So how people are actively participating and securing Ethereum. But those are some of the things that are stark differences. And I think also another thing that is very dependent on whether it's an ecosystem or a local community thing is local communities are very different from one another in terms of the things that are needed in one country or one city are going to be very different from what's needed in a different country or a different city. And how we framed as well the round that Ethereum Mexico ran is we want to fund the edges where impact is happening. And this impact might not be clearly understood by someone that is very far away. So say even if this is impact that they are onboarding everyone into optimism, this might be very difficult for optimism to recognize from such a far away point of like, oh, why is this relevant in Mexico in this type of scenario? But this is something that local communities, if they are the ones running the rounds, if they are the ones designing and determining what is impactful in this specific situation or in this specific reality, they're going to be better able to identify and understand why this was impactful and why it should be rewarded. So I also think that one of the big differences is how close you are to the impact that is being generated so that people are able to identify it and fund it. That's a very broad spectrum. Hello. So I have a question. I had a question about how you can measure the post-mortem analysis. And I was thinking, like, should that be done? I was thinking about how an ecosystem that runs retro-PGF rounds can make the post-mortem analysis, and I was thinking this is a whole design space on how you can do that, how you can measure how much you incentivize the project from the previous round compared to the next round after one year. So, yeah, my question is how, for example, the analysis of previous Optimism rounds happened. Perfect. So for each of the optimism rounds, there has been hypotheses that are set at the beginning of the rounds in terms of lessons learned from the previous rounds. with, it's sort of like, we are, say, now we're going for impact metrics-based voting, and we're doing this because we expect that people are going to have an easier time deciding which type of metric is more relevant to them. They won't have to, we're going to be reducing time because they're not be reviewing each of the projects. And then once the voting comes to an end and allocations are done, there's usually also an exit survey that is sent. And all of this data is then aggregated to understand, do we, I think this one has been more of a sentiment analysis, and like, do we feel confident that the allocation was done better in comparison to round three. There's still room, a lot of room for improvement, to be quite honest, in terms of analyzing the postmortem of the rounds, specifically when it comes to the impact that funding has had on the projects. I think for some of the retro rounds, this is still to be done. And there are some, I think Open Source Observer has done some in terms of one year after Retrip BGF3 to have projects continue to build on optimism and continue to ship and generate value in the ecosystem. I think that's one good approach. Something else that I know, like Sejal and myself and others have been exploring is, can we create counterfactuals to understand if this injection of capital or injection of funding to the projects actually made a difference? If we were to look at this in the future, had they never received funding, would it still look the same? Which can help us understand if this actually makes a difference for this type of projects or in this type of situation. But I think that is still pretty nascent. And not a lot of it, yeah, I know that not a lot of it or none of it has been done yet. OK, perfect. has been done yet. Okay. Perfect. So... Okay. So we're going to get into breakout groups of three to four people. They can... Let's see. I think five can also be good. But if you can just get together. So... Perfect. I think we have three here. CJAL is going to help us as well in terms of putting together the groups. Do you want to get together? Hello. Perfect. Do you have things to write with? Yeah. Okay. Perfect. And what we're going to be doing is we're going to go through a set of prompts. And the first one is we're going to go through a set of prompts. And this is going to be the process through which we design a retroactive round. You can choose whichever, whether you want to design a retroactive round. You can choose whichever, whether you want to design it for a community. You can choose whether you want to design it for an ecosystem. For example, I see the Gitcoin team here. Maybe they want to design a retro round for members in their ecosystem. I see different people from LATAM communities over there. Maybe they want to design it for a Latin American community. Something that Sejal and I were also brainstorming earlier on this workshop is someone, and it might be an experiment that Sejal and I do right now as well, is what if we were to design a reach around for DEF CON? So anyone can take that prompt up as well. And so the first prompt is we're going to ask you to introduce yourselves in your group in case you don't know each other. Please make it short because we are going to have eight minutes for this first prompt. So introduce yourselves and choose your adventure. Choose who are you designing this retro round for. Is it for a local community? Is it for an ecosystem? And just agree with the other members of your team which audience you're looking to engage with. Is it going to be, again, a local community? Is it going to be an ecosystem? And if you have any questions throughout this process, please raise your hand and Sejal or myself will walk to you and answer any questions that there may be. We're going to have eight minutes for this. So please introduce yourself, share why you're in this workshop, and if you're part of a community or an ecosystem, and then decide who are you designing this round for. Go. Yes. If you have any questions, just raise your hands and Sejal or myself will come. I feel like I'm focusing on something. Oh, yeah. I'm going to take a picture. Can we focus in Mary? Can we carry on something? I feel like I'm focusing on something. I feel like I'm focusing on something. Thank you. ¶¶ Thank you. Hello? Thank you. Thank you. Thank you. Terima kasih telah menonton! Thank you. Thank you. We have one minute left, so please come to a decision of what adventure you're going to follow for the next prompt ¶¶ And time is up. So please, I hope you've written down either in one of the post-it notes or somewhere else. What is the adventure that you're going to follow if you're going to be designing this retro round for a local community, or if you're thinking of it as a Web3 ecosystem or Ethereum as a whole or DEF CON. So the next prompt that we're going to be looking at is based on your groups and based on who you've decided to design this round for, you will be defining what is the North Star that you want to achieve. And here is what the North Star is. What is the end goal that we want to reach? What is the end goal that you want to reach? What is the end goal that you want to reach within this community? What is the end goal that you want to reach within this ecosystem? And it's important as I mentioned before to know this because if we don't know where we're going then we're not going to be able to tell if we've gotten there or how we're gonna get there. And this means we're not going to be able to measure what's the type of data and what are the type of metrics that would be useful for us to leverage when we're deciding who should be funded. And we're also not going to know what are the type of behaviors that we want to incentivize for people to follow that are going to get us closer to this hypothetical goal. And I will incite you as well to think of this not as a short-term goal. It could be a short-term goal, but it's better to think of it as a long-term goal. Like if you're doing this for a local community, how do you want this community to look 50 years from now, 60 years from now? What is this like bright future that we're aiming for? And as I mentioned before, you can have it be a one-year goal. I know that a lot of the ecosystems plan in more short term because it makes sense for what they're trying to do. But try to aim for this long-term vision. Even if you're an ecosystem, we want to play infinite games, which means we're incentivizing behaviors that are going to be healthy for what we're trying to achieve in the long run. And so the second prompt is define with your team members what is the North Star that you're going to be fulfilling for this either local community or ecosystem. What is the overarching? And here are some questions that will help you in defining this North Star. And here, feel free to use the post-it notes. And remember, one idea per post-it note. That's going to make it a lot easier when you're looking at the different ideas that you have. What is the overarching objective that you hope to achieve by running this round as well? Not only this round, but like in the long term. And how does this round, how can this line around with this overarching goal? Then what is the long-term vision that you have for your community or for your ecosystem and what are going to be your goals for this round? And so for this one we're gonna have about 10 to 12 minutes. I think it might be too much but we'll be going around and if it is too much we'll just cut it short so go ¶¶ Thank you. Vielen Dank. Thank you. Yeah, and we just had a really good question on whether it's like, oh, should we just have one North Star? What I would suggest is each of you write what are some of the things that would be very relevant to have in this round. Write it in a Post-it note, and then you share them, like you paste them in the middle for everyone else to see, and then people can either agree or disagree with the things that you've identified are valuable for there to be, or that could be the North Star that you're going after. Thank you. Let's pray. Amen. Thank you. There we go. And I think it's a flex. so Thank you. © transcript Emily Beynon I love you. Thank you. We have one more minute. So please start gathering your ideas and choose what the North Star is going to be for your round. We're done. And we're up to the next prompt. So now you've already defined what is going to be the North Star for your community or your ecosystem. And the thing that you're going to have in mind as you're designing the rest of the steps of the richer round. Next is we're going to identify who are the audiences that are involved in these richer rounds. Who are, one, the projects or people that you expect to fund through this round, it's going to be either the projects that are applying or if you're not asking them to apply, you already have a way in which you identify who's going to be eligible to participate in this round. Who are they? What do they look like? Are they individuals? Are they individuals? Are they groups? Second, as I mentioned before, most of the retro rounds as we've seen them is people vote on who gets allocated the funds. So who are going to be your voters? Is it going to be everyone that belongs to the community? Is it going to be a selected group of people that are very and have a lot of high context into your ecosystem. And third is you're also going to identify who is benefiting from this overall impact that the people that you're funding or the projects that you're funding are generating. Who are the end users or end receivers of the impact that is being generated? And map out who these audiences are so that you can understand what are their incentives, what are they trying to achieve, whether it is within your ecosystem or within your community, but what moves them forward and what are they trying to achieve so that you can then continue to design something that will ensure that you're aligning their incentives with the incentives that you're trying to achieve through this round. And here there are some questions to guide you through thinking through this. The first one is, oops, this is not, OK. So apparently this slide is wrong. But here it is, as I mentioned before, identifying who's the audience in terms of who are benefiting from the impact that's been generated, who are the people that would be voting in year round, and who are the projects or people that would be receiving funding through this round. And for this, we are going to give eight minutes. Let's start. Vielen Dank. Thank you. Amen. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. And And the time is up for this next prompt. And we're going to move to the next prompt in which we are going to be looking at setting behaviors and defining what these behaviors are. So what we are, as I had mentioned before, in these retro rounds, what we're doing is we're incentivizing very specific behaviors that we want to see, that we have a loose idea that are going to take us to this North Star, that are going to enable us to get to these goals. So for this next prompt, what you're going to be doing is you're going to be thinking about what are the type of behaviors that you want to see, the type of benefits, the type of recognition that you want people to have or you think people will appreciate from participating in this type of actions that are going to get you closer to the North Star. So think also about what are some of the milestones that are in between where you are right now and getting to the North Star so that you can identify what are the type of behaviors that I'm trying to incite for people to do that are going to get me closer. If you're looking at an ecosystem round, maybe this behavior is contributing to the open source code. Maybe this behavior looks like deploying smart contracts and attracting new users into my ecosystem. If we're thinking of a local round, if one of the things that you're incentivizing or you value the most is, say, protecting the forest, maybe the type of behavior that you're looking for is planting trees or protecting trees that are already in a specific place, taking care of the trees. These are protecting trees that are already in a specific place, taking care of the trees. These are the type of behaviors that you might want to incentivize that people have or do as you're going through, and that is going to be rewarded at the end of the round. So we're actually going to give, I know it says 12 minutes, but we're only going to give eight minutes for this part as well. And you can start now. And try to be as specific as possible when you're thinking of the behaviors that people are going to have. Because this part is going to help us to identify what is the data and what are the metrics that we can use to measure the type of behaviors that people have already completed and that we could reward in the round. So again, sort of like if we are looking to protect the, like if it's a local community and what they want is to preserve a particular area, then maybe we're going to be looking at planting trees or having a more healthy ecosystem. And then how do we define what a healthy ecosystem looks like? Is it the number of trees that are in a specific space? Is it the type of behavior that people are having when going around? So this is what we're going to be looking at. Thank you. Thank you. ¶¶ ¶¶ Thank you. um ¶¶ Thank you. Gracias. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay, we have one more minute before we go to the last prompt. Okay. We're going to move on to the last prompt. Since both, since the groups are already done with this prompt. And as I mentioned to both of the teams, the way in which you're designing your round is going to inform the evaluation process that that you're able to already extract from existing sources or that you need to create. The best thing there is to create it in a standardized way so that people can cross-compare it amongst different projects. So for the last prompt, we're going to be looking at data and metrics for this evaluation. And there's two actions here that you're going to be looking into. Or questions that you can use to guide your process through. The first one is, now you know, based on the behaviors that you want to see happen from these different audiences, what you're aiming for, and what are the potential actions that the members or people or projects can undertake to get to the North Star, how can you measure these actions? Like over here, we were talking about, oh, we want to have more developers building in this particular ecosystem. And we talked about developing looking like launching their own smart contracts. It could look like contributions into the githubs and into the repos from the different libraries or tools that are relevant to the ecosystem. Then you can also explore, are there already existing data sources for this information? And if there are not existing data sources for it, how can we measure this in a standardized way? And we're going to give three minutes for that. And then we'll start wrapping up. Thank you. Gracias. Thank you. Is it moving forward? We have one more minute left. Okay, so you were talking about . GitHub repo commits, stuff like that. And we were talking about maybe that's spammable, it's not good, and maybe we should look at the quality of the codes. And I was like, how do you even standardize that? Is it all right? It's very subjective. It's the same level we have in OPP. Exactly, no, but it is subjective. I think the good thing, though, is how can we do subjective evaluations to become as most objective as possible? And that can look by, like, for example, if it's very, very subjective, maybe you have five different things within the code that you're reading. And then that helps, like, reduce the subjectivity of it. But I'm reviewing it to a degree. No, you're welcome. Perfect. So we're coming to the end. And I think now what would be really interesting, this was the last prompt. And so as I mentioned, this was a taste of what the design phase looks like when we're looking into mechanism design. The second, there's several other steps that would follow, but right now we only focus on the design space of the problem. And so we're going to just ask real quick the two teams if you can share what your round, whom you were designing this round for, what is the type of incentives that you wanted to generate, what was going to be rewarded and who the audiences are. We have very limited time, so if you can explain it in like one minute or two, that would be great. Hello. So retrofinding the round is for ecosystem. Specifically, it's unique swap ecosystem. And the round we were thinking about is the hook round that they just released. So the goal would be to increase the builder that build on the Uniswap hook ecosystem, increase the numbers of high-quality hooks, or build hook tools, making it easier for developers to work on it, and also like onboard builders on the ecosystem. Some of the audience would include Web3 builders, educators who help and also the benefit, this will benefit like the Web3 developers, Uniswap users, Uniswap token holders. And who will vote on this? We're still debating on this, but we think probably like the Uniswap token holders. But yeah. Yes, so the behavior we wanna see is we want them to build on the Hoke ecosystem, them to deploy the smart contracts, and also we want some educators, content creators to create content for Hoke to spread the information to help onboard people and also yeah and people to participate in voting and stuff like that perfect thank you and so please I know we're missing the last group but I don't think we have time for that anymore as we're well just make it really really really brief. We still have two minutes, but this is the QR code for the group. Please feel free to join as well, and we can continue exploring what these designs look like. We designed retrofunding, retroactivity funding to incentivize more women developers to onboard to Web3. que nos incentiva a crear o tener más workshops para esta audiencia específicamente. Y la audiencia o el financiamiento serán de personas que pueden ser individuales y también comunidades desarrolladoras. Y los comportamientos que estamos buscando son, primero, communities. And the behaviors that we are looking for is first onboarding more women with workshops, focus on them. And also more women contributing to open source projects. In the milestone, we have increased women attendance by 25% in six months. And in Greece, and then have more women meetups during a year. Okay. Thank you. Awesome. Thank you. So please feel free to share the designs in the Telegram group. Thanks a lot. Sejal and I were really happy to host this workshop, and we're excited to see more people approaching mechanism design in a more methodological way so that we can have better outcomes for how we're allocating funding in this space. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1GTU723iYMOTD9COHjYQdSKNFi7gSZc88-BnP7Co9jE4",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "enhancing-ethereum-p2p-network-security-through-fuzzing",
      "sourceId": "7SR77E",
      "title": "Enhancing Ethereum P2P Network Security through Fuzzing",
      "description": "Security is a big deal for Ethereum's p2p network. We think fuzzing is a great way to make it more secure. We developed a time-series-based fuzz testing tool for the Ethereum network layer. In this tool, we integrated mutation mechanisms and seed selection algorithms, and introduced a new time-series feedback model. Using this tool, we can spot and fix existing vulnerabilities while also spotting new risks.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "network,p2p",
      "keywords": "Fuzzing,p2p network",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "318caf8e3c3d2cefe58bdf4b84464ca84773412519e1ec72bf190e1abd87608d",
      "sources_youtubeId": "e7T7naJRWVg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T08:10:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1B-0SsGH9Jbgo3njphxqoa7CInPi0Ftq_r5Ivuuvi8zg",
      "resources_slides": "https://drive.google.com/file/d/1lGEiPRa3DHZxDl4uHSdhOHp-sgxkXeLJ/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "ethereum-real-world-economy",
      "sourceId": "JSYMFD",
      "title": "Ethereum Real World Economy",
      "description": "Ethereum’s role as universal settlement layer is growing fast. Tradfi companies like Stripe are building on-chain, while native projects like Polymarket are increasingly impactful in the real world.\r\n\r\nThis panel will debate the future of “Real-World Ethereum”. What does that mean? How do we maximize growth opportunities while avoiding capture? What can we learn from history? How do we best compete, and how do we ensure Ethereum values as we power more and more of the world outside crypto?",
      "track": "Real World Ethereum",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,Use Cases,e/acc,case,use,e/acc,Ethereum Roadmap,Use Cases",
      "keywords": "stablecoins,real-world-use,use-cases",
      "duration": 3314,
      "language": "en",
      "sources_swarmHash": "0744b811712e4fc06596318988aea1107a479258a4a14e81c272d0eefccfc715",
      "sources_youtubeId": "3A0b9y7OmUI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736bb429dbb7a90e12ebbde",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1UVP1zLQ1cszDLmjMKl61KN2rP616jJTze1YhwSPWhms",
      "resources_slides": "https://drive.google.com/file/d/1LhECBOcENoagT8fgXyxbFrerlrSlKZd9/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "mood-uplifting-singing-bowls-handpan",
      "sourceId": "H7Y7L8",
      "title": "Mood Uplifting (Singing Bowls + Handpan)",
      "description": "By Most Handpan X Ice\r\nThis session fills you with positive energy, boosting your mood and clearing your mind.\r\n- Lift your spirits with the bright sounds of singing bowls, handpan, and soft percussion. \r\n\r\nNov 14 15:00 - 15:45",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T08:45:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/1vnIacRdbAcvTa2ioFdaqS_vlSqjDw2GnNcAukvszKyw",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "optimize-zkevm-throughput-series-ii",
      "sourceId": "HRDW3R",
      "title": "Optimize zkEVM throughput: Series II",
      "description": "There are different ways to optimize the zkEVM, the one exposed in this workshop is through optimizing the zkASM (zk assembly) code itself so that it consumes fewer counters for the same execution.\r\nThe first 40min of the workshop is a deep explanation of the zkASM language, instructions, operations, counters, build... And the rest of the time we will be live coding and explaining in detail two optimized core functions of the zkEVM so that attendees can appreciate the before and after optimizing",
      "track": "Layer 2",
      "type": "Workshop",
      "expertise": "Expert",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZK-EVMs,EVM-equivalent,ZKP,l2,EVM-equivalent,ZK-EVMs,ZKP",
      "keywords": "L2",
      "duration": 4575,
      "language": "en",
      "sources_swarmHash": "b05317cd28522296f044ca89921a6b99f8b13dc344f61b21056c1ee972f4682c",
      "sources_youtubeId": "EdUfOvoIhNc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c86b9dbb7a90e164abae",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1j-dXA_XZk45fwe4mOSLfaBUXA0DVQTMQ1GLhESBsAZM",
      "resources_slides": "https://drive.google.com/file/d/1ybYlkLTrFXcumfQdFQss_tgZqE37kxSv/view",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "permissionless-p2p-with-the-waku-network",
      "sourceId": "N9WRM3",
      "title": "Permissionless P2P with The Waku Network",
      "description": "This workshop will be oriented around showcasing how p2p networks are pivotal for dapps and just Privacy oriented applications. We will show how Waku can be used to strengthen many concerns about censorship resistance and decentralization. Another section of workshop will be about conscious choice of tradeoffs and those that are present in Waku or any other p2p network. We will try to leave you with some patterns that can be implemented into your daily development and reasoning.",
      "track": "Cypherpunk & Privacy",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Privacy,DePIN,infra,p2p,DePIN,Developer Infrastructure,Privacy",
      "keywords": "p2p,infra",
      "duration": 3588,
      "language": "en",
      "sources_swarmHash": "cd279ae97ff4fd6476822817592ef641d9cb81d5c24f41c0b18dc70c195e9d17",
      "sources_youtubeId": "QbpNrcD0MvI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735bec29dbb7a90e1c39d62",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1-0QAKQAwAZ11MiH9PyyPFFxZJJ76rz1xsmKj_FWlbEM",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "web3-poetry-day-3",
      "sourceId": "GN8LTB",
      "title": "Web3 Poetry - Day 3",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:00:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/16EbmLxT3rCfmlW9Mc5CErRzSrp05fgvj7stvb6H3iaY",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "whats-in-your-dose",
      "sourceId": "BRUGUL",
      "title": "What's In Your Dose?",
      "description": "Pandemic responses require robust technical tools such as molecular diagnostic tests, novel immunization reagents, and recovery surveillance tools.  Pandemic responses depend on public trust in these tools and their good faith deployment.  Verification strategies to enhance public trust and cooperation will improve the performance of molecular tools in future pandemics.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Public good",
      "keywords": "Molecular,Biology.,,Public,Health.,,Public,Trust.",
      "duration": 907,
      "language": "en",
      "sources_swarmHash": "be8d4b2923608f8527fd4ad82d690569c33c76afabc4fce1d2968e8cd0993e26",
      "sources_youtubeId": "F_SxA6W5hzQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735b9069dbb7a90e1872a41",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735b9069dbb7a90e1872a41.vtt",
      "transcript_text": " So thank you all for inviting me to come and speak to you. I want to talk to you a little bit about some of the molecular biology work that's going on in my lab at the University of South Carolina, but I'm going to talk a little bit about the philosophical implications of how pandemics do more than just harm individuals. They actually harm the social trust that exists between individuals, and a proper pandemic preparedness plan has to have in place widgets that can deal with the virus, but also information management schemes so that we can prevent the social harm that happens to societies by the loss of trust that naturally occurs between people when a virus is creeping through and harming people and you don't know what is happening. So you're going to see that this is a tour through some molecular biology work, but I want you to notice ways that information management and blockchain kind of technologies can make our societies more robust to future pandemics. So early on, I and several other people noticed that the SARS-CoV-2 virus had some really weird characteristics. If you do an alignment of this virus against its most recent common ancestors, you find out that it has a lot of mutations that are what are called non-synonymous mutations that cluster in the receptor binding domain. It looks very engineered. It's a circumstantial evidence, but it looks like it had help. If it didn't have help, it's a remarkable coincidence. Nonetheless, we made a bunch of PCR primers. Regardless of where it came from, we made PCR primers and started developing a test in our laboratory just so that we could figure out who had it and who didn't and maybe keep the University of South Carolina functioning properly. We made this kind of a PCR test with our own primers and protocols and proved that the PCR really will detect the virus and it'll detect it down to single molecules in the well. This PCR assay does not have a false positive problem. You can cycle forever on a negative sample and it will never fire, but it can accurately determine how much virus is in any sample. We then kind of accidentally discovered that raw saliva works better than the nasal swabs. We discovered this for a funny reason during the pandemic lockdowns. We ran out of nasal swabs and ran out of RNA purification reagents. So we just tried it on raw saliva and it turned out that it worked fine. It worked even better than the standard procedures. And it's kind of funny given all the gatekeeping that has gone on about nasal swabs and the type of swabs that you have to use and all that sort of stuff. You don't even need any of that stuff. I tracked the virus as it went through lots of individuals, students at the University of South Carolina, and families. And I could prove to myself that the virus was real and that the PCR protocol was actually predicting illness. This is a family here where some of the people got it and some of them didn't. And the PCR signal predicted perfectly who was going to get sick and who never did. This family on the right, that's me. And the blue curve is me whenever I brought it home and gave it to my wife and two daughters, and then they got sick later. So I had access to validating, verifiable information that the PCR results were tracking with symptomology. Most people didn't have access to this kind of verifying information, and after a while, they kind of became suspicious of, hey, is this thing really telling me the truth or not? And they didn't know anybody like me that they could ask to see, is this real or not? Another kind of interesting phenomenon about this virus, which probably we'll see this again, is the asymptomatic super spreader phenomenon. Now, you may have heard about this. I know it's real. This guy here was the brother of one of my students that was working in the lab. And we were testing everybody every day as we were developing this test. And we were using all of our friends and family members as negative controls. This guy blew a positive and he had more virus in his saliva than there are people in China. But he had zero symptoms. And on day three, he went on a seven-mile run and clocked his normal time. But this guy could infect the whole world if he tried, and he would never have known about it. So this phenomenon is actually real. There are silent super spreaders that are walking among us, and reasonable people grew suspicious about, hey, maybe this PCR is bogus. Maybe this is lying to us or something like that. Because it's not really working when we're trying to just quarantine people based on being sick. And again, future pandemics need to respond to this kind of situation in a way that preserves social trust. We need to plan on this as being a real thing. And I think that the information blockchain might be one solution to this problem. Here's a result of a whole bunch of positives that I'm showing you from just one month of testing. We did like 300,000 tests over a two-year period, and this is one month of positives. And I want you to notice the distribution of CT values. In PCR space, it's minus the log of the CT value is kind of related to the log of how much starting material there is. So these very low CT values down here at 10, they have a lot of virus. And these higher CT values around 30 or 32, they have a small amount of virus. Those people at the bottom in that circle have a billion times more, it's a log scale, they got a billion times more virus than the average. Those are our super spreaders. And that's probably what happened in the Wuhan market. Someone was infected and didn't know it because they were an asymptomatic super spreader, but they had enough virus that they could infect everybody that was shopping that day or that week. This happens. It's a very sneaky virus and it causes mistrust to develop because people are getting sick, and you don't know who they were exposed to. Are they somehow worshiping the wrong god, or are they in the wrong political party, or are they guilty of wrong think? And then the natural human brain's tendency to turn on each other takes over and it fractures social trust and society begins to deteriorate. This is what this really, really happened during the pandemic and we're still dealing with the consequences of it now. Okay, a few words about the mRNA vaccine. The mRNA vaccines came on the scene. They were deployed under an emergency situation. We didn't have time for public trust to develop organically. We had to go for broke because it was a dire situation. Initially, it was reported that these vaccines were 90-something percent effective at preventing infection. That only lasted for about a month. And then they became very effective at preventing infection. That only lasted for about a month, and then they became very ineffective at preventing transmission. They do have what appears to be durable protection from death about 80% of the time. It does reduce the probability of dying. This is still hotly debated because people don't trust the information that they're getting from various sources. We need ways of having raw data being uploaded to some sort of an information blockchain so that people can see how the data is accumulating and not be suspicious of whoever is gathering the data so that they can come to their own decision about whether this is real or not. Anyway, there's a lot of side effects that are unexplained and they tend to be flippantly dismissed, and this causes more distrust. I use a lot of sequencing technology in my lab. We're gene jocks. We're cancer gene jocks. We sequence things all the time, and we can discover mutations or DNA sequences that we didn't know we were looking for. I did this on vials at a vaccine, okay? This is basically an internet dare that somebody put me up to. Somebody else had done this, and I said, ah, this is crazy. So I did it myself, and lo and behold, I discovered a bunch of DNA pieces in the mRNA vaccines. I thought, well, this might be the explanation for some of these side effects that people are seeing. We should check into this and see if these little bits of DNA are getting into the human genome of vaccinated people. The public deserves to know what they're taking. If it's a vaccine or anything else, the public deserves to know what's in it. And molecular biology tools can do this for you. This is kind of the sequencing result that came out of my lab using a modern, very cheap, deployable sequencing platform. I'll tell you about it later if you ask me about it. This technology, you can use it in the field if you want to. You could sequence something in the jungle on your laptop. It really does work. Anyway, these little pieces here, all the little pieces of DNA we discovered in the vaccine, there's a lot of them. And using this technology, you don't have to know what you're looking for and you can see what's in your saliva, what's in your vaccine, what's in your food, what's in your environment. All these things can be answered by sequencing, but public trust can only be generated by making this data on some sort of a public ledger and it be collected by multiple sources, and everybody can just inspect it to see what's there. Think of this kind of data as a form of a contract, you know, or some sort of a blockchain of currency that you might want to generate public confidence in. So we are doing some studies now where we're monitoring the genome integrity of vaccinated people just to see if this stuff is ever getting in there. I made some PCR primers against the DNA sequence that I found. This is what real-time PCR looks like. In case you want to know, the cycle number here at the bottom is how many cycles of the machine does it take before you start seeing product. And the more target was in the well to begin with, the fewer cycles it takes before you start seeing stuff. And the less product is there, the more cycles it takes. So that's why it's backwards and it's a log scale. But we can quantify down to the molecule number. We can literally count molecules in any sample with this kind of technology. And it doesn't suffer from a false positive from too many cycles. It's reliable. So we did an experiment. Can the little pieces of DNA that are in the vaccine ever get into the human genome and modify it? I know this was theoretically possible, and I said this publicly about a year ago. Well, we did an experiment to prove it. We took some normal human colon organoids. These are normal cells, and we vaccinated them in the lab by just putting the vaccine in the media and let it incubate for a little while and then washed them and then grew them and washed them over a month and kept washing them and passaging them. And then we did the PCR on the genomic DNA. And lo and behold, there's stuff there. And it's the exact same frequency that I predicted about a year ago. About 1 in 1,000 to 1 in 10,000 cells have taken up different pieces of this vaccine and it's a permanent fixture of their genomes now. This is not surprising based on molecular biology first principles. We've been doing this for years in the laboratory. This proves that it happens with this contaminating DNA, which is why I was so weirded out about this stuff when I first discovered it. We've been looking at a bunch of tumors from cancers that have shown up since the vaccine rolled out, and we just looked at like 50 tumors so far that were not really selected for anything other than the fact that they developed in the last three years and we've got a couple of them that appear to be positive for bits of this DNA. Now we don't know if this is a driver event or if it's a passenger event and we're doing more investigations to try to figure out where it landed and if it has anything to do with the tumor or not but it does happen so this is what we need to the public needs a careful accounting of this so that their confidence can be, or lack thereof, can be appropriate and based on data, not based on marketing or anything like that. So a final couple of words about another kind of genome modification. It's called DNA methylation. So your genome has methyl groups added to cytosine residues that's CpG loci in your genome and these modifications are part of a developmental program that's been going on since you were first conceived and continues until you die where you slowly change the methylation landscape of stem cell genes and eventually those stem cell genes begin to be silenced, and you lose stem cells as you grow older. Aging is just a natural extension of the developmental process that goes on in utero, and you eventually just run out of stem cells, and you die. We can tell how old you are by looking at the methyl groups on lots of loci in your genome. We've known for a little while now that when your DNA methylation age is disconnected from your physical age, especially if your DNA age is too old, you're at high risk for all-cause mortality. This is really a thing. And there's a lot of interest in finding out what kind of things or interventions can I do that will make me younger, right? That will delay aging. And we can have surrogate markers for this with this DNA methylation aging. The same sequencing platform that I was telling you about earlier will measure methylation on the fly at the same time we're figuring out sequence alterations. So, you know, sequence analysis of normal blood cells can agnostically monitor for sequence changes and methylation changes and figure out if some sort of an intervention has altered the methylation age. All right, so I did this on a sample that was just available to me. This is me pre- and post-vaccination, and there are spots in the genome that the methylation age changed. This is a real thing. This is one sample, so it's scientifically meaningless right now, but it's to prove that this is something that is worth doing, in my view, to monitor what happens to people pre- and post-COVID and pre- and post-methylation to see are these interventions helping our methyl age? Are they harming our methyl age? Once you know what you're looking for, it's very cheap and easy to do it. Final thoughts. Pandemics stress test the trust that people have in each other. Low-trust societies fare worse than high trust societies. Molecular biology tools can measure these things, but molecular biology tools can best increase public trust and future pandemic preparedness if we make the information available to the public so that anyone can see it. We should do this now. And I'll stop there. Thank you for your attention. Thank you so much, Philip, for one.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:05:00.000Z",
      "slot_end": "2024-11-14T08:20:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1RgW3g8Dx3KqmsQIkx6vtDH-Q1Sykokl4An1TOH01ltI",
      "resources_slides": "https://drive.google.com/file/d/1slAuBmp5HLngSMVxEP9X-arUQ8NbW_6e/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "blockchain-and-the-economy-of-online-information",
      "sourceId": "LCU3FW",
      "title": "Blockchain and the economy of online information",
      "description": "Blockchain technology started as the ledger for Bitcoin and bloomed as the backbone of DeFi. As a database, blockchains are ridiculously slow and expensive. Where it has the potential to reshape the internet, is in nothing else but the economy of information. So let's take a look at the economics of the information that we consume online, and see how and where blockchain technology fits in.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,Digital Sovereignty,Regenerative Applications,digital,space,Decentralization,DeSci,Economics,Mechanism design,Use Cases",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "b6c47db46047deeab07b894d50997db7f6ccbcd1dfd8e4b67dff71704cb29e09",
      "sources_youtubeId": "AZ7ctYLxstQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:10:00.000Z",
      "slot_end": "2024-11-14T08:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1OD0-nwsjhb5qQwxWCHD0G1UeVtcNpvNFHNOz8Qb2e8U",
      "resources_slides": "https://drive.google.com/file/d/126QEmd6RTO8l43d1tr5b4IvA7ib-xjEM/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "optimizing-full-node-costs-with-monitor-tools",
      "sourceId": "D9UAVG",
      "title": "Optimizing full node costs with monitor tools",
      "description": "Running a full node is a fundamental component of participating in a decentralized network. However, the operational cost associated with running a full node can be prohibitively high, even for an archive node, it needs a lot of CPU/Memory and SSD disks. At our organization, we have successfully implemented a cost reduction strategy by using the pprof tool, along with grafana and prometheus in our node infrastructure.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Developer Infrastructure,Best Practices,service,level,improvement,Architecture,Best Practices,Developer Infrastructure",
      "keywords": "performance optimization,service level improvement",
      "duration": 389,
      "language": "en",
      "sources_swarmHash": "f6620ea38e11b7ab65c0402392377e73334050231b49731c9ab35dac21d1a8c0",
      "sources_youtubeId": "-ZCcYVuEKVM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f8d01b0f83434dbd1a8a",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736f8d01b0f83434dbd1a8a.vtt",
      "transcript_text": " Hello everyone, this is JS Visa and I come from Amber Group. Before this project, I want to introduce Amber Group. Amber Group is a leading digital finance company and in the meantime Amber Group is also holding Amber AC which is launched for the BuildQuest Web3 innovation challenges. And I'm the DA leader of the Amber Group Web3 security team and besides of my job I'm also an independent Ethereum ecosystem contributor and I'm also managing execution clients for Gaia, REST and ELEGAN. And here's a picture about my contribution over the past six years and yeah I'm continually contributing and let's continue contributing. So today I will introduce a small piece of our fund in the history, which is, let me show you what we found in the monitoring panel. You can see there are some abnormal points in the picture, and so we just found that maybe there is something wrong, and so we need to find out what's wrong. It is really very hard to find out which process indicates abnormal points. But we are lucky. We found a project, a process that uses WebSocket to connect with the guest node and pull some data back. So we found this process, then we just pip of it. This is a piece of the proof frame graph and in the picture you can see there's many CPU used to do this module and yeah very much. So we found this maybe there's something wrong in the process, maybe in the client or in the server. And luckily, we finally found there's something wrong in the server. So what's the problem? We just write some simple code to reproduce the issue. So in the picture, you can see we have two structures. One is the load result and the other one is the interface result. And the load result which uses JSON.loadMessaging and the other one uses interface. And we also load some benchmark functions to just benchmark it. It is really simple. You can just reply it with the same code in your machine. And here is the result. In the picture we can see, yeah, let's translate into the chart. In the chart you can see, so in the first one, this uses less CPU and in the meantime, so memory usage is also smaller than the load messaging. So here's the question. In the subset, they store the data as JSON.loadMessaging and they use it to mature and unmature again and again. But what we want to improve is just to store it as an interface and on the end we just return the interface and mature it into JSON messaging. And so this is a PR way proposal in the Goethe theorem and It is much. Yeah. So fix is really easy. Not very much. You just need to find where's problems and fix it. And add some test case. Yeah. That's all. Thank you. All right. Let's give him a warm round of applause. JS Visa. You speak as fast as a choo-choo train, man. All I heard was JSON file and you're saving. So if I understand correctly, you're using JSON file to save on memory space and that's how you save your costs. Is that correct? Yeah. Okay, good. So I'm not completely an idiot. You know, I don't come from a tech background, so it can be quite challenging for me. But nevertheless, let's quickly look at the screen. We've got two questions for you, JS Visa. So let's look at the first question at the top. What do you dislike about the clients that you worked on? Get Eragon Red. Actually, I select the clients I like. So I like Gail, I like Eragon, and I like Luis. But in the meantime, for some other clients like Zawa, I really dislike them. Okay. All right. I appreciate the honesty. You select clients you like. Fantastic. How much of... Oh, excuse me. One got voted to the top. What other problems did you find in Getz P-Prof? I think maybe a lot of other issues. But in my point is you need to improve it. It depends on your workload. It depends on your company's workload. You need to first measure it and you need to monitor it and then say you can find an issue you want to fix or you want to improve. Alright. We got 30 seconds. You think you can do one more question? Let's try. Quick answer. How much of a performance improvement do you get after the PR? Actually, let me see. Maybe 13%. 13%? That's pretty good. That's pretty good. But it's just for the WebSocket and Notify, says RPC. Okay, you've got 20 seconds. Let's do one more. How come interface is faster than raw message unpassed bytes? I think I'm not really into that one. But I think because inside the same one to marshal and unmarshal, because every time you marshal it into better understand,",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:10:00.000Z",
      "slot_end": "2024-11-14T08:20:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1DOTMyJmIPI5tdLiG_5PoOmjA44ieroq22BSvZjFN9no",
      "resources_slides": "https://drive.google.com/file/d/1lG06XlSxA9fABevhQdaB45fMNMafvdM7/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "staking-on-power-efficient-and-low-cost-hardware-from-arm64-to-risc-v-boards",
      "sourceId": "J3SWYT",
      "title": "Staking on Power Efficient and Low Cost Hardware: From ARM64 to RISC-V Boards",
      "description": "The entry barrier to staking on Ethereum got lower, as ARM boards, the tooling and OS support have improved massively. We show the current landscape of hardware options and the software stack to go along with it. \r\nAs a glimpse into the future we will talk about RISC-V, an open CPU architecture, present the current state of RISC-V based single board computers. We will discuss the progress we have made to run Ethereum nodes on these boards and the road ahead to optimize clients.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Validator Experience,Home staking,Decentralization,optimization,hardware,Decentralization,Home staking,Validator Experience",
      "keywords": "node running,RISC-V,Hardware optimization",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "5a873a44d8361c9e25d38237c41c3dc8af477ee3fc922209b9d3d7f25e0e05b1",
      "sources_youtubeId": "owzXocC1biE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:10:00.000Z",
      "slot_end": "2024-11-14T08:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/120GkPug8WQzGtUpAMbWnOOcB7P72J5K2YG_ZVHAuEF0",
      "resources_slides": "https://drive.google.com/file/d/1hfqIoD4BO6zEoIc8kLYNfgVixMq9QRud/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "rohingya-decentralized-identity-and-community-building",
      "sourceId": "G8W8MU",
      "title": "Rohingya Decentralized Identity and Community Building",
      "description": "The Rohingya Project is a transformative digital platform addressing the critical needs of the Rohingya community, focusing on empowerment and cultural preservation. Key services include R-ID, a decentralized identity verification system ensuring privacy and access to opportunities, and R-Academy, which offers courses on Rohingya culture and personal development. The Heritage Archive provides access to cultural resources, while the Community Exchange fosters collaboration & economic development.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Digital Sovereignty,Ethereum for Good",
      "keywords": "Rohingya,Decentralized Identity,inclusion",
      "duration": 667,
      "language": "en",
      "sources_swarmHash": "78d7bdda2dbaf97a4154df19ae2900b51e6fc485197e9aa0abdc505f8628ca2d",
      "sources_youtubeId": "3GBkaOSBuT0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735b6219dbb7a90e16c11bf",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:20:00.000Z",
      "slot_end": "2024-11-14T08:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1UYUaHo5Qavbvjs-V4IY1wgEZga3-zWvPCG7PXENX-k4",
      "resources_slides": "https://drive.google.com/file/d/11A5-7-t8giOFThBmze9Jv5u2eeVmWIjN/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "updating-gas-cost-schedule-based-on-reproducible-benchmarks",
      "sourceId": "TZVK7F",
      "title": "Updating Gas Cost Schedule based on reproducible benchmarks",
      "description": "Sponsored by the Ethereum Foundation, our project evaluates the real cost of executing OPCODEs and procompiles in EVMs across diverse clients. We present the up-to-date benchmarks, the new Gas Cost Schedule proposal, a do-it-yourself solution to reproduce measurements in your environment, and an automated way to generate new proposals for each hard fork.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gas,Decentralization,infrastructure,node,Decentralization,Gas",
      "keywords": "Gas Cost Schedule,EVM Internals,Client Diversity,Node Infrastructure",
      "duration": 479,
      "language": "en",
      "sources_swarmHash": "7f453befdfb576ea007ae870d4419b6bda34be91a758cd779132803505e89e90",
      "sources_youtubeId": "lHutPWuF3EY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f9981b0f83434dd9a416",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:20:00.000Z",
      "slot_end": "2024-11-14T08:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1Dzcuj-EPyhFVz3jUb7kd535irDd3n7X0WxNqRVI5Rgs",
      "resources_slides": "https://drive.google.com/file/d/1ZLhcSjEgO0A73_rxwq14DA8CcPkMuUMW/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "verifiable-open-source-vaccines-to-save-millions-of-lives-from-the-developing-world-up",
      "sourceId": "S7LEHK",
      "title": "Verifiable Open Source Vaccines to Save Millions of Lives from the Developing World Up",
      "description": "Viruses & bacteria like HCV, Strep A, and TB cumulatively take millions of lives each year – effective vaccines against them would considerably reduce that death toll. Unfortunately, big pharma isn’t interested in investing in developing these vaccines, and even if they did exist, rising vaccine hesitancy may prevent many from benefitting. PopVax is pioneering a new model of developing first-in-the-world verifiable vaccines at dramatically lower cost in India with radically greater transparency.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DeSci,Effective Altruism,Public good",
      "keywords": "vaccines,biotech,public health",
      "duration": 1022,
      "language": "en",
      "sources_swarmHash": "59e01008d62db1db4ee007dd926f53ffa4d235af99b1d8c6d3bbc2729e6a1fa1",
      "sources_youtubeId": "c4upJlvW_fE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ba4a9dbb7a90e18ca9f7",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ba4a9dbb7a90e18ca9f7.vtt",
      "transcript_text": " Hey folks, I'm Soham Sankaran, I'm the founder of a company called Popfax, which works on verifiable open source vaccines, intended to save millions of lives from the developing world up. And I'll explain what all of that means. First, let me say something that I think has unfortunately become controversial of recent times. Vaccines are one of the most effective public health interventions known to man. Over the last 50 years, they're estimated, and this is just childhood vaccines, are estimated to have saved over 150 million lives across key pathogens that attack younger children and adolescents. And when you add adult vaccinations, they're likely tens of millions more. But there are pathogens, important pathogens that are quite prevalent for which there are no existing vaccines. These include tuberculosis, strep A, and HCV in particular. So just these three pathogens, and here I'm talking about tuberculosis in adults, not tuberculosis in children, kill over 1.8 million people per year across the world, and there are no effective vaccines for them today. Unfortunately, instead of speeding up, vaccine development is slowing down. It took the malaria vaccine, which is one of the more recent infectious disease vaccines to be licensed, 35 years to go from concept to licensure. And these programs now routinely cost $1 billion plus, which is a very large amount of money to invest in a single drug. And then because they're only dosed, let's say, between one and three times usually, unlike GLP-1 agonists like Ozempic, which are dosed considerably more frequently, pharma companies do not see them as a particularly high return on investment. And so they have pulled back, especially in the post-COVID era, from investing in new infectious disease vaccine programs, in particular for pathogens like TB, which they see as being only relevant in developing countries which are poorer. But there's a potential solution here. So this is a sort of graphic of the top 10 vaccine manufacturers in the world by volume in 2021 during COVID, but excluding the COVID-19 vaccines. And what you can see here is that Indian vaccine manufacturers, including the Serum Institute of India, are very substantial in their presence. They account for almost 40% of the volume of vaccine doses shipped globally. And so why don't we V-ACK, so to speak? Why don't we accelerate vaccine development by relying on this capacity that we have in India and other developing countries? And also the fact that you know you can do things cheaper in countries like India, right? We can do this, we think, 10x faster and 10x cheaper. And you might ask why aren't people doing this already? There are already these big vaccine companies in India. Surely they must be thinking on these lines. Unfortunately, India as a country spends almost nothing on research and development. It is a joke how little we spend on R&D across government and across private industry. And Indian companies, including vaccine companies, are not particularly interested in taking technical risks, such as developing new vaccines for pathogens where vaccines have been hard to develop. And so PopFax is trying to solve this problem. We are trying to solve long-standing problems in vaccine design, including the three pathogens that I mentioned, where people have made attempts but no success. Using mRNA and computational protein design, tools that have just reached the point where we can actually use them to make novel vaccines that couldn't have been made before. As I'm sure many of you are aware, Demis Hassabis, John Jumper, and David Baker just won the Nobel Prize in Chemistry recently for their work in protein structure prediction and protein design. And so these tools are just now reaching the maturity where we can actually use them productively. And we want to leverage, as I said, the talent, cost, and speed advantages of operating in India, which is to say we are the only company or one of the only companies doing this kind of novel vaccine development in India, and so the best talent in the country, folks who want to work on these world-changing problems but don't have the opportunity, essentially come to us. Unfortunately, no one wanted to fund PopFacts. When I started this three years ago, VCs and non-profits essentially didn't believe that the talent pool existed in India to do this. The Gates Foundation gave us an early 100K check, but that was not enough to build a vaccine platform or take anything to clinic. As I said, these vaccine programs, they routinely cost a billion dollars. Even if you can do it for 10x cheaper, that's still hundreds of millions of dollars, right? And we had some good early data about, you know, two and a half years ago, but it had rendered both me and the company bankrupt. Then there was a massive pump in Shiba Inu coin, which I'm given to understand is a meme coin of a meme coin. And Vitalik founded the biosecurity and public health organization, Balvi, and one of their investments was us. And so they've now funded us a cumulative $15-plus million. And I think it's interesting that it took Vitalik, somebody who is outside of the sort of general hierarchy of public health funding, to see what I think is obvious, which is that the talent is there. It was really a resource constraint that meant that folks in India had not had the opportunity to go after these problems. So let me talk a bit about what we've actually done with that money and what that means for the future. So this is extremely dense, and I apologize in advance, but I'll explain what's going on here. That green thing that you see there is the receptor binding domain or RBD of the SARS-CoV-2 virus. It is the immunodominant antigen of SARS-CoV-2. Antibodies against that particular protein are the key drivers of neutralization and protection in the COVID vaccines. So it's a subset of the spike protein that I'm sure all of you have heard of. It's not the entire spike, it's just a part of it, right? What we've done is we've pioneered an approach where within mRNA, we can encode a self-assembling virus-like particle, which is what is in the blue, that basically displays a whole bunch of copies of this RBD protein. And in doing so, we've achieved two things. One is we've achieved considerably more potency. So what that, if you look at our sort of highlighted, you know, bolded bar here, two micrograms of our RBD-VLP display mRNA, which uses this approach, provides 22x greater neutralizing antibody response, which correlates quite highly with protection, compared with two micrograms of the full spike mRNA sequence from one of the U.S. approved COVID vaccines. And this is all in mice. And what's particularly interesting about this is, you know, we can use it to make a vaccine that's more potent. We can also use it to make a vaccine that's potentially safer. Because we can achieve with 95% less dose, as you can see at the top, with just 0.1 micrograms, we can achieve the same result as you would without using our strategy with two micrograms. And so I can inject much less mRNA, much less lipid nanoparticle, and have potentially a much safer product overall. And the kinds of things that Philip was talking about, you know, all get worse as you increase the dose of whatever your vaccine product or drug product is. The other interesting thing that we got here is breadth. So if you think about COVID-19 and why the vaccines were less effective as we got later into the pandemic, the reason isn't that they didn't have a durable response. The reason is that the pathogen evolved. Variants of the pathogen that were no longer susceptible to the antibodies elicited by the original vaccines became prevalent. And as a result of that, we ended up with a situation where what were very effective vaccines at even preventing transmission became relatively ineffective as new variants evolved. However, using this mRNA-encoded VLP strategy, what we've been able to do actually is elicit antibodies that neutralize a whole breadth of variants. So our construct is the red line, which is able to potently neutralize, you know, even as it's diluted, a whole bunch of these other variants, wild type, gamma, lambda, Omicron, BA1, whereas the US approved COVID vaccines, the original ones in blue, are not able to neutralize those other variants. So this approach, which we are, you know, among the first to pioneer in mRNA and protein design, has potential substantial gains for COVID and for other pathogens like influenza. Now, let me talk a little bit more broadly about what strategies like this are actually trying to do, right? If you think about any pathogen, you know, it has a whole bunch of antigens, which are, in the case of a virus, those are going to be proteins that elicit certain kinds of immune responses, certain types of antibodies, right? And then some of those antibodies are going to be functional. You know, they might sort of neutralize the pathogen, right? Some of the antibodies are going to be non-functional. They might bind but not neutralize or not bind at all, right? And then within the context of these functional antibodies, there's some subset of antibodies that you might consider broadly functional. And what that means is even as the pathogen evolves, even as there are additional variants that come up, those antibodies, which are some subset of the original antibodies, remain effective. So the intention of our vaccine design approach is basically to move the distribution of antibodies listed, whether it's in an animal or a human, towards these broadly functional antibodies, which have a chance of providing durable protection against even new variants that might emerge. And this is relevant in the context of COVID for this variant evolution story. In the case of some of the other pathogens that I mentioned, for example, HCV, it is absolutely essential. Because HCV as a pathogen like HIV evolves within its own host. And so, you know, if you were to be infected with HCV, you might have thousands of different variants of the HCV pathogen within your own body. And so, an antibody response that is effective against just one of them is not going to be effective at clearing the pathogen. So, we've used this, you know, we've used a variant of what's called an epitope scaffolding approach to basically attempt to elicit specific types of antibodies that provide this kind of broad functionality and broad protection. And what we've been able to do in just three years is to run these processes of designing and testing in animals these novel immunogens, which can elicit these antibodies, much faster than anybody typically does in vaccine R&D programs. Again, as I said, leveraging the fact that we can do this research much more cheaply and much more quickly in India. And what we found is mRNA in particular gives us an advantage here. mRNA is, of course, not the actual immunogen. What you're injecting in mRNA is almost code that encodes a specific design of immunogen that's intended to elicit some antibody response. And so because we're using mRNA, which is a standardized process, we can test thousands of different immunogens and then when we alight upon one that that actually gives us the response we want, we can translate it not just into a sort of lab environment but into an actual GMP manufacturing environment. Apologies. We can translate not just new a lab environment but into an actual clinical environment, into a clinical study very quickly, which is not something that you can do with conventional vaccines as easily, which is why you get these extremely long timelines, right? And so what we know so far is basically this VAC approach in India, it works, right? We've been able to do this at 20x cheaper than you would usually spend on a preclinical vaccine program to build a new platform and get to phase one. We've 10x the testing throughput of a normal vaccine program. We've tested, you know, thousands and thousands of novel LNP formulations of novel designed immunogens, which are basically vaccine designs. We've tested them know thousands and thousands of novel LNP formulations of novel designed immunogens which are basically vaccine designs we've tested them in vitro but we've also tested them in vivo 500 plus and we've been able to achieve the results you saw in the beginning which is that our vaccine appears to be much much better than the existing COVID-19 vaccines so what that means is I can take 10x the number of shots on goal for the same amount of funding as a company in Boston or San Francisco and potentially save 10x the number of shots on goal for the same amount of funding as a company in Boston or San Francisco and potentially save 10x the number of lives in the long run. Right? If somebody gives me $300 million, a company in Boston, and this is all based on a real comparison with a biotech company that's developing vaccines and doing quite well doing so, they might take one candidate through a phase one clinical trial and be getting ready for a phase two clinical trial and be getting ready for a phase two clinical trial with a new platform. For us, we've been able to build a new platform and with that amount of money, with $300 million, we would be able to take 10 new candidates to phase one and two and potentially bring multiple products to licensure. And again, in the long run, that saves a lot of lives, right? Because every year we wait to develop these vaccines, millions of people are dying. Another key piece of this is that we've built our own GMP manufacturing facility. So what that means is we have the ability within our own facility to make clinical doses in a way that's safe to inject in humans, such that we can very quickly take these new candidates and advance them to clinical studies. And here I'll reference what Philip was talking about a little bit, right? I think it's critical to ensure that we build a process that is not only, you know, safe by the standards of existing regulatory norms, but is legible enough and is understandable by the general public that they trust what comes out of our work, right? And so the good news here is, and in fact, I'm announcing this today, we just announced it via press release this morning, is that NIAID, which is part of the US National Institutes of Health, has selected our next generation COVID-19 vaccine as part of the US government's project NextGen for a phase one trial, which will happen in the US early next year, which to my knowledge is one of the first clinical trials of an Indian design vaccine in the U.S. In fact, no Indian vaccine or Indian design vaccine has ever been approved in the U.S. No Indian company's vaccine has ever been approved in the U.S. And our intention is to have our vaccines available all over the world, right, not just in rich countries, not just in poor countries, but to everybody. And so this is very exciting news, because I think what this means is our approach, which, as I said up top, is an approach that at the beginning, nobody supported, right? Nobody believed that we had the talent in India to do this. Nobody was certain, you know, or people, in fact, were quite certain that we wouldn't be able to do this, right? And it took an early bet from Gates and a substantial bet from Vitalik and his team this has now been essentially co-signed by one of the most credible organizations in public health, right? So NIAID thinks that this vaccine works very well, at least pre-clinically, and they think it's worth evaluating in humans. The COVID vaccines have, you know, have been somewhat maligned, but best estimates suggest they saved tens of millions of lives. We can quibble about specific numbers, but it's likely a very large number. But again, as Philip was talking about, there's been a lot of questions about whether the process followed in approving these vaccines were processes that are appropriate for human health. And at the same time, on the other side of the coin, the reality is that a lot of people who wanted these vaccines in developing countries didn't get them, because Pfizer and Moderna were not that interested in selling their vaccines at knockdown prices in developing countries didn't get them because Pfizer and Moderna were not that interested in selling their vaccines at knockdown prices in poor countries, right? And so what we've done is we've worked with Balvi to come up with a model where we will be open sourcing our COVID-19 vaccine. We will not be enforcing our intellectual property on that vaccine for a number of years. And what we'll be allowing people to do is to verify not just the R&D processes that led to that vaccine, but also the manufacturing processes. So that manufacturers across the rest of the world can not only make copies of this vaccine themselves, they can distribute at low cost, but also that people can understand whether the processes that we've used in developing and manufacturing this vaccine are compatible with what we say out in public and are safe for them, right? They can have independent scientists evaluate these processes in a way that existing vaccine companies don't make enough information available to do. And that's something that we can do because we are comfortable releasing far more information than anybody has ever done for a vaccine development or manufacturing program before. Let me just briefly talk about what we call the Popvax Million Lives Mission. As I said, there are these three pathogens that collectively kill 1.8 million plus people per year. We want to develop effective vaccines against all three of them that we think can save over a million lives per year. We want to do this using a new sustainable model for funding for public health vaccines, similar to what we've done with Vitalik and his team, where, you know, we will provide full details needed for vaccine verification, R&D improvement, and manufacturing openly. We'll cap profits in developing countries and allow for open licensing on transparent terms of this intellectual property, and we will have uncapped profits in rich countries. And this is a model that we want to advance, that we think could be the future of sustainable vaccine development. Yeah, I'm happy to take your questions outside. And please feel free to email me if you're interested in working with us, funding us, or collaborating with us. Thank you so much. Thank you so much. That was great.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:20:00.000Z",
      "slot_end": "2024-11-14T08:35:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1sK71lOtl_9Q8SbWOBVtDNhLVBhc--pIc-AxaYE2toIM",
      "resources_slides": "https://drive.google.com/file/d/1ZnLUJlsKkwKWtFHaJ-j4b2KAgjWukxqm/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "blockchains-unique-potential-transforming-lives-and-the-humanitarian-sector-for-refugee-solutions",
      "sourceId": "KR9NSV",
      "title": "Blockchain's Unique Potential: Transforming Lives & the Humanitarian Sector for Refugee Solutions",
      "description": "This talk covers blockchain pilot projects in Uganda and Thailand, demonstrating how technology transforms lives through local collaboration. The Uganda pilot uses public goods funding to fund a  refugee-led music production studio and produce music NFT's. The Thailand pilot utilizes crypto to send aid to Burmese refugees, and tests crypto->fiat off-ramps across the country. These initiatives highlight the effectiveness of hyper-local solutions and offer insights for future humanitarian efforts.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Use Cases,Ethereum for Good,Public good,disaster,relief,Ethereum for Good,Public good,Use Cases",
      "keywords": "humanitarian aid,local collaboration,disaster relief",
      "duration": 561,
      "language": "en",
      "sources_swarmHash": "3d7eaead786b24f7a15500be64abae02e1a4ae7abb1041ad74691cef92495c7b",
      "sources_youtubeId": "26Sag2EIlYM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735b8819dbb7a90e182a512",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:30:00.000Z",
      "slot_end": "2024-11-14T08:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1BEfulRwWXyv3ETY4qD5zkkb47m78LVdcVcRHGFpP07o",
      "resources_slides": "https://drive.google.com/file/d/1IzPViXIZte38sGYx-IN9vum-BGVO4k0h/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "scalable-and-sovereign-evm-data-modern-data-engineering-best-practices",
      "sourceId": "KEEUYL",
      "title": "Scalable and sovereign EVM data: modern data engineering best practices",
      "description": "Collecting and analyzing large historical EVM datasets can pose a significant challenge. This has led many teams and individuals to outsource their data infrastructure to commercial 3rd-party platforms. However, over the past year a new style of data workflow has emerged, using entirely open source software and local-first processing. This new ecosystem of tools allow anyone to cheaply, easily, and robustly collect and analyze any EVM dataset from the comfort of their own laptop.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,data,analysis,Developer,Infrastructure",
      "keywords": "Data Engineering,Data Science,Data Analysis",
      "duration": 1242,
      "language": "en",
      "sources_swarmHash": "177fc071fd2593ac885081c221ecbd202fb829eb9768f9bef54a4e3456bea753",
      "sources_youtubeId": "bKrnOnfx9io",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cc8b7982f234a1257b1be",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:30:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1ArYtVYufUwHpFKb-cm8W6DCWGSPca78nUlpjKQDTmiY",
      "resources_slides": "https://drive.google.com/file/d/1HhBZWf4Y3LQba0Q5ZITTgRrtu3g6xFB9/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "usability-changes-in-a-post-eoa-world",
      "sourceId": "P9FRCH",
      "title": "Usability changes in a post-EOA world",
      "description": "The wallet world has evolved to embrace contract accounts (4337 and 7702), app-owned wallets, session keys (CAIP-25), and permissions controls (7715). How might we on the app layer design and build upon these new account types? In this talk, we will demonstrate the possibilities for novel user flows given these new account standards, compare how these new standards can introduce pitfalls, and provide best practices on how to design for app layer in a post-7702 world.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Design",
      "featured": false,
      "doNotRecord": false,
      "tags": "ux,wallet,Account Abstraction,Design,Key Management,UI/UX",
      "keywords": "Wallet,UX",
      "duration": 1443,
      "language": "en",
      "sources_swarmHash": "c9499f3505fdc7dbc21b9bcae9814112702fbbd925537261068b032270172cdb",
      "sources_youtubeId": "MvFforeD1SU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e3f49dbb7a90e171c913",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735e3f49dbb7a90e171c913.vtt",
      "transcript_text": " So today we're giving a talk about what evolving standards, different account models, and how that changes usability. This talk is basically going to be about endgame wallets and the connection between wallets and dApps. Okay, sorry. All right, so a little bit of background of what the UX looks like today is that dApp UX today is constrained by the concept of the account model. So what that usually means is that when you visit a dApp, the purpose in life of that dApp is constrained by the thing that you're trying to do on chain, and also the private key signatures and how you interact with that dApp. All right. There are a lot of account abstraction talks, I realize, at this conference. So I don't want to spend too much time. But these are some properties that you get when you're signing with a single signature. So your entire account gets connected, key management is all on you, no native batching, and you have to have native gas tokens. These are the common pitfalls that I think most of us are familiar with. So no recovery or upgrades, signing a message for virtually every action that you have to do, and also having blocked experiences on dApps where you need to connect your wallet to be able to see a quote or you need to be able to connect before you do anything. All right. So what's at stake here is something that I'd like to call the appification of dApps. So instead of the dApp experience being one thing and then using a legacy app and being another thing, it would be great if they're more blended. And the properties of that would be enhanced security, smooth onboarding, and what I want to say, convergent applications where you're blending on-chain, off-chain, and cross-chain components. Oh, I'm blocking the QR code. I'm so sorry. Maybe I'll come over here. All right. So what I want to do now is give a tiny glimpse into what greater programmability and security on smart contract wallets actually looks like on a UI. I don't want it to be too detailed or implementation specific. So these are just general examples of what can be done in the future. All right. So with smart contract wallets, they can run code. And with that, you can get conditional logic. One use case that I think would be pretty interesting is like, let's say that you are a DAB and you manage portfolios and you want to add a security service that says if there is an on-chain condition that is met and we are able to prove that there is an exploit or there's a vulnerability in this DAB, please remove my exposure and unwind my positions. So in order to do this, you have to presign it and then you would be waiting for the condition to be met and then you won't have to unwind it manually as you do today. Other things you get with smart contract wallets, you can do cross-chain calling and balance aggregation because you can query the state of other contracts as a smart contract wallet. So this means that let's say I want to buy an NFT on Zora, and I only have funds on Optimism and Mainnet, this doesn't have to be a problem because we can aggregate the balance and bridge it for the user on their behalf. So this means that the click path is getting much lower, so you're not clicking as much and approving everything, looking for which bridge has the liquidity that you want, and numerous other things that we have to do today. Okay. This use case, this example is the most important one to get right because user ops are not highly adopted right now. It's still pretty early. So the thing that we need to nail is the onboarding and the issuing of the smart wallet. I mean the smart account. So what I say here is modular authentication allows users to define the dApps level of access. So this is not unlike when you sign in with Google, and Google is telling you, oh, when you're going to Calendly, I'd like to see your contacts and your calendar and what you have set up there. We could do this on chain as well with smart accounts. So in this example, I want to show that it's looking for a specific domain that you have set, and then it wants to request access to a spend limit or maybe usage of modules that this dApp wants to use. Okay, so those are some very high-level examples of what can be done in the future, or even now. So the upshot of all of this is that you can get better blending of off-chain, on-chain, and cross-chain components. This is a nod to the convergent application thing that I was talking about, so that dApps don't feel as single-purpose. The thing is, though, like many complex systems, when you introduce new solutions, you can also risk introducing new frictions. And I'll gloss over this very quickly. Now that we want fewer approvals, currently right now with the standards that we have is that you do migrate to a new account and funds get fragmented. Even though there's fewer approvals, that is a user friction that we might have to deal with. Obviously the evolving standards and there's different ways in which that we can try to deal with this. So there's also enhanced programmability. This means like the contract can run code. It's like currently the EOA, they don't have, like it can't actually read state or anything like that. So EOA is still initial signer for it. So that's another user friction that is possible. Gas abstraction, paying gas and other tokens other than the native token of the network. This is not gas optimized yet because we're still prior to 7702 being on Pectra, I think. And then we have account recovery. These can introduce unfamiliar user patterns. So the one that I, again, like the one that I want to focus on is the onboarding aspect, which would be the modular authentication, because there's a plethora of solutions that you can choose from for implementing the onboarding of smart accounts. And so I want to hand it over to Greg so that he can show you some best practices on how to implement a helpful experience. Thank you, Sydney. Sorry. Okay, perfect. Sorry, I forgot where we were at. How many people here are familiar with, like, the CAPEs, Chain-Agnostic Improvement Proposals? This is good. We need to change that. How many of you are familiar with EIPs? Keep your hand up. How many of you actively check the EIPs or the ERCs? How many of you are wallet developers? How many of you are applications as opposed to smart contract accounts? Yeah, we have to change all this. Basically, in order to get any of this done, we need to start focusing on standards more. And so I'm going to verbally assault you all with some very important ERCs and show you exactly how we get to, frankly, what's probably the end game wallet to dap interaction where those pieces fit together and how we do it. It's going to take a couple of ERCs, capes to get there. Frankly though they're all available. They're live and there's other versions being iterated on. So we're not too far away. It's just going to take some work because we need more people actively working and looking at standards. These are the main ones we're going to be talking about. 275, 4337, which we should all be familiar with, 7702, 7755, 17715, 7679, and RIP7575, which I'm not going to cover. Let's start, though, with some, like, visual stuff. And then I'll kind of stitch it together at the end. When you first go, this is a website that's live, basement.fun. When you first go to the website, you get to experience the entire website without ever needing to get a wallet prompt prompt without needing to actually install anything without even needing your funds the way they do that is by embedding a embed they put an embedded wallet behind the scenes for you and there's a reason they go with the embedded wallet behind the scenes it's because you don't have to actually they don't want to force you into a smart contract account right at the beginning because that would just cause further fragmentation if you come to the app later with your own account that has a smart contract wallet enabled. Now we're already just fragmenting. We're splitting up your accounts where they don't know about each other. So now you're having to maintain effectively two separate bank accounts. I don't know if anyone's done banking with two bank accounts, but it's, like, unenjoyable. So I don't know why we'd force our users to do it. Similarly, which is 7555 and K275, we have this issue. Do we have any mobile app developers here? Maybe mobile game developers? Have you ever tried to actually connect to somebody else's smart account or even someone else's external wallet? It's borderline impossible. And that's because you can't discover. The ability to discover a wallet and then know where it's deployed is practically impossible. That's what Cape275 tries to do. It's the idea that you basically look up gregthegreek.eth and instantaneously I'll figure out where the address is, what type of account is it, EOA, smart account, safe account, is it someone else's es. And we can then use a wire protocol, which is the Oeth one, to basically go and connect to them. Because we do know everyone's familiar with pass keys. We know that that's probably the best way to remove seed phrases. But how many of you know that your pass key is scoped to the website you issued it from? Or the app you issued it from? Or the app you issued it from? That means if I generate a public key from the passkey, it's only going to be found there. If I go to chainsafe.io and I issue passkeys that way, if I go to then sprinter.tech and try to issue a passkey there, I'll never get the same pub address. And this is a fundamentally big problem because we're all issuing pass keys with our smart accounts. So we need to create a way to bridge that gap. And that's what 7-triple-5 aims to do. And I'll get into it a little bit. We're going to stitch this all together. Don't worry. It's going to be less confusing, I promise. 7-7-1-5. This is pretty ‑‑ this is also an interesting one because what we do is we start to say, hey, if I'm going to remote into somebody else's smart account and I'm going to ask to use that smart account with a passkey that I don't have, I don't want to constantly go back and forth. The UX breaks. You're constantly flipping out of a mobile game that's supposed to keep you in the game, and now you're going out to sign every single time you need to do something. So what 7715 tries to do is basically says, hey, we're going to let a random key, aka this is where we tie the embedded wallet back into the picture, the embedded wallet can now act as a signer to the wallet for a given period of time. And if you're thinking about the Web2 model, this is a JWT token. All we've done is brought back the JWT and said put some cryptographic properties to it. And that's how we can maintain it. So you can maintain your UX in the app and use an embedded wallet. Now you don't need to deploy them in a smart contract account. You can use what they already have in preexisting infrastructure. Finally 7579, this is an interesting one. You do want a smart account feature. There's stuff that the smart account doesn't actually provide you. It's simply a plug-in. Build it as a plug-in. You tell the wallet to register the plug-in. You get the full benefit of deploying a smart contract account for someone without fragmenting their balance. You remain the existing experience. And the big thing to discuss here is, which I'll get into and then is it the next slide? No. Oh, yeah, it is the next slide. Perfect. And so this is these are all the EIPs. ERCs. That stitch it together. And what we'll do is we'll go through an interactive game of actually stitching together to show you kind of like how the end game in one to two years' time will actually look like. Because we have a lot of teams that are doing really good infrastructure work at the application layer, but frankly, because we don't have all of these things plumbed in, we are frankly setting up the applications for a little bit of failure and there's going to be a period in time where we're going to see mass migration of self-deploying app infrastructure outwards. And my main argument I have for this is there exists two spectrums of UX that we need to maintain as an app developer. There's your local app UX, which we have full control over, and that's why we're suggesting you deploy with embedded wallets, because you can fully control it. You can make your perp decks that's fully on-chain look and feel exactly like Robinhood. You can make your perp decks that's fully on chain look and feel exactly like Robin Hood. You get all the benefits. So there's no reason to go anywhere else. The other thing is we have wallet land UX. And most app developers think we don't have control over this. And that's where we're hoping to tell you something else. Because you do. You have to control that you don't change it. The minute you go and change someone else's experience of how they bank effectively with your app, it's breaking it. And when you deploy a smart account into the application layer, we've gone back to the old principle of, you know, think about like Starbucks. You have to put money into the Starbucks app. Your bank now no longer knows how much money. If you moved $100 in there, you don't know that you have your bank account value plus $100 that you have unspent in Starbucks. You just have minus 100. We have the ability to not do that. We have the ability to not fragment and deposit funds but rather borrow funds directly from the bank with a scoped permission. And that's what we're going to achieve here. So what do we need to do to get this done? The whole pipeline is going to look like this. We want people and wallet info teams to deploy 4337. We want them to be the smart account issuer. If they're not the smart account issuer, if the app ends up being the smart account issuer, you need to be able to let the user rage quit. You need to be able to set permissions. Like, are you comfortable as an app developer saying, oh, yeah, I want to make sure when I get into a chain-abstracted multi-chain future that the user always has one eth and mainnet because they always want to make sure they have gas to pull out some Aave positions or compound positions that I don't know about. They might want that preference. But are you really ready to actually deploy all that info or do you just want to focus on your app? Let the wallet handle that part. We then always pass keys are the way to go. It's already like this today. And it's only going to get better. We have solutions to do like multi-account pass keys where the address persists. The app is where you're going to deploy the embedded wallet. Within that app, we see CAPE 275 come in. And I want to think about clicks, so keep your hand up thinking about the clicks we're going to do here. When the user first auths onto your website, they're going to type in their identifier, email, phone number, ENS name. They'll click enter once. I'll then go and look up in the registry, you figure out who issued it, who their provider is, the SmartCon account provider, and you go to that website directly. This is now going to look like an iframe. This is what 7755 does. Think about that iframe you're used to, you know, when Google says, hey, someone says can you log in with Google so I can get access to your calendar information. It's the exact same flow. That's going to pop up the destination where the smart account is. It might be the NOSA safe website. Once you get in there, along with that message, we're going to pass in 7715. That's now going to say, hey, I'm a perp desk. I want to trade 1,000 USDC every hour for the next 24 hours. That means your total bound is 24K worth of USDC that could be taken by this application but it's defined, strictly defined the same way Google strictly defines your log in process. 7579 I talked before. You want that extra smart contract functionality on the same wire protocol, the same 7555 wire protocol. That should be 7555. You'll be able to attach whatever else you need. You need that extra functionality as a plug-in. It's simple. You just add it in. The screen shows the exact same view. 7811, for anybody that follows standards, you probably haven't seen this yet. It came out a few days ago. But the simple thing is, you know, when you're an app, do you how many of you develop your app and then go, okay, cool, step one, hit the RPC for every available token balance that this user has? There's no need. It's already been done for you. The wallet knows what the user has available. And in a cross-chain, multi-chain world, the wallet also knows how much is available against other chains as well as what the spending power is. I have USDC. I need to use ETH to go buy an NFT. It knows that I can convert USDC. The app doesn't need to know about that. So this makes an extension for it. There's a plethora of other ones we can add on to here. But if you think about it, at the end of the day, what happens with this experience? Remember how I said to think about the number of clicks we did for the user here? There's two. We've reduced it to two. You go from looking up your account, automatically generating the pop-up iframe. If anybody's used Coinbase's wallet recently, there's that iframe that pops up. That's your OAuth window. The user clicks. One click, OAuth window is up. One more click to confirm. And now you have the full power of that smart account locally while only using an embedded wallet, and you can ensure the security remains constant because the permissions are there in place. And you now have the full flexibility. You never had to deploy account of any sort of wallet infra. You don't have to care about the wallet infra. You don't have to even care about what app, what type of implementation do they have because you can attach the plug-in you need. So you can ensure as long as they're 4.3.3.7 compatible, you can boot in anything else you want. You don't have to go and deploy them a custom implementation. Your onboarding experience, just 10x's right out of the gate. This is what the end game realistically will look like. How close are we? We started with 7.5.5. Realized we needed 7.1.5. 715. This one is almost done. There's a few last little things needed. Modules by rhinestones been done forever and frankly if you haven't looked at them, you should. This will be pretty quick. 275 is live. Lit's using it. And we already know we've got pass keys and 437. We just need the permissions module to finish. And we need to get the main wallet providers to start adopting the slugs so that we can have those pop-ups happen. And you have a full OAuth journey that when you go to onboard your grandmother, it just looks, or your mother, it looks exactly like how they do by logging with Google. And you can issue all the infra underneath blindly. And so that's, this is the endgame for wallets. This is how we envision it. And frankly, we need more people looking at standards and helping just see what's out there and realize what they can do with what's available. Because all this already exists. We can have this. We can have a two-click journey, and you get a smart account without ever having to deal with that info yourself. Oh, there's more slides. Ah. Yeah. Time for a... I don't know. Yeah. Time for ground zero good Dapp experience. Thank you, guys. Let's keep in touch. Okay. We have time for questions. So if you have questions, you still can ask them. We have two questions already. So first question is what is that we're missing or need to change on layer one in order to enable better experiences for everyone and not just a specific layer two? Native account abstraction that doesn't look like 7702. Something more closer to 3074 that actually achieved its goal. There's a lot of chains that actually have that at the base layer. And 7702 is good. It's a migration tool. It's a stopgap. We're going to have to release more account abstraction features down the road, natively. But unfortunately, that's the biggest one. Okay. So our next question is about passkeys. You said that the problem with them is being scoped to specific sites, but later you said they're actually good, that we should use them. So what is the solution here? Two separate problems. So the one problem is that when I issue a passkey, you get a public,. Like when you do a passkey authentication, you know, face ID, touch ID, whatever it may be, that issues you a public address. And it works the same way that a YubiKey works. What happens is they basically say, cool, what's like the DNS record? And they match that DNS record to a new public key generated on the passkey chain. So the problem is if I go to another website, it's going to either get a new one or create a new passkey. And so you can never have that same address persisted through. And so that's why you have to push outwards. If you want access to the signing key of the smart account, you actually need to go to whoever generated the passkey in the first place. And that's why we need that OAuth window. The same way Google doesn't let you just like OAuth inside of an API call inside some random website. It would just be very insecure. Okay. Next question is if we connect our wallets to apps in this way that you described and there's all our savings on these wallets, is it going to be safe enough? How many of you trust Gnosis Safe? Or safe? Lovely. Do you trust safe to provide you the UI you need to ensure you're signing the right message? This isn't rhetorical. Okay, good. In that case, if you trust them to provide you the correct UI when you sign a message, why would they not provide you the correct UI when they say, hey, by the way, this thing's going to drain your account? Instead, it's going to say, hey, it just wants to spend like a thousand USDC for the next like two hours. And then the token expires and they have to refresh that token. Same thing as a JWT with your bank. If you trust the people providing you the wallet, then you should have no problem actually using it in the system. It's the same problem with, it's exactly why I said, you as an app developer, are you actually comfortable deploying all this infra? Even the person giving you the smart account, are they giving you all the infra you need to ensure your users are safe? Push the barrier to the wallet. There's a last question, I think. Next question. In the endgame, are token ballots still held on external wallets or smart contract address? 4337. So, smart account. Okay. And banks often have the backup of support. Will the wallets provide the same backup? I mean, pick a good provider. MetaMask support is pretty good on Twitter. I think this question is really interesting. It's more like an insurance type of question. Because I think banks are like they have FDIC. It seems like this question is asking about support of user funds, which I think is external to wallets themselves. Or recovery mechanisms. Yeah, exactly. Okay, if there are no more questions, that's it. Please give it a round of applause for Greg and Cindy. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:30:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1Qe6obqukS9lTToSF06QtJ1Ovqj8Dzv1P-Vi0z9-wI7w",
      "resources_slides": "https://drive.google.com/file/d/1TaIFDPviXHm1cdsJcOiUxZ9OldqpXQCZ/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "web3-security-is-embarrasing",
      "sourceId": "VNFNDM",
      "title": "Web3 Security is Embarrasing",
      "description": "The explosive growth of Web3 has brought about innovation, decentralization, and financial opportunity. But let’s be honest—Web3 security is a disaster. In this talk, we’ll confront embarrassing truths: drainer attacks, weak wallet protections, and overlooked vulnerabilities. But we won’t stop there; I’ll share practical fixes to protect users and show how Web3 developers can raise the bar. If we want Web3 to thrive, we have to stop attackers beating us with low-effort attacks. We can do better!",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Sustainability,User Experience",
      "keywords": "phishing,protection",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "cbe9a41a18b6b6f38c6379e9112f03daa416a430143cc5aacad5d75ef4fa3041",
      "sources_youtubeId": "4dr7sL42GAw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:30:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1lEsNi0su_iRPEMbDkw-4CNthY3CMQvM_6ClpF3sBGNM",
      "resources_slides": "https://drive.google.com/file/d/1znq8jpKAR9gwVW753mlGIwpgUrBHEKn5/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "viruses-and-chronic-aging-building-a-research-community",
      "sourceId": "FX8UQF",
      "title": "Viruses and Chronic Aging: Building a Research Community",
      "description": "Did you know that mitochondrial dysfunction, inflammation, and cognitive decline are directly accelerated by viruses? In fact, the viruses that infect us over a lifetime are technically not even alive, and therefore must “hack” our human cellular metabolism machinery to do anything at all. This talk will overview the first-ever global collaborative network studying & treating chronic viruses as drivers of aging, including how certain lifespan-promoting drugs may help combat viral activity.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 996,
      "language": "en",
      "sources_swarmHash": "4aff1867805b00275edb2ba9798287686622d38009dc1a7f1cae7feacc201c23",
      "sources_youtubeId": "OcwxbzqP8Jc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735bc959dbb7a90e1a2fe20",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735bc959dbb7a90e1a2fe20.vtt",
      "transcript_text": " Hi, I'm Amy Prowal. I'm the President and Research Director of PolyBioResearch Foundation, and today I'm truly excited to talk to you about viruses and chronic aging, building a research community. So where am I going with this? First, what I want to talk about is viruses as drivers of aging processes. This is really not discussed as understood as it should be. And there's so many compelling examples of how viruses can contribute to chronic aging processes. This is really important for the longevity community and for people who are just dealing with many forms of chronic disease. If we understand these interactions, we can better figure out how to combat them and how to best create strategies to mitigate viral effects on aging. So let me give you some examples of viruses and how they contribute to aging processes. So there's definitely many drivers of human aging, but here are some of the three main factors. Mitochondrial dysfunction, these are the just energy powerhouses of our cells, and they become dysfunctional as we age, often. Inflammating, which is basically just the fact that over time, as people get older, tend to have more chronic inflammation, more immune cells that are just activated in an unproductive way. That's called inflammation. And cognitive decline, obviously. People who are getting older and aging have memory problems. They have brain, you know, dementia, even post-Alzheimer's type phenomenon even sometimes. So one thing to understand is that as we age, and in fact, as we are born and across the scopes of our lives, we inherit viruses from our parents, chronic viruses sometimes. They're passed, in some cases, in the womb. Plus, we also, over the course of our lives, and this happens with exposures with other people, it happens from our environment, and sometimes the foods and other things that we consume, we accumulate different viruses, and they become part of what is known as our human virome, the viruses in us. And there are many viruses that become a burden in our systems as we just live. These are the herpes viruses, the papillomaviruses, and increasingly a growing number of RNA viruses are also understood to be persistent viruses that are with us for life. Now, what does this virome then do to aging? Well, consider just some of these virome components. This is, you know, sometimes when people are, let's say, in college, they get mono. And people understand that people get sick, they get a sore throat, they don't feel well, they're feverish, they try to avoid people for a while. What really means when you get mono is that you get the Epstein-Barr virus. It's a herpes virus. And that virus stays with you. Again, this is a persistent virus. Once you have it, it does not clear your system. It stays with you for the rest of your life. This is the thing. If someone's immune system is robust, if it's in a good shape, if it's active, what the immune system does technically is keep these chronic viruses like the Epstein-Barr virus, if you get it, and over 90% of people in the world, by the way, harbor Epstein-Barr virus, keeps these viruses in check. It keeps them in a dormant form. The immune system keeps them in a latent form. And that way they technically cannot activate and create more proteins or things that can actually drive disease or potentially aging processes. Now, however, if viruses do become active, if they are moving out of a state of dormancy because something happens to the immune system, and this could be many things, it could be another infection, it could be just exposure to pollutants, to chemicals, to many things that wear down our immune systems over time, viruses can become more active. And one of the things that viruses do when they activate is that they affect our mitochondria directly. In fact, this is one of the key things to understand about viruses, our health and aging, is that viruses are actually obligate intracellular pathogens. And what that means is by definition, they are not even alive. They must, in order to replicate and create new versions of their cells, they must pull the substrates, the backbones to do that, to create new versions of themselves from our human cells, from our mitochondria. So they do this. Every single virus hijacks our mitochondria in order to create new versions of itself and do basically anything that it does. This is a paper that I wrote about this phenomenon with my colleague, neuroscientist at Polybio, Mike Van Elzucker, who also is a neuroscientist at Harvard, we wrote about how pathogens, bacteria, viral, fungal, parasite pathogens even, hijack the metabolism or the mitochondria of the host cells they infect to just gain those basic substrates again to just create new versions of themselves. This is core to what they do. This is a diagram from our paper. This is basically a human mitochondria in the diagram. At the intermediate of that paper, of the figure, you can see the TCA cycle. That's a very important part of just gaining substrates for our own energy metabolism so that we can function, burn glucose, burn other fuels that allow our mitochondria to make us energy producers in a good way. The blue boxes contain different human viruses that are part or can be part of the human viral persistent chronic viruses. And those are just the different parts of metabolic pathways that they hack or hijack as part of their ability to just create new versions of themselves or replicate. Okay, so I mentioned before that inflammation, just chronic inflammation often accelerating over time is also strongly associated with human aging and issues with longevity, for example, in this paper. Now, what inflammation often results from is the activation of immune cells, including cytokines, which become active, and one of the questions is why, but we certainly know that especially cytokine immune which become active. And one of the questions is why, but we certainly know that especially cytokine amine cells become active. Now, IL-11 is an example of a cytokine. So it's an inflammatory molecule in the human body that can become more active when things become inflamed. Now, this doesn't seem too surprising then. In this paper, this team showed that inhibiting this inflammatory molecule, the cytokine IL-11, extended mammalian healthspan and lifespan, suggesting that inhibition of some forms of inflammation is helpful in that regard. Now, what though, in the first place, was causing that cytokine IL-11 or interleukin-11 to be active, to be more active than it should be. Well, one of the biggest driving factors, again, that so many viruses and other bacteria or pathogens or parasites do is they activate immune cells as part of their persistence. What happens is the immune system recognizes them, tries to target them or to keep them in check, and in the process becomes more active, more perpetually active. So here is a study showing that that cytokine IL-11 or interleukin-11 is actually stimulated both in animals and in the lab by viruses including respiratory viruses. So these are drivers of inflammation. Most pathogens can be direct drivers of inflammation. Okay now we just have the ability, the viruses and bacteria, but I'm going to focus on viruses here, can basically just hack the signaling pathways that are the heart of our longevity health networks. For example, there are pathways in the human body that control processes associated with cellular senescence. For example, that's the ability of a cell to correctly divide, to correctly grow, not overdo that, not sort of underdo that process, but to do it robustly. There are networks in the human body that scientists have calculated as mattering in terms of human signaling associated with longevity. Well, this team did a network-based analysis, and they uncovered dozens of viruses that encode proteins experimentally demonstrated to interact with proteins associated with these human aging networks, including senescence. So just dozens of viruses and thousands of interactions between these viral proteins to the point where they ended up calling dozens of viruses in the study that they identified age distorters because of the study that they identified age disorders because of the fact that their proteins could have such detrimental or modulatory effects on these aging networks. In other words, their reproduction and their ability to replicate benefits directly from interference with their host's aging processes. And here on this chart are just some of the top viruses that were basically shown to have proteins that interfered with human cellular senescence pathways. There are the herpes viruses, which most of us acquire over the course of our lives, the papillomaviruses. Interestingly, influenza A virus was one of the top drivers of aging. And we never even think about the flu type viruses, which is influenza in an aging capacity, but we probably should a little bit more. So with that in mind, then, this is a final takeaway from that paper. This is what the team concluded. Owing to the considerable number of human viruses, this evolutionary-minded view encourages a reconceptualization of the locus of aging, no longer exclusively focused on our own genetic material, but expanded towards a larger set of genetic entities interacting with our species, such as viruses. So boom, the heart of aging. All right, now what about cognitive decline? What about just direct mechanisms by which viruses or other pathogens can drive cognitive decline? Here is one. This is the team that we work with at Harvard Medical School. They're really cool. They've been using models of a brain in a dish where they actually recreate the neuron structures of a brain in a model or just experiments in mice to show that the Alzheimer's plaque, the amyloid beta plaque that is the plaque that forms in the brains of patients with Alzheimer's disease that defines the disease, actually acts as an antimicrobial peptide or part of the immune response that forms in response to pathogens directly in order to combat them. So basically what happens is there's a virus that gets into the brain tissue model, and then the plaque forms around it as part of the response to the virus. That's what they were showing in this study in response to the herpes viruses. But this team has also shown the same phenomenon with bacterial pathogens and with fungal pathogens. This places infection at the heart of the driving of amyloid plaque in the Alzheimer's brain. Here's another example of a team who's working on the same phenomenon. Van Riesheids' group at Arizona State University. This is an image of cytomegalovirus, which is another herpes virus that many of us just carry with us for life. It's look in the image here, it's concentrated in the microglial immune cells of the brain around the plaques of these cells along with the axons and dendrites of neurons, again, that are inflamed and directly part of the Alzheimer's disease process. So there are a growing number of teams connecting viruses directly to neurodegeneration. Now, this is an interesting study. Okay, what do we do? Well, there's some really low-hanging fruit. No one's even doing anything about this. And this is an example of just easy measures we could take to control the impact of viruses on aging if we made it a priority. This is a team in Taiwan. And what they did is they tracked people over time, some of whom were given just affordable herpes virus existing generic medications. So for example, let's say someone gets genital herpes, they're given Valchex. It's just an over-the-counter herpes, anti-herpes virus drug. So some of the people in the study were given more of those anti-herpes virus drugs and some weren't. When they looked at the group that was given these anti-herpetic medications, they had a much lower risk of dementia than the people who didn't. In fact, up to a 10 times lower risk of dementia. So really, it's extremely low hanging fruit to maybe start to use some of the drugs that we have to inhibit viral activity in the context of human aging. Now, what about in COVID, long COVID? Now, I know we're all a little burned out on COVID, but really, part of what our group does is study still the chronic consequences of SARS-CoV-2. We have to. It's because it's one more virus that is one of these players that can contribute to chronic disease and unfortunately aging processes. And you'll hear about long COVID and it sounds like a vague phenomenon when you hear about it in the news often. Really, it's not though. A lot of us that are directly studying long COVID realize that the persistence of the SARS-CoV-2 virus in tissue in the human body over time, in other words, SARS-CoV-2 potentially becoming just another member of the human virome, is happening in at least a decent number of people with long COVID. And here's a paper that a group of us of long COVID researchers wrote about SARS-CoV-2 persistence as a driver of post-COVID symptoms. Here's an example of a team that we work with. This is in the bottom right, gut tissue from the lining of the gut collected from someone almost in one case over two years after they got COVID. And in this case, the person did have symptoms. They had chronic symptoms. But still, what you're seeing there in the purple, that pink part is the SARS-CoV-2 virus still there in the gut tissue after over two years, sort of embedded there with immune cells around it, clustered, preventing it potentially from being cleared. So it's there in a persistent capacity, which means that at least in some people, SARS-CoV-2 may be acting or seems to be acting as a persistent virus that can also contribute to chronic disease and aging processes. In fact, this is a table from one of our papers that just, this is just some, there's some other studies that have shown persistent SARS-CoV-2 up to then over two years or more after initial infection in at least a subset of people. Okay, so then what do we do about this? Well, one of the drugs we're actually looking about in the long COVID world is called CORE, which is treating people with conditions initiated or exacerbated by infection, is we're actually running a trial of rapamycin in patients with long COVID. Rapamycin is an mTOR inhibitor that has, you know, different properties on the immune system. Now, one of the things that's really interesting about rapamycin is that in some studies, at least rapalogs or analogs of rapamycin, drugs similar to it, have been shown to, in a low once a week dose, not in a high dose, in a lower dose, to enhance parts of the immune response that can better control viral infection. So for example, that trial that I showed you gave patients two rapalogs over the course of six weeks, and a couple couple things happened. First, they showed the people taking rapamycin an increase in interferon-induced antiviral gene expression, with interferons being one of the primary molecules or parts of the human response that combats viruses and keeps them down. Also, the people in the rapamycin group, everyone in the trial was given the influenza vaccine, but those who took rapamycin group, everyone in the trial was given the influenza vaccine, but those who took rapamycin had a more robust response to the vaccine. In other words, their immune system seemed to activate more and create more antibodies in response to that vaccine. Also, the participants on rapamycin, even though they just took the drugs for six weeks, reported a lower rate of infection for a full year after being on the rapamycin. This includes respiratory infections, UTIs, though, multiple types of infections, suggesting, again, that rapamycin was helping to control viral activity. And in a related study, the team found that rapamycin in some patients improved T-cell exhaustion. And again, when viruses persist, they tend to knock down T-cell cells and their activity, which are parts of our immune system, making them literally exhausted. And rapamycin was shown to potentially improve that. Again, so rapamycin, we're trialing now to see if it might help patients who have persistent SARS-CoV-2 or other virus problems in long COVID better control those infections. And what this means is there's a you know, a use of rapamycin in terms of potential viral control that is also probably relevant to human aging.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:35:00.000Z",
      "slot_end": "2024-11-14T08:50:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/17eofu9OtkjONNHPpAdEmPg8MIz7E8ahPAxLdRwJsfNY",
      "resources_slides": "https://drive.google.com/file/d/13rvMfS2uyuibX_a3uOoFiSmF9Q5yXf5M/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "single-slot-finality-and-the-future-of-staking",
      "sourceId": "LZCP8E",
      "title": "Single Slot Finality and the future of staking",
      "description": "Discussing the evolution of the thinking around future upgrades to the Ethereum consensus protocol (single slot finality project) in relationship to the future of staking. For example discussing things like https://ethresear.ch/t/orbit-ssf-solo-staking-friendly-validator-set-management-for-ssf/19928/3",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Ethereum Roadmap,Home staking,Single-slot Finality,Consensus Mechanisms,Security,economy,Consensus Mechanisms,Core Protocol,Ethereum Roadmap,Home staking,Single-slot Finality",
      "keywords": "Economic,security",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "82fbf9013dca892d6a0f02a4a78bd16cd2cd1448127fd267782bdeb1e9cbf5e9",
      "sources_youtubeId": "6VEEAemYaeI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:40:00.000Z",
      "slot_end": "2024-11-14T09:10:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1198JUW8nHiS-gIHBkbDTKrorHlxq2jJXKTiMaVCMvcI",
      "resources_slides": "https://drive.google.com/file/d/1634mOw_plgIJ5zzOzx_9Wq1b_ypPpluz/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "tackling-east-asias-population-decline-issues-with-local-coops-subsystem-for-local-governance",
      "sourceId": "QKMVPC",
      "title": "Tackling East Asia's Population Decline Issues with  Local Coop's Subsystem for Local Governance",
      "description": "Local Coop envisions a world beyond nation-states and capitalism, fostering mutual aid and co-creation. It promotes self-reliant community autonomy and public goods, targeting East Asia's declining population. The system includes digital resident IDs with NFTs, democratizes emissions trading, and manages resources sustainably. Partnerships with local governments facilitate transferring public goods and services to Local Coop, optimized through technology and resident participation.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Local/SEA",
      "featured": false,
      "doNotRecord": false,
      "tags": "Public good,Local Impact,service,public,Autonomous World,Local Impact,Public good",
      "keywords": "Population Decline,Local Government,NFT,Public Service",
      "duration": 558,
      "language": "en",
      "sources_swarmHash": "970e2579b6bacb4f94a244e5139a9f6388fba8e2a7198feced8dc071c16bd62d",
      "sources_youtubeId": "pmWRDoV3ug4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735bb619dbb7a90e19e862b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735bb619dbb7a90e19e862b.vtt",
      "transcript_text": " Thank you. I believe that the most important thing is to implement new ideas and mechanisms into society. If you ask me the way to start the implementation into real society, I will answer in rural area of Japan where the population is rapidly declining. And Japan's population peaked in 2018 and has continued to decline and it is fair to say that Japan is one of the countries in the world with the most declining aging population. And it will become a serious issue not only in Japan, but also in East Asia. Thailand is about to enter a time of population decline. The map of Japan is colored by red by local governments that predict it to likely disappear by 2050, but 40% of the more than 1,700 basic local governments are in danger of disappearing. We have decided to focus our attention on Japan's local community and local government to develop and implement a new community operating system, Local Coop. By taking back the laws previously left to government agencies and putting them in our own hands, we will build a foundation for a more flexible and fulfilling way of life. Numerous small communities will be created, and the people who move freely between them will solve problems and create new values through co-creation and mutual assistance. Local co-op is truly a mechanism for funding local commons whereby local assets such as vacant houses and best forests in a community can be made accessible and nurtured as commons while raising funded external resources. The goal is to optimally redistribute and invest common funds without the involvement of the former government or council in governance. Now let's describe the specific case study. The first case is Yamaguchi village, a type of depopulated area in Japan. Currently, Yamaguchi village has a population of less than 770 and is facing difficulty in maintaining the infrastructure that is essential for daily life. With a sense of crisis that their village will be disappeared if nothing is done, we are contacted by the villagers about three years ago and offered this proposal. Let's let people around the world know what is going on in Yamakoshi Village and get our friends. So let me show the video. This is the rural community of Yamakoshi, Japan. In 2004, a massive earthquake hit this part of Japan. Many left and never came back. If my parents and mother are gone, if the young people don't come back, this village will be destroyed. To help deal with this, Haruka's got some ideas in the digital space. To avoid losing our identity, I started using NFTs. When you buy an NFT, it's almost like your passport to be a digital villager. How many do you have now? There's 1,012 people. There's 1,000 and a half people that have bought these NFTs. Yes. Whether you live in Japan or in the world, anyone can buy an NFT and become a member of Yamakoshi. It's worth not underestimating the value of that legacy. And I hope Web3 technology can help bridge the divide between young generations and historical legacies. Currently the number of the digital villages both domestic and international exceeds 1,700 more than double the number of the real villages. The second is about a relationship between the sustainably regenerated nature in the sustainable local community. We have focused an abundant of nature in Japan. 70% of the country land is forested and the archipelago is surrounded by the sea. On the other hand, as a result of the decline of the forestry and the other industry planted as a result of the decline of forestry and other industries, planted forests have become desolate, creatures are disappearing, fish are harder to catch, and the risk of landslides and other disasters is increasing. It is important to guarantee economic feasibility while restoring the cycle of nature. SINLA makes it possible to raise funds for the maintaining of Japan's local forests and the oceans by allowing pre-holding rights for the carbon credit to be created to be exchanged on the blockchain. By using the funds raised to maintain natural resources, carbon credit can be created. The project is also promoting the democratization of the emission trading so that individuals as well as companies both domestic and international can participate in the trading. As our first site for creating a carbon credit we have a partnership with Oasis City. On the other hand, carbon credit are not only things generated from forest maintenance. We are working to create biodiversity forests and restore the nature that humans have destroyed. To create a watershed where diverse creatures can return and disasters are less likely to occur. is now in the position to generate carbon quality worth 330,000 US dollars per year. This model, which can be financed by caring for the local nature, is beginning to be deployed in other regions. Local coop, the practice is still in the process process but we have discussed with a specific example. We are forming the common fund with a steady stream of the funds as a resource through the multiple channel. The common funds are matched using methods such as a quadratic funding. In Japan, we are currently conducting demonstrations in four areas and plan to conduct demonstrations outside of Japan in the near future. Local coup is a strategy to start by creating a subsystem of local government in Japan, which is facing the rapid population decline, and it is an excellent opportunity to design the original radical self-government. We intend to develop local coop as the main system to implement a plural world where people freely move between multiple communities and help each other, creating the communities that are self-serving but necessary for survival. Thank you. Thank you. It's very interesting because some of the community members in Taiwan actually gather resources to buy one of the NFT. So we're technically one of the villagers. Any questions from the audience? Anyone? Okay, death. I'm going to throw it. Oh, oops. Very far. I'm going to throw it. Oops. Very far. Nice. Thank you so much for that very brief overview. I'm wondering if there's ways for people outside of Japan who maybe don't have a Japanese passport to participate in these kind of schemes. Is that part of the conversation? Is there any way to design in digital nomads to come in and experience even for short periods of time? In Japan, there's some kind of discussion about the digital nomad visa ID, but it's kind of separated. So now we are just for example, Nishikigoi NFT is just digital resident certificate NFT. So you can participate in a discussion and you can participate in voting. And also you can visit to the village and stay and talk with local residents and people and participate in a festival, something like that. And it's interesting. There are so many elderly people over 80, 90 years old, but they all realize digital villages. So if you're a digital village and and go to the Yamakoshi village, probably elderly, the little grandmother, grandpa, but they realize the relationship between the local resident and the outside of people, digital visitor. That's wonderful, I think. Thank you for your question. Thank you. I'm sorry, but we only have...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:40:00.000Z",
      "slot_end": "2024-11-14T08:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/105LJog6X4qLZc6Fr_TdY9gMTLhUukbrbE677s9fsW6E",
      "resources_slides": "https://drive.google.com/file/d/1pXURERD6HMluuQR3lPCLOiTsNv3OFk2Z/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "leveraging-ethereum-for-sustainable-solutions-in-southeast-asia",
      "sourceId": "F7Z87P",
      "title": "Leveraging Ethereum for Sustainable Solutions in Southeast Asia",
      "description": "In this talk you will learn how Ethereum can shape a sustainable and regenerative future in Southeast Asia. We will dive into the challenges faced by communities like Thai farmers, and how cryptoeconomic solutions can drive resilience, fair markets, and renewable energy adoption. Discover innovative projects tackling coordination failures through cryptoeconomics, from parametric insurance to decentralized energy exchanges, and see how you can contribute to this transformative vision.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Local/SEA",
      "featured": false,
      "doNotRecord": false,
      "tags": "Ethereum for Good,Climate,SEA,ethereum,case,use,Climate,Ethereum for Good,SEA",
      "keywords": "Ethereum,Use,Cases",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "faa954cc0d587a16e9dac5073d053dbe28772d3b17a31ca5bd910439b488f60d",
      "sources_youtubeId": "G87k3c_cLLc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:50:00.000Z",
      "slot_end": "2024-11-14T09:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/103WQKb3Z0-Knd415-KUFx0TbNISdUujVoQzaXW3xd3Q",
      "resources_slides": "https://drive.google.com/file/d/1zfHNGm9EufK4kThv5QSBw7Rl63KBdBsL/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-silicon-hospital-and-a-new-way-to-make-medical-devices",
      "sourceId": "D8UTDS",
      "title": "The Silicon Hospital and a New Way to Make Medical Devices",
      "description": "Could silicon be more effective for medical treatment than drugs someday? We think that day could be soon.  Openwater has spent nearly 9 years developing new tech to treat a range of diseases.  It's not pulse ox, fNIRs, HIFU or EEG ... it's new hard tech and it's open-source.  We will demo the tech on stage, and share with you our clinical results to date and explain how the technology works.  We expect to be in over 100 clinical trials next year.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DeSci,Open Source Software,Scalability",
      "keywords": "Healthcare,,Medical",
      "duration": 939,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "hNFQtpNHufk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735bf739dbb7a90e1c73a9e",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735bf739dbb7a90e1c73a9e.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\" Peter van de Ven Hi, great to be here. Just first a brief intro to myself. I'm a hard tech pioneer in moonshots. I've created multi-billion dollar products on the hairy edge of physics. Former MIT professor, Brown University PhD in physics, and about eight years ago I started my latest project, it's called Open Water. And in rooms like these we've been building, trying to look at this idea of using the fact that our bodies can be penetrated by infrared light, ultrasound, and electromagnetics, and a belief that maybe we could leapfrog current healthcare systems and even drugs by modulating the phase of light and sound. So we've been doing that for a while. And the reason that I did it is I've shipped a lot of innovative silicon chips. So using silicon manufacturing that's coming online in consumer electronics and phase wave steering, I knew that we could steer beams of light, decode phase with interference, and even selectively select or destroy certain cells or stimulate certain cells. So we built these systems up for about four years, got pretty interesting results, filed a bunch of patents, a lot of work, and decided in early 2020 to get them into hospitals, start testing this on people. And the results are somewhat spectacular. First in stroke, large vessel occlusion stroke is the number two killer in the world. If you could diagnose that stroke in an ambulance, you could get the patient to the one of less than 5% of hospitals that could treat that stroke to the one of less than 5% of hospitals that could treat that stroke so the person wouldn't die so we built a portable that gives higher specificity and sensitivity for large vessel collision stroke than we can find published anywhere in the world that's 6.5 million lives per year now we go over to number three glial we gave some mice glioblastoma and treated them and move these mice into remission and on autopsy found that we didn't harm any healthy cells Charles River couldn't find any while we were waiting to get into humans for that we take that same device and put it into humans for severe depression and a 20-person trial and we were able to Turn off the over firing neurons in the default mode Network sending these patients into remission So with that we're pretty excited It was time to go get more money We've raised a hundred million dollars today and we're announcing today, for the first time, that Vitalik gave us $50 million from the Shiba Inu coin. Woo-hoo! Thank you. Nondilutive. And it's made all the difference. With that money, we've been able to reduce those carts from really a laser the size of a room to this that's going into production next month and those carts that were the size of the size the room as well to these to little consoles and all that's going into production next month this is a platform with a life-saving pan disease potential for not just stroke but all cardiovascular disease not just glioblastoma but all cancers not just mental disease but potentially all mental disease and beyond so open source is cool we're all believers but where this impacts the health care is even if you make hardware and software open source we open sourced all of our patents you still need approval to use it on somebody it's FDA it's regulatory approval the reality is in a comprehensive paper published recently the average cost of a novel therapeutic medical device approval is now 658 million dollars in capitalized cost and 13 years. So even if you open-source it, huh, what do you do? Well the key is here. 85% of that cost isn't in clinical trials. It's in the device. So that takes a lot out. Further, another 7% of that is in safety. So share the safety. Do we want a safer device or a less safe device? Will the regulator be able to approve a device, pan-cancer disease with lots with a hundred times more safety data will that make a choice easier or not will a doctor and patient find it easier to be able to access the safety data we think it's yes on all of those running those numbers our hypothesis is we take 653 million dollars out of the cost of developing a novel medical device with a de novo approval. So down to 5 million sub-three years. So how does this work? Phase wave modulation. Well these are the waves in light and sound. And what we do, for example, on these yellow emitters is we delay the phase from emitter to emitter so we can focus that sound near or far right or left up or down anywhere we wish to in the brain or the body so glioblastoma cells can hide out among healthy neurons and the neurosurgeon you know that you can't take them all but these cells have a mechanical property that no other cells do all aggressive cancer cells have really large nucleuses and small cytoplasms and we can exploit that like an opera singer can ping this wine glass hit that note and destroy that wine glass and harm nothing else in the room so that's what we're doing with glioblastoma. We did sonification parameter sweeps, found the frequencies. This is low-intensity ultrasound. Lower intensity than used for diagnostic reasons on pregnant women and their fetuses for the last 50 years in rich countries. But these cells can't take it. We're destroying them. And here, at a different frequency, I can tell you the frequency of our open source, this is 400 kilohertz, the other one is 150 kilohertz, we're able to turn on, stimulate, or destimulate neurons. And that's been helpful not just for non-invasive brain computer interface but also for pulling people out of mental disease addictions and so forth it's all together I think we're enabling read write brain computer interface while we're trying to solve mental diseases it's they're impossible to separate so another module this is our read module, uses that laser I just picked up that we've reduced down. This laser and a bunch of camera chips, four camera chips that are in your smartphone. And those camera chips are super small. We take the lens off and they've got, they're cheap, because they're small. They have pixel sizes, the size of the wavelength of light. So we let that light go into the head, and what we see on that camera chip is this. And it looks like the waves on the ocean, and we can read the waves on the ocean like a sailor can read the waves on the ocean and know where the fish are, know where the land is. Here we interrogate a blood vessel and the pattern changes because where the light hits the moving blood cells, it ricochets differently changing that pattern. And we can read blood flow very accurately with this way. In fact we can read it 20 times more accurately by some measures than multi-million dollar MRI or CT scanners with this laser and some camera tips. So that's not just good for stroke, but all cardiovascular disease and all micro motions of the body, we believe. So to get this far, we've been working with a whole bunch of great institutes, but everybody wants it, so that's why we shrunk the system down and decided to go into production. And also, you can build it, it's all on GitHub, you can build it yourself if you'd like to. Everyone that wants it, you can all have it, you don't need permission, go to our website, take it. I want to mention here there's also evidence we may be able to suppress cytokine storms as amy just discussed the impact of them treat inflammation even deactivate covid with the same way we're deactivating leoblastoma cells there's some evidence we're working on that in our labs and we're using our devices to do research on long COVID and microclots. We can see if we can ID them and if we can break them apart. So as we discussed, this is the problem. $658 million in 13 years. It's why we don't get any new devices. It's why healthcare... We've got 20 to 40-year innovation modes. So here's what we're doing about it we're flipping it on the side to make a open source generally general-purpose platform and the apps on top it is either treatments so those are the ones with regulatory approval three years five million dollars then we get into the call to 510 K substantial equivalents said one million, sub one year. So the question, we're a for-profit company, I hate to be crass, but people sort of scratch their heads and ask me quietly afterwards, and so I thought I'd just come out like, look, it's free as in beer, it's free as in speech, not free as in beer, like you can publish the recipe of the beer. You can go make your own at home. Most of us just go to the store to buy it. But by making more of something, it's cheaper. We enjoy a portion of that savings as profit. We engender trust. That's new. We don't charge $1,400 for an EpiPen one day. If we misstep on something, if we screw up, you can go get it made yourself. But we ensure quality standard that's IRB ready and FDA ready, we're ISO 13485 compatible, by safety sharing. We have the cost of clinical trials and double the speed for our customers. Plus, everybody just benefits. That's a cycle of innovation that's not a 20-year moat. So here are these new units. I've got them out here, LIFU and OpenMotion. All the electronics are going into the mechanicals and being tested, so they'll be ready next month. Here's more. And this brings me to a moment. We also did something just to save another $150,000 using our smartphone. And Vitalik, are you going to come up to get scanned in? So this is just in, so we prepped a little in this so this is the whole unit that's replacing that room and so if we put this on you I can actually turn it on see it so and then I will oops the strapping is being perfected it's Belka I don't think it's sticky enough. Can you? All right. So what we do, whoops, thank you for doing this. Oops. I just opened the wrong nap. Oop. So, I'm just putting in a number here, and then I start, and, Just putting in a number here and then I start and it's just saying unable to focus, okay. Try again. Okay. Okay. So I'm just taking some pictures of him with the camera. That's all you have to do is walk around and then we have to get the top of his head. Okay. That's it so it's all so we can take this off now although we should leave it on if we were actually going to sonicate him so now we scan him in and Vitalik has an amazing brain and we get to look at it right here although this is the interesting part a bit this is an MRI he did last week and what we're gonna do is pick a spot for target the interior singlet is gonna make him happy we know that and so okay with standard so these are what the pictures look like. And then this is making him 3D. And this is so we can know where the... I'll show you what this does. So here we overlay the 3D MRI with a scan so we match his bone structure. And then we look at where the transducers are and we know exactly where they are. And now we set the target and we compute what phase delays we need on the transducers and then we nail that target. I would love to sonicate him on stage but we don't have permission from the Thai government yet. However, I met somebody that got permission to do 200 people at Esalen recently. So next year, we'll do something. But yeah, this is how it works for any part in your body. And the interesting part is, as we develop our read part, the part seeing blood flow, we can get away from the MRI machine and use different scanning technology that's lower cost. So with that, this is somewhat like a smartphone moment from 20 years ago. Smartphones changed our lives. Look, a silicon hospital can happen. If you think of your smartphone, there's a dozen radios in it, four cameras and so forth. As we populate this, it can treat us for the diseases we have. Again, you might be asking, can I have one? My country, whatever. Yes, everybody can have one. Everyone has permission. You can build your own. I'll be showing these prototypes out in the hall shortly, and thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T08:50:00.000Z",
      "slot_end": "2024-11-14T09:05:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1cscUxEQdkm5QVkLDeEDz09MWGMPqPDhGH5xZlEf1yRQ",
      "resources_slides": "https://drive.google.com/file/d/1gFOq5_x-vCxTLqykwut0WLMaWJp8GRuF/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "cross-l2-with-intent-addresses",
      "sourceId": "BEWE3Q",
      "title": "Cross L2 with Intent Addresses",
      "description": "Ethereum today is more fragmented than ever before. We'll talk about how CREATE2 intent addresses and ERC-4337 can be used to enable fast and smooth cross-chain interactions for consumer applications.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Cross-L2,Account Abstraction,chain,abstraction,Account Abstraction,Architecture,Cross-L2",
      "keywords": "CCTP,bridging,chain-abstraction",
      "duration": 1467,
      "language": "en",
      "sources_swarmHash": "357fe5c92b73f63d2f7a80622e435d3f2259295c1227433add5efdfa0ad8bac8",
      "sources_youtubeId": "ioCdBWLmuI8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c5159dbb7a90e10e8725",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/19NEsXDqwrsMeZ3hvkerJHBNN4pTD7oRWn6fSazU8JWU",
      "resources_slides": "https://drive.google.com/file/d/1A4_Pa2cbDHHe6_L6gm2ktHCnOzWEdQW_/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "elevate-your-vibration-reggae-sesh-w-rafamilkz-and-friends",
      "sourceId": "NNFDDB",
      "title": "Elevate your vibration! (reggae-sesh w/ rafamilkz & friends)",
      "description": "A reggae jam music sesh performed with soul and heart by web 3 builders & musicians, with the goal of elevating the vibration of Devcon, fostering an environment of peace, love and community! \r\nI have a list of songs to play (guitar and voice), and will have other musicians (cheers to Shaka!) to perform with me and increase the vibrations!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Art,Marketing",
      "keywords": "music,relaxation,fun",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:45:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/14nyL7Ln8KMC-c1thokTKnggtUR8lxRb5WI3bRH2a-uQ",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "ethereum-and-robots",
      "sourceId": "9G9LSH",
      "title": "Ethereum and Robots",
      "description": "I will describe how Ethereum can be used in the emerging consumer robots industry (and generally for autonomous machines).\r\n* privacy preserving surveillance\r\n* autonomous transport\r\n* factory to consumer - tokenization models\r\n* Laws of Robotics - zk hardware",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Collective Intelligence,Civil Resistance,DePIN,Autonomous World,robots,Autonomous World,Civil Resistance,Collective Intelligence,DePIN",
      "keywords": "Robots",
      "duration": 1531,
      "language": "en",
      "sources_swarmHash": "1f7a566e27b32382aa1713acd471469239bf022cfe7569f8241c430a8ebb9577",
      "sources_youtubeId": "SQzTSNurehU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c5c19dbb7a90e1266802",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735cae29dbb7a90e1adef32.vtt",
      "transcript_text": " Hey, everyone. My name is Marcus. I'm building EthOS. So the first question is what is EthOS? So three years ago, we forked Androids, more specifically Graphene OS. We started building Ethereum, putting Ethereum into the OS layer because we think current OSs see Ethereum as an app. We want to leverage the protocol. The first thing we we did was basically put a light note on it. And, yeah. And so, for example, we have on E4OS, we have a system level wallet or OS level wallet. So we took basically a signer, put it into the OS, and any app can basically connect to it and make signature requests to the user. And the upgrade for the users, they don't have to switch apps or anything and it just works. There's also basically upgrades to privacy and security and basically the reason is it doesn't do any outbound internet transactions so you also don't need any internet to sign anything. Also, yeah, as I mentioned before, we put right now on EthOS, we have two light client proxies. So when we first started, the first thing that we wanted to do was put a light node on it. And at the time, we didn't have much to choose from. So we took the Geflight node, put it into the OS as a system server, and basically what happened was it really into the OS as a system server, and basically what happened was it really turned the phone into a pocket warmer. The battery really, really drained pretty quickly. But it did work when you had a stable internet connection, it would connect to peers, and then you would have a local RPC with which you can send transactions, get blockchain data and also any app could connect to this RPC. Yeah, as I mentioned right now, we have Helios and Nimbus verified proxy. They're both currently Liteclient proxies, not full Liteclients, because of the merge, the Geth Lite node stopped working because the beacon chain was added to Ethereum. And so right now, basically, you still need an essentialised RPC, but it does verify some pieces of data that it gets from this RPC. So this is pretty cool. And also, like, for the UX for the user, we wanted to make it so that it's, like, as simple as turning on and off Bluetooth or Wi-Fi. You can turn on your light client and just, just like have a local RPC on your phone. And so this brings me to the first issue that we have. Basically there is almost no mobile native dApps currently. So basically I'm a big fan of like native dApps. Currently like all of the dApps are like web dApps and then there are some that are progressive web apps, but they still face the platform risk of Apple and Google can shut you down if you are a PWA. And so basically what we created last week actually is like basically an app wrapper that is native. It looks and feels like a native app, but it actually opens up a web view that auto-injects the OS level wallet into the web app. So for users, it's just like opening Aave and the wallet is connected and you can start like basically doing transactions. And so this brings me to basically our announcement from last month. We announced our first hardware. So basically what we first did when we first started was basically bring your own device. We still have a web installer where you can basically buy a pixel by yourself at a local store. It has to be bootloader unlocked. You connect your phone and it just installs ethos on it for you. But people ran into some issues. So basically, you know, because you had to get a specific pixel and it had to be bootloader unlocked and some carriers actually lie about it being bootloader unlocked. And so basically, we then started like selling Pixel 7As running EthOS. But then we also came into a roadblock and basically the roadblock there was that it's very hard to reliably get pixels. And so we were just like, hey, let's do some hardware. And so we started talking with some phone manufacturers and they sent us like some phones that they did in the past but they all really looked bad. We were just thinking, okay, nobody is going to replace their iPhone with a random crypto phone. So, we thought to ourselves, hey, let's first make a really cool, fun and exciting secondary device that because it's a secondary device, it doesn't have to be boring. Basically it's called the DGEN1 because it's our very first device. It's our first device on our journey that we want to like our big mission is like self-sovereign hardware. And basically what is self-sovereign hardware? I'm glad you asked. So basically I wrote a blog post about it last year, and for me self-serve and hardware is where basically not only the OS that runs on top of it is open source, which we have currently, Ethos is fully open source, but also the hardware is open and inspectable. Like literally from the CPU instruction all the way to the app. It's a big mission, but I think it's very important. Because currently with like, CPU instruction all the way to the app. It's a big mission, but I think it's very important. Because currently with like if you look at the current hardware wallet landscape, for example, if you take a ledger, you really rely on security for obscurity. And that's very, very bad, especially if you're storing large amounts of crypto on it. And basically not only that, but also basically last year there was like this big controversy, if you remember. The ledger, they announced like this ledger recovery program where they basically said like yeah, with a firmware update we can extract your private keys. And it was like, oh, my God. Okay. So, basically if a ledger can, a firmware update can extract your private key, what stops a malicious actor from pushing a malicious firmware update? It's basically the ledger firmware signing key. Now I assume and I hope that they hold it very dearly and very secure, but this is like a huge trust assumption for like every ledger to be secured by basically one ledger firmware signing key. So this is very bad. And so for self-server hardware, it's a natural use case for crypto because you don't have to trust. There's no trusted modules. Everything is open. And that's one of the big, big things with the current phone landscape. The iPhones is the worst because it's like, okay, closed OS and closed hardware, you can't inspect it on any level. But with Android it's a little better, you can inspect the OS, but there's still like proprietary blobs of code, like for example, like play services and like device drivers for hardware, which they don't allow you to open source just like ledger, they can't open source some things on the low level stuff. And so this is like very bad as soon as you have crypto in it as well because you're like, okay, I have this phone, it has this OS and it's open source but there's still like some trusted like blobs of code that are closed source and you don't know what they do. And so basically, yeah, that's like our big mission. And yeah, the DGN1 is like our first step towards that mission.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1s1aFTwzOBXNg9v3Cu1EnNW22GUWNxNYFneRubREaJXE",
      "resources_slides": "https://drive.google.com/file/d/19jj064cf1jQRV8xcl5AYOsrj3OMhFl48/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "from-web2-security-with-love",
      "sourceId": "VYEKSS",
      "title": "From Web2 Security With Love",
      "description": "Web3 organizations often rely on Web2 for infrastructure, communications, and development, yet their Web2 security posture is often neglected. This leaves them vulnerable to a wide range of adversaries, from well-funded sophisticated attackers to opportunistic script kiddies. In this talk,Joe Dobson will share hard-earned lessons from the Web2 trenches that can help secure Web3.Don’t make it easy for the adversary. Learn from the past: strengthen your Web2 security to safeguard your Web3 future.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Hacks,intelligence,Hacks,Security",
      "keywords": "Intelligence",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "b2b9cd3ea5ae963a562ed64dde0eca9d48ca6dbb93cd638d5a55c9e0f287485f",
      "sources_youtubeId": "s-8nDKk_kkM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1Q9J9HaQFEJ3SPx50bpp3xIlPzaHzn4kJ8ESPA0lnVoI",
      "resources_slides": "https://drive.google.com/file/d/13qm_aUZUBrab3BRLDDAzhCPB86v5zVeM/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "little-things-weve-learned-about-fhe",
      "sourceId": "9JFDZA",
      "title": "Little Things We've learned About FHE",
      "description": "Recently, at PSE, we have been exploring the field of cryptography, specifically focusing on Fully Homomorphic Encryption (FHE). FHE enables secure interactions with encrypted data between different parties.\r\n\r\nIn this presentation, we will introduce key concepts and essential information tailored for developers and application designers. This will help them quickly grasp the fundamentals without getting bogged down by complex mathematical details.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,Homomorphic Encryption,eli5,Cryptography,Homomorphic Encryption",
      "keywords": "ELI5",
      "duration": 1181,
      "language": "en",
      "sources_swarmHash": "abd4e937b9caaecc4dc7c05e26ceccf32cd810e419e9f50e4b17fe00e8a55fce",
      "sources_youtubeId": "4AEKFmPftSY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c37a9dbb7a90e1fa8ae4",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735c37a9dbb7a90e1fa8ae4.vtt",
      "transcript_text": " Thank you, Teresa. Hi, welcome here. So, I'm C.C. Liang from PSE. We're talking about FHE today. So, here's my selected resume to fit my narrative. First, like in August and July this years, I've been working on ZK-EBM projects with Kimi there and a bunch of many people. And more before that, I was working on different various applications in PSE. So I think in summary, what I have seen is a rise of computational primitives on cryptography. So what does that mean? Like we discovered a bunch of computation legos. This legos starts with gates like this. You can add numbers together or you can multiply numbers together and we call these gates arithmetic gates and if you're coming for a programming world you're probably seeing this N gates or gates XOR gates and not gates and you can fit bits inside it and do operation with it. These are called Boolean gates. So these are kind of abstract models. Once you get this, you get the whole computation you need. You can serve user with different kind of application logic. How does that go? So first you have gates and then if you combine these gates together you get circuits and then if you build language and compilers on top of them and you feed developers some coffees and teas and they'll build amazing applications and to happy users. So I think that's what happened in the ZK past. In a ZK we kind of have some arithmetic gates that allows you to perform computations on secret values and now FHE we are getting new sets of Legos and they unlocked new sets of capabilities and this is what we'll dive into today so before that let's do some recap on ZK calculation So when we talk about computations, we usually start with calculations, and then we build more abstractions, and then we can do computations on actual application logic. like a Sudoku board. They have some pre-filled values 7 and 3. You can fill in some initial value 5 and then you multiply together you get 35. You plus 3 you get 38. You can witness some private value 2 and then you get a result 40. And then you can send input and output to the verifier without sending all the computation trace in the middle. And in this way, you get privacy for your private value. You also get the computation compression for people call this the sickness. And if we accept some more abstractions we wrap developers are very good at turning your application needs into the computational bits and bytes so we can do some high-level logic like this. For example, you have ID 123 that resides in the ID registry. So this computation result tells us this is a valid ID. As this doc, you can insert some secret video in this secret field of ID. This ID has some certain attributes. And then the whole thing is combined together. This shows you know the secret. So you are the owner. This dog is the owner of ID 123. And then furthermore, this ID 123 has an age field, say it's 19. So this dog is 19 years old. But the dog wants to tell people it's 19 years old. The dog wants to tell it's greater than 18 years old with just one additional computation step. So that works well if your user is just one dog. But now, like, if you want to build applications that interact with multiple users, like this dog and cat, and they want to decide what to have for dinner, either khao-soi or pad thai. But their opinions are very sensitive, so secret they need some cryptography that can protect you from state-acted attackers. So they kind of... Let's imagine we don't have any cryptography and we host this computation and trusted third party like the Horse below and then the dog sends their vote one and the preference for pot I 0 and the cat sends the And the cat sends the vote one, and the vote for Pad Thai. Then we perform some computations. We want to add the votes together and to get the final preference for the dinner. But then, you see, this requires the interaction of the dog's secret and the cat's secret, and then it's not achievable by the ZK gate. So that's where FHE comes to help. So FHE stands for, it's short for three words, but I'm not going to torture you with that nerdy details. I'm going to redefine FHE as just one word, and it's a computation over encrypted data. As people say, the ticker is FHE. So what does it mean, like, computation over encrypted data? So you basically get these two new Legos, and then you can have encrypted data from Player 1, encrypted data from Player 2, and then you add them together, you get a computation result. And so when I heard, like, the encrypted computation over encrypted data first time I didn't realize how powerful it is or like a significant it is but it if you see here it kind of builds a boundaries between two players and then you that there are secrets to interact with each other, and you get a computation result. So now, like with FHE, you can do the dinner voting properly. So the dog sends the encrypted preference, cat sends their encrypted preference, and then you use the new FH gate and to add them together to get an encrypted computation result. And then you have to decrypt it to get the actual result. So I think Vitalik has a saying, like, all the technology we have is kind of a simulated, trusted third party. And I think, like think from this example, we're kind of seeing this phenomena. This horse on the bottom is kind of like a trusted third party. And when we're using FHE and without it, the application logic looks similar, but it's kind of wrapping on another layer of security. And from the perspective of user, all they see is the same. They input some value on their browser, and then they got some values from their browser, but there are differences behind the scenes, and there are different security and trust assumptions behind the scenes. So why are we not already using FHE now? We kind of already, like, so in the basement, you have FrogZone from 0xPark I highly recommend you to play it but there are three main problems of FHE right now first is the computation is costly let's say you want to send one bit of plain text from the stock to the server you have to encrypt it and in this encrypted message is a data itself so it has some size and how big is this data so this data takes six bytes is kind's kind of amortized, and it's kind of 100 times of the plain text. And it grows linearly. It means if you have 100 bits, you multiply 16 bytes by 100. This is already very good. But the terrible part is the computation part. You need a server of 192 vCPU. It burns 10 USD per hour. So I highly recommend you to play the frog games in the basement to feel that $10 per hour feeling. So the second problem is the verifiability. So you are getting this computation result, but is this computation result coming from the correct computation step or the incorrect computation step or the incorrect computation step. To be honest, like, you cannot really tell. Like, it needs extra work. You either wrap another zkProof on top of it, you either do some, like, message check or something to either guarantee kind of integrity. It's a problem people are actively solving, but it's still a problem. So the problem number three is the decryption problem. So after all the computations, we're getting the encrypted computation output. But how do you decrypt this? How do you remove that elephant from the fridge? So you need someone to decrypt the message, but you can give the decryption key to a centralized this is like you're not doing FHE at all. But the concern is like if you, like if someone has the ability to decrypt the output, can they also decrypt other users' input? And this, you know, this makes the whole like FHE a security theater. So the other way to solve this is you use some threshold decryption scheme. the whole FHES security theater. So the other way to solve this is you use some threshold decryption scheme. So every encrypted output, you need all the party to derive a decryption key share. And together, they have to collect all the decryption key together to actually see the output so if the dog did not give its decryption key to their input the other people were not able to decrypt the dog's input but the problem here is the cat if the cat ghosted, then nobody can actually decrypt this output. And then this secret will be secret forever. So let's give a quick recap. The message is simple. In ZK, we kind of have computations on top of your own secret. And for FHE, we can engage with other secrets. So you can build applications for multiple peoples. And that opens a lot of doors to many new applications. So I expect you will hear the word FHE in the near years. There will be more talks in the next DEF CON. This is a kind of trending topic. And lastly, I want to direct you to play the Frog Crypto on the basement to feel like how early days of FHE feel like. Also, if you are coming from a developer background, here are some reports you should check. The first two are from Gauss Lab. It's a library for multi-party FHE. Pay attention to the branch because the latest code is living on some branch. And if you want to deploy your frag zone yourself, check the last link. You can deploy a frag zone. And it's early days. Things break. So expect some hiccup from the code. And that's all I have for you. Thank you so much. Alright, thank you, CC. We actually have a lot of time for QA, so feel free to submit questions. Let's go from the first one on the screen. What are the best use cases you've seen for FHE so far? That's a very good question. So for me personally, like definitely not for others, like I'm more from like economic background, and I'm very interested in like auctions and votings. These are very, FFG applications. They are very ancient. But I feel like if we can do proper voting and proper auctions, that would be cool. I think what makes people exciting in the future is if you can build some private state, you can build a, let's say, P2P network. You have a private state, and then you have some state transition function. You can continuously update the states, and then you can build a kind of private blockchain and private application to many, but I cannot say anything concrete at this moment because I don't know. And we have another one. Which optimizations do you see possible to solve performance issue oh so so this is what I heard from our colleagues PA he said like the last so the latest performance update was already in 2016, and there hasn't been new papers for that performance improvement. So we might be looking for a new generation of bootstrapping technique to improve their performance. When should you use ZK, FHE, or MPC? So my over-simplistic understanding of this is if your application involves just one party, then use DK. If you have two parties, use MPC. If you have more than for QA. What do you think of TEs as a solutions for shared state, shared private state computation as a performance solution over FHE? Good question. I don't know yet. I haven't dive into this topic, and so I haven't developed opinions on this. All right, perfect. OK. All right, let's give it another 10, 15 seconds to see if there are any more. Maybe a question for you, like who here have heard FHE before? Oh, okay, almost everyone. Okay. Who here have heard indistinguishable obfuscation before. OK. Oh. Nice. All right, cool. Thank you, CC, for your amazing talk. Our next talk will be in 10 minutes.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1yFyLyjYjdDzT6MDPS4LGolPm0BsYYfhsoxLz5fezE_k",
      "resources_slides": "https://drive.google.com/file/d/14J68hMRQHzjC88VWeNPTxdegCreGl9CT/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "scaling-crypto-theres-an-app-for-that-onboarding-millions-in-africa-with-minipay",
      "sourceId": "EXCPST",
      "title": "Scaling Crypto? There's an App for That. Onboarding Millions in Africa with MiniPay",
      "description": "Post-EthCC, everyone’s talking about the industry’s influx of infra & lack of consumer apps. These conversations overlook the strides made in Africa with MiniPay, a self-custodial stablecoin wallet with 3M+ activated accounts since launching less than a year ago. In this panel, Rene, Yoseph & co-panelists will discuss building, scaling, & updating a truly user-friendly crypto wallet, introducing net new users to Web3 and dApps, & the power of ERC-20 stablecoins for payments in emerging markets.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Scalability,UI/UX,Mobile,Protocol Design,Scalability,UI/UX",
      "keywords": "payment,p2p finance,mobile",
      "duration": 575,
      "language": "en",
      "sources_swarmHash": "7851df1a8fb38bb024cefa2524794dd6ea145f15f791a2a89b592b81f5121777",
      "sources_youtubeId": "cxrKuY7XQoc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c0b19dbb7a90e1dd49c5",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735c0b19dbb7a90e1dd49c5.vtt",
      "transcript_text": " All right. Hi, everyone. I'm Charles from Opera. I'm leading the product team for Minipay. And I'm here to tell you about how we're onboarding millions of users in Africa to crypto and to stable coins. Opera, as you may know, is a browser company. We have many apps, and we have over 100 million users on the African continent. Opera Mini is our most popular app. It's a browser that allows you to surf the web, but in a very efficient way that means that you don't have to buy as much data to browse the web, so people love it. And MiniPay is built right into Opera Mini and distributed through this channel. We've launched about a year ago to five African countries, and we've been getting tremendous traction. MiniPay is not only built inside Opera Mini, but it's also built on the best of Celo. And it's different in many ways to other crypto wallets. Minipay doesn't try to be a Swiss army knife that sees all your assets and all the chains and all the swaps. It's really focused around a payment use case using stable coins. And Celo is perfectly suited for that. It allows us to use phone numbers instead of wallet addresses. So similar as to mobile money systems already in market today. It's super fast and almost free. So a typical stablecoin transfer on Celo is 0.1 cent. So we don't need things like account abstraction to really get the user benefit of gas abstraction. It also has automatic key backup, so when users activate MiniPay, their keys are backed up, they don't lose them, which is a common thing in self-custody wallets. And it's a UI that really tries to speak to users with familiar terms and has this Web2 experience to it with the full power of Web3. And the last part, which very few wallets do, is really have a localized approach to on and off ramp by working with smaller partners to really make sure that it's possible to buy and sell stable coins for very low amounts using local payment methods. Zooming in on one of these differences, the phone number to wallet address system is actually built on Social Connect, which is a solo protocol that allows us to map this in a privacy-preserving way, from number to address, but not the other way. And it really improves discoverability. You can see which one of your friends is already on MiniPay, and it makes copy-pasting addresses a thing of the past. Another interesting feature of Opera MiniPay is that although we've added support for more stablecoins, dollar stablecoins, we wanted to make it easy for users to move in and out of them if needed by having this concept of swaps using pockets. So you can simply drag and swap from one pocket to the other, one to one. And we use the mental protocol for this, which allows us to have super low slippage and excellent fees. As this is a developer conference, I'll mention that we also have mini apps, which are web apps that are integrated within MiniPay. We have over 10 of them now. Also getting tremendous traction since launch and this is something that you can do and you can build using existing web and ethereum technologies and the cello team is there to assess also in making this a reality I'm happy to share that since launch, we've activated over 4 million wallets, mapped to phone numbers, and as of October, we've done over a million, during October, we've done over a million P2P transfers between those users. Thank you. You know, going past the tech and the numbers, right, we're starting now to tell a bit more of these stories that we're seeing in market using MiniPay. I'll go a bit quick now. I see time's running out. But this is a story of a freelancer getting paid in stables and being able to use them locally. This is another one in Kenya, being able to use USDT and pay local merchants for groceries interacting directly with the payment system locally M-Pesa so it requires no merchant integration. This is another one someone has lost their wallet in Kenya and was able to basically pay for everything during a week, thanks to MiniPay and this integration with the local payment system. MiniPay is now growing outside of Opera Mini, so I'm also glad to announce that we've made available now the Android app as a standalone app, so you can just go on the Google Play Store and try it out. It's enabled in over 60 countries, and we basically ramp up as we have good cash in and cash out in those countries. This is what it looks like, really trying to focus on the dollar use case and the payment use case. And we foresee that users in country in Nigeria or Kenya will invite people from outside Nigeria to the standalone app, and you can imagine cross-border payments being powered through this system. Also happy to reveal that the iOS version is coming very soon. So if you are keen to be one of the first to try MiniPay on iOS, scan this QR code, sign up the form, and we'll be sending invites to the test flight very shortly. And if you want to build on MiniPay, please reach out to us on x at MiniPay or at cellodevs. And the cellodevs are organizing bounties during EVE Global. So if you're joining the hackathon, make sure you check it out. Thank you. Thank you, Charles. That was incredible, all the attractions. Any questions from our audience? Any questions? Okay, well, a few. I see the gentleman in the red shirt first. So my understanding is that in Africa, there's not much of a savings culture because many of the currencies are losing their value every year. So there's more of a borrowing culture than a savings culture. With access to stable coins and mini-pay, are you seeing more of a savings culture develop? Are people using this for savings and accruing those savings over time? What sort of data are you seeing around that? So savings is certainly something that we hear could be useful in the context of mini-pay. This is not a use case we're really focusing on now. So for us right now, it's a marginal use case. It's really around money movement at this point. Another one over there. With using USD there, is that somehow in conflict with the local currencies? Is that something that you see as a risk? Like, for example, when integrating with tax payments, that should be happening, or just with local governments not being happy that their original money is not being used, or how is that in Africa? So, I mean, these economies already have a lot of demand for dollars. This is just a different channel to access them. And, you know, users can use Opera Mini and, sorry, MiniPay in a way that they can think in local currency, like they can denominate everything in local currency. So a lot of people are doing that to alleviate the kind of mental friction. One more over here. Good job. All right. What is a mini app? Is it like an iframe or? Yeah, it's an embedded web app. Yeah, it's not an iframe, but we just invoke your app. And each app is within a sandbox, so we only allow known blockchain interactions. Is the output dumping HTML? Yeah, this is existing web technology. This is existing Web3 API. It's not exotic anymore, but it's packaged in a way that users don't have to connect their wallets. It's integrated, and the signing process is very human-readable. Okay, cool. I see that there's a few people wanting to ask more questions, but the time is up for this session. Feel free to reach out to Charles for further discussion. Let's thank Charles for another day. Thank you, Charles.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1lk319WDhop2qBsR_BdMLAl1tdzOwri17ao4IPguI7Ac",
      "resources_slides": "https://drive.google.com/file/d/1IWeENduiUQrxullo7FQJbPPCzmc0EBA9/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "solidity-then-now-and-the-future",
      "sourceId": "HZ3DEF",
      "title": "Solidity: Then, Now, & the Future!",
      "description": "In this talk, I will be presenting the prospect of Q1 2025 release of the Solidity language compiler including the following sections:\r\n\r\n- Latest features and developments\r\n- via-ir: what's happening and what's next\r\n- Experimental Solidity: The future of the language\r\n- Timeline & roadmap",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Languages,solidity,Languages,Tooling",
      "keywords": "Smart Contract Development,Solidity",
      "duration": 1612,
      "language": "en",
      "sources_swarmHash": "f0d38870de6eb47e00161622e7047dec080197153c6387f0b560a0d6c505b0fa",
      "sources_youtubeId": "56JNxjPH-QY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c6dc9dbb7a90e139e25d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:00:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1GmwHGEiPwMU4yfyA7ipBeOYh8M7CK0BgtepZdbx3JFA",
      "resources_slides": "https://drive.google.com/file/d/1V_p-KHAYMNNyteGUsy4WU3jpZfxabh-I/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "building-the-eve-frontier",
      "sourceId": "7SCWQD",
      "title": "Building the EVE Frontier",
      "description": "This is a project demo as part of the MUD Day CLS: autonomous worlds, onchain games, and non-financial applications.\r\nA decentralized sci-fi game built on the MUD framework. We’ll walk you through launching a smart assembly, writing custom logic using our Builder tool, and configuring it in-game. Witness how on-chain mechanics empower players to create and control autonomous game elements in real-time.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": true,
      "tags": "Autonomous World,Decentralization,Gaming",
      "keywords": "Decentralised,Sci,Fi,Gaming",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:05:00.000Z",
      "slot_end": "2024-11-14T09:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/14-gheTUG-tAY311u_ro8WU5QDEn9UD6qoIs10GwD2uk",
      "resources_slides": "https://drive.google.com/file/d/1-EdITc0JQ02NJa7F2-0T17zJepF74r3P/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "from-peerdas-to-fulldas-towards-massive-scalability-with-32mb-blocks-and-beyond",
      "sourceId": "EVSLDH",
      "title": "From PeerDAS to FullDAS: towards massive scalability with 32MB blocks and beyond",
      "description": "PeerDAS is expected to be one of the most interesting improvements of the Pectra hard fork, enabling long-awaited sharding on Ethereum, unleashing L2 scaling.\r\n\r\nPeerDAS is however just the start with up to 1-2 MB of blob space per slot. We look into the techniques jointly developed by our Codex Research Team and EF researchers to improve this by orders of magnitude, targeting 32 MB (and beyond) of data availability space.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Danksharding,DAS,Scalability,fulldas,Danksharding,DAS,Scalability",
      "keywords": "PeerDAS,FullDAS",
      "duration": 1441,
      "language": "en",
      "sources_swarmHash": "d1c8176ff1b4c933326ddae3ac900465ccbc6f4bde4090fa63cc7af602715e09",
      "sources_youtubeId": "Y8VKmyJMAUk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736cc799dbb7a90e18c1d03",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736cc799dbb7a90e18c1d03.vtt",
      "transcript_text": " Hi, hello. So, yes, I have a long title. So I will be speaking about data sampling. And it's a long title and I didn't want to make it longer, but actually I have to make a small remark, it's not even blocks. It's data space that we are speaking of. So let's see who we are first. I'm Chaba, I'm from the research team from Codex. We are working on decentralized storage network, providing high durability guarantees. And our research team is also working with the EF on data sampling and also the client teams. OK, so I think you are all familiar with this chart. You have seen it like 10 times during this DEF CON, I think you are familiar with this chart. You have seen it like ten times during this DevCon, I suppose. We had the IP 4844 and the transaction fees on L2 went down, right? And the right side we see those little, little ones. That's very good. That's very nice of 4844. But I hear that we have a problem. And yeah, to see that problem, I have to zoom in. Sorry. So if we zoom in, we see this problem. So there is a slight increase in L2 transaction fees. And actually, there's a problem. L2 transaction fees are driven by blob space, by data space, and that's running out. problem, and if that happens, prices can rise fast. data availability solutions, in codex we are also building one. speak of the Ethereum data availability solution, so we should scale that Ethereum data availability solution. a little bit behind these numbers. What you see here, maybe you have already seen this one as well, this is the daily average size of blocks on the network. And you see that it was rising and then after the merge it started to rise and then with 4844 it reduced. So this looks fine, right? Now you see this less often. This is the problem. So this is what our node operators are seeing. This is the blobs, so this is the blocks and the blobs cumulative plot together. So this is the data amount that they are seeing. And now we can start worrying, right? There is more data to handle. But we are scientists. We can look at this differently. So let's see it on a log scale. And then it's not that bad, actually. Just like Moore's law. It's actually a different exponent, but we are better than those CPU guys, so why would we? Okay, we just have to do that little technology evolution which keeps this running. And then we have a solution, right? We can draw that line. We just need more Bob space. And that's a plan. And here's a plan. We need to build this and then forward this. And we just have to be a bit lousy about the scale. So it's an unknown scale here and an unknown scale there. And then we solve the problem. What we want is more Bob space and what we want is less node traffic for our operators. We just have to work out these little networking details. So let's see those little networking details that we have to work out. But first, let me have a quick detour, networking. But this is, I was speaking of average box size till now. But average was a daily average. And in the day many things happen, right? Every 12 seconds something is happening. So what you see here is the tail distribution of block size during a long period. You don't even see the small blocks, you just see the large ones. And this was before 4844. And what you see that there were blocks beyond two megabytes. And our nodes have to handle those. What if two blocks of such come one after another? And you have to handle that. So our technology has to handle these things as well. And we were measuring whether you can handle that. And the good news is that you can handle that. So the network was running and live. And what you see here is for different block sizes. These are the bins. Small up to 32 kilobyte blocks. And then between one megabyte and two megabytes. How much time nodes need to get the blocks. And the bigger the block, the more time is needed. This is a distribution. So the nodes which are receiving it only here. and there are many nodes which are receiving it before. Almost everyone is receiving it before four seconds. So things are fine. But clearly it doesn't scale very well, as you see. If we want to increase the block size, then it would go to the right. So we arrive to our point how to scale layer one from the networking side. things. change the slot time, the slot timers, we can change resource requirements. It's a good discussion. It is a bit controversial. We have small gains. We have a lot to lose. So let me not go into that one. Then we can change the gossip sub, the protocol, the networking protocol. We can do improvements on gossip sub for large messages. And this is in the making. It's important. But it's not my talk today. And it's relatively small gains compared to what we need. Still important. Then we can do sharding and data with sampling. And this is my talk today. This is large gains, I hope. And then there's the fourth thing. We can do distributed block building. This is kind of intertwined with many things, but this is large gains. But basically, that would mean re-syncing the whole data flow of the blob transaction from submission of the transaction until the end. Okay, so let's focus on point three, the sharding and data sampling over the network. So data sampling, let's just key concepts. So you need to prepare the data. We were speaking of blobs. There are blobs. There We were speaking of blobs. There are blobs. There's a package of blobs. And then we need an encoding. We need an encoding because we want to sample from it and for the sampling to be effective, we need a laser coding. For the laser coding, we need segmentation. So we need to segment it, chop it to pieces. We need to extend it with a laser coding, then we have to commit to it so that it cannot be, nodes cannot send false pieces basically. And once we've done this, we are ready with the data preparation. And here we have different structures, we have what we call the one-dimensional PIRDA structure where you have blobs which are extended and committed to, or we have the two-dimensional structure where you have blobs which are extended in two-dimension. If you look carefully, you see that actually this is also two-dimensional. The difference is that it's not extended by code in this dimension, and that has consequences. Okay. And then you just have to sample. It sounds easy, right? The question is where you are sampling from and how you are is. And then you just have to sample. It sounds easy, right? The question is where you are sampling from and how you do this sampling. So I wasn't saying anything about that. So let's just see that. So we were typically speaking of DAS, but itA-S, but it's actually a D-A plus an S. So there is the D-A part, which is a property of the network. It's a global property. The data is available. The data is not available. And to have the data available in the system, I have to disperse it in the system. So I have to send it out, different pieces to different nodes, which are custodying it, and only then I can start sampling. So this is the DA part. It's ensuring data ability, it is doing the sharding, nodes are only receiving pieces. And then there is the sampling part. Once the data is in custody at different nodes, you can start test sampling, asking for pieces and getting pieces. Little graphical representation. So you have a builder. It does, in the example I was doing it with the 2D encoding. So it does the 2D encoding. And then you have those beacon nodes in the system. And what you are doing, you are sending out these things, pieces of this, actually rows and columns, to different nodes. Someone is getting only two rows and two columns. Someone is getting a bit more. Someone is getting the whole thing because he's learning hundreds of validators and he's securing so much money, but it's better for him to get the whole thing. Plus, doing that, he can support the system. That's one side. That's just the data availability. Now you have to sample, and that's an individual node, and actually every individual node has to sample. That's his own view. So he's picking a few pieces, and then asking for those from custody. So you're getting this piece from here, that from there, that from there, and if it manages to sample, then things are good. Okay. So we have this kind of two sides of things, and you can see that on the network these are a little bit different. And we need them to behave differently. So we need them to be secure, robust, fast, and cheap. And we need sampling to be secure, robust, fast, and cheap. So we need the same properties for the two things. It's just different protocols. Okay. So I was saying that there is PIRDAS and there is FURDAS. And there is a difference in the data structure. Actually there are a number of differences. If you scan the QR code, we have a light up on FURDAS which is starting from PIRDAS. And some of these techniques apply to PIRDAS, some of them to FURDAS. So what you see here is PIRDAS. Some of these techniques apply to PIRDAS, some of them to FURDAS. What you see here is PIRDAS. In PIRDAS, you have one-dimensional extension, and that also means that when you are sampling, you cannot just get a small piece because it would not tell you probabilistically enough. You have to get a full column. So your sample is a column. Which is more data than a cell that you can sample if you have this two-dimensional extension. But to make this work, you need changes in the networking, changes in the other coding, changes in many things. So, let me just give you a comparison of PIRDAS and FURDAS. Actually let me go back and just go through this list. So we have different techniques here. The first category is about sampling, it's about how I'm selecting what to sample, how I'm selecting those small pieces that I will be looking for. Then the second category is about erasure coding and the using of the erasure coding. Because what I can do is if the data is erasure coded, I can use that as part of the transmission. So when I'm sending the data, I can already use the erasure coding to recover data and then send it on to others. And that's a very important property. We are always using erasure coding when we are doing transmission, actually. Take your mobile phone, take whatever. Here, we can use erasure coding very efficiently to send data. And when we are sending a row, we can use erasure coding very efficiently to send data. And when we are sending a row, we can take a piece and we can send it on a column. And when we have received half of a row, we can extend it to the whole row, for example. And then there are techniques which are protocol changes. And I'm not going through all of them. Just showing you the comparison. So peer-to-peer and foo-to-foos. One is cheap, robust, fast and secure. And the other is cheaper, more robust, faster and more secure. And I was inclined to stop here. But let's see a little bit more in detail. Okay. So FooDAS is cheaper. It's cheaper because it's sampling at the cell level. And the cell is 512 bytes, actually it's 590 bytes because it's the data plus the proof, which is still much smaller than a whole column. So whenever we are so the sampling becomes cheaper. It's more robust. Because it can do a little coding which is allowing what I was calling local repair. But what local means is important to understand. Local means in the sense of the code, in the sense of, let me go back here, in the sense of this square. So I only have a few data, I only have half of a column, and I can repair something. I can generate these. I have these and I can generate these. I have half of the pieces here and I can generate this one. I cannot do that up there. And that's a huge difference. That's a huge difference because that means that every single node can do repair. Not just big nodes who are having old data, but every single node. Actually in that construct we don't need big nodes, which by the way we have in Ethereum, so it's not like we don't have them. But we are not relying on them doing the repair in the data structure. And it's faster. Why is it faster? Because we are working on new protocols, both for the PubSub part and both for the sampling part. So and yes, is it more secure? So the light size is still in research. Should it be more secure actually when this is secure enough that we are changing to this? Actually it doesn't. It shouldn't be less secure. That's still in research. This is still in specification and changing. So let me not do more comparisons, because this will change. And actually, this is still in changing. OK. Sorry. Last slide. As I said, it is not just PIRDAS and FURDAS, but we can go beyond. So, I will show you one slide. slide. As I said, it is not just but we can go beyond. we have to rethink the data flow. So how is this data flow? What is happening? about is what is happening in the consensus layer. So, when a block is generated, how that is distributed. But actually, these blobs are coming as transactions from the world. So let's say one blob is coming in. And that goes to the execution layer. And the execution layer has the mempool, and the mempool is redistributing this, sharing this blob between nodes. Actually when it's blobs, it's not shared like when it's normal small transactions. When it's normal small transactions, these transactions are pushed to others. When it blobs, it's just information that I have this blob and then the others are pulling it. Overall effect, these blobs are spreading in the network. Let's see there's another blob that's spreading also in the network. block proposal. block proposal. block proposal. network. network. block proposal. the data is being distributed. the data is being distributed. the data is being distributed. the encoded version is getting spread in the consensus layer. I think you start to see the problem. It's not the problem, it's an inefficiency. We have an inefficiency in the system in which we are distributing the data here, and then we are actually redistributing the data there. And it's the same data. So we can optimize it a lot, which is good. We need these optimizations for Moore's law to happen. So we found another place where we can save a lot and have that curve go up. So what you can do in first instance is that we do the distribution up there, but whenever the node is realizing that he has a piece, he would need this and this, but he has the blobs, he has the data. He can just pop it up from the execution client. Because the data is also there. Now, of course, this plot is optimistic. Because I was putting this wide blob almost everywhere. And I was putting this red blob almost everywhere in the network. In reality, you don't have every blob everywhere in the execution layer. Because if we could do that, like now you actually have, but when we are scaling, we will not be able to do that. Because if we can do that, then we don't have a problem. Every node can handle all the data which is there. We want to scale beyond that. So in reality, you would have some blobs here, some blobs there, some blobs there. But then when you do that distribution, data can pop up. And that's one of the optimizations. Of course, that's a simple one. We can do much more complex ones where we have interaction between the two, and we are avoiding some of these duplications. But for that, we have to rethink the networking stack here, which is that P2P, here, which is using the P2P. We are doing different kinds of gossiping distribution. This is using GossipSub. This is using other protocols. So this is all. This can all be the same. But that's just that, I think. Okay. Thank you. . Okay. We are now here to take some Q&A. So starting from the top, there are blob fees that play blah, blah, blah, blah, blah, blah, Thank you. Thank you. We are now here to take some Q&A. Starting from the top. There are blob fees that play blah, blah, blah. There are blob fees that pay for blob storage. Is this verified at all? What happens if I get the fee but don't actually store the blob data? Okay. BROP fees that pay for BROP storage. I don't know. Yeah. I'm not sure what the question is about. No worries. Question answer, question asker if you have the question still. Yeah. Fair to clarify. Clarify. But I'll mark this as answered. What are standing problems that hinder current adoption? Okay. So it's just in implementation. So the peer-to-peer is in implementation. It's still compatibility problems. Peer-to-peer will be here very soon. It will not be in Paxra, but it will be in the next fork. For full-to-peer, we still have to figure out many things. So there was a question mark on the security, and we have to work on that to make sure that there are no issues with that. Well, why do we need 2D erasure coding? Can't we put all of the points on one polynomial and extend it? So putting them all on one point and extending it would be one huge code, one dimensional, and that would be very bad for repair in the sense that then you can only repair if you have all the data. And that just means that you have difficulties repairing. The 2D has this advantage of having this code local repair property so that the code distance is small, and you can repair pieces. So you can repair a single column, you can repair a single row, and that gives nodes the possibility to start repairing and contributing even before getting the full data. If full DAWs is way better than pure DA DOS, why not choose to implement it in the first place? As I said, peer DOS itself is changing. We were thinking of techniques for full DOS, then we realized they are actually applying to peer DOS, and then they are part of peer DOS. We're still changing that. Full DOS is still much more research. changing that. It's a simple fact of life that there's too much PC still moving in full DOS to implement that. It seems full DOS approach are much, much better than pure DOS. Should we skip pure DOS and implement the full DOS? Is there any other trade-off? off? So, is using many things that PIRDAS does. So skipping is not a direct skip. It's not fully different. It's giving the 2D Azure core. It has advantages, but it has also complexities. Whether we skip or not might save something on the implementation. When we are introducing it, it might also skip something, but we are losing also the graduality. So overall, I'm not sure it's good to skip, but we can think of it once FUDAS is fully evaluated. What are open research questions? So the open research questions is, so the last slide was fully open research question, or everything around that. On full DAS, there are several questions around the sampling, how we can do the sampling safely in the sense that Sibyl nodes cannot attack the system, for example. So there are still things to clarify there, especially attacks, network partitioning, and other things, how they affect security. There's some fighting and upvoting going on. This is fun. If 2D is better, would there be any benefits of 3D? Absolutely. Multidimensional encoding is fun, but no. Actually not. So we just need the local repairability property, and we have enough of that in 2D. 3D would not contribute to that too much. Can full DOS be implemented progressively? I think so. But we didn't define the plug list. So it's something to think of. But we didn't define it yet. Cool. And then I think this question was already answered. I think we already had that. Right. Yeah. Unless someone's aggressively or I just forgot to mark as answered. Cool. So we have a minute and 34 seconds left. Oh, I was actually spending time. Yeah, no, but so I guess there's one piece to this. You can give him feedback on the QR code, so we'll give you some time to tell him how great he did. Collect a card. But other than that, I think our next session, well, I guess...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:10:00.000Z",
      "slot_end": "2024-11-14T09:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1lz7gYMVKQCLb5914Y9OWEh4uWk8dcQ8g132fAtGQIuQ",
      "resources_slides": "https://drive.google.com/file/d/1zjFRZKqxjPOuJJrQZt0scUJHCnxRlg3w/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "running-ethereum-node-in-africa",
      "sourceId": "XT8ZWL",
      "title": "Running Ethereum Node In Africa",
      "description": "Running an Ethereum node in Africa presents both challenges and opportunities. It enables participation in the global blockchain ecosystem while contributing to network security and decentralization. Key points to highlight include overcoming infrastructure limitations, leveraging community support, the potential for economic empowerment through staking, and fostering local innovation and adoption. Emphasize the importance of education, collaboration, and strategic partnerships to",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Home staking,Distributed validator technology,Decentralization,diversity,geographical,Decentralization,Distributed validator technology,Home staking",
      "keywords": "Geographical,Diversity",
      "duration": 611,
      "language": "en",
      "sources_swarmHash": "17771851685309b8c91301fcc0013484c2d9732fb2aacfc27bbefa76c6c204d5",
      "sources_youtubeId": "_AywwOgu2zY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c1c39dbb7a90e1f43d28",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735c1c39dbb7a90e1f43d28.vtt",
      "transcript_text": " Hi. Okay. Okay, I want to start by asking a question. How many of us want to be here in the next, let's say, 10 years, 20 years, to see that Ethereum has gotten to the great potential it should be? How many of us? Okay, good. Awesome. So I think we are all in one spirit. I'm David. I'm David Uzotuku from Nigeria and I'm here to really spoil our hearts to see how we all come together to make Ethereum truly decentralized, right? And I'll be talking on from my own perspective as a participant and as a user and someone who who believes in the future of the ATM network what I'm trying to do and how I'm doing it so I live in Africa in Nigeria Africa and Africa has some few unique challenges and it has a lot of potentials. So today I'll be talking about running nodes in Africa and other few things. So why geographic decentralization is vital for Ethereum? If Ethereum has to be truly decentralized, It means we need to have these nodes, the information infrastructure points, distributed. And it has to be distributed from a geographical perspective. So currently right now in the world, we have like seven continents. And Africa is one of it. And just to also add, I believe in one day that some of us, one day, possible me or you, might go to Antarctica to actually run a node. Right now, in Africa, we have less than 1% of node operators in Africa. But in other parts of the world, we have more than 30% in other side part of the world. And that's not true decentralization. True decentralization is where we have a good balance to these infrastructures running together. The interesting thing about Ethereum vision is it has a vision of security, trust, resilience, but all of this can happen if we don't have true decentralization. Because one, if we have an imbalance, right, it means the majority will actually have high rules or policies, which might not favor the rest. So we all have to be like, it has to really be decentralized for us to really have this vision come to pass. Geographical diversity is also one thing, one of the vision or core vision of the Ethereum network itself. Seeing Ethereum really, really being decentralized is what the network needs. So, this is just a map showing us the nodes we have in globally. Just like I said, we have less than 1% of nodes in Africa and also Latin America. So these two continents kind of have similar unique challenges. So I'll be talking on some of these challenges also. One question also I want to or another thing I will be talking about is why should we also think about or supporting Africa in trying to run nodes, right? Because of the potentials. At the moment in Africa, we have approximately about 1.7 billion people, right? And majority of this, a good number of this number is they are all youth, right? And one thing I love saying is in Africa, we have a lot of challenges, right? Where there is challenges, that's where you talk about finding solutions to solve these challenges. And one great solution right now, it's the Ethereum network, it's the Ethereum blockchain. We solve challenges we face in Africa so that tells that there's a great potential in Africa and that's why also we want to see more nodes running in Africa. Building a decentralized future, our work at NodeBridge. In trying to solve this issue of true decentralization, from my own standpoint, I came up with a community called NodeBridge. The idea right now is just to see how we kind of have more home node operators, right, support people's minds to also be participants of this network in a way to really make this Ethereum truly decentralized, right? Some huge challenges we have in Africa is education, lack of internet infrastructures, lack of power infrastructures, economical issue, right? So, and, you know, in my own small way, right, one of the things we try to do at NodeBridge is to also break this barrier, right? And we find a way to also break this barrier by, for instance, doing workshops. I've been doing a lot of workshops in a few African countries, also educating them, finding better ways to actually tell them that, yes, we all have to be part of this network, making this network resilient again. And then also, power. So because we have to find a way to also, like, use a very small system, so it's sustainable and it's actually also solved the problem. So this is just like a Raspberry Pi running on an arm, which also solves that problem. So I also, like, custom build all of these things to solve the problem for an African person who is not able to actually buy this infrastructure. Who is not able to buy this infrastructure. Because setting up this infrastructure is quite expensive. Yeah, this is some of the workshops also we kind of get to do. Yeah, this is also some of the setups. We've seen more Africans running these nodes, right? So they kind of have different setups. So actually at the moment, we also kind of use styling. We use power solar systems to generate power for sustainability. And also with the custom nodes we put together. Also, yeah, this is like the whole power system, the styling, some of the node operators we have at the moment are actually using. Also, education. So why security depends on global node operators and solo stickers, right? So, just like I said, we all have to come together, right, to really see, to push Ethereum to be truly decentralized. And then, the point here is, Africa is trying to solve its own problem. But because we're trying to solve a global problem, which is trying to solve its own problem. But because we're trying to solve a global problem which is trying to solve decentralization, we all have to be like, all of us have to all come together, like helping each other. For instance, there are different system right now, like the DVT system. Okay, so like the DVT system, which actually you could also delegate to permissionless system, you could actually have like a cluster to delegate also your stakes to a particular node that is sitting in underrepresented location, right? So we all have to come together to actually do these things together because it depends on us all, not just one geographical point, but it all depends on everybody who loves this network and who is ready actually to see this network in the next 50 years to come. Yeah, how you could also support, like, there are different ways also you could also put this support, just like I said. Supporting node operators from your own point of view, let's say it might be you delegating your nodes, it might be you delegating your stakes, it might be you setting up some resources, which is for education and its likes. So I also just want to quickly ask, what does true decentralization mean to you? And we can think about it in the next session. I'm sorry, David, but the time is up and we have a really tight schedule. If you want to talk to David, I think the work he's doing is absolutely amazing. Definitely connect with him.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:10:00.000Z",
      "slot_end": "2024-11-14T09:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1buMXIg1gOhRzKk22wUllHQbcl9xVPk1mQ7_JHDKF_oQ",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "giga-undepin-to-connect-every-school-in-the-world",
      "sourceId": "JXH3T3",
      "title": "Giga: (UN)DePIN to connect every school in the world",
      "description": "Giga (a startup built by UNICEF and ITU) has built a long-lasting friendship with the Ethereum community, starting w/ the 2019 Devcon launch of UNICEF's Crypto Fund, to the first Eth staking with the Government of Rwanda, putting schools onchain, and now working on a global Connectivity Credits Marketplace.\r\n \r\nBlockchain, and particularly Ethereum, is fundamental to scaling connectivity for the 1.8 billion people who aren't online.   http://giga.global",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "DePIN,Ethereum for Good,Politics",
      "keywords": "Connectivity,real world digital assets,",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "801527a603a3ccb989baca376f305b79064af4e39bdd8edde90678215994fdcb",
      "sources_youtubeId": "45Ma4-p_rZc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:20:00.000Z",
      "slot_end": "2024-11-14T09:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Kux95LlPqrqyaIMbQZgE8OhOIzJM8A61evcBSSNF7dY",
      "resources_slides": "https://drive.google.com/file/d/1T_-tkKWxpusU0dt5GDCYxuwowurHRLFU/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "hardware-security-from-sand-to-stone",
      "sourceId": "UZDFEK",
      "title": "Hardware Security: From Sand to Stone",
      "description": "All software runs on hardware. The assumptions on which many of our systems rest are often shakier than we realise. This talk explores hardware security, its shortcomings and the path to a firmer foundation.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Hardware wallets,Security",
      "keywords": "TEE,Hardware Trojans",
      "duration": 858,
      "language": "en",
      "sources_swarmHash": "e0116d1a6b9a82bc416ec617599f1e64550eaea718855a6d17b96d6ab8cb51bd",
      "sources_youtubeId": "LDGXAd14DJY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c4869dbb7a90e10712d6",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:20:00.000Z",
      "slot_end": "2024-11-14T09:30:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1wcISJi-Q9aswEj-3R97yb_9jp5DN6P_38XsaGdEq3B4",
      "resources_slides": "https://drive.google.com/file/d/1QNri28c9QHvsJ_157yGgsTO3HjR4S9lm/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "bringing-ai-on-chain",
      "sourceId": "QJCEPL",
      "title": "Bringing AI on-chain",
      "description": "Ethereum serves as a settlement layer for trust. It presents itself as a great option to offer trust-as-a-service for AI agents, which are normally oriented for individual profit and non-collaborative.\r\n\r\nTo best leverage Ethereum as a building block for AI agents, we want to give a workshop focused around collaboration between agents which will focus on interacting with Prediction Markets, a particular application we at Gnosis AI believe has lots of potential.",
      "track": "Coordination",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,e/acc,ai,Coordination,e/acc",
      "keywords": "AI,web3xAI,Artificial Intelligence",
      "duration": 4536,
      "language": "en",
      "sources_swarmHash": "a35197c269493b3fc0be77ad4cd07ba79b5477ef9c406af90aafb5c5fbb24341",
      "sources_youtubeId": "5U1kLXSODCA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735dd219dbb7a90e190bebf",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1u3q_4IFfuXzZDfanHnjTsWcClgoN_wAS6_wEi2qSxiU",
      "resources_slides": "https://drive.google.com/file/d/1oMumy6nRT27rkf_iIUqr5tDr2wvdMDqt/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "eth-escape-winner-revealed",
      "sourceId": "WXS8BH",
      "title": "ETH Escape Winner Revealed",
      "description": "We'll announce the winner of ETH Escape.",
      "track": "[CLS] ETH Escape - Speed Hacking Challenge",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "5304e1f0420f40e6a4f48b035a6c651c52dacb2925780a2ad61a0bddcbe95480",
      "sources_youtubeId": "K4pfXsEwCHQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1lSwPhaKp0iIdGqbNHH0Wq_hG_vGPCW1ja_5qbLVLScg",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "ethereum-a-force-of-good",
      "sourceId": "HUZP7J",
      "title": "Ethereum a Force of Good",
      "description": "Ethereum as a Force for Good\r\nWhat does it mean for Ethereum to be a force of good? How can real-world applications of Ethereum such as RWA, DeFi, and Web3 social right current inequities in the world? What are key blockers that we need to overcome to bring Ethereum into the mainstream? In this talk, Stani will elaborate on how Ethereum is a positive force of change in the world.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "RWA,Ethereum for Good,Economics,micropayments,Economics,Ethereum for Good,RWA",
      "keywords": "stablecoins,supply chain,agriculture,scalability",
      "duration": 1570,
      "language": "en",
      "sources_swarmHash": "fcbb11b0fbb59cfb6316106365ba8065682b5fe889588b22de59a4c48016395a",
      "sources_youtubeId": "Z86QO0WgWtE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735cf379dbb7a90e1f9620f",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T10:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1zwoxKxRNSg1zW4w3I3Ad1I6aSDAtCo3sBRkenui4eQ4",
      "resources_slides": "https://drive.google.com/file/d/1KvgGkHtyX3OF85QD41bM9nP87Jnyp8Pc/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "kickstarting-impact-funding-with-hypercerts",
      "sourceId": "VGZ7PP",
      "title": "Kickstarting impact funding with hypercerts",
      "description": "Create hypercerts, evaluate their content and fund what matters by building on top of the hypercerts ecosystem. Building on top of a decentralised registry of impactful work, the hypercerts ecosystem empowers impact creators to explore novel forms of impact funding and resource coordination. \r\n\r\nDuring this workshop we'll explore the hypercerts stack and help you mint, evaluate and trade your first on-chain impact certificates.",
      "track": "Real World Ethereum",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,RPGF,Best Practices,funding,Best Practices,DevEx,RPGF",
      "keywords": "Impact,Funding",
      "duration": 4872,
      "language": "en",
      "sources_swarmHash": "68b95037f727bd07f5ee9fb3035c4efc4b0cd90bf2ba621b902764ea826d33ef",
      "sources_youtubeId": "Ozf8X4UeDY0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e4169dbb7a90e176e775",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735e0479dbb7a90e1e78f69.vtt",
      "transcript_text": " Session of the day. Well done. I think you guys should all give yourself a round of applause for making it to the day three, last session. Well done. Awesome. My name's Mark, Englishman living in New Zealand. Any Kiwis here? No Kiwis? No, they've all gone home. Okay, last session. We're going to talk about corruption, KYC, and the cost of compliance. So Jared Hope, co-founder of Status, and Lagos, welcome, Jared. Cool. All right. Well, yeah, thanks for making it to the end. I'm sure you're hungry. I'm sure you're tired. I am too. So I'm here to talk to you about corruption, KYC, and the cost of compliance. And, I mean, I think a lot of people intuitively don't like KYC when it's applied to them, but don't really have, never really kind of looked into it or maybe some other alternate narratives. So I'm here to give you another narrative form. So let's rewind back the clock, back to September the 10th, 2001. Pay attention to this date. It might look familiar to you, right? Donald Rumsfeld, the Defense Secretary of the United States, held a press conference in which he admitted that $2.3 trillion of the Pentagon's budget was completely unaccounted for, had no idea what it was spent on. In fact, in that press conference, he went to say that the adversary wasn't terrorists. In fact, it was closer to home. It was the Pentagon bureaucracy. Now, the following year, Jim Minery took it on himself to track down just $300 million of this, right? He traveled all over the United States, and unfortunately, couldn't find it. He said, he's on quote saying, we know it's gone, but we don't know what they spent it on. Kind of a problem, right? Now, fast forward to 2015, right? Catherine Austin Fitz, who is the Assistant Secretary for the Housing Department of Housing and Urban Development, looked into her own department as well as the Department of Defense, and found for the year of 2015, $6.5 trillion was known as unsupported adjustments, right? This was 54 times the actual approved spend for the DoD authorized by Congress, which is $122 billion. Now, Mark Skidmore, not sure why he's not appearing there, but he has a face, thought that this might have been some kind of typo, right? Surely it can't point $6.5 trillion. Surely Catherine meant $6.5 billion. So he got together with Catherine, and they really dove into it. What they found through their research is that there was actually 21 trillion dollars unaccounted for between the period of 1998 and 2015. Oh, okay. There he is. Now, none of this would have been... This is old news for Franklin Spinney, right? He was a 1983 military whistleblower who had already come out in Time magazine trying to alert the public of this kind of spending, right? He's on quote saying the books are cooked routinely year after year. So this isn't something that happened just in that period. It's been happening for a long time. In fact, it's still happening, right? So here is something that's from this year where $8.2 billion of improperly valued material went to Ukraine, right? Again, no idea what that material was, it's just the money got spent. And here is Senator Chuck Grassley of 2024 on quote, saying that when it comes to catching fraud, the Department of Defense internal controls are a complete failure. We've known it for decades, And he's looking quite grumpy about it. And the reason for that is because he's basically made most of his term tracking down this kind of spending. So he's found a gas station that was created in Afghanistan that should have cost half a million dollars actually went for 43 million dollars, right? He's found soap dishes that cost $117. Hammers, $400. Pliers, $1,000. Forks and spoons, $57 each. I hope they were plated in gold. But the most comical one is probably toilet seat lid covers. Maybe you remember this, maybe you heard about it. It's happened a few times. Once in the military where they were going for roughly $640 per toilet seat, just the lid, right? Like not the entire thing, just the top part, right? In the Air Force recently, it's gone for $10,000 for each lid. It's a hell of a lid. So how does this happen? How on earth can you spend 54 times your allocated budget in a single year, right? Well, it's actually because of the oldest living program that's still in operation today. This is called MoCos, right? It's written in COBOL. I think it was deployed in 1958. Now, what its purpose is for is for tendering, procurement and so on. But what it also happens to do is it finds unspent money or funds in other programs that have kind of gone stale. This is completely illegal to do for any other governed department, but here apparently everyone seems to look the other way. completely illegal to do for any other government department, but Here apparently everyone seems to look the other way now Just to give you a sense of proportion going into this right on the on the left hand side here We've got the the unaccounted adjustments. I've just been talking about right this 1.2 trillion dollars on average For that's being spent per year according to that time span of six and a half trillion, right? This $1.2 trillion on average that's being spent per year according to that time span of $6.5 trillion, right? Adjusted for inflation, that might be around $2.5 trillion, let's say, right? On the other side, we've got the United Nations estimates for illicit financial flows on a given year, right? It's somewhere between 2% to 5% of global GDP. So that's the 1.6 trillion and the 4 trillion balance. Now, the 3.1 trillion that's on there, this sort of middle bracket, is an estimate of illicit financial flows for last year according to NASDAQ's global crime financial crime report that came out this year right just to give you a sense of proportion right this is just one government agency the entire planet's illicit financial flows okay so going back to that press conference if you recall at the beginning of this right the day after 9-11 happened, right? Tragedy, absolute tragedy. And as those towers fell and hit the ground, the world changed, right? Not only in terms of the surveillance, in terms of like our communications, but also the deployment of financial surveillance around the entire planet, right? In just one month, the Financial Action Task Force issued out the eight of their nine special recommendations, and actually, with the installment of the Patriot Act, they have basically a whole bunch of provisions for KYC and AML, specifically around Sections 311, 326, 351, and 352. So the 311 is about giving the Secretary of Treasury the authority to target specific money laundering and terrorist financing risks. 326 is about establishing minimum standards for financial institutions. So this is why you have to hold your passport up and take a photo of yourself whenever you sign up for an exchange or so on. 351 is immunity for anyone who likes to squeal, right? So reporting, and if there's any issues around reporting, they're kind of immune if it's in the name of this particular goal. And 352 is basically the mandate for financial institutions to implement the program, right? Now, I say this goes around the globe because it was felt in Asian banks. So, according to the LexisNexis Resolutions, the True Cost of Anti-Money Laundering Compliance Survey in 2015, even though local regulations were cited the most frequently as having the greatest impact, it was known that the US regulation embodied by the Patriot Act, which was actually what enforced these particular implementations of policy. Now, we've established agency, illicit financial flows, just to give you a sense of proportion, right? Now, we've also seen that it was implemented in the name of terrorism. Can you see on here the amount that has gone to terrorism? Yet? Maybe? Oh, there we go, right? $11.5 billion have gone to terrorism financing according to the latest NASDAQ Global Financial Crime Report. That's what we sold out our civil liberties for, just to give you an idea of that, right? Now, of course, you might look at that and go, well, you know, it's been two and a half decades since then. Surely we've solved terrorism. Well, actually, if you roll back the clock and look at some of the estimates of what that terrorism financing looks like, the World Bank and the IMF basically viewed the terrorism financing organizations for the entire year of 2001 to be less than half a billion dollars, right? Just to give you a sense of proportion here. In 2005, Rand found that al-Qaeda's annual budget was between $30 to $50 million. In 2015, the Department of Treasury estimated that ISIS generated roughly a billion dollars. And in 2016, Europol estimated that that was doubled to $2 billion. Now, in contrast to that, we have this great report from the Simpson Study Group on counterterrorism spending, right? And they believe that, according to that report, that between the period of 2002 and 2017, this came to a total spend of $2.8 trillion, or $186 billion on average per year. So this spending on counterterrorism is 16 times larger than what our estimates are for spending that goes towards terrorism, right? That's a pretty good margin if you're playing both sides. Anyway. So, I mentioned this notion of illicit financial flows, right? What are they? Well, there isn't really an agreed consensus on the definition. But generally speaking, it comprises of tax evasion, multinational tax avoidance, the theft of state assets, the laundering of prostates of crime, and they cover a broad range of market and regulatory abuses, such as corruption, drug smuggling, and human trafficking, right? And it is important, like I'm not trying to dismiss this at all, right? So don't get that from me, I'm just trying to give you a sense of proportion on this, right? It is important because Donato believes that for every dollar that goes towards crime gets reinvested in crime and therefore you get more crime, right? Perhaps a better analysis of this is actually Bjorn Lomborg who got some of the top economists in the world together and tried to rank the United Nations Sustainable Development Goals on a dollar-for-dollar spend, right? And they found that, you know, addressing illicit financial flows is a top priority. So why is this important? Why is addressing IFFs important? Well, they're basically associated with ineffective state functioning, illiterate use of state power, and they're kind of an international problem because even though you might solve it within your own state or within your own country, that capital will probably flow to other countries that support it in some way, right? At the end of the day, it results in lost GDP, which means there's lost tax revenue, and the argumentation is that that leads to poor governance. And higher quality governance is obviously desirable. So what exactly are IFFs? And, well, that depends just like the definition. So there's a myriad of different methodologies, and it's unclear exactly what comprises IFFs. And this is kind of part of the problem, because it's actually hard to understand what they are due to their nature. So Raymond Baker here basically views that 65% goes towards trade misinvoicing. Trade misinvoicing is like when a multinational corporation will do, say, for example, natural resource extraction in one country, but misprice the cost of extraction and the value of resource that's extracted, and then when moving it to another country, changes those numbers, right? 30% towards criminal activity and 5% towards corruption. The Cardamon view is that 80% goes towards trade misinvoicing, so this same problem, and 20% towards corruption. And the NASDAQ Global Financial Crime Report focuses more on the humanitarian aspects of illicit financial flows. So 25% towards drug trafficking and 12% to human trafficking, with 63% just towards other, which is corruption, crime, and so on. So even though we had the Financial Action Task Force and the Patriot Act implementing these regulations, it wasn't really until 2015, sorry, 2005, that illicit financial flows as an argumentation came onto the scene as a public policy issue, right? And this was actually through Raymond Baker's book, Capitalism's Achilles' Heel. The issue is that he's pushing a model and this idea that isn't well substantiated, and Peter Rudd was quite critical of it in 2015, where we didn't really know much more about the issue 10 years beyond, right? And in fact, there's more people who also feel the same way, right? Even though we use different measures and models to use to estimate money laundering, there is actually no model that can be used to quantify and accept it globally. Even the Financial Action Task Force's own methodology for assessing compliance with the FTAF recommendations and effectiveness of AML systems is completely vague. It relies on the general use of crime statistics, anecdotal evidence, and subjective conclusions. And this is part of the nature of what makes IFFs so difficult to deal with. Now, to their credit, there is an ongoing pilot program by the United Nations that is trying to provide the highest quality data around IFFs. But, yeah, I mean, it's largely focused on drug traffic in developing countries and it's roughly $1 billion we're talking about, some of the bigger ones. So, are follow-the-money methods effective? Well, according to the Financial Action Task Force's own admission last year, less than 1% of global illicit financial flows is recovered. Ronald F. Paul believes that less than 0.1% of criminal finances is impacted by follow-the-money methods. And according to the UN, this is roughly 0.2%. In other words, 99.8% of criminal proceeds are completely unaffected by all of the KYC that we've implemented. So what can we measure? We can measure the cost of implementing compliance, right? And according to the LexisNexis Risk Solutions report, for the United States banks alone, this is roughly $26.4 billion annually. That's double the amount that's going towards terrorism financing in the US alone. In fact, Ronald F. Paul believes that compliance costs exceed recovered criminal funds more than 100 times over. And the banks and the taxpayers and ordinary citizens are penalized more than criminal enterprises. That's you and that's me. In fact, this has been known at the OECD and B20 for several years. In fact, all of those costs get passed on to us. And in particular, when it comes to traditional trade finance, 50 to 60% of these costs, or more than half the price of your trade, is charged to clients. And a large portion of that is associated with compliance. Now, so why am I talking about this? Well, there's another inverse way to think about this. And there's a great thing that we understand in institutional economics, that if we can reduce transaction costs in the abstract sense by 0.1%, this enables a country to be able to quadruple its wealth. That's the difference between Argentina and Switzerland. So we can actually create better and more efficient institutions that lower transactions. I mean, when you line up in the DMV or if you're filling out a form, that's associated with transaction costs in an institutional sense. If you can reduce that by using, say, smart contracts or smart paper or whatever, we can enable more wealth, which will probably lift a lot more people out of... No, it will just make a higher quality of life. In fact, Michael Levy basically says that no one could rationally think that AML controls in general or financial investigation in particular will solve organized crime completely or eliminate high-level offending. For there even to be a chance to achieve that, there would need to be a step in change in transparency and effective action against high-level corruption along all possible supply chains. It's a huge problem. Why is that a huge problem? Well, before I get into that, Amnesty International created a report called Weaponizing Counterterrorism. I highly recommend you read it. But they basically showed how the Indian government has been exploiting the 2010 and 2013 FTFS assessments to target human rights activists, journalists, students, academics, and so on, right? And so this is one of the issues with these kinds of recommendations. What happens is that if you don't implement them on time, you can be excluded from the economic system, right? You can be excluded from member states. And so this leads to rushing bad law through their parliaments. And when they're in place, politicians can then use them for political gain. They've been used to suppress political opponents. It's not just Amnesty International talking about that. It's also the Royal United Services Institute in the UK. This is a 100-year-old think tank, very prestigious and well-respected. They focus on authoritarian regimes, but as we've just seen, it doesn't apply just to authoritarian regimes. So, who is perpetrating it? Are we really ones to blame? Alex Cobham doesn't believe so. In fact, he says that ISS are likely to be, by and large, an elite phenomenon. So the financial and political elite. And the reason for this is because that's where the systems of abuse are likely to be strongest and where the capital is concentrated. They're the ones who make the rules, and therefore they're the ones who can break them. And to give an example of this, you might recall Jamie Dimon, CEO of JPMorgan Trace, is on record basically saying crypto is a tool for criminal drug traffickers and money laundering and tax avoidance. And if he was the government, he would shut us down. There's a certain irony in that, that basically the following year, no, following weeks, I think, after that statement, he was hit for, well, not him, but JPMorgan Chase, was hit for fines totaling roughly $39 billion for doing exactly those things. This was part of the FinCEN files, which basically shows it's not just exclusively the JPMorgan Chase, but many major banks. Now when it comes to financial secrecy, you might think of tax havens and nice, lush, exotic Caribbean islands. You might think of even Switzerland. But I wanted to point out the financial secrecy index, and particularly look at the global scale weight here. The United States stands out at roughly 25%, in contrast to the second position of Switzerland, which has it less than 4%. Their policies are literally, as Rumsfeld said, closer to home. Puerto Rota goes on to say, asking a kleptocratic state to create an effective AML system is like asking the fox to create a better hen house. It is the governing political elite that benefits the most substantially from the weakness of existing systems and controls. And this is a problem, because this kleptocracy issue is real. According to this Bloomberg report, trust in most US institutions has been withering across the board. It doesn't matter where you look. Public, you know, civil society, the public is losing confidence. This is not just happening in the US, but it's happening globally. And whether you look at the United Kingdom or in France, you'll see that trust is declining. In fact, if you look at the OECD reports, the average public trust in governments hovers around just 41%. We can do a lot better in terms of providing quality of governance. In fact, to corroborate the point, Transparency International came out in 2023 saying that over two-thirds of countries score below 50 out of 100, strongly indicating that they have serious corruption problems. Now, why is this important? Well, another way to view this problem of kleptocracy is that government debt is growing. In fact, it's reached unsustainable levels in many of the advanced emerging economies. And this is from the Peterson Institute in International Economics in 2011. Even though they were talking about 25 years, we're kind of already quite into that. And this is also evidenced by foreign sovereigns not buying US debt. In fact, since the 1800s, 52 countries saw a debt-to-GDP ratio above 130%. 51 of those have defaulted, and today the United States is at 120%. By 2033, they're going to be above 130. And that's according to the Congressional Budget Office in 2018. Now, those challenges are also not unique to the United States. According to the National International Council, they projected that many governments of the OECD countries are likely to experience the same economic difficulties. I'm running a little bit over time, but so the Financial Action Task Force is also quite interesting in of itself, right? Because it has a unique form of sovereignty as a non-governmental organization. It has no legal basis. It enjoys this sovereign immunity granted to it by its member states. It has no oversight. Even though it is based in Paris in the OECD building, it is not the jurisdiction of the French government, neither the OECD countries. So they can do whatever they want and have no real pressure to reform unless all member states start participating in that. As I mentioned, if you don't follow them, it results in economic lockout of the global financial system and there's no way to opt out. So why does this matter? Because if we can start solving governing institutions, we can start unlocking trillions of dollars worth of value rather than just disappearing into the ether. There's this great report from the World Bank called Where is the Wealth of Nations? And what they found is that human capital and the value of institutions as measured by the rule of law actually constitutes the largest share of wealth in virtually all countries. This is beyond natural resource extraction. So at the end of the day, what I'm trying to convey to you is that we should stop thinking from a scarcity mindset when it comes to follow the money methods. And instead, what we can actually do is improve our institutions. If we can make them faster, more efficient, and instead what we can actually do is improve our institutions. If we can make them faster, more efficient, and maybe even develop them in parallel, then we can actually create a much better and thriving world and a higher quality of life for all. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1-2n2zwPdIpfxkXDYIJI5vN-Bz4JCM93vP20YXjSCQ4I",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "non-native-arithmetic-via-crt-codes",
      "sourceId": "B7CJU8",
      "title": "Non-Native Arithmetic via CRT Codes",
      "description": "Non-native arithmetic is an important and costly operation in SNARKs. It is essential for proving validity of general cryptographic data like RSA signatures, non-native elliptic curve arithmetic like secp256r1, and general SNARK proof composition. We investigate a new approach to prove non-native integer arithmetic using Residue Number Systems and a batch proximity test for Chinese Remainder Theorem (CRT) codes, as well as surprising connections to STARK soundness.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,SNARK,Zero-Knowledge",
      "keywords": "Coding Theory,Math",
      "duration": 1400,
      "language": "en",
      "sources_swarmHash": "ee183d21ef8cc107dffe649816f123f9afc2ef6ecc7f2906454a6c588c5bdf3c",
      "sources_youtubeId": "OucSo6xjlBQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735cfae9dbb7a90e109be5d",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735cfae9dbb7a90e109be5d.vtt",
      "transcript_text": " Coming to my talk. In writing the talk, I found that I think it probably makes sense to mostly go over what non-native arithmetic is and build up to the question. Because this is sort of an idea I've been casually thinking about for a while and I don't have like, you know, definitive answer to yet. So, yeah. Did I push the button or how does that? Is this working? The clicker? Oh, wait, it is working. Okay. Okay, so, for people who don't know, what is the context for all of this so non-native arithmetic is something that comes up in snarks which are succinct non-interactive arguments of knowledge may have heard of like zero knowledge proofs and this is the same kind of thing so in a snark, the prover, wants to convince the verifier that they know some witness. And they want to do this by generating a proof without interacting with the verifier, so non-interactively. And the proof should be small. And if it's zero knowledge, it keeps the witness secret. Yeah, and sorry for the formatting. I did the slides in Beamer and had to import them to Google Slides, so it might be a little weird. But, yeah, so the way that we usually write this is there's some C, which is like a circuit that has some public input X and some private input W, and the protocol allows the prover to convince the verifier that they know W by sending a proof. So, for example, right, you could have some, like, chain state X, which is public, some transaction W that is private, and you want to prove that you know W for a given chain state. And so not all transactions will be valid valid and so that's what C checks Yeah, so the the quarter important takeaway for this from this for our for our topic today is That C is defined typically typically, over a field. So a field is like a place where we can do arithmetic, addition, multiplication, etc. And that field will be defined modulo some prime P. The field is sometimes sort of like a free parameter, depending on the proof system. You can pick the field freely. And sometimes it's not. So if you're using like a Stark, you can sort of, in principle, use any fields, just some caveats. But for like an elliptic curve-based proof system, it's usually fixed as part of the curve, which, again, depending on the particulars, is something you probably can't control. So the question that motivates non-native arithmetic is what happens if we want to prove a statement about some arithmetic that lies in a different field than the one that we're defining our relation over. So right suppose you want to verify a signature that is defined over some other curve externally to your proof system, right? You want to verify a Bitcoin transaction or something inside of BN254. And in that case, both of your fields will be fixed and you can't, you know, change them. And they're different. So you have to simulate one field inside of the other. And yes, we need to emulate or simulate the non-native field inside of the field that our relation is defined over. So just as a simple example of why this doesn't work, right? Suppose we wanted to check that like 4 times 5 mod 7. You can check that that, I think all the arithmetic there is right. And 4 times 5 is 6 mod 7. But then if our proof system is defined mod 5, then directly carrying out the computation will yield the wrong result. So they're fundamentally different spaces for doing arithmetic. And so you need to simulate the one inside of the other. So what can we do? The situation is not hopeless. Like in principle, right, this is a solvable problem because you can simulate or you can encode any NP relation into an arithmetic circuit over any field. You know, you can encode it into binary digits and check that the digits are binary and encode the bitwise operations. And so this is just sort of like an existence proof. But this is, you know, very slow. We would prefer to do things more efficiently than this. And intuitively, you'd kind of expect it to be possible to do something better, because fields are similar, right? They wrap around, but for small values, they behave kind of like the integers. You're just adding numbers, and if you don't exceed the modulus, then maybe we can exploit that somehow. That's what we're going to try and do. So the first observation is to check modular arithmetic. We just have to check integer arithmetic, because if you want to check something mod R, this is the same thing as just checking some integer relation for a quotient. And in some cases, this is actually sort of enough by itself. So depending on the relative sizes of the fields, it might be possible to do this. Like if you check that A, B, C, and Q are all small, you can just check this relation, as written here, directly in the larger field. And the reason this works is because it will never wrap around the larger field. So if all the sizes of everything are small, or provably small, then if it's equal to 0 mod p, it has to hold mod r. You can convince yourself of this, I think. It's not that strange, right? Because if something has magnitude less than p and it holds mod p, that means it's 0 mod p. And since it has magnitude less than p, the only thing that's 0 mod p with magnitude less than p is 0. And so this works sort of like up to square root of the size of the field, basically. And that's coming from the fact that we're checking a multiplication. If you were checking a degree 3 thing, then you'd want a cube root or something like that. But what if our field's not small, or our modulus is not small? Maybe it's even bigger than the prime that we're working over. Then you can't expect your encodings of the small values in the larger fields to behave well. In fact, it might not be possible at all. If your elements are larger than the field that you're encoding your relation into, you can't even commit to a big value in a single field element. So this technique, I think originally, or at least I first heard about it from Aztec, but there's this ancient theorem, the Chinese remainder theorem, that allows us to work around this problem. And so what that says is if you want to work in a big modulus, it's sufficient to work in two smaller moduli that divide the larger one, or that multiply to give the larger one, if they're co-prime. And it's sort of like both sides of that are rings, right? So you can take a value, and instead of working mod AB, you work mod A, and you work mod B. And these two rings are isomorphic. And so people call it a residue number system, and you can represent a large number, mod AB, by representing it by encoding all of its residues mod different small primes that divide AB. Here I've written it with two, but you can sort of recursively apply this equivalence to include arbitrary numbers of co-prime divisors. And so remember we were working mod P, and we had had some R and our R might be too big. But now we can pick whatever modulus we want, right? We can pick a bunch of small primes. So it's always large enough to recover the value that we're operating over. And so long as everything kind of remains small, that Q is supposed to be a product. So long as everything remains small, this works. And like before, how we didn't want to wrap around P, here we don't want to wrap around M. But here M is a free parameter. It's not fixed by the proof system. Okay. So now like a slight digression to explain the idea. Read Solomon codes are an error correcting code scheme that is ubiquitous in in snarks if you've heard of starks or fry this is you know essential to how they work and the basic idea is we have some message that we treat as a vector of dimension K and we encode it as a polynomial as the coefficients of a polynomial and then that polynomial we evaluate at more points. So remember a polynomial of degree k, or k minus 1, is determined by k points, and by evaluating it at more points, you're adding redundant information. And so, like in the original conception of Reed-Solomon codes, this was to deal with the fact that some of the information might be corrupted. So you only need, even if some of the data is maliciously altered, you can still recover the original message. Certainly given all of the points, you can recover. So given a polynomial, and we evaluate it at some set of endpoints, this is equivalent to reducing the polynomial mod this kind of degree one ideal, X minus RI. And so this insight allows us to generalize Reed-Solomon codes to other domains, ideal domains. And for example, algebraic geometry codes, which replace polynomials and evaluations with functions of some curve and divisors can be cast in this language as well and this is also how we're going to introduce CRT codes here CRT is still Chinese remainder theorem. So the idea of a CRT code is rather than encoding our message as a polynomial, we encode it as an integer. And then, rather than evaluating the polynomial at multiple points, we take the integer, mod, a bunch of small primes. And so you can think of these mod small primes as analogous to evaluating a polynomial at a point. It's not exactly the same, right? Because the ring you get from reducing mod different primes is not the same. But the ring you get from evaluating at a point is always the same. It's not really important for our purposes. Yeah, so if you have enough primes so that the product of all the primes exceeds the bound of your original sort of space of integers that you're encoding into, then even if some of the primes are wrong, in the same way that with the Reed-Solomon code you can correct errors, with the CRT code you can also correct errors. And this encoding is the same as a residue number system. So you're taking an integer, you're reducing it mod a bunch of small primes, you're keeping the residues and you can just work with those. And for those who are familiar, there's a lot of similarities here with how, you know, Starks work and other kinds of algebraic proof type stuff. Okay, so this slide I just screenshotted because it was horribly mangled by the formatting. So the sketch of the protocol is, suppose we want to verify some non-native arithmetic. So we have some system of polynomial equations, the f in different x's, which are integers, and the f's have degree d, and all the integers are bounded. So in the original version of this, we'd be checking arithmetic mods some other prime, and you can encode that as an integer relation. And so the primes are fixed in advance, and you have some M, which is larger than the kind of maximum value achievable by F. As written, it's a little wrong because you probably want to account for the additions, but it's something like that. And then you encode your integers, model the small primes, as we described, and also provide quotients for the evaluations of these functions. I guess as written, those should all be zero, so maybe there's a slight confusion there. But yeah, so you prove your encoding of all of the integers is sort of within its appropriate range, and then you choose a random subset of the primes to test the relations over. And for simplicity, assuming the primes are all like roughly the same size, then you can calculate the success probability of a dishonest prover pretty straightforwardly. You know, it's just the probability of having like some code word, right, some set of residues that does not correspond to a small integer, or that's like wrong in some position, and then, yeah, anyway. Yeah, so it's similar to how starks work. Okay. Yeah, okay. So this is the interesting stuff. As described in the previous protocol, we're testing each encoding of X is close to a small integer directly. You're just taking your set of residues and checking that it is close to a small integer or an integer within the right bound. This used to be how fry-based proofs did things, but it turns out that there's a much more efficient batch proximity test. The proximity test here is you're testing the closeness of the set of residues to its space of correct messages. And what people do now with Starks is take a random linear combination of encoded columns or encoded integers and test that the random linear combination is close to a code word. So instead of testing each one independently, you can just take a random linear combination and test that one thing. So the question is, can we do the same thing here with CRT codes? So I think so, but it's very different, right? Because in the Reed-Solomon setting you have polynomials and you're working over a field and you're taking a random linear combination over the field here we have a set of integers and we're taking a random linear combination of these integers but there's no obvious base field in the same way that there would be for polynomials to take a random linear combination over. So instead we would pick a random integer linear combination to take of the integers. And this has a lot of interesting issues. So for example, this is not sensitive to encodings of small rational numbers. If you encode it like one half, then there's like a one in two chance that a random linear combination will cause the one half to go away. And you might think, well, we could pick primes to multiply by, but that doesn't really work. So it is in some sense fundamentally different, but I think that it's possible to do this. The question is just like how good is it and if those things can be worked around. So for example, for working mod R, this is kind of fine because small rationals are also valid sort of to reduce mod R for large R, but there's lots of details to be worked out. So, yeah, just some overview of the things I haven't talked about in this talk. So I mentioned a little bit that CRT codes, the notion of distance is a little bit subtle, especially if the primes are not all, like, the same size. They can't sometimes always be, I mean, right, like every prime is of a different size, so you have to account for that. It's not like a fundamental problem, it's just a little more complicated. As well, I mentioned this decoding to rationals rather than integers, which in some applications is fine. It might not always be fine, so I have to work that out as well. And then another thing I haven't talked about, but that could actually be worth considering in practice, is all of this generalizes from integers to number fields. And in number fields, you have a sort of different situation where the first point, you actually can have primes that are like of the same size, right? If you have some prime over rationals, integers that splits over number fields, then that might yield like a more convenient thing to work over. And in some cases, this sort of works. And then you have to ask, how do you define the size of things and so on? But just more technicalities, I think. And then one other interesting thing worth considering is, so we moved to the integers, and we were no longer able to take random linear combinations over an extension of the base field. But what happens if we work over Starks with polynomials, and instead of taking a random linear combination over the base field, we took a random linear combination of columns by random polynomials, not over an extension field. In this way, it's sort of analogous to taking an integer-linear combination of integers, and it has a lot of the same problems, but it would allow us to avoid ever working with extension fields, potentially. So I think that would be an interesting direction to explore if this work ends up making sense. And that is all I have. Thank you. All right. Thank you so much, Liam. We do have a question here. Could you repeat how you calculate which primes are small enough to use for creating a CRT encoding? So I guess there's two ways that I'm not sure what the question is referring to but For encoding into like a residue number system the primes can be any primes That you you usually choose them to be small because working mod small primes is much more efficient. This question could also be talking about the example that I gave where you can simulate arithmetic mod a square root sized number in a larger field. And that is just coming from the fact that the relation that you're calculating over can't exceed the field you're operating in. So you fix some field that you're actually working over, and then if your field that you're simulating is small enough, then everything kind of just works out, as long as everything remains smaller than the characteristic of the larger field. All right, perfect. We do have a little bit of time for questions, so let's give it another 10 seconds to see if we have any questions. What is the size of size 2 to the 128 sized space. All right, perfect. Thank you so much for the amazing talk, Liam. So we will resume at 5 for our next talk.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T10:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/15NH3bC1NnjmkyRycEK1VaWR9dgZMJsH0PJMf-OTgOyA",
      "resources_slides": "https://drive.google.com/file/d/1roAOatZGYwzShqJODF3ECwVwVJNYEUOT/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "security-frameworks-by-seal",
      "sourceId": "A7TNUF",
      "title": "Security Frameworks by SEAL",
      "description": "Comprised of dedicated security specialists, SEAL aims to spread awareness and educate the community about Web3 security best practices and pitfalls. We address various challenges, compile accessible resources, and create new content. Open to all backgrounds, our guidelines provide comprehensive security frameworks for Web3 projects, offering best practices and practical solutions throughout their lifecycle. We aim to make Web3 a safer space for developers and users alike.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Hacks,Public good,framework,Hacks,Public good,Security",
      "keywords": "Best practices,Guidelines,Frameworks.",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "41a583919688b5e2f6ad7b3fa35a6f8a8fa49a66f9c7cd395a60a280ae8a22ae",
      "sources_youtubeId": "XTrR7aQLeWQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T10:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1HmUewjGmXzH3e1271bv_rXsd73TpbSS90ZBFslgi4ic",
      "resources_slides": "https://drive.google.com/file/d/1vvOCM-7zuriJcDo3qpapOcyrj3Hd46xk/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "solidity-inline-assembly-for-developer-experience",
      "sourceId": "F7XJZW",
      "title": "Solidity Inline-Assembly for Developer Experience",
      "description": "We demonstrate how inline-assembly is used at Solady to improve the account abstraction developer experience, write concise code, and create novel features.\r\n\r\nSolady is a Solidity library (MIT-licensed). \r\n\r\nSome of our biggest users include Coinbase, Optimism, Uniswap.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gas,Account Abstraction,solidity,Account Abstraction,Gas",
      "keywords": "Solidity",
      "duration": 1000,
      "language": "en",
      "sources_swarmHash": "82e9412440d2f77b0682445fbd724edc9ece1726af1fa89fec760eb0ec69b406",
      "sources_youtubeId": "CUHov__69b0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ca1e9dbb7a90e1955648",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ca1e9dbb7a90e1955648.vtt",
      "transcript_text": " Intro Baiklah, saya sudah berumur berkali-kali. Dalam masa percuma saya menjaga library ini yang dipanggil Solady iaitu library yang berbeza keterangan yang dikirakan kebanyakan dalam pembinaan dalam. Jadi hari ini saya akan bercakap tentang bagaimana kita menggunakan assembly dalam dalam untuk membuat perkara lebih mudah untuk pengguna library. Baiklah, jadi untuk mulakan, beberapa masa lalu saya menulis tweet ini mengatakan pemula menggunakan assembly untuk menyelamatkan gas kerana itu adalah perkara yang paling jelas. Pemula menggunakan assembly untuk mengelakkan limit Dragon Spirit. Kerana assembly membuat kode byte anda lebih kecil. Seperti bahawa walaupun tanpa EOF, jika anda menggunakan Solady, anda mungkin dapat melihat seperti 20-30% kekurangan kode byte. Kemudian assembly pengguna lanc untuk menghidupkan masa. Setelah anda terbuka untuk menulis pembukaan dalam dalam, anda hanya menulis kod U Optima secara langsung. Jadi pembukaan dalam dalam dan U, kita boleh gunakannya secara bergantung dalam konteks ini. Okey, jadi hari ini saya akan menerangkan sedikit kedua dan tiga, kedua dan ketiga. Baiklah, jadi beberapa contoh untuk dimulakan. Yang pertama adalah leap penyelesaian aman. Saya juga akan bercakap mengenai leap yang telah dilakukan di Solady. Jadi di Solady, kita mempunyai library ini untuk proksis. Dan proksis kami adalah kebanyakan, saya akan mengatakan kebanyakan jika tidak semua proksis kami seperti yang telah ditulis secara langsung dalam kode kata. Kemudian kami juga menerangkan beberapa keadaan yang lebih tinggi seperti contohnya, lip salt dan bagaimana menggunakan assembly inline-z dapat membantu dalam ujian. Okey, jadi contohnya, kita gunakan ini, katakan jika anda ingin menghantar ether kepada seseorang dalam kontrak aksi tetapi bagaimana jika pembawa adalah kontrak yang mengusir atau tidak memanfaatkan fungsi terima. Jadi di Solady, kita mempunyai fungsi ini yang dipanggil force save transfer if yang menggunakan self-destruct untuk menghantar ether kepada orang. Jadi sebagai pengguna, anda tidak perlu peduli tentang semua ini. Anda hanya menggunakan fungsi dan anda boleh sangat selamat bahawa ether akan dihantar kepada orang. Dan mengapa ini berguna? Kami tahu bahawa wrap if tidak berada di adres yang sama pada setiap L2. Jadi jika anda ingin mengembalikan kontrak anda ke adres Create2 yang sama dan anda menggunakan RepIf, ia agak mengalami masalah. Jadi, sebaliknya anda boleh menggunakan fungsi ini dan anda boleh menyelamatkan ruang dan masa otak anda. ruang otak dan masa. Di DeepClone, kita mempunyai fungsi ini yang dipanggil deployERC1967 yang memanfaatkan proksi brightcode yang minimal yang boleh diubah. Jadi ia berdasarkan ERC 7760 ini. Proksi ini telah ditulis selama lebih dari sebulan. Tetapi baru-baru ini kami ingin menyatukan semua ini menjadi ERC supaya penjelajah berbuka dapat mengimplementasikan auto-verifikasi dengan mudah. Oleh kerana ERC ini telah berada di sekitar, seperti proksi ini telah berada di sekitar selama semasa, ia juga telah mendapat sedikit penggunaan. Jadi dalam jenis basa, ia adalah proksi pilihan yang digunakan untuk wallet Smart Coinbase dan untuk Polygon, saya rasa mereka menggunakannya untuk sesuatu yang lain juga. Jadi ada lebih dari 100,000 contoh proksi ini yang digunakan di sekitar. Dan kelebihannya adalah bahawa ia cepat, ia kecil dan dalam beberapa kes ia auto-verifikasi. Jadi ia menjaga masa anda. Anda tidak perlu melakukan beberapa alat API atau kebutuhan. Ia hanya memeriksa sendiri. Jadi DeepSort juga mempunyai beberapa fungsi yang berlaku tinggi yang boleh digunakan untuk tujuan ujian. Seperti kita ada operasi set pada array yang disort. Anda boleh gunakannya untuk ujian invariant. Dan untuk ujian plus, kita tahu jika anda menulis ujian di Foundry, anda perlu menentukan variable random dalam argumen fungsi dan ada jumlah ruang yang terbatas yang boleh anda lakukan. Dan anda perlu memikirkan variable yang menarik yang saya rasa sangat mengambil masa. Jadi, saya menggunakan banyak magik hitam untuk menulis metode yang memungkinkan anda untuk menggunakan untuk menghasilkan serta-merta nombor yang serta di mana-mana di dalam kode anda. Jadi anda boleh melakukan sesuatu seperti pengujian cepat yang diperlukan. Dan jika anda ingin tahu bagaimana untuk mengintegrasikannya ke dalam projek anda, anda boleh mengikuti link di bawah. Okey, jadi ini adalah beberapa kajian pelajaran yang anda mungkin ingin periksa. Satu perkara yang saya rasa agak menarik adalah dynamic array leak. Contohnya dalam SOID, jika anda menentukan array, anda tahu anda tidak boleh melakukan pen atau pushback tetapi di mana dynamic array leak ini mempunyai fungsi itu. Okey, jadi mengapa assembly dalam dalam adalah baik untuk penulis bibli seperti saya. Pertama, ia memberikan beberapa teknik seperti ia membantu saya melakukan beberapa penyelesaian kode supaya pengguna bibli tidak perlu risau bahawa bibli saya menyebabkan mereka mempunyai stack terlalu dalam. Anda juga boleh mencari kumaf yang tidak terkendalikan dalam soliditi normal. Dan jika anda tahu bagaimana untuk memperkuat kompiler dengan arah yang benar, anda dapat mencapai beberapa ekstrak masa kompil tanda tanpa kos. Jadi, contohnya, mengelakkan stack terlalu dalam. Kami mempunyai fungsi ini yang dipanggil lon-what dalam fixed point math lib. Jika anda menggunakannya sendiri, kadang-kadang kompauler cuba untuk menggabungkannya dan ia menyebabkan stack terlalu dalam. Tetapi jika anda menulis fungsi ini dalam assembly yang bergabung, anda boleh menggabungkan penggunaan yang diberikan, anda membantu kompon menggabung dengan cara yang mengelakkan stack terlalu dalam atau tanpa IR. Jadi dalam SOAD, kita sangat khas yang ia mesti berfungsi dalam sebanyak situasi yang mungkin. Jadi sama ada anda menggunakan via IR atau sama ada anda memilih untuk tidak menggunakan via IR, kita ingin memastikan ia berfungsi. Anda tidak perlu risau bahawa library kita adalah kalibran stack2d. Itulah sebabnya kita sangat khas dengan semua detail kecil ini. Okey, kemudian, inline-SMB juga membolehkan kita membuat Math yang keren. Anda tidak perlu memahami apa itu, tetapi ada dengan Math, tetapi ada beberapa kode-kode op seperti kode byte dan kode op yang tidak berkaitan dengan branch, yang membolehkan kita membuat Math lebih cepat. Contohnya dalam metode log2 ini. Dan keuntungan mempunyai fungsi yang tidak berkaitan dengan branch yang boleh dilakukan dengan assembly dalam dalam adalah ketika kompauler soliditi cuba untuk mengoptimisir, ia akan mula memeriksa apakah fungsi anda mempunyai beberapa pangkalan. Jika ia mempunyai pangkalan, maka fungsi itu mungkin tidak boleh dilakukan dalam dalam dengan pipeline VRIR dan jika anda menggunakan pipeline legacy, ia pasti tidak akan berada dalam dalam. Oleh itu, dengan menulis barang dalam InlineSMB, kami membantu kompiler menghasilkan kode yang lebih efisien dengan memberitahu kompiler bahawa fungsi ini boleh diterapkan dalam garis. Oleh itu, menggunakan trik ini, kami juga dapat mencapai teknik lain seperti jika anda ingin membandingkan string pendek dengan string pendek lain, sec, tradisional anda akan menggunakan jenis kata-kata Kikak 256 tetapi jika anda tahu assembly inline, anda boleh melakukan sebuah jenis magik hitam yang akan memberitahu kompiler bahawa daripada melakukan alokasi memori yang tidak berguna, hanya menggunakan opcode Optima. Jadi semua ini, walaupun ia kelihatan seperti banyak kode op, tetapi pada akhirnya setelah anda menekan Compile, ia secara sesuai menyelenggarakan menjadi hanya beberapa kode op. Dan ini hanya mungkin jika seluruh cat kode itu tidak berkumpulan. Baiklah, jadi perkara tentang Solidity adalah bahawa ia adalah bahasa yang sangat indah. Anda boleh menulis semua perkara yang rendah dan pengguna tidak perlu bimbangkan detail. Jadi semua ketakutan, kegiatan, kegiatan hitam telah dikeluarkan dari pengguna. Pengguna perlu hanya peduli tentang apa adalah API tinggi dan apa adalah penggunaan semua fungsi yang berbeza. Jadi anda boleh menulis dengan bersih dan berkesan, pen soliti yang berada di tingkat tinggi yang dapat dibaca tanpa bergerak dalam lubang rabit yang tidak ada di dalam. Ada beberapa tujuan yang diperlukan seperti North Star of Solidity. Kami ingin memastikan soliti adalah bahasa kontrak yang terbaik dengan mengeluarkan setelah menggunakan utiliti yang sangat optimis yang menjadikan Compiler Solidity sekarang dan ke depan. Jadi idea adalah orang-orang berkata kita ingin menulis bahasa kita dalam Rust kerana ia mempunyai library yang lebih luas. Kita ingin menulis dalam C++ because there's a wider library. But in SoilDT, we have this exotic math function called LambertW01 which is only available in Python, maybe some math language in SoilDT. You can find it in any other major languages. So that is so ladies go. If it's Turing complete, if it's possible within the Turing complet kita akan memperbaikinya. Kami juga ingin mengukur Ethereum melalui pemotongan layer app dan juga membuat kode cantik. Kami juga mempunyai pelan untuk EOF. Walaupun saya agak, ia agak besar, tetapi saya melihat pentingnya kerana kode EOF sebenarnya lebih optimal untuk perkara seperti SP1, jadi akhirnya, saya rasa hampir segera, SoLady akan mempunyai EOF. Ia tidak akan menjadi jenis yang berbeza. Jadi kita mempunyai direktri sumber quite soon, Solady will have an EOF. It won't be a breaking chain. So we have a source directory, and then we have an EOF directory. So you can use either one. We won't force you to use either EOF or the legacy. We'll give you an option. Okay, so here are some links. You can visit solady.org, which is a shortcut to Solady. Or you can visit my GitHub or my ex. Sometimes I post like small Solady lessons on my ex. Okay, I think that's all for my talk. Thank you. Thank you, Victorize, for this talk. I guess we have all now a better idea of how to leverage assembly, inline assembly in our code. In the meantime, we have plenty of time for questions. So if you have any, make sure to please send them through this QR code. I'll be starting with the first one on the leaderboard. Did assembly ever bite you in the butt? Where vanilla Solitude would have been more safe? Okay, when I was starting out like maybe two years ago, I think the safe transfer leap forgot to clean some upper bits because I directly potted it from soulmate. But then someone spotted that bug. So these days, I am super paranoid about unclean, dirty upper bits. So it becomes like a second nature. So I think recently it should be quite safe. All right, next one. When to use let and when to use mstore? Okay, let is a way menentukan variabel dalam pembinaan. Jika seperti katakan, bagaimana-bagaimana anda tidak boleh memaksa kompiler untuk mengganti jangkauan terlalu dalam, maka anda mungkin perlu menggunakan mStore untuk menggunakan ruang kacau untuk variabel sementara. Tetapi anda paling mungkin hanya boleh menggunakan dua bahagian have to use mStore to use the scratch space for temporary variables. But you at most can only use two parts of the scratch space, like two slots, because the other slots are for other purpose, for important stuff. Next one, why do you prefer Solidity Inline Assembly over other low-level languages like Huff? Okay, so the thing about... I like Huff, but the benefit of Solidity is that it allows you to recycle your efforts. And also, it allows you to generate more compact bytecode. For example, in Huff, the inline is really inline. But whereas in Solidity, if you have like 10 use, if your function is a giant chunk of bytecode and it's used like in 30 different places, you might not actually want to inline it every single time. Next one, at L2's call data with call data feedback function, I guess from Libzip, has greater gas versus non-compressed call data feedback function, I guess from LibZip, has greater gas versus non-compressed call data, but has slightly cheaper gas at L1. Is that correct? What happens under the hood? Okay, this depends on your L2. Some L2s already forward the gas savings of compressed call data to the users. So if you use CD4BET on Optimism, you will most probably not be able to save any core data money. So if you're deep on OP stack, just don't use that. When using assembly-based libraries, is there a higher likelihood that it is less compatible with future Solidity versions? I would? Saya akan mengatakan bahawa sebenarnya itu kurang kemungkinan kerana, contohnya jika anda melihat penyimpanan transien, Solidity mengimplementasikannya pada tahap pembinaan pertama, kemudian kemudian ia menjadi seperti perkara yang lebih tinggi. Jadi satu perkara yang perlu anda risaukan adalah bahawa Solidity mungkin ber in ways that the high-level parts interpret the memory slightly differently. So that you have to keep up with the change log. Victoria also has a nice friend, a nice hat, and a panda on his hat. Do you want to show everyone? Yeah, I guess that helps you as well in coding. Next one, I guess how Sol-AD's features saturated? How many more libraries and functions do you plan to add? Okay, I think right now we have mostly saturated. We have a lot of work to do to port to EOF. Especially the lib clone is going to be... But the Solidity team has actually much more stuff to do compared to me. Another one about Sol-AD. Will the new Solidity compiler in Rust impact Sol-AD? I think this new Solidity compiler called Solr, they try to become like a feature parity with the official Solidity compiler. Like for example, in C++ you have Clang, you have GCC. I think we might go that route down the road. And if you are just writing in a high-level language, you don't need to care whether you use Clang or GCC. All right, that wraps up for our Q&A session. Thank you very much for the questions and give a big round of applause for Vectorize.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T10:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1ww4IN7FSAReDpOBeMK96jT38LWmsqkRdbQBoBnUIH-k",
      "resources_slides": "https://drive.google.com/file/d/1FlbxteD9YFQooQvu_TjjjOA7Ug97kf-k/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "structuring-censorship-resistant-privacy-protocols-risks-and-considerations",
      "sourceId": "MVJFDX",
      "title": "Structuring Censorship Resistant Privacy Protocols: Risks and Considerations",
      "description": "This workshop is aimed at developers, legal professionals, and project managers involved in the creation and maintenance of privacy-focused projects and will guide participants through the various considerations and risks that need to be managed during the structuring, development and launch of these protocols.",
      "track": "Cypherpunk & Privacy",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": true,
      "tags": "Frameworks,Privacy,Censorship Resistance,legal,Censorship Resistance,Frameworks,Privacy",
      "keywords": "Legal",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1hNJE0EKTqY7KkSQmnZdpNsxrFfsKPlhwl0VFWn9f3pA",
      "resources_slides": "https://drive.google.com/file/d/1_NF6zWt1cV0YWs0lyG8M-obAtbpAUQHD/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "the-chain-abstraction-master-plan",
      "sourceId": "DCSCA7",
      "title": "The Chain Abstraction Master Plan",
      "description": "Chain abstraction is vital for Ethereum’s future competitiveness and interoperability. This talk will dive into why Ethereum apps need chain abstraction to avoid fragmentation and ensure open, trustless, and modular systems. We’ll explore approaches to abstraction, the importance of open standards, and a roadmap for upgrading the ecosystem’s core infrastructure—spanning JSON-RPC API improvements, resource locks, and intent settlement—to unlock new layers of usability and decentralization.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Account Abstraction,Cross-L2,Developer Infrastructure,DevEx,Ethereum Roadmap,Gas,Intents,MEV,Paymaster,Rollups,Token bridging,Transaction fees mechanisms,User Experience",
      "keywords": "Chain Abstraction,OneBalance,Resource Locks",
      "duration": 883,
      "language": "en",
      "sources_swarmHash": "41949c931075e883c80aba1313f8f7f87470af99f2053e0e96485b9145d4b4bf",
      "sources_youtubeId": "9fH-de8v53g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e4739dbb7a90e18580fb",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T09:50:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1aMlbfC7Va_bqN5fI43BFPOB0iIennWgUwyiQxb7D3q0",
      "resources_slides": "https://drive.google.com/file/d/10EkGiZcp3_B5sTR_FMEgG2WFbJ1DXC56/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "tlsnotary-applying-mpc-and-interactive-zk-to-prove-web2-data",
      "sourceId": "RTVKJC",
      "title": "TLSNotary: Applying MPC and interactive ZK to prove web2 data",
      "description": "Diving into TLSNotary, a protocol which leverages multi-party computation and interactive ZK to prove the authenticity and provenance of any data on the web to another party.\r\n\r\nSummary:\r\n1. What it is and what it can do\r\n2. High-level overview of how it works\r\n3. Details on the underlying MPC and ZK protocols that we use\r\n4. How to use it",
      "track": "Applied Cryptography",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Identity,ZKP,MPC,oracle,Identity,MPC,ZKP",
      "keywords": "User Sovereignty,Infrastructure,Oracle",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:30:00.000Z",
      "slot_end": "2024-11-14T09:40:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1XH5xVNY-eLNdwvYduookcntMG3Z4qjU319sqNmXxUXo",
      "resources_slides": "https://drive.google.com/file/d/1SOVbq_b6OBIWkwl06aLNMMKgOa6VY0NY/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "a-fast-confirmation-rule-for-ethereum",
      "sourceId": "F7RFTH",
      "title": "A Fast Confirmation Rule for Ethereum",
      "description": "In this talk, we present an algorithm that, assuming good network conditions, reliably determines in less than a minute whether a proposed block will always be part of the canonical chain.\r\nThis represents a considerable speedup compared to waiting for the full security guarantees provided by block finalization, which takes 13 minutes in the best-case scenario.\r\nAlso, this algorithm provides a far better metric than chain depth, which some services still use. Paper https://arxiv.org/abs/2405.00549",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Consensus,User Experience,confirmation,rule,Consensus,Layer 1,User Experience",
      "keywords": "Confirmation,rule",
      "duration": 1458,
      "language": "en",
      "sources_swarmHash": "d5a8607677858333b0aae7830955e04f92c041b7b3b2107cb7392939b2eb0ae8",
      "sources_youtubeId": "p7JPRTELnJc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736ce9e9dbb7a90e1f57ba7",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736ce9e9dbb7a90e1f57ba7.vtt",
      "transcript_text": " Hi everyone, I am Luca Zanolini and today together with Roberto Saltini we are going to give you an overview of our fast confirmation rule for the Ethereum consensus protocol. This is a joint work with Aditya, Francesco, and Cheney. So let's start by answering this question, what is a confirmation rule? Before that, let me give you a quick definition that will be useful throughout the course of the presentation. What is a canonical chain? A canonical chain, very roughly speaking, is the chain that is followed by the honest validator, so the honest participants in the consensus protocol. For more technicality, it's basically the output of the fortress function, but let's just stick with the chain followed by the honest validator. Our fast confirmation rule is an algorithm that enables nodes that run this algorithm to identify blocks that will never leave the canonical chain, assuming good network conditions. Basically, this algorithm outputs whether a block is confirmed or not. And it satisfies these two properties. One is safety, meaning that if a block is confirmed by a validator at a given time, it will always remain in the canonical chain forever, basically. Moreover, we have also monotonicity, meaning that if a block B is confirmed at a given time, it will always remain as confirmed, meaning that if we confirm a block today, tomorrow it will remain confirmed. Sorry. Sorry, something is going on with the presentation. Okay, sorry. Today we will focus only on safety property, given that monotonicity actually requires some technicalities that are out of the scope of this overview. Okay. Why do we need a fast confirmation rule in the first place? So today, the way to know whether a block will always be in the canonical chain is through finalization. The problem is that with the currently implemented protocol, finalization takes up to 13 minutes in the best case scenario. protocol, finalisation takes up to 13 minutes in the best case scenario, and for some kind of users, this might be too long to wait to get a confirmation in some sense. Moreover, we cannot really use block confirmations as we were used to do with proof of work, for example, where the more blocks are appended on top of a block, the higher was the likelihood that this block remained confirmed. Because in the current protocol, we have seen some attacks called balancing attacks where the adversary could manipulate, in some sense, the view of honest validator, making them believe that they were participating in a chain. So in some sense, this might not be the real chain under this attack. So block confirmation doesn't really say anything about whether a block will always remain in the real canonical chain in some sense. So with a fast confirmation rule, what does a fast confirmation rule give us? For sure, it gives us improvement on the user experience in Ethereum because having faster confirmation rule implies that small values transaction can, in some sense, don't have to wait for finality, but they can already immediately be assumed as confirmed, implying that they will eventually get finalised. Moreover, it reduces also risk trade, basically trade reversal made by forex changes that allow to optimistically trade immediately upon deposit. And moreover, it improves also reliability of some wallets. In particular, there are wallets that tell users that a transaction is already confirmed upon entering, upon including in the block. But as we said before, because of this possibility of a balancing attack, this doesn't really tell us anything meaningful. So having a fast confirmation rule can really improve also reliability of wallets in general. It's also useful for PBS in the sense that it provides block builders with an indication whether the block that they are building upon is actually not going to be re-org'd. So in some sense this fast confirmation rule can give us more reliability and can help users that want faster confirmation time instead of having to wait finality. Before going into details of this confirmation rule, let me very quickly give you an overview of Gasper. This is because Roberto will introduce some technical terms that is better if we are all synchronized on that. Casper is the name of the consensus protocol. This is a proof-of-stake protocol that can be seen as made by two sub-protocols. One is LMD Ghost, which is a synchronous consensus protocol that determines the canonical chain. And Casper is the finality gadget on top of this that finalizes blocks that are outputted by this LMD ghost. Basically, it finalizes blocks on this canonical chain. The time unit in Gasper is a slot. A slot lasts 12 seconds. 32 slots define an epoch. For every epoch, the set of validators is partitioned in 32 disjoint committees. epoch the set of validators is partitioned in 32 disjoint committees and at the beginning of a slot a proposal is randomly selected from a committee and it proposes a block on top of the canonical on top of the head of the canonical chain and and the other validators in the committee will just cast a vote ideally for the block that has just been proposed. How validators choose which is the head of the canonical chain, they do it through the fork choice function. Fork choice function selects all the latest messages ever cast by validators, and it chooses the heaviest chain in terms of stake that these messages carry. So, for example, with this image, this is the canonical chain, and proposers will just append a block on top of B9 for example. One last thing, in case of a validator sending two conflicting votes for the same slot we do not count these votes for determining in determining the weight of this chain so we discount equivocations. Now let's dig into the information that we have. So we have to be able to determine the weight of this chain. Now let's dig into the rule and Roberto. Thank you Luca for setting the stage with all the concepts we need to understand how the confirmation rule works. So let's start with some assumptions we rely on. The first one, we rely on the fact that votes sent by validators are received by other honest validators by the end of the slot. The second assumption is that we assume that there exists a maximum beta fraction of dishonest stake across any set of committees. So take any set of committees, we assume that maximum beta of the stake of this committee is dishonest, and 1 minus beta is for sure honest. Okay, as Luca mentioned, we cannot rely on chain depth at all. We do need to rely and look at the weight of the LMD ghost vaults. How do we do this? Well, let's start by assuming this block B10 is canonical for now. For our example, we use beta 0.2, 20 per cent. Then we assume also for simplicity that the weight of a committee is 100. Now what we are going to require is S represents the support, the weight of LMD GOST that goes towards block B11. We require that the S is larger than W multiplied by half plus beta. We'll see why. But just running through the samples, in our case, this means S of B11 more than 70, and we pick 71. We know that any other block for this slot that is conflicting with V11 has no more than, actually, it has less than half W volts, which means less than 50. Let's pick 49. What is the worst case scenario? Well, the worst case scenario is that beta of the stake equivocates. With beta of the stake equivocates. the stake equivocates, then 20, weight of 20 is subtracted from 71. We have 51 for B11 and 49 for anyone else. Well, B1 wins the choice. With this rule, we can say B1 is canonical. Are we done? No, we are not done. All we can say is B11 is canonical in this slot. We want to say B11 is canonical forever. We can do that, but we see why we can't do that just right now. So what happens, for example, if we move to the next slot? example, when we go to the next lot. So let's move to the next lot. example. the committee weight goes to 200. votes for B13, these votes, 80, goes to B11. we have B11 as a weight of 131, but what we need is higher. We need 200 multiplied by beta plus half. We need 140. So we're not quite there, and what does this mean? Is B still going to be canonical? Well, the answer is yes, it is. But in order to prove it, we need to do some clever trick. This clever trick is assuming that we somehow, for now, we know who is honest. So we are going to use this H. H represents the votes for B11 that are from honest validators. Bear with me, we'll go over this honest thing later. And J represents the validator, the stake in the committee. committee. represents the stake in the committee that is honest. is that H of B11 is larger than half the committee. clearly this will make B11 win the fork choice. the fork choice. we are going to work with which is this, we are going to require J, which is the honest committee, divided by 1 minus beta. condition implies the first one. Can show you by running, using our numbers, we get HB11 more than 50. We just pick 51 for now. So what happens when, OK, sorry, I forgot this. We use a, we define an indicator, p, for usefulness of the fraction of H over J, which in our case turns out to be around 0.63, which is larger than half divided by 1 minus beta. So what happens when we go to the next note? Let's have a look. Okay, the committee increases to 200, the B11 increases to 160. Now everybody that is honest vote for B13. goes to B11 as well. If you now calculate P, we have 131 over 160, which is roughly 0.81, which is larger than our target of 0.625. That's exactly what we wanted. As we showed before, this actually implies that the B11 is at least more than half of the possible votes. So B11 will be canonical. Now, for those who actually know intimately GASPR, they might ask, well, when you cross the epoch, things are not exactly like that. But I show you that that's not a problem. So what happens when you cross the epoch is that it's possible you have a new validator set that is sampled, but it's possible that in the new slot you have validators that have already voted before. So in this example, we assume that the honest committee only increases by X, some X of some sort. So in the new slot, 80, or the honest one, will vote in support of B13, and the honest one will vote in support of B13 and because X stake is new, they have not voted before, so the support for B11 will increase by X. Now if you calculate P, it's 51 plus X over 80 plus X. Because X is added both above and below the line and 51 is less than X, so the new slot is 51 plus x over 80 plus x. Because x is added both above and below the line, and 51 is less than 80, we know this is larger than 51 over 80, which already shown is larger than our target. So what we have achieved, what we have shown is p, this indicator p, never decreases. So once it's higher than our target, which is half divided by one minus beta, we know B1 will be canonical forever. But now, how do we measure P? Well, lucky for us, if we use our initial fraction, which is S over W to be larger than half plus beta, Actually, this implies the threshold that we want for P. So I left it one last thing. Oh, sorry. So basically what we have is that if you have this, the Q, which is what we mean to be defined to be S on W to be more than half plus beta, this implies that our threshold on P and that B11 will be canonical forever. One last thing, we assume that B10 was canonical. How do we solve the problem? That's very easy. We ask the question is Q more than half plus beta for B11, then we go back to B10, to B9 until we get to the finalised block. So in terms of LMD cost, that's all we need in terms of confirmation rule. Things get a bit more complex when we add FFG-Gasper. So with FFG-Gasper, the fault choice gets a bit more complex. We start from the latest justified checkpoint, which is roughly the checkpoint that is ready to become finalised. We remove some blocks that are problematic, and then we apply the fork choice that Luke explained before to the remaining blocks. So what we need to do to ensure that B and some block B is canonical forever, we apply the rule that we saw before for LMDGhost, and then we need to work out a series of conditions that ensure that B will never be filtered out by the effect of integrating LMDGhost with Casper. This amounts to, in our case, we require that the checkpoint justified by block B is from the previous epoch. This ensures that in the current epoch, block B is not going to be filtered out. And then we have this more complex condition, which I'll break down soon, which ensures that B will not be filtered out in any future epoch as well. So what we're going to do in this condition, the the will not be filtered out in any future epoch as well. So what we are going to do in this condition, we look at the weight of check point of B, we remove the maximum amount of those votes that can be slashed because perhaps they are Byzantine, so this corresponds to beta, the weight of the committee from the beginning of the slot to the current slot. And then we add the minimum amount of weight that we keep voting for the checkpoint of B, which is the one minus beta, the remaining weight between the next slot and the end of the slot. And we require this to be more than 2 third of the total stake. What this gives us, it gives us the checkpoint B the the this gives us the checkpoint B has enough votes to be justified. assumption, this checkpoint of block B will be next epoch, and then because we know that And then because we know that every honest validator only votes for a descendant of B, if there is any checkpoint in a future epoch which will be justified, that for sure will be a descendant of B. Which means that B will never be filtered out and therefore will be canonical forever. Just going through some initial experimental results that are due toi Circle. Hvala Circle, ker jih je pripravljen. Načrtovno je tudi postavka na blogu, tako da se pripravite. Svetovnika je izpralna za šest dni. V tih šest dni so izpralni 56 blokov, ki so bili preizv 56 blocks that were reorgs, some also were two block reorgs. The implementation was a proof of concept, there were some approximations. So for example, they were polling VCO nodes every 10 seconds rather than running the confirmation rule at the beginning of each slot. Also they were estimating the FFG weight using just the LMD weight. However, the results are very encouraging. First of all, none of the reorgs were ever confirmed, no matter the value of beta. So this gives quite good confidence of the security provided by this rule. security provided by this rule. Second, in terms of performance, what they saw was that if you fix beta to be around 20% of the stake, you could see confirmation in four, five slots. If you go up to 30%, you get to eight, nine slots. However, we think, this has to be proved in practice, that if the rule is implemented by running it at the beginning of a slot, then we should be able to confirm a block in 12 seconds. So this leads me to the last slide. So if you are interested in testing this and you're interested in implementing this confirmation rule on a Bitcoin node, reach out to us. Our contacts are in the talks description also one with the QR code for the paper which is also linked to the talks description and the second QR code is to the Ethereum consensus if you are interested that's all, thank you applause okay cool I'm going to approach from behind and push you towards the middle as we address questions. So we'll start from the top. Seems like confirmation rule will get obsolete with single slot finality. Does it make sense to work on it at this time? Seems a question we asked ourselves. . Yeah, the point is that we still don't know exactly when single slot finality will come out and the confirmation rule is something that we already have, so it can already be helpful in some sense while waiting for single slot finality. But yes, once single slot finality will be out, probably there will not be, it's not needed anymore, but yeah. It's not, say, it's, in our opinion, not too difficult, that doesn't require any hard fork. It's not that difficult to, I wouldn't be the one implementing it, but I don't think it's difficult to implement, so it wouldn't take that long, and provides good benefit in the meantime, yeah. Okay, cool. The UI is not updating. How do you estimate beta? Is it a new parameter? It's user subjective belief on the value of beta. So the user picks their beta that they believe is the maximum and they query. And according to what they believe is they get the result accordingly. Well, can a confirmed transaction still be affected by a reorg? As I said, one of the assumptions is that votes are delivered by the end of the slot. If there is a network partitioning, this is not true anymore. Yes, it can be affected by reorg. So, it's not as safe as finality, but it's quicker. It's a sort of trade-off. What is the recommended value of beta you suggest? Is it 0.2? and how does it impact security? That's a very good question. Of course, beta cannot be larger than 30%. In other aspects and some, for example, that work on EPBS that's been researched out, usually 20% tend to be what people are comfortable with in setting beta, so that seems like a good, reasonable value. What are the security guarantees? Do you have any probabilistic assumptions? We have some probabilistic assumptions, which is the one that said, you know, we have this assumption saying we take any sequence of slots and the maximum beta of the stake is dishonest. As you know, the committee sampled randomly, and so when you cross actually the epoch, you have some probability measure that if you assume that over the entire stake, say beta 1 is dishonest, the fact that over some set of committee this is less than beta is a probabilistic measure, but the probability goes up very sharply. So, yeah, there is some probability from that perspective. Can you explain validator's honesty scoring more? I'm not sure I fully understand the question. I'm trying to fully understand the question. If not, we can move on and whoever asked the question can, I guess, re-clarify. So then we have, the example has an honest amount of validators quite optimistic. What about a more pessimistic scenario? Can validators be honest most of the time until they find something they vote adversarial? Okay, that's right. If I understand the question correctly, I mean if this is the behavior you're thinking about that should be accounting the data, so thinking about that should be a count in the data. So this rule relies on belief that one minus beta, they're honest and they will vote according to the rule. If some point, if in the future, some of them become dishonest,",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:40:00.000Z",
      "slot_end": "2024-11-14T10:10:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1eztv0xy8RI4T_eMei061J--yX-7gDRGN4ZnQYsasWbU",
      "resources_slides": "https://drive.google.com/file/d/1cez0UtMmY4mbonX-QvJz3ZahZVi_MhPE/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "giga-staking-for-school-connectivity",
      "sourceId": "ZU3AEJ",
      "title": "Giga: Staking for school connectivity",
      "description": "Giga is a joint venture between UNICEF and the ITU with the mission of connecting all the world's schools to the internet. Over the past years, a novel approach to fund the ongoing operating expenses of school connectivity has been running as a pilot in Rwanda and Giga is currently scaling up operations.\r\n\r\nAs part of this pilot, one staking node has been generating returns that are being spent on connectivity in a school in Rwanda. All of this has been done in compliance with local regulations.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking,Sustainability,Ethereum for Good,Social,impact,Ethereum for Good,Staking,Sustainability",
      "keywords": "connectivity,schools,social impact",
      "duration": 472,
      "language": "en",
      "sources_swarmHash": "4ba71b291e596a21671ea12501fbde925f46699752ba06188c0e734f1f72af2e",
      "sources_youtubeId": "nqe0zeu_z7w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735c98d9dbb7a90e17afef8",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735c98d9dbb7a90e17afef8.vtt",
      "transcript_text": " Hello everybody. Can you guys hear me well? Good. Thanks to the organizers for putting me right after Chris. It saves me a lot of introduction for what Giga is and does. A quick summary. So we're a project about connecting every school in the world to the internet and every young person to information, opportunity, and choice. A little bit of a summary of what we've achieved so far. We've connected about 14,500 schools benefiting almost 8 million students. And we're currently active in 34 countries. So it's a big operation. One of the opportunities that we found, one of the challenges that we have, is in once a school is connected, to fund the operational expenses of the school. So we're kind of looking for a sustainable financing model. And one of the projects that we've looked into is around staking. So I thought it'd be interesting to talk to a crowd that's very familiar with staking and talk a little bit about the challenges that we faced in implementing this in the real world. So what we want to do is we want to create staking nodes and use the staking rewards to pay for the operational expenses of Internet connectivity. So it's kind of like an endowment. A little bit about the size of the opportunity or challenge that we have. We did a case study a few years ago where we looked at what it would cost to connect the 86,000 schools in the 17 countries that we were active in at the time. And we discovered that it would cost approximately $305 million in operational expenses. So it's a very significant problem that we're looking to solve for. But the benefits also speak for themselves. So with sustainable funding, about 25.5 million students and teachers will be able to benefit from connectivity. What we've done so far is we started with setting up a staking node and loading it up with 32 ETH. So you can look at the validator node there. And what we are using this funding for is to pay for connectivity for one particular school, the Murama Primary School in Rwanda. So about 1,000 students in Rwanda are benefiting from this connectivity at the moment. But we ran into some problems. So currently we have to jump through all kinds of regulatory hoops in making this system work. So we have to off-ramp the ETH into fiat, then the ISP needs to send a paper invoice to a company that pays this invoice, and then the school needs to be connected. So you kind of lose track of the payment process in that way. And it's something that we feel hinders the scalability of this a lot. So there's no problem in sort of paying for one staking node ourselves and connecting one school, but doing this for all the remaining schools becomes a problem for impact investors if you can't prove the impact. So this is where we are. We have completed this pilot and we want to scale. But we're currently sort of stuck in the middle where we're trying to set ourselves up for success. One of the things that we need to do for that is create an end-to-end crypto flow in cooperation with the central bank of Rwanda. And we have a pathway for doing that. And the second is to set up a pooled impact staking product. But I'm here today with a super specific call to action. And Vivian, I might ask you where that group of African community was gathered. But we're essentially looking today for a crypto company based in Rwanda who could join us in this regulatory sandbox of the central bank and help us make this more scalable going forward. I'll leave a few minutes for questions. Thank you. Do we have anyone from Rwanda here? We're not here. Nobody. It's okay. We have the whole hub. Okay. We have one person here, one, I think, over there. Let's start with this gentleman with the white shirt. Yes. I've got a crypto company in Rwanda. I'll talk to you after. Oh, good. I told you it would work. So I'm from Myanmar, and we are having all kinds of crisis. Can you speak up a little bit? Yeah. So I'm from Myanmar, and in Myanmar, there are a all kinds of crisis so is there can you speak up a little yeah so I'm from Myanmar and in Myanmar there are a lot of civil wars going on and we the schools needs a lot of this internet connectivity issue so the first challenge is that not only the internet connectivity but also we only got around six hours seven hours electricity a year. So in that kind of scenario, will the GIGR infrastructure will be work? Or that's the first question. And second is, will there be a future plan to come to Myanmar to support this kind of internet infrastructure development? Yeah. So this particular project was really about raising the money to pay for operational expenses. There's of course this whole infrastructure question and electricity providing question that also needs to be solved for, which is part of a bigger problem, I suppose. We do intend to see if this solution will scale on a global level. So far, we've always hit this regulatory glass ceiling in this solution. So we are quite excited to be working with the central bank in Rwanda in trying to break that glass ceiling for this particular use case, and then hopefully using that as a pilot study to show central banks that there is an opportunity in this space in other countries as well I think for the Robonert they already have you know support of the central bank but in Myanmar the central bank and all kinds of financial transactions are always checked by this dictator military so I think this will be not happen in like next a few years I believe. Yeah Myanmar sounds like a very complicated situation I agree with that. Thanks for raising the question. Any other question? We have time for one last question. One over there. So I'm from Kenya, and I really appreciate the work that you are doing with Chris. So my question would be in terms of locating, for example, the institutions, you had mentioned that you have an ML model that helps you identify what is a school and what is not a school. Is that model available for use to other use cases? So are you working agriculture identity for farmers? I mean, the answer is yes. It will probably have to be a different model. But we do have a data team that looks into that. And I don't know if you mentioned it. Did you mention that we're also doing hospitals and health centers? Or am I spoiling something? Yeah, so we're also branching out into other public infrastructure, like hospitals and health centers. Okay, Luc, for you afterwards. Thank you. And thank you, Gerben. Thank you, Giga team. That was a lovely half an hour. Okay, that's another round of applause.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:40:00.000Z",
      "slot_end": "2024-11-14T09:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1rmmBw3SZZEyNNDi7PgdUEMlN6Wfogmt3EIpC8WZe-5I",
      "resources_slides": "https://drive.google.com/file/d/1AhKmqFIfP4_kfIJKdenhbX1DVgOp8Ipk/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "unlock-web2-data-with-tlsnotary-hands-on-workshop",
      "sourceId": "VPMQGM",
      "title": "Unlock Web2 Data with TLSNotary: Hands-On Workshop",
      "description": "Join our hands-on workshop to master **TLSNotary**! Dive into multi-party-TLS and learn to prove and verify online data authenticity to a third-party verifier while ensuring privacy. We’ll start with small examples in Rust and build up to custom browser extensions in TypeScript to collect and verify private user data.\r\n\r\nBring your laptop, bring a friend, and learn together. Get ready to unlock and compose Web2 data in innovative ways.",
      "track": "Applied Cryptography",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Live Coding,Privacy,MPC,oracle,Live Coding,MPC,Privacy",
      "keywords": "oracle",
      "duration": 5123,
      "language": "en",
      "sources_swarmHash": "08a2c85bb2455f9e3ab6c2e0b2181df657e1370607a8a686c1627266923ca9d3",
      "sources_youtubeId": "FhKjScuaNxw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cbc4b982f234a12f066e9",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673cbc4b982f234a12f066e9.vtt",
      "transcript_text": " . Thank you. How about now? There we go. So transport layer security, TLS is an old protocol. It's ubiquitous. It's all over the Internet. Basically everything supports it. If you're not using it, you probably should be. And the important thing is that all websites already support it. And so, yeah, basically, it's a protocol between your computer and a website that you're connecting to. It's using cryptography. So you can query data from a website and know that you're talking to the website you expect and your communications are protected. But one thing that it can't do for you is you can't query data from a website and then present it to another party. There's reasons for this that I won't really go into, but you can just know safely that it's not possible to just take a TLS transcript and send it to someone else. They won't know whether or not the data is actually authentic. But TLS, there are signatures there. There's cryptography there. And maybe we can plug into it and somehow leverage it for what we want to do, which is to allow people to privately prove data to a third party. I'm not sure if you've come across this jargon before. There's multiple different terms, multiple different approaches for generally the same thing. MPC TLS, CKTLS, web proofs, TLS middle boxes, Windows proxy. They're all essentially trying to do the same thing. We've implemented MPC TLS, and this leverages multi-party computation and zero-knowledge proofs in order to allow a prover to privately disclose data to a verifier in an implicitly secure way. And then there's other approaches, like a proxy approach, which kind of switches up the topology here, which puts the verifier in between the prover and the website. But, oh, right. One thing to note here is that both these approaches, both the MPC TLS approach and the ZK TLS proxy mode, whatever you want to call it, these are both designated verifier protocols, which means that it's really between two parties, and the only party who really receives cryptographic assurances is the verifier that's actually running the protocol itself. So sorry for everyone that wants to do things on-chain. This does not give you trustless oracles, but it does give you a trustless off-chain designated verifier protocol. And yeah, all approaches kind of all go towards the same affordances, which is that it allows you to basically compose with any application. If there is data on the Internet, which is queryable using TLS, you can trustlessly compose with it and without asking for permission. So all servers are already running TLS. So you can basically take our protocol Wired into your application And now users can just Prove arbitrary data to you In a privacy preserving manner And just to Elaborate on the Privacy part there, user can Query data from some arbitrary data Source and present it to your application While redacting their secrets Such as their cookies and their HTTP request. They can redact data in the response. And we're probably not going to get into it today, but you can also do zero-knowledge proofs over the top of that data, such as hiding your address but proving you're not in the United States, and then the classic example of proving that you're above some age without revealing your exact birth date. Yeah, so those are the general affordances that these protocols provide. TLS Notary is free, open source. The core protocol is written in Rust. We have TypeScript bindings. We're going to demo a browser extension slash plugin system, which is just one way of distributing the TLS notary protocol. And yeah, it's Apache, MIT, dual licensed at your option. Do whatever you want with it. Yeah, so that's an introduction to generally what TLS notary is and what it does. And yeah, let's just hop straight into building something. So good afternoon. I'm Hendrik, also from the TLS Notary team. And this is our schedule for the workshop today. We are plenty of members of the TLS Notary team. So if you have questions, there's Tanner, Ryan. Who else do I see? Thomas in the back, Chris. And then Tsukino and Sinu. So just if you run into any questions, just ask. So we just had this great introduction to TLS Notary. We'll start with coding part one. So that will be mainly on your own machine, everything offline so that we don't run into any network problems. Then we'll do some experiments with TLS Notary where you work together with your neighbor. So just distribute the roles. And then we'll also switch to the browser extension. Where we'll do a demo first, and then Tsukino will give an overview of how all browser setup works. Then we'll switch to coding part two, where we'll build our own plugins. And then we'll do a short, an extra slide with some, where we go into the future of TLS Notary and discuss what comes up next. And then there's some play time where we invite you to build your own plugins or build on top of TLS Notary and we're here to help you. And then we're also very open to hear all your questions. So some tips and tricks for the workshop. So don't rush through all the items. We have time. There is a HackMD document. It allows for adding comments. So if you run into things that don't make sense or you see typos or whatever, just comment so that we can improve it for next time. Also, check the Wi-Fi. So there is a local Wi-Fi network here that allows us to connect to each other. So make sure you're on the, yeah, I can't read the name, but I think it's a classroom network. The network, the password is also over there. Also for the web devs that will do a little bit of rust, but it's only reading Rust code, so don't be intimidated. You should be fine. And also, as I said, the TLS Notary team is here, so if you have questions, just ask them. So, if you go to this URL, you will find the classroom notes. And yeah, if you have any issues, just call us and then we will help. So maybe a quick overview of what we will do there. So we will start with the most basic setup of TLS Notary, where we just have a server, a prover, and a verifier, where everything is on the same networks on your local computer. So we'll start with starting all the necessary services. And then the prover will do the request to the web server with the cooperation of the verifier. This will return in an attestation. We will then create a presentation where we redact some information, and then the verifier will verify the info. And then the second part, we will work with a neutral notary, so where the roles of the verifying parts are split up. So this is all in Rust. And then in the third part of the example, we will run the prover in the browser. And then in the nodes, you will see that then we will also distribute the roles. So you could run the prover and your neighbor could be the test server and another neighbor could be the verifier. Should be fun. Any questions so far? So the question was which repository for the first item, so that's the TLS and repository Sorry, so if you didn't bring your laptop, this will be a very boring workshop, I'm afraid. So maybe then just chat with some of us and ask us your questions. Vielen Dank. Thank you. Thank you. Amen. Vielen Dank. Thank you. If you have an error message running the last command, don't stress, that's for further in the workshop. So the prerequisites is mainly to have all the dependencies on your laptop so that you can go through the following steps. Thank you. Thank you. All right. Thank you. If you run into a Rust C error, check your Rust compiler version and then just do Rust update to get the latest Rust compiler. Thank you. Thank you. Vielen Dank. Thank you. Thank you. Thank you. Thank you. Vielen Dank. Thank you. So they couldn't install the PDK. Thank you. Thank you. Thank you. Vielen Dank. Thank you. Thank you. Thank you. Thank you. ¶¶ ¶¶ Thank you. ¶¶ Thank you. ¶¶ Thank you. Thank you. Vielen Dank. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:40:00.000Z",
      "slot_end": "2024-11-14T11:10:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/18dMKK1NHUfq3W_cP2sm0ttim6fH4ZLV0KlzLOZdAiZ0",
      "resources_slides": "https://drive.google.com/file/d/1AkKRK-frg9D-yoY-XfuUKKZH_fVpR58W/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "ethos-dgen1-self-sovereign-os-hardware",
      "sourceId": "TALWUM",
      "title": "ethOS + dGEN1: Self sovereign OS + Hardware",
      "description": "In this talk I will talk about ethOS, the dGEN1 and the concept of self sovereign software and hardware.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DePIN,Mobile,UI/UX",
      "keywords": "",
      "duration": 482,
      "language": "en",
      "sources_swarmHash": "ea1cab369dd91136941e6ea8e57b72d60156dc26dbee471bcb30235241d5a2cb",
      "sources_youtubeId": "Eb-wJh1PK5k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735cae29dbb7a90e1adef32",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:45:00.000Z",
      "slot_end": "2024-11-14T10:00:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1_547FFGifntr2F9NLRt6mgJnjr6QzNRpm-JcA8hqP_c",
      "resources_slides": "https://drive.google.com/file/d/1gpkHojawFWW5S9r_sbheRTMrM-XQ8Yt4/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "warren-winter",
      "sourceId": "9PWLDW",
      "title": "Warren Winter",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:45:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1KC8s2MGqxozkSjf4Ogbdu9s8XFZLZgkj32ySca-LrnQ",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "simulating-economic-systems-of-an-autonomous-world",
      "sourceId": "KWKW3W",
      "title": "Simulating Economic Systems of an Autonomous World",
      "description": "This presentation reviews the basics of token systems design and their onchain game applications. This will be specifically tailored to onchain complicated economic systems and simulating them in interactive notebooks for real-time graphing; aiding in parameter tweaking and finding gaps in systems designs. The goal of this talk will be to begin to bridge the gap between complex token systems designers and onchain game designers.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Gaming,Protocol Design",
      "keywords": "Token Engineering,Simulations,Complex Systems",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:50:00.000Z",
      "slot_end": "2024-11-14T10:15:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1JGirNWdZq9HEHUw7sdVF-0QUGOk9fJFHX5UmLIB_6hk",
      "resources_slides": "https://drive.google.com/file/d/1WbHadz-O2MJY3w-YoLENCtorEaisyeZ6/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "solving-multichain-ux-lessons-from-cosmos-for-the-rollup-ecosystem",
      "sourceId": "QKRCF7",
      "title": "Solving Multichain UX: Lessons from Cosmos for the Rollup Ecosystem",
      "description": "This talk addresses how we tackled challenges in the Cosmos ecosystem like liquidity fragmentation, multi-chain accounts, and cross-chain contract standards, and how these solutions can be used to improve cross-chain UX in the rollup ecosystem. \r\n\r\nIf time allows, we'll also dig into designing flexible and scalable abstractions for rapid deployment of integrations (bridges, dexs, wallets) across not just many chains, but many diverse tech stacks.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Fragmentation,UI/UX,Account Abstraction,defi,cross-chain,aggregation,Account Abstraction,Fragmentation,UI/UX",
      "keywords": "DeFi,Cross-chain,Aggregation",
      "duration": 1470,
      "language": "en",
      "sources_swarmHash": "bfcbec6589bfc1a512d43c1f597aa804af938b8641ac913c6cd2aa96b53a8edb",
      "sources_youtubeId": "2J2XDbN8Q6M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d1009dbb7a90e1500f8e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T09:50:00.000Z",
      "slot_end": "2024-11-14T10:20:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/10vnF2ObOK5u8Z8XcfbB0o6Q0DIS1LwGHZA_ieNhsIXg",
      "resources_slides": "https://drive.google.com/file/d/188K8egWhH8b_3TPWp4zFMKL7Qu9OfUQt/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "deep-dive-into-fork-choice-compliance-for-ethereum-clients",
      "sourceId": "D3XZKF",
      "title": "Deep Dive into Fork Choice Compliance for Ethereum Clients",
      "description": "In this talk we will share the design of the methodology checking the compliance of Ethereum consensus layer clients to the fork choice specification. The core of the methodology is based on the constraint solver models which allows to generate huge number of distinct test scenarios providing comprehensive coverage. At the current stage we have ended up at around 13,000 fork choice tests, but the test suite we developed allows to generate a million of tests and even more.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Fuzzing,Testing,Core Protocol,Testing",
      "keywords": "Fork choice,model based testing,fuzz testing",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "14177e03121feac769f91c2e8d4d3a396ab599adb4c47087f768bcd41cca9aac",
      "sources_youtubeId": "N5WzD8ptGJA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:00:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1MDK3dwXPQcTMGQIVnxa-4Kpkp17RJexPuQt0c3zp1_Q",
      "resources_slides": "https://drive.google.com/file/d/1IDmfIELnPZ1hfh0IVFUDyE1MEDVltMmE/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "interpreting-solidity",
      "sourceId": "GQAEZX",
      "title": "Interpreting Solidity",
      "description": "In this talk, we present an alternative way of executing Solidity: interpreting it.\r\nFoundry popularized writing more in Solidity, including tests and scripts. However, the compilation model is limiting for some use cases, such as interactive environments or general purpose scripting. We first describe how interpreting can solve many of these limitations, then, we explain how to build such an interpreter, finally, we present a Solidity REPL that we built using this approach: https://eclair.so",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": true,
      "tags": "Developer Infrastructure,Tooling,Languages,Developer Infrastructure,Languages,Tooling",
      "keywords": "NA",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:00:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1YKUtPFBeb26s1YkKpnXAOT5YJuWFJaIAKmQLoipb0oM",
      "resources_slides": "https://drive.google.com/file/d/1kq4F6OS57K5O0wbORU7X9OkHbULYHCFB/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "programmable-cryptography-and-dacc",
      "sourceId": "PNA8NU",
      "title": "Programmable Cryptography and d/acc",
      "description": "This short panel will explore the role of advanced programmable cryptography, beyond ZK and MPC, in d/acc. Programmable cryptographic primitives like functional encryption (FE), witness encryption (WE), and indistinguishability obfuscation (iO) have become theoretically feasible and even moving towards real-world practicality. This panel will explore how these primitives can be used to improve trust-minimized infrastructure and applications.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,Use cases of cryptography",
      "keywords": "d/acc,programmable cryptography",
      "duration": 1223,
      "language": "en",
      "sources_swarmHash": "c51ad64f5cc04390985fbfe82798708f9cf1c378086fcb689bc0ee5ee3d75a64",
      "sources_youtubeId": "NrhmX3yHNdA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d0a79dbb7a90e146832f",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:00:00.000Z",
      "slot_end": "2024-11-14T10:20:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1NOKA9WOe3iWdApB0QmpWreDTMUpsQvJeG7afyjEMBSQ",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "the-verifiability-vision",
      "sourceId": "KXRMGY",
      "title": "The verifiability vision",
      "description": "Imagine all data was guaranteed to be correct. We could build a trustworthy digital world based only on correct data. In this presentation, we will sketch layers and techniques that can realize this dream, in particular proof carrying data and succinct proofs.  We will also discuss the connection to the proof singularity vision for Ethereum as well as highlight caveats that apply; humanity is still in the early stages of the journey and there are obstacles and constraints to tackle",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Scalability,Vision,ZKP,proof,succinct,Scalability,Vision,ZKP",
      "keywords": "Verifiability,proof carrying data,succinct proofs",
      "duration": 1670,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "5l6XY2lX244",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d5949dbb7a90e184c02b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735dbd09dbb7a90e1646d41.vtt",
      "transcript_text": " Okay, and we're live. Welcome, everybody. My name's Andrew. I'm with Xerox PARC. And I'm here to talk to you a little bit about pods. How many here have been using pods this week? I think the answer should be everyone because you got into the building somehow and that actually required one. So what are pods? So your devs con ticket is a pod. The proof of attendance to this talk that you can claim from the Q&A app that's up on the screen there right now. If you're in the room, I don't think this works remotely. That's also a pod. Some of you have been playing Frog Crypto this week. I see some frog hats out there. All those frogs are pods. A pod really can be anything. It can be a secret message. It can be your identity credentials. It could be your driver's license if we get any governments involved to actually do this. If we get any governments involved to actually do this it can it's cryptographic data of any sort So what is the pod framework? So pod is is a technology a framework that makes it easy for apps to issue cryptographic data and to make zk proofs about that Data, it's a data format that's optimized for fish improving It's a standard of how that data format can be sent around and things can be proven about it And it's a framework of how that data format can be sent around and things can be proven about it. And it's a framework with some developer SDKs. Check out our documentation site. I'll have a link at the end if you want to try it out. It's mostly in TypeScript, but can be used on other platforms as well. We have ports in a few other languages. So I'm hoping some of you will get some use out of it. So one last WTF is zero-knowledge proofs. How many people here have used ZK proofs before? I feel like you understand them. Okay, a few. It's kind of obscure technology. That's kind of the point of pods is to make it easier to use so you don't have to understand how the underlying math works. But in brief, a ZK proof lets you prove the validity of any private data or any computation on your private data without revealing that private data itself. And that proof is trustworthy because there's some math that basically you can only calculate if you did it validly. At Xerox PARC, we think of ZKProofs as a universal cryptographic adapter. Basically, I've got lots of different kinds of private data. By doing computations on that data in a verifiable way, I can present to somebody whatever I want that is validly proven from that data. The example in this diagram, which you'll find in our blog post, is like, what if I could calculate my own credit score from signed data I got from my bank or from the IRS? I don't need to ask a credit reporting company to gather all this stuff together. I can gather it myself, and I can make a provable statement about what my credit course score is and apply for a loan. This is part of the vision of something we call the Parknet, programmable cryptography internet, which we think is going to be much better once programmable cryptography catches on in all of these ways. ZK proofs are a big part of this, but it's only the beginning. See the other talks being given by my colleagues this week. Also, we have a whole day CLS tomorrow about programmable cryptography. But today we're going to be focused on ZK proofs and what pods let you do with them. So this is the pod ecosystem that we envision. You need issuers who are issuing this cryptographic data. They're mostly using a private key to sign it. Those takes the form of attestations, which users themselves can hold on to. They hold on to their own private data. They don't need an intermediary. At some point, some consumer asks the user, please prove something about yourself, your identity, your age, the fact that you went to DevCon, things like that. And then the consumer can generate a ZK proof and send, sorry, the user can send the ZK proof to the consumer who can verify that proof. They do need that third arrow in the diagram, which is a little bit of knowledge about who the attester is. You need at the very least to know that that attester has a public key that you should trust. There might also be things like what is the event ID that represents DevCon on a ticket, things like that. But that kind of completes the diagram. Okay. So why are we doing this? So I work with the team that builds ZooPass. You've all been using that to check into DevCon. And we believe that the best learning on this kind of technology comes from contact with reality, meaning we want real users to try this. We want to do it at scale. There are 12,000 people at DevCon this week who are stress-testing ZooPass for us. Thank you. I'm sorry if the performance has not always been as great, but it seems to be standing up. And we want to use these opportunities to onboard new users by bridging data that is not ZK-friendly into our ZK-friendly world and take advantage of people who are willing to be early adopters like the crypto community. So by bridging, what I mean is we're bringing data in the red boxes on this diagram Into the green world red in my diagrams of this talk and the next one means like non zk friendly systems Whereas green means ek friendly systems we can bridge it in we can then issue it like your devcon ticket Which is loaded from a database that isn't cryptographically signed And then you can the verifiers can get you into another system like telegram in order to join the the DEVCON chat group. All that is working today. In order to bring this in front of the most users, we do have to accept some constraints. So we're not using the most cutting edge of ZK technologies. We want everyone to be able to use it, which means we've built a mobile friendly web app. Which means everything we do has to run in a browser on a phone. Even an older phone, even on a bad network when the Wi-Fi is overloaded at the conference. So that became a bit of a mantra when I was building some of these technologies. There's a lot of cool ZK technology out there that is great, but it needs to run on a big back-end server and I don't have one of those when I'm in a browser on a phone. So we've got to use tried and true technologies. For people who are in the know, we use SIRCOM and GROSS16. You may or may not have heard of those, but that's kind of the underlying technology. They've been around for quite a few years, so they're pretty well battle tested at this point. So I want to talk a little bit about the systems we built along the way. So this is what ZooPass ticketing looked like a year ago at DevConnect when we were in Istanbul. So it's the same triangle that you've seen here. We were pulling data out of Precix and issuing the tickets. We used a format called an EDDSA ticket. That's a signed piece of data, but it's not a pod, which I'll explain a little bit later. And then we had a proving circuit where you could prove your ticket, you could reveal some fields, you can prove that you owned it, etc. So what did it take us to build this? Don't pay attention to all the details here, but look at the line counts on these PRs when we wrote these things. It's pretty large. That's quite a few lines of code that it took. And in just the ZK proof, there's about 15,000 lines of code that are still there, not including tests and documentation. So it's kind of complicated. So that was the first thing we built. The second thing we built was Frog Crypto, the first version last year, which used a very similar data format. So frogs were issued by the server as what was called an EDDSA frog, very similar format to tickets, and then you could make a proof about it, you could present it to our Zucat telegram bot who would let you into the secret frog holders chat. This all happened last year in Istanbul. So what did it take to build that? It turns out it was very similar. There was a lot of duplication of effort. There was a lot of similar patterns, but you couldn't actually reuse the underlying data. So there clearly is a pattern here, right? We want to issue some signed data. We want someone to request a proof and then to be given a proof of that signed data, we want someone to request a proof and then to be given a proof of that signed data, but it turned out that each time we had to build it, we had to rewrite a whole bunch of code in order to customize it. So I'm an engineer, I don't like this kind of complexity, I'd rather do things once because I'm lazy. So why is this so hard? So the signed data part, the EDSA PCD that we were using as our underlying data format. Used as a fixed size hash, it hashes 16 numbers in an array. And therefore, every new data type that we wanted to put in there, we had to do some custom coding to decide how those numbers in that array go together to make this data type. I would analogize this to imagine you were processing all your data in a hex editor directly as bytes. It's kind of inconvenient. We have better tools than that now. And on the proof side, ZK circuits are a little bit awkward to program. Like they don't use a normal programming model. You don't write it in a language you're used to. Every variable is what's called a field element. This is a mathematical concept. It's a very big number, modulo some big prime number, and you've got to like write equations on those field elements. So it's kind of complicated. And also once you build a ZK circuit, it's very fixed. In order for the prover and verifier to agree on what's valid, the circuit can't change very much. You have to publish a whole new circuit. So that makes this a bit hard. I would analogize this, again, to in the hardware world, this is like an ASIC. It's a chip that does one thing. It In the hardware world, this is like an ASIC. It's a chip that does one thing. It might do it very well, but it still only does one thing, and every time you want to do another thing, you've got to build a whole new chip. It's kind of inconvenient. So what do we need here? Well, what we'd really like to have is what's called a ZKVM. Basically, if you have an ASIC and you want something more general, why don't you use a out there that lets you basically write code, run it inside of a ZK circuit and validate that this is the correct output. It's great. Some other people are giving talks about it this week. But unfortunately for our situation, it's a little bit too much. Like I said, my mantra has to work in a browser on a phone. ZK VMs are pretty big right now. You're not going to be able to do that on an older phone in a few seconds. So we have to do something a little bit more limited than that. But again, I'm an engineer, I like working within constraints and coming up with clever solutions. So here's what we came up with. So on the data side, I'm finally gonna explain to you what a pod is at some level. So a pod is just a collection of names and values. Think of it like a JSON object, except that it's flat. There's no hierarchy of nested objects, just names and values. It can have multiple data types in it for those values. The data is then cryptographically signed in a way that makes it easy to make proofs about it. And I'm going to get into more of that a little bit later. Also, I forgot to mention this at the beginning. We are having a deep dive session after this intro session. So stick around for that if you want lots more detail. But I'll give you what I can in the next 15 minutes. On the proof side, we also can generalize. So we have what we call a general purpose circuit, which means rather than having a fully CPU-like circuit in a ZKVM or having the ASIC fixed circuit, we can do something in between. I would analogize it more to an FPGA. We've got some logic blocks. We call them modules. You can feed in some inputs to your circuit in order to decide how those logic blocks are connected to each other and make a different proof every time using the same circuit. We call this framework GPC for general purpose circuit. And in addition to the circuits individually being configurable, we precompile a set of circuits in what we call a family at different sizes with different sets of modules. So when you want to make a proof, you can pick the circuit in the family that has enough modules for what you want and not any more because having a bigger circuit means more time to prove, more memory, etc. So you can make the right trade-offs there. So with that, we get the generalized version of the ZK ecosystem where every issuer is issuing pods. They might contain very different kinds of data. It might be a frog, it might be a driver's license, but it's still a pod. And then when you make proofs about it, you can freely decide what you want to prove and write a configuration to represent that proof. So with that in mind, at this point, what is a pod? So a pod is a data format that makes zkproofs easy. It's a key value store. It's going to be hashed and signed in a very specific way involving a Merkle tree, which I can explain more of later. And it's optimized for efficiency zkproving. Here's an example of a pod. So we've got some names and values. Most of these are very straightforward, so I'm not going to go through them all in detail. The one that's maybe a little bit interesting is the cardholder. So this is meant to look like a driver's license in some fictional country. The cardholder is my semaphore ID. This is what Zupass uses to identify you. It's really a public-private key pair. So the public key is what's going to go in the pod to say that this is my pod, or in this case, this is my driver's license. What you see on the right is the JSON format for this. It's optimized to be a little bit terse and also human readable. So things that don't need a type annotation, you'll notice don't have them because the JSON type itself is enough data for that. Once because the JSON type itself is enough data for that. Once you get down to actually building the Merkle tree, like everything does have a type, but in this table I call them type hints because the type is not part of the cryptographic data. Instead, it is guidance to how do I hash this data into a piece of cryptographically verifiable data. More on that later. So the first thing I do to make this into a pod is I build a Merkle tree tree i'm not going to go into detail on that but basically you arrange the elements into a tree you hash them all together until you get to a root and that root is what we call a content id the content id is derived from the data so if you have the same data you can derive the same content id regardless of how it was formatted in json one detail that you might notice on the right is that the names have been alphabetized. That's how we make sure that it is deterministic and you always get the same content ID. But everything else is just hashing. And then now once I've got the content ID, that's the thing that I sign. So if I'm an issuer and I want to issue a pod, first I get the data, I Merkle-ize it, I get a content ID, and then I just write a signature on that content ID, and that's enough to validate that the entire pod is valid. So we have a ZK-friendly data format. We'd probably like to do some ZK proving on it. So let's talk about the GPC side of this that is what lets you do that. As I mentioned earlier, GPCs are circuits made of reusable modules, as well as a family of multiple circuits so you can pick the size that you want. Let's look at what that looks like. So this is an example of a GPC configuration. This is how you say, what do I want to prove? And you're gonna present this as this JSON object that says what you wanna prove, and the system is gonna do the rest rest compiling this down to what to do with the circuit so here's a very minimal proof i'm going to try and prove that i have a driver's license that says i'm allowed to drive right so i my configuration says i have a pod i'm going to call it id card this is actually an arbitrary name that's just part of the configuration to refer to it later it has some entries and one of those entries is driver that is not not an arbitrary name. That's a name that was in the pod and is going to be hashed and checked. And what do I want to do with it? Well, I want to reveal it. So is reveal is true means this is a proof. It's going to prove that I have a pod, that it contains this entry, and it's going to reveal that its value is hopefully true because I'm going to try and drive a car. So that's simple enough. There's one detail that wasn't on the previous slide. That's because it's done by default, so I didn't need to include it in the config, but it's important to talk about. What I proved if I don't have, think about the signup key, is I just proved that I have a pod containing the word driver with the value true. That doesn't mean it's actually a driver's license. In order to do that, you've got to do something cryptographic. So the easiest way to do that is you check that the pod was signed by a public key that is well known. That might be the government of California, which is where I live. Hopefully we'll get them to issue pods eventually. But that is implicit. The signing key is also always revealed by default, but you can choose to not reveal it if you want to, in which case you can constrain it in other ways. You might constrain it to be equal to some other element without actually revealing it or constrain it to be a member of a list like maybe i have a list of all the signing keys of the 50 u.s states and i just want to prove i have a driver's license from one of them i don't want to tell you which one okay let's get straight and get a little bit more complicated um so i've proven that i have a driver's license that says driver equals true. I haven't actually proven that it's my driver's license yet. I could have stolen somebody else's. The thing is that pods, because they're just data, they are transferable. I can copy them. The way we make a pod bound to a single user is by putting that user's public key in it, which I showed earlier when we were looking at the entries. And the way you prove that you are that user is you make a proof that you hold the private key that corresponds to that public key. And the way you say that in the gpcconfig is this is owner ID field. You say is owner ID, and I give the type of public key I'm using, which is semaphore version 4 from our friends at PSE. And that basically means that this proof is going to be configured to check that I have the right private key in my private inputs. And in this case, it's not even going to reveal what my public key is, just that I own this pod and this pod says I can drive. Okay, let's get to a little bit more ZK and hiding some more data. Instead of proving that I'm a driver, what if I just want to prove I'm over 21? Maybe I want to go buy some alcohol. I don't know what the age is in Thailand, but back home it's 21. So I can just say I have a pod containing an entry called date of birth. That entry is not going to be revealed, but it's going to be in this range, and that's the numeric range for the date that is 21 years ago. We should make this more friendly and let you just pass in a date object, but for now it's a number. So this is a proof that I am over 21 and that I own this pod. I didn't take out that field, but everything else is not revealed and I'm being very anonymous. One last example, we can make proofs of multiple pods at once if we have a circuit with enough modules. So here's one that I'm proving I'm over 21 and also proving that I have a ticket to an event that maybe I'm going to go to an after party after DevCon. And in this case, the ticket, I'm proving that its attendee name is the same as the name in my driver's license. I'm proving that I own it and I'm also proving that the event ID of that ticket is in a valid list. I'm not revealing what I have a ticket to, but it's maybe a list of like DevCon related events that are happening in Thailand this week. So this is kind of a minimal anonymous way of checking into a party. Of course, if I'm there in person, I'm revealing some more about myself by being there, but you get the idea. Okay. So last piece of this, I've now configured my proof. I've decided what I want to prove How do I actually make a proof and all of this is an example of what you can do with the the GPC libraries So the three things I need in order to make a proof one of them is the proof config that I've already given you some examples of The second thing is the inputs. That's the actual pods Which I need to have in order to make proofs about them There are also other inputs like my private key or like that list of valid event IDs that I want to prove that my event ID is one of. Those are all inputs. The third thing I have to feed in is something called an artifact path. That is, where do we find the binaries that know how to generate this circuit? So when a ZK circuit is compiled, it generates a proving key, a verification key, and also a witness generator. Don't worry about what those are, but there's some like big binary things that the prover and verifier have to agree with. We distribute these via NPM. We also put them on various CDNs. You can download them. So you have to just decide for your app. Are you going to download them, put them on disk, give a path to them? Are you going to download them from a URL? There are options. give a path to them, are you going to download them from a URL, there are options. Once you've got these things together, the gpc proof function will generate the proof. It puts together that configuration, it picks a circuit that fits that configuration with enough modules, it downloads the corresponding artifacts for that circuit, and it generates the proof. And then the last thing it does, oh, I should have gone to the next slide, here we go. So it needs to compile down all those inputs into circuit signals that can feed into the actual ZK circuit, which are mathematical field elements, as I mentioned. And then after it's done and it gets a valid proof, it will decompile some of the outputs and turn them into what's called the revealed claims object. So it comes out of a proof. You've got the actual claims object. So it comes out of a proof. You've got the actual mathematical proof. That's just opaque numbers that are needed by the verifier. That's the actual ZK part. You've got a bound config, which is exactly like the configuration that you fed in, except that now it contains the identifier of the circuit that was selected so that the verifier knows how to verify it correctly. And then you've got the revealed claims. If I revealed that I am a licensed driver, driver equals true, that would be in this object. If I revealed my name, et cetera, that would be here. And that's what the decompiling is for. It's taking the circuit outputs and turning them back into a string or whatever the representative thing is. Okay, so those three things are exactly what I should send to a verifier, whoever I'm gonna prove this to. They need those three things. They also need an artifact path to download the corresponding verification key. And then they can verify the proof. They just do very much the same thing. They're going to compile some of those inputs back down into ZK land where there are circuit signals. They're going to verify the proof and they're going to say yes or no, whether it's valid. And, you know, gravy, we're at the end and hopefully everything went right and I've proven what I wanted to prove to you. So final takeaways, summary of what this was a bit of a speed run through. So pods are data that's designed to be proven about. Any pod is a signed attestation of something, whether it's I have a ticket, whether it's I have a driver's license, etc. GPCs allow you to flexibly make proofs about those pods by using modular circuits, which can be configured using a JSON-like configuration language. And the system will auto-select the circuit that you need depending on your configuration. So all your app needs to do is say, please make me a proof of this with these inputs and everything else is handled for you. Then the last step is the verifier verifies the proof, and then the apps do have to decide what things they trust. How do you trust that this is the correct proof? Like I alluded to before, you should check that this ID card was actually signed by the government. You should know the public key or you should check that this ID card was actually signed by the government. You should know the public key or you should know the event ID for DevCon. You should also check, and I'll say a little more about this in the deep dive, that the configuration that was sent to you was actually the configuration you asked for. So you don't want the prover to say, oh, I have a proof of something, but not necessarily the thing you asked for. That's something that you should check as well. But once you do all of that, this end-to-end should be very solid and you should be getting the information you need. Okay. That's it for the speedrun intro. Please check out our documentation. They're on pod.org that just went live yesterday. And also there's a link that just went by, t.me slash zoo pass to join the telegram group. And yeah, let's go do some Q&A. All right. Where do you store the sound for identity secret for users in Zupass? So that's all client side. Zupass stores all of your private data client side. The Zupass server is aware of your public key because that's how it can make sure that you get issued the right Devcon tickets and things like that But yeah, zoo pass is a client-side cryptographic data manager To what extent is pod an open data standard, so I consider it open we haven't like published a spec for it I should work on that but all of our code is open source, so people can do interoperability with it. The pod format itself is very generic and interoperable. It's the GPC compiler that turns a pod into the specifics of what you need to prove with a specific GPC. So the GPCs are kind of less standard and generic, though they also could be used on multiple platforms. We do have an example of GPC verification on chain that just started working a couple days ago, so all that is possible outside of a browser, but we don't have as many examples there as we do on the pod data. Can we scroll down? Is there anything more? Can you compare pod to verifiable credential? Yes. This is something I looked into. Pod is simpler. It doesn't really have a fixed schema or anything that ties it into a specific standard. You could put JSON-LD-based verifiable credential data in a pod if you wanted to. But a pod is much more flexible. At the cryptographic level, there is a difference in the kind of Merkle tree we use. The pod uses the lean IMT, which is something that Semaphore created, which is much shallower because pods tend to be relatively small, as opposed to the sparse Merkle tree that is used, at least for the implementation of verifiable credentials that I'm aware of, which is the one from IDEN3. That is a much deeper Merkle tree, but it can do things like prove absence of an entry, which pods can't do. Okay. What else do we have? How frequent is pod refresh? Very frequent so far, but we're hoping to keep it much more stable after DevCon. I don't have a strong answer to that. What else? How do you convert JSON to a Merkle tree? Please stick around for the deep dive session that's coming up. I'll tell you all about that. What else? Yeah. So, the—in the example of prover and verifier, the user's device can generate the proof and that's why everything has to work in a browser on a phone. Client side proving is definitely the default in ZooPass. Not every app has to do it. These are libraries. You can call them wherever you want. There's much more difference between verifiers, whether they're doing server side verification or client side verification. That depends what your use case is and what you're protecting against Are the issued credentials signed and the proof that the crunch loops we scrolled away We do not use BSS signatures to verify partial properties, that's what we use the Merkel tree for again more details on that coming up Is it possible to make information in ZooPass portable? I think that pods do make that possible, yes, as long as it's a pod and there are APIs for getting data out of ZooPass if you want to. That's called the ZAPI, at which point you can take this to whatever platform you want. We have implementations of pods in Python, C, and Rust for various projects, so it's not too hard to do. How do apps know whether a proof from a verifier is legit? Well, the framework tells you that it is a valid proof. And it will confirm for you that this configuration and these revealed claims and this proof match up and are valid. So the prover couldn't have cheated about that. What they could cheat about is app level semantics. So if you ask for a proof of a driver's license and I sent you a proof of a frog instead, that's something that the framework can't tell you because it just says that's a valid proof. So you do have to check, is that the configure I asked for? Is the signer of this driver's license the government, etc.? But yeah, that's the kind of level of verification we got. Okay. I think that's it. Can we go back to the slides briefly? Okay. Those of you who are collecting frogs, I've got something for you if we can switch back to my slides. Oh, yeah. We'll leave that up for a minute or two. I think we've got like three minutes before the next session starts anyway. So feel free to frog away. Okay. And as I said, we're going to go straight into a deep dive session, which is going to be 90 minutes. We probably won't use the whole thing, but that's what we're scheduled for. So stick around if you want more details to answer any of those questions.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:00:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1D13mwNG569Eo7vRzSRs1BRHF7sCXAys5mnZEJpklwtg",
      "resources_slides": "https://drive.google.com/file/d/19LsUDgMe98h3PEuB2Yx7Qe2oaaE7ikY0/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "why-defi-matters-on-ethereum",
      "sourceId": "E7GFJC",
      "title": "Why DeFi matters on Ethereum",
      "description": "Why DeFi matters on Ethereum, and why Ethereum is the best place for DeFi.",
      "track": "Real World Ethereum",
      "type": "Panel",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 3320,
      "language": "en",
      "sources_swarmHash": "df5c65e42388ca8d8c9582a368d558bba237216fb411806b3238eb8a8168a461",
      "sources_youtubeId": "C4MIV9oQUYk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735dfaf9dbb7a90e1bf8303",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:00:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/14OuUArkp-1DdYuHEylurELQO49RZZh5IHebMv6N4LAU",
      "resources_slides": "",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "state-contention-rules-everything-around-me",
      "sourceId": "XGHU89",
      "title": "State Contention Rules Everything Around Me",
      "description": "State contention causes MEV, prevents parallelization, breaks gas simulation, causes transactions to revert, etc. etc. We'll discuss state contention in practical and theoretical systems (e.g. OS threads and type systems) and how/why synchronization primitives developed. We'll cover why state is contentious, what state is contentious, what can be accomplished by making state non-contentitious, and strategies for refactoring existing systems to reduce contention.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Architecture,Cross-L2,concurrency,Architecture,Cross-L2,Layer 1",
      "keywords": "Synchronization,Concurrency",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "58a4d430fa264cb64fc8af1f5aaec20d3e7b3280ed0426b56bb3e15c1cc2e82a",
      "sources_youtubeId": "QrbJbjWKNX4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:10:00.000Z",
      "slot_end": "2024-11-14T10:40:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1cS2GTJFjotanBsdxY8DrP-qcMwV7ijAs3-hVV-oIS40",
      "resources_slides": "https://drive.google.com/file/d/1vQwgbFkFgFDUOlO2X2e-tsY9jZIfdcrZ/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "financialization-in-games",
      "sourceId": "EF3P9X",
      "title": "Financialization in Games",
      "description": "This talk will cover different financialization strategies we explored while building Project Mirage, and our lessons and learnings throughout the journey.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "n/a",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "5c8a8fb6d0d12eb090c158168905777b8ac1977d8de29e41317e18172fc20ebe",
      "sources_youtubeId": "49oV_zgRoN0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:15:00.000Z",
      "slot_end": "2024-11-14T10:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/15r4rPTnKvKjpyxmg1BaFjdTPs2MB_Up2KIg320EKBjc",
      "resources_slides": "https://drive.google.com/file/d/1645nYcNYuXS59sQqQ8JfuhvLuTTWhqs0/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "depin-pushing-decentralization-beyond-blockchain",
      "sourceId": "Q8QPSF",
      "title": "DePIN: Pushing Decentralization Beyond Blockchain",
      "description": "Explore the revolutionary world of Decentralized Physical Infrastructure Networks (DePIN), where blockchain meets real-world applications. This talk delves into DePIN's core concepts, from token economics to governance, highlighting its potential to transform industries. We'll examine successful projects, technological underpinnings, and future trends. Using Huddle01's innovative approach to decentralizing real-time communication as a case study, we'll demonstrate DePIN's practical impact. Join",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Decentralization,DePIN,Tokenomics,communication,real,time,rtc,Architecture,Decentralization,DePIN,Tokenomics",
      "keywords": "Distributed Systems,Physical Infrastructure,Real Time Communication (RTC)",
      "duration": 512,
      "language": "en",
      "sources_swarmHash": "129ea5033085c9b928a70f26b2114423ca392d90b566ababe2ee77d89b6da631",
      "sources_youtubeId": "UDNOGtZUxc4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d59f9dbb7a90e186ef54",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735d59f9dbb7a90e186ef54.vtt",
      "transcript_text": " Hi everyone. Am I audible at the back? Yeah? Okay. Awesome. So hi everybody. I'm Akshit. I lead the engineering team at Huddle01. And we are a deep end dedicated to making real time communication affordable, cheaper and performant. So we are a deep end project. So if you're familiar with deep end, deep end stands for decentralized physical infrastructure. And we saw a gap in the market that real time communication is very hard and it's very pricey as well. So people like Zoom, companies like Zoom, Google Meet, all these rely on heavy infrastructure and have to host it on cloud providers like AWS. So this is what my talk is about, how Deepin can make it cheaper. So this is what my talk is about, how Deepin can make it cheaper. So as I said, Deepin stands for Decentralized Physical Infrastructure. And in blockchain, we've seen a couple of trends from the start. Things like NFTs have reshaped ownership, digital ownership. In games, you have these skins and everything, and these can be sold via NFTs to make sure that only one person owns it and stuff. And smart contracts have automated money transfer as well. So you can program how smart contracts transfer money based on a particular condition. But we're still deploying these things on centralized infrastructure like AWS. So if you were to build a complex app which requires heavy infra, you still host it on AWS. Things like storage and bandwidth still require centralized infrastructure. So that's where Deepin comes in. We saw this with Filecoin. So Filecoin is a really good example of a Deepin project, which is decentralizing storage. So similar with Filecoin. So Filecoin is a really good example of a deep end project, which is decentralizing storage. So similar to Filecoin, we have it for real-time communication. So charging for stocks and paying for flows. So this is a very interesting trend we saw. And AWS was charging crazy amounts of money for egress costs. So as you can see in the slides, AWS charges like 700x amount of pricing for egress costs. So say like you fill a bucket full of water, water being the amount of storage, then the amount of water that's being transferred out of AWS, which is data being transferred out, they charge really costly amounts for that and not for the stocks that they save. So that's how AWS is charging, and it's like 700 markup. So why I'm bullish on bandwidth projects like DeepNR? So internally at Huddle01, we did an interesting study where we calculated the bandwidth costs for hosting on AWS, like hosting a real-time communication app on AWS. And as you can see, for a single participant, the bit rate consumption was around 700 kbps. And for eight speakers in a room and 20 listeners, the data consumed was around 112 Mbps. And if we were to take like 100 such rooms hosted on a particular app, you'll see that it comes around 11,200 Mbps. So data consumed in an hour would be around 40,000 gigabits. So that's a huge amount. And if you were to calculate the data transfer cost on AWS, that comes around to $400. So if we were to use a Deepin project and these services were hosted on a physical infra where people are incentivized to run these nodes, then the price goes much lower. So these are some popular deep end projects. You can see Huddle01, which is the project we are working on. And some popular projects like Grass are also working in this bandwidth department where they're saving cost. All you need to do is just install an extension and you'll be good to go. So if you are looking to build a deep end, the best way would be to look for a market which is super overpriced and you can easily save costs by allocating that job to a particular person who can run it on their own machines or host it somewhere, maybe with less P. So as you can see, the less P the better. This is one thing to follow if you are looking to launch a Deepin project very easily. And it's very easy to deploy a project with less P as there's no physical part involved. There are really good projects with more P as well, as you can see on the left. But with less P, you'll be able to run it and make it work. So thank you. That was a small lightning talk by me. And I am happy to entertain some questions. Thank you. Okay, okay. Do we have any questions from the audience? Anyone? Let's wait for another five seconds. Okay. Oh, do you want to throw it? Yes. Perfect. So you talked about essentially that the less the P the better. You need to put it, it's a mic. Hello. Oh, wow. So you talked about a point where essentially the less P the better. I just wanted to understand if you have less P, there's also a chance of having more chance of gaming the system. That means verification can become tricky if your system is more digital rather than having a physical network. For example, Helium, it's very difficult to game a Helium system because you have a Bobcat miner, but in case of something like Hudl, it's more of a digital network, that means you require verification systems, more like QoS systems. So have you thought about that at Hudl01 where you can have a QoS system implemented which can act as similar to a physical infrastructure network as well so that nobody games it? So the best way to go about it is to first",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:20:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1frO1LBrX3h2e6LoJqHpvDElvChyEEDg6CFrkzwQt5VY",
      "resources_slides": "https://drive.google.com/file/d/1yNRDCzxQdJOZ4Dx6lpGFjDfJzK6JNy1y/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "empowering-a-safer-internet-community-driven-scam-reporting-and-prevention-in-thailand",
      "sourceId": "FGUAQX",
      "title": "Empowering a Safer Internet: community-driven scam reporting and prevention in Thailand\"",
      "description": "In today’s digital age, user-driven solutions are vital for online safety. This talk explores Whoscall—a free mobile app trusted by over 17 million users globally, offering call and SMS identification, phishing site scanning, and personal data breach detection—and Thailand’s Scam Alert feature. Both initiatives empower users and promote public-private collaboration in scam prevention.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Public good,SEA,Security",
      "keywords": "Anti-Scam",
      "duration": 533,
      "language": "en",
      "sources_swarmHash": "706ba7fe2748e77f900452e4d1f1ba71883ef043c73c9fc738f17c3dac914a8b",
      "sources_youtubeId": "guXtHSEhZgQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735cb649dbb7a90e1b64fac",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735cb649dbb7a90e1b64fac.vtt",
      "transcript_text": " Hello everyone, greetings from Taiwan. I am Michelle Shen, Product Director at HUSCO. Today I am going to share our HUSCO empowers internet safety through a community-driven approach to scan reporting and prevention in Thailand. In the digital world, trust is fundamental to the online interaction. At HOSCO, we are committed to building that trust by combining cutting-edge technology like AI with global partnership. This approach aims to create a safe collaborative space where user activity and participants in combating scans. Hoosco was founded in 2012. We are founded by the trusted company, Gogolook. Gogolook now has a dual headquarter in Taiwan and Thailand. So for over a decade, we were connected with governments worldwide, including the police agencies across Asia, and we enhanced anti-scan solution. We now have more than 250 employees around the world. So for those who never heard of Who's Code before or you never used Who's Code, so here give you a very quick introduction to the videoอีกที ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตอนนี้ ตทำร้ายตัวเอง แล้วทำร้ายลูก ๆ 1 tbs of sugar 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey 1 tbs of honey Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Ketua kota Okay, so we just published a book. This book includes more than 1,000 of the scammers' calls, and weight is more than seven kilograms. So this book is just a function that we want to give a simple introduction of the host code. That is, if you cannot remember all the scammers' calls, the easy way to protect yourself is to install HoosCode. So why we can do it? I think now because we have the largest number of databases in East and South Asia. Every year we block more than 2.6 billion of spam calls and messages. So we help users identify the stress and stay protected every day. So now we reach more than 100 downloads worldwide. So Hooska offers not only the color identification as an SMS filter, but we also evaluate our new features. So right now we also can help users do the personal data linking monitoring, and also we can detect the suspicious website. And also now we are trying to provide some defects scan detection. The reason why we are consistently involving and to meet the new threat is because the scan become a new normal. According to our annual scan report in AIPAC, only 1.6% of people, they never account for scan before in a year. So means that people will at least account once a month. Even some of the people, more than 50% of people, they will account for every single day. And I think the most important is that the scammers, they are so smart. They were involved with technology. So with these new technologies, such as social engineering, phishing, and defect, they leverage the data science, big data, and AI to power themselves. So that also gives us a learned. Even at Asper's, acknowledging that is a battle that we can not win only by ourselves. We need to fight with the network. So that's why we want to build an involving network to combat a scan. A part of our self and AI technology to present scans and public database, we also couple that tip with government, including the lawyer type police. Most importantly, we believe that we should work closely with our users. Sooner we post, we can discover the new trending scan is happening. So who are these hidden heroes? What outcome did they contribute? In the past years, these report users, they submit over 4 million of scan code reports through the Who's Code. So who are they? After speaking with them, some of them just the uncle or auntie that you will meet every single day. So this community driven effort reflects a strong sense of justice among our users. And the only purpose they want to report is that they want to pretend they are loved. Like they believe the report they built can help more users pretend as guests. So we also found that this value to share will transcend age, even gender, and demographics. So while community-driven reporting is inactive, it also raises some challenges, such as how we're going to ensure the report is trustworthy. And we also have some challenge to meeting the regulator compliance. So I think we also see some opportunity such as incorporating blockchain to facilitate report origins and data integrity will help us to overcome this challenge. So in closing, Unity is essential in combating scan, at whose core we are built for trust and join forces with users, partners, and governments to create a safe internet. Together, I think we can make a meaningful impact. Thanks for your listening. This is Michelle from Google Look. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:20:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1PhodWX8WCiq6P9Vsm9h6TVZlVmdMbAjQkvyVws1MlFw",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "interoperability-between-l2s-latest-developments-framework-and-challenges",
      "sourceId": "3ZH9ST",
      "title": "Interoperability between L2s: Latest developments, Framework and Challenges",
      "description": "The number of L2s is growing rapidly and it’s crucial to create strong interoperability solutions to reduce liquidity fragmentation and friction for users. We provide a framework for analyzing interoperability solutions that defines 6 levels of interoperability. For each level, we deep dive the consequences on UX, DevEx, scalability, fee structures, and MEV potential. We also provide an ecosystem map categorizing the level of interoperability offered by existing projects.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Fragmentation,Cross-L2,Developer Infrastructure,interoperability,Cross-L2,Developer Infrastructure,Fragmentation",
      "keywords": "Composability,Interoperability",
      "duration": 434,
      "language": "en",
      "sources_swarmHash": "f312319bdc29280d6466e892fe79207f272961439803ba5130a9043225029290",
      "sources_youtubeId": "-G6oOQTb5AI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e5539dbb7a90e1a8b6f4",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735da889dbb7a90e132b668.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\" Peter van de Ven Hello. So, I'm Dave, a co-founder of Alliance to Feed the Earth in Disasters, or AllFed, also a professor at University of Canterbury in New Zealand. So AllFed's mission is to build resilience to global catastrophes, and its vision is feeding everyone no matter what. AllFed is all around the world, has board members including Robin Hanson, Andrew Sandberg, Jan Tallinn and Martin Hellman who won the Turing Award for cryptography. One example of the catastrophes we work on was there was an eruption in 1815 that caused the year without a summer in 1816. And there was famine in many parts of the world, including Europe. And there's the catastrophes we focus on disrupt food supply greater than 5% of total food production in the world. And there are many such catastrophes. One of them is abrupt climate change. Another one is extreme weather on multiple continents at the same time. And a UK government study estimated that that alone had an 80% chance of happening this century. But there are other catastrophes. There could be a super pest that attacks crops that's resistant to pesticides or a super weed that outcompetes crops. You could also have disruption of pollinators, disruption of beneficial bacteria, asteroid impact. And most extreme is nuclear winter. So if we have a large-scale nuclear war, we'd have burning of cities, smoke would go up into the stratosphere and stay there for up to a decade, and the global climate would be severely disrupted, around 9 degrees Celsius drop globally and the agricultural output would fall about 90 percent and to put this into the perspective you can see the yield the number of tons per hectare per year of global food production and if we had a severe nuclear winter, basically our yields would go back to before the Industrial Revolution. So what can we do about this? Well, we could relocate cool, loving crops closer to the equator, things like wheat and potatoes. Also, mushrooms don't require sunlight to grow. But there are a number of other options. We could scale up seaweed. Seaweed can grow around 10% per day, even in nuclear winter conditions. We could turn fiber or wood into sugar, so that's cellulistic sugar. We could take crop residues and make leaf protein concentrate from them. We could build greenhouses and we could also do fermentation. So there are several companies that are now turning natural gas into protein and also some companies turning hydrogen into protein. But these companies are not thinking about how to do it fast in a catastrophe, so that's what we focus on. We also look at catastrophe scenarios that could disrupt infrastructure, such as electricity. And these include extreme solar storm, a detonation of a nuclear weapon at high altitude, causing an electromagnetic pulse that could destroy electronics, or a cyber attack, which could be AI-enabled, or an extreme pandemic that could cause people to be unable or unwilling to report to critical industries. And then this would cascade across industries. cascade across industries. So as we've talked about today, future pandemics could be far more severe than COVID. They could have the transmissibility of measles and the fatality of rabies and have no vaccine. And we're not prepared for that. And we could have a collapse of of critical industries. We've done some research on potential backup plans for meeting basic needs of food, energy, and water in these scenarios. But there's another line of research I want to talk about. And that is, could we scale up some of the technologies that we've heard about today, like UV or in-room filtration like we have in this room. And it would be better if we could scale these ahead of the catastrophe, but we're not ready yet. And that's probably going to take more like tens of billions of dollars. So what we're interested in doing is figuring out whether we could scale them up very quickly with our current capability in a catastrophe, in an extreme pandemic. And another option is massively increasing ventilation from outside and also sequestering workers. And in terms of cost effectiveness, we think that for the resilient foods that I talked about earlier an investment of something like $100 million could get us research and piloting of the technologies and planning and we have some peer-reviewed papers making the case that this is a very cost-effective way of saving expected lives and also improving the long-run future. But in this case in particular, we don't need to build big industrial pilots. This could be done potentially for millions of dollars, so it would be extremely cost-effective. So some of the pilots, the paper factories actually have most of the equipment already that we need to turn wood into sugar, but we'd like to actually try it out and convert a paper factory into a sugar factory. Another pilot we'd like to do is a resilient satellite. So if we had an extreme solar storm or EMPs, the satellites we have now would be destroyed quickly. But if we had a satellite that was resilient to these catastrophes, we could get emergency communication. And with just one satellite in a polar orbit, it would be able to get information to everyone on Earth with just regular cell phones once per day. And here, again, the investment is just in the millions of dollars. So now I'll have Yash talk about some opportunities with crypto. Thank you, David. You might have all seen already some of the solution sets that we could have for such a scenario. But there's still more work to be done. So for the next couple of minutes, I want to explore with all of you about how we as a community can come together and build resilience to global food catastrophes. There is a lot more work to be done. Some of the things that we can do using the decentralization ethos and the defensive ethos from the crypto ecosystems could be communication of crucial information. We would want to have the capability and the capacity to distribute things like disaster resilient guides or response plans when these catastrophes occur. So one thing that we can do is we already have decentralized hosting and storage through IPFS, but we would still need mechanisms that would help us when we do not have access to internet. So we need to have such technologies to be able to communicate very crucial information that could save a lot of lives during such a scenario. We could also build software solutions to enable coordination with multiple stakeholders who would do decision-making. This need not be during a catastrophe, but this could also be done without a catastrophe, too. We can utilize all of the things that the Web3 ecosystem is working on. For example, more pluralistic decision-making and using prediction markets, as Robin Hanson was talking about today for more informed policy decisions but this time only to sort of predict the different catastrophes. We can also build, govern and launch our own community-owned resilient satellites. As David previously explained, we need satellites which can be resilient to solar storms or hams. So I want to leave this with all of you about a potential idea of a food resilience DAO so that we as a community, we can together build and govern resilience at a planetary scale. Does a DAO structure suitable for this? I'm not sure yet, but would love to allow all of your inputs. But there is so much we can do together and take control of our own resilience and defense as a community. But there are some other simpler things that we can do. For example, directly supporting the work that AllFed does through Ethereum. You can just send some ETH to AllFed.Eth. Some of the things that Alfred's working on currently is sort of mapping out the entire technology roadmaps, building a technology tree for food resilience. One of them could be research on precision fermentation. You can make fats from microorganisms, but we would need to still map out what sort of bottlenecks are there in the industries. And as David pointed out, when a catastrophe occurs, how can we actually scale up very rapidly? So we need to have that research and plans available beforehand. We can also do some really cool pilots. We can simulate a nuclear winter condition in the Australian interior. We can do a test launch of our resilient satellite as well. So if you're interested to contribute and help, my signal is up there at h.88. You can hit me up. So I want to leave you with this about potential community-owned resilience. And if you have any questions, we can now take it. I think you can go to the QR code on the side and have some questions there.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:20:00.000Z",
      "slot_end": "2024-11-14T10:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1DgmkfIFJfD0vf-bVsGTFZt1Nv09KHD5RE7ct8x0puek",
      "resources_slides": "https://drive.google.com/file/d/1sgrmEztgz8rBpKfjG-NUOOJYQPvVrRXh/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "advancing-ethereum-scalability-architectural-innovation-trends-in-the-layer-2",
      "sourceId": "E3YRQV",
      "title": "Advancing Ethereum Scalability: Architectural Innovation Trends in the Layer 2",
      "description": "In this session, Joshua will explore recent research findings and upcoming Ethereum Improvement Proposals (EIPs). He will explain the technical details of EIP-4844 (Shard Blob Transactions), single-slot finality, and other significant protocol upgrades. The talk will provide insights into how these changes impact Layer 2 scalability. Lastly, he will discuss latest architectural innovations in Layer 2 solutions both optimistic and ZK-rollups, offering a neutral perspective for assessment.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Architecture,EIP4844,Blobs,Danksharding,Layer 2s,Data Availability,Optimistic rollups,Zk Rollups,Blobspace,Commitment Scheme,node,analytics,performance,Architecture,Blobs,Blobspace,Commitment Scheme,Core Protocol,Danksharding,Data Availability,EIP4844,Layer 2s,Optimistic rollups,Zk Rollups",
      "keywords": "node,performance,analytics",
      "duration": 360,
      "language": "en",
      "sources_swarmHash": "d3aee961f55b3f8d283f2e8a6e9853fa958efba19de0e45aec10006d06eba55e",
      "sources_youtubeId": "dvBaZBaMKVE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d6519dbb7a90e19f7129",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T10:40:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1HPCOiNJTibtU-aBvYZEHb5wvCv6Fuf3ujBxdsuBTFYo",
      "resources_slides": "https://drive.google.com/file/d/13sGxI5-BKIfcVIgylQSKqKUVvQ9wm7bI/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "dacc-social-tech-from-a-swiss-perspective",
      "sourceId": "QXJYLY",
      "title": "d/acc social tech from a swiss perspective",
      "description": "Zuitzerland‘s thesis on how a permanent network state sandbox can serve as a „free social space“ and aid in scaling d/acc governance models beyond Swiss borders. Zuitzerland aims to build on the social technology developed over 700 years of decentralized governance in Switzerland, a thriving nation state, that’s no. 1 in innovation per capita, and one of the world’s only direct democracies.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Economics,Governance",
      "keywords": "D/acc,Network states",
      "duration": 604,
      "language": "en",
      "sources_swarmHash": "202e671c802f4e47806892af0a402f9a72b5adccf3307150985b397f496ef6f5",
      "sources_youtubeId": "BRjAO-5s2MA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735ccb29dbb7a90e1c4d953",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735ccb29dbb7a90e1c4d953.vtt",
      "transcript_text": " Thank you. Hi everybody. I'm really excited to be here talking about DIAC. So we want to tell you about DIAC the Swiss way. Una and I are working on Zwitserland, which is a network state and society sandbox that has come out of all of this Zuzulu madness. Yeah, so how did all of this begin? We were at Zuzulu in Montenegro and had just an incredible time, literally life-changing experience. If you haven't been to a Zuzulu pop-up city yet, it's definitely a very different experience to a conference experience and really meaningful. And it's led to us actually really coming up with a concept of how we want to launch a permanent network state sandbox in Switzerland. And we're working on a joint venture now to create a livable living lab in Switzerland in a beautiful Alpine region. So I'm going to tell you a little bit about that. Why a permanent sandbox? When Vitalik made the call to decentralize the Zuzulu movement, I was very honestly slightly upset. decentralized the Zulu movement I was very honestly slightly upset the experience in Montenegro had been so amazing and I Couldn't really imagine a space like this being created again in the same way by a random bunch of people in a decentralized manner And I wrote a big post on the agora forum about how free social spaces were so important and that this, what was created with Zuzulu and Montenegro was kind of like a social technology that had been created and a free social space that had been created where radical social innovation and technological innovation, but personally most interestingly social innovation can happen. And a place where we can work with existing models and new models and experiment and iterate in really interesting ways. And I was worried that we wouldn't keep that free social space alive and that sandbox alive if we decentralized. But interestingly, we took the challenge on and we said like, well, Switzerland is the home of the Ethereum Foundation and it's home to many, many early crypto companies. And if not in Switzerland, then where should there be a pop-up city? So we went a bit deeper and we really started looking at Switzerland and what can we actually do in Switzerland as a pop-up city that's very meaningful and you may know this quote from Vitalik Stiak or defensive accelerationism essay where he says actually Switzerland is often considered to be the closest thing the real world has to a classical liberal governance utopia and it's very much true you You know, values back and forth, and everyone's going to have their favorite thing. Switzerland is an incredible country. I've lived there now, both of us have lived there around 10 years, and it's incredibly successful. So number one for innovation per capita in the world for the last 13 years running. It's a direct democracy, and it's still working. And I think that that's what makes it fascinating. It's one of the only direct democracies and I think a lot of people are scared that democracies don't work, especially direct ones and Switzerland has made it work in a way that we think is very, very interesting and they've actually been working on decentralization and democracy for the past 700 years and they've been very much a part of the humanism movement and they have this incredible social technology that's developed over literally 700 years of iterating and working together and we realized as we started thinking about this more that this is a social technology that we should be building on top of rather than starting from scratch. And we realized also as we looked into it more that the secret sauce that a lot of scholars actually write about why Switzerland is successful is actually defensive accelerationism in other words. So they define their mentality as having a very good balance between caution and progress. They manage to innovate, but they do it in a way where they move reasonably fast, but they don't tend to break things. And so Switzerland has managed to become not only fast and a great innovator, but also a very stable and strong economy. And actually even has, for example, a kind of a UBI. If you become unemployed for whatever reason in Switzerland, you have up to two years your income covered by an insurance. So instead of leaving people on their own, when something happens, they reinvest in the people and in their human capital. And it's quite an interesting model because it's based on sort of insurance rather than a purely social support. So it's an incredible place and there's a lot that we can learn and we believe that it's a great place for network states to iterate and to build a sandbox where we can build our own kind of network state and society but also iterate and to build a sandbox where we can build our own kind of network state and society but also iterate and build other network states and societies and have them build with us. And core piece here is the balance of the state between the individual interests and collective interests and this is really what we think that Switzerland does well, is they balance the cypherpunk values with the solarpunk values and when in doubt they protect individual freedoms and things like privacy. Interesting example of kind of some of the weird stuff in Switzerland. So when you first move to Switzerland, they actually they give you your residence and your address and then they say, and by the way, here are some iodine weird stuff in Switzerland. So when you first move to Switzerland, they actually, they give you your residence and your address, and then they say, and by the way, here are some iodine tablets just in case there's nuclear war. And it's funny because it's like Switzerland is probably the last place in the world that actually expects to have nuclear war. But there you go, you get your iodine tablets. They also have bunkers, for example, that could house the entire population in the case of war. And so even though they do everything in their power to actually protect against that situation, they have ways that they can be protected from the worst possible outcome. So, yeah, I would hand over here to Una. Based on all of this, we've decided to do an eight-week event in May and June 2025, and have a bunch of you amazing people over for a really long sleepover, and to nerd on AI, privacy, ZK, rationality, DAOs, X-Risk, governance, and in particular, really learn from Swiss democracy. But there's actually more to this, and in particular, really learn from Swiss democracy. But there's actually more to this, and Una is going to tell you about that. So, instead of, not only conceptually, Ayla mentioned a lot about how we're conceptualizing Switzerland as a perfect DX sandbox place, and also for fast iteration bridging web 2 to web 3 through governance and what we can learn from Switzerland But at the same time we want to build it into a permanent living lab And Ayla has talked about how important it is to create a this social space as part of social technology For social for creating this safe social space as part of social technology. For creating this safe social space, we think that we want to build this permanent place at this potential park that we can have residents to come in and different people come in and have fast iterations and different products that we can test in a scale that matters, which means that we can actually accommodate 6,000 to 10,000 people on site and have testing all different kinds of social technologies on site. And one thing that we firmly believe is that why we need a sandbox in Switzerland is also change within the regime tends to be incremental and path dependent. We need a little safe space that we want to create and carve out from existing good society to have a path-breaking innovation. And this is where we see half the potential with a permanent location. And as for Switzerland, we believe we have the best ingredients to make it happen. We have really great people, all the Zuza people, I think I see half of you are probably Zuza-liens, and we're great. And then we have the culture and philosophy as Ayla explained, the D-AC and solar lunar punk integration. And also we want to have space and infrastructure to make rapid iteration happen with all of this we think that we can make meaningful radical social technical social technology happen and our rough plan from 2025 and to 2030 is that with 2025 the first two three years we want to have pop-up villages and pioneer residency and have a basic renovation at the physical location. And from 2026 to 2029, we want to have experimental villages and scale up a little bit and with living labs and sandbox and have deep tech on the park. And with 2030 onwards, we want to have a real high tech civilisation accelerator village on site. I was meaningful of being careful with the time. I think that's our slot. Thank you for your attention. I hope you are as excited as us about this. And welcome to connect with us and the community. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T10:40:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1f6N0D4m-xFEHIkJO7OzkhfJlUtBWAno5sr5jRV3Q5kk",
      "resources_slides": "https://drive.google.com/file/d/1pkPZzUe3L6PI12hbSwDs78WfmRbGxhq4/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "decentralizing-consensys-to-catalyze-a-network-state-in-the-emerging-decentralized-web3-ai-global-economy",
      "sourceId": "STX9DW",
      "title": "Decentralizing Consensys to Catalyze a Network State in the Emerging Decentralized Web3 + AI Global Economy",
      "description": "Supported by MetaMask & Linea infrastructure, this open network state will be one of many interoperating token economies. This talk will briefly trace the arc from web1 to web3.  Two technologies are maturing that will together serve as the foundation of web3 – a user-owned and controlled information technology infrastructure for the planet. They are AI and decentralized protocols.  This complementary tandem must evolve together in order for humanity to evolve beyond the current adolescent state",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Ethereum for Good,Network State",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "8b4b3cf0da0672c8c54310cdfa69005eb4d812e2213e0c328ef76b2c5f1e32bf",
      "sources_youtubeId": "47dUdOvhKtM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/11eX1oRXoI4urF046XwUWj6LWwTjgZKotWz4aBKhGwB4",
      "resources_slides": "https://drive.google.com/file/d/1nGgfMy90j1SVWRdXrnRQLEcAWLuedDL1/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "future-of-onchain-credit-scoring-for-farmers",
      "sourceId": "BBEDYL",
      "title": "Future of Onchain Credit Scoring for Farmers",
      "description": "This talk will illustrate how a farmer's farm records alongside verified government issued ID and mobile money statements (M-Pesa) form the basis for anonymized real time credit scoring onchain, as a foundational layer to build unique farmer DIDs. This talk features Antugrow, a startup in Kenya re-imagining credit scoring and record keeping for farmers.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": true,
      "tags": "Identity,agriculture,Identity",
      "keywords": "Agriculture",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T10:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/143aux2LnIoaZxJqy3DpwpFTohgfllg9LWtuYzwx2v78",
      "resources_slides": "https://drive.google.com/file/d/1GxWSJZ93c26iDreHhRckS-2svFb6dWJs/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "inch",
      "sourceId": "AWQHPU",
      "title": "INCH",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1XIExS1_AoQ1qy7x-JA9-WtnrbRg6bJqnZ5hnpK-w-Sw",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "lazarus-how-to-stay-safe-from-the-biggest-threat-actor-in-crypto",
      "sourceId": "HCXCXB",
      "title": "Lazarus! How to stay safe from the biggest threat actor in crypto",
      "description": "Lazarus has stolen by far the most funds in the blockchain space. They use the same or very similar attack vectors every time yet we see the biggest crypto companies falling victim to them one after another.\r\n\r\nIn this talk, i'll go over some of the attack vectors used by Lazarus and how people can keep themselves safe from Lazarus.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Best Practices,Hacks,lazarus,Best Practices,Hacks,Security",
      "keywords": "Lazarus",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "c4250f32ff689a42bcb4f8a9158944ab1c783e4f1d985a1b531bbe7096128ce5",
      "sources_youtubeId": "W5wcGsh3UVE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/15zVK369DMEaAyZgEYl7ytDPnVtTcqgBbNjAZaVtPUfk",
      "resources_slides": "https://drive.google.com/file/d/1k3fV-hc66ZofGtyB0e97lSKdAfe_26fE/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "the-next-700-evm-languages",
      "sourceId": "QE7RWH",
      "title": "The Next 700 EVM Languages",
      "description": "What is the role of programming languages in helping smart contracts become reliable and scalable technology? Are our current languages for the EVM up to the task? Has Ethereum lost the lead in this regard?\r\nThis talk explores these questions and proposes a roadmap for the development of the next generation of smart contract languages for the EVM.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Languages,Formal Verification,smart,contracts",
      "keywords": "programming languages,formal verification,smart contracts",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:30:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1xFEtAafqxxm1b1UAUHGb8bnoWg9x6qZQdGRk_3lPM8Y",
      "resources_slides": "https://drive.google.com/file/d/1el0gFJAWBYjssHUS2rHQ_GupEbyJIkdK/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "l2-evm-common-core-a-path-beyond-evm-equivalence",
      "sourceId": "9RJ3MA",
      "title": "L2 EVM Common Core: A Path Beyond EVM Equivalence",
      "description": "Network effects of the EVM have locked many of the L2s into equivalence with the L1 EVM. L1 is optimized for moderate throughput and maximal decentralization, but L2s need higher throughput and can rely on heavier full nodes.\r\n\r\nThe talk will present a vision for an L2 EVM Common Core as a new base VM for participating L2s. It aims to offer a way to ship more ambitious EVM changes without increasing L2 fragmentation. It is a result of our work as leads of the RollCall L2 coordination process.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "EVM-equivalent,Rollups",
      "keywords": "",
      "duration": 1552,
      "language": "en",
      "sources_swarmHash": "be5774410c333f9038b5b9f1d8f38553237468d423f216e7da33a56b045bf8a7",
      "sources_youtubeId": "eLO6N-99CZE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d1881b0f83434d4b82fe",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d1881b0f83434d4b82fe.vtt",
      "transcript_text": " . Hey, everyone. Yeah, I'm here to talk about the L2 EVM Common Core. It's basically this initiative, that's a recent initiative that came out of the roll call process. If you've never heard of the roll call process, that's something we started earlier in the year. It's basically a L2 coordination effort. We have a monthly call where many of the major L2s on Ethereum send representatives, and it's basically for now mostly used as both coordination across them, also trying to be a bit of a connective tissue to the L1. I'm personally spending most of my time in L1 core development. And then we also have what we call RIPs, roll-up improvement proposals. And the idea there is basically that they are similar to EIPs. EIPs, you might be familiar that they are proposals to add new features to the EVM on L1. And so the IPs are the equivalent for the L2 side of things. And yeah, the process was originally kind of started, the roll call by Karl, who can't be here today, unfortunately, Joaf and me. And then more recently, Nico also started joining us more actively. And yeah, I want to basically talk a little bit about EVM equivalence in general and then kind of the specific kind of direction that we see the EVM equivalence go in the future. And I wanted to start a little bit with history. It's going to be a bit more of like an easy kind of beginner-friendly talk, at least in the beginning. I like pictures, so I have some pictures. In principle, the way I always think about scalability for blockchains is basically as this fundamental trade-off between the cost to run a full node and the throughput you get, right? And so Bitcoin, Ethereum, two early examples. Ethereum has more throughput than Bitcoin, but it's also a bit harder to run an Ethereum full node than it is to run a Bitcoin full node, right? And I call these kind of trustless chains because the important thing is everyone on earth who wants to can verify the chain. At least that is the goal. And we are sticking pretty close to that. And then there is the approach of just going further up the diagonal and just picking Solana as an example here of chains that basically just say we want higher throughput, and how do we get there? Well, we basically go up and we accept that now it is harder to run a full node, but we get more throughput. And of course, the trade-off is that now you actually need to have a quite beefy machine to be able to still trustlessly participate, otherwise you have to trust the majority of nodes. Now, ideally, we want to go there, right? The ideal chains, they have very low cost to run full node. Everyone can verify, but they have high throughput. So how do we do this? We basically, this is the key roll-up trick, right? We realize that actually what matters is the cost to verify the chain. And if there's a way to verify the chain cheaper than running a full node, that's sufficient. And that's basically how we get to rollups. Because rollups are basically a way to compress verification of a chain. And so what you can do now is once you get to that place, you can do either, you know, basically the thing you could immediately think of is just, hey, take the high performance chain and just run it as a roll-up. There are some projects now on Ethereum that do this, that are Solana VM-based. It's a bit of a challenge, though, to adapt them for L2s. Then, of course, what you can do is, as well, you can just take the EVM and you can really scale it to a high throughput. People sometimes have this mistaken conception that the EVM is fundamentally not able to go to similar throughput levels as the SVM that's really not the case the reason why Ethereum runs on lower throughput is specifically because we want the straight off we want to be verifiable by everyone on earth in a trustless way right it's a choice it's not a technical constraint so once you go into the roll-up you can actually push the EVM to its limits but the variant of this that we've seen the most is actually what I would call, I don't know, the multi-rollup cluster, which is just many smaller, lower throughput individual rollups that basically together form a very high throughput system. So that's kind of like the background of how the ecosystem evolved as Ethereum went through its roadmap. And if you zoom into the chain itself and then eventually the EVM, I have some more pictures. I call basically this, this is just like the execution chain. It's just a symbol that's representing chains where actual activity happens, right? Like chains that actually offer execution, where you can build apps on top. And the most basic version of a chain is just like a self-contained chain. It's like an L1 with just normal apps on top. But now you want to turn it into a rollup. How do you turn it into a rollup? You need settlement and data availability. And those two things can be provided in a combined way by a settlement chain. So then once you have a settlement chain you can take your chain there and just turn it into a roll up and the nice thing there is that now you inherit the security of the settlement chain but you can still now yourself run with much higher throughput, right? But because you have that security that you inherit from that low throughput but high security chain. That's kind of like how with much higher throughput, right? But because you have that security that you inherit from that low throughput but high security chain. That's kind of like how this kind of this best of both worlds solution works. And that's kind of how you get to this world, right? Where you have a settlement chain and you have a bunch of rollups on top. Now this was basically the original vision for what people back then called ETH2. You might remember before the merge, people used to call kind of like the beacon chain ETH2. And we moved away from the name, and that has a specific reason. Why? Because ETH2 was this vision that we would basically give this new chain some fancy new execution and settlement system, and you would take the old ETH1 chain and the old EVM on top, and you would retire it. Back in the day, there were a bunch of different thought experiments. You could turn it basically into some sort of roll-up on top of the new ETH2 chain. You could just have it parked somewhere on the side with some, you know, either like proof of work forever with some bridges to the new system, or just even over time, deprecated or something. But then at some point, basically, as we moved closer to the merge and to these ideas, people realized that, hey, actually, the ETH1 chain is kind of good enough for what we need for the settlement chain. And so that's why then instead the plan became, let's merge the existing system into this new ETH2 chain. And that's why we kind of stopped calling it ETH1 and ETH2 and instead just call it Ethereum again. And so that's why today what we have on Ethereum is this hybrid settlement and execution chain. So down there, that's what Ethereum is supposed to be. That's both the settlement thing, settlement and data availability, but we also still have execution. You can have your UA on L1, you can use Uniswap on L1, all of that still works on L1. And it's all still powered by the original EVM that we used to have on Ethereum from the early days. Right. So that's basically like how the role of the EVM evolved and basically how for a long time the EVM was supposed to have an expiry date and then it turns out it actually didn't. Now how do L2s and their execution systems fit into this? Well, actually when we first pivoted from sharding to rollups, the idea was, well, this is going to be amazing because now all the L2s can go on this wild exploration and see what type of new fancy execution system works for them, works best for them. But then it turns out we have seen a little bit of that, and I said that earlier, we now see experimentation with the SVM and move VM and like more fancy throughput EVMs and whatnot. That happens a little bit, but the vast majority of L2s actually basically followed the network effects and they were like well there's already a lot of dApp developers for the existing EVM today so what we are going to do is we're going to just copy the EVM right we're going to copy the EVM and we're specifically not going to make changes to it because otherwise apps would not be compatible and it doesn't mean that the L2s were not innovative I would say quite the opposite I think we've had we've like three, four years of pretty rapid innovation in this space, but that was very much focused on the specific new challenges for us. So that was focused on new functionality and fault-proof, ZK-proof. Of course, in the ZK space, the innovation has been amazing to see. And then when it touched the EVM, it was more like extension. So I'm not sure how many of you are familiar with Arbitrum Stylus. That's an example of an extension to the EVM that left the core of the EVM unchanged and just added something on top. And then one more wrinkle to this, the consequence of the EVM equivalence was also that most L2s ended up just going with the dominant L1 EVM client, which at the time was Gath. Of course, Gath is still",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:40:00.000Z",
      "slot_end": "2024-11-14T11:10:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/12XdvKPNbvuPDHnrej4p-WzreCiZV7ATA5gFRxh1Vejk",
      "resources_slides": "https://drive.google.com/file/d/1Tfnis6TUHKD6mGUcncuZmmXYYCNB8ygt/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "open-decentralized-ai",
      "sourceId": "WDMSDF",
      "title": "Open + Decentralized AI",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 25,
      "language": "en",
      "sources_swarmHash": "e66f035630ef4fca4408ce06903e5ad43ca226ed677f0113ab4083ad4d808fa3",
      "sources_youtubeId": "ZMDpq5_zXvA",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d8799dbb7a90e1dbbd8a",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735d8799dbb7a90e1dbbd8a.vtt",
      "transcript_text": " So when we think about open and decentralized, I think, you know, you've got your models, not your mind. I'd like to think that that's the equivalent of not your keys, not your crypto. I think open models will run the world. It will be the infrastructure upon which our societies run. And I think, you know, let's make them awesome and let's think about leveraging all this technology and innovation we're thinking about to do that. Because I think open source and decentralized AI has a massive advantage over these centralized ones.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:40:00.000Z",
      "slot_end": "2024-11-14T10:50:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/185D2a1dcM0Mnygg246mzs0j_kcxYkRpeWxUnWh_d0cs",
      "resources_slides": "https://drive.google.com/file/d/11kckQWAIdqgnPw4CnF1eXJhpaRNgd-BA/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "transitioning-from-an-l1-to-an-l2-a-case-study",
      "sourceId": "KHVZ9M",
      "title": "Transitioning from an L1 to an L2: A case study",
      "description": "This talk will cover the learnings from cLabs' experience rebuilding Celo from the ground up as an L2. We hope that it can be a useful case study for other L1s to follow.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 1,Layer 2s,Rollups,Scalability,Optimistic rollups,Use Cases,learnings,technical,Layer 1,Layer 2s,Optimistic rollups,Rollups,Scalability,Use Cases",
      "keywords": "Layer2,case study,technical learnings",
      "duration": 516,
      "language": "en",
      "sources_swarmHash": "a8c9c4eae5b8bb85116277e1923cea79ee156b70ba11481801a3d6a23aac001e",
      "sources_youtubeId": "JerIZmTt-tE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d79e9dbb7a90e1be2c82",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735d79e9dbb7a90e1be2c82.vtt",
      "transcript_text": " Hi everyone, my name is Marek Olszewski and once again we're going to be talking about cello's transition from an L1 to an L2. You've probably heard that we're making this big transition and we really hope that we can become a case study for other L1s to follow suit. And so today will be a talk really targeting those other L1s with our learnings. And I tried to make this fun for you all, so I made this in the style of Zelda. Any Zelda players in the audience? Cool, we got a few hands up. Great. Well, hopefully you'll enjoy the slides.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:40:00.000Z",
      "slot_end": "2024-11-14T10:50:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/14jswR8SSkWsHdCj5ky0DG_01yQVUwV7nJtS5K18ynHg",
      "resources_slides": "https://drive.google.com/file/d/1YETIuGF5J7tcIEbo7nSf5sN0oygnGXF-/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "unlocking-the-future-onboarding-the-fixed-income-market-to-ethereumchallenges-and-opportunities",
      "sourceId": "N3JJFU",
      "title": "Unlocking the Future: Onboarding the Fixed Income Market to Ethereum—Challenges and Opportunities",
      "description": "Discover how Ethereum can revolutionize the world’s largest market: fixed income. This talk will explore strategies for onboarding fixed income markets onchain by collaborating with regulators, adopting progressive compliance, and streamlining UI/UX. We'll also discuss how to tackle challenges such as chain navigation, liquidity fragmentation, and fiat-to-crypto onboarding to drive the next wave of mass adoption.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Regulation,UI/UX,Account Abstraction,Economics,defi,Account Abstraction,Economics,Regulation,UI/UX",
      "keywords": "DeFi",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:40:00.000Z",
      "slot_end": "2024-11-14T10:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/15KHZ8vK6GD9sf4oCsV5ZRJ5sKkMhq4oPgvFv-uAVHsY",
      "resources_slides": "https://drive.google.com/file/d/1kRioMh546g5ESssfRZeHUAuAkfbUw0RQ/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "yeomenai-elevate-your-game",
      "sourceId": "WLKTYW",
      "title": "Yeomen.ai - Elevate your game!",
      "description": "Web3 games bring about possibilities for autonomous worlds that traditional games are not able to offer. Yeomen.ai makes the on-chain data available to the masses in simple dashboards. Yeomen.ai also offers on-chain extension of autonomous worlds to automate and transform game play. Command has an Intents based services layer to offer services on Autonomous Worlds that can expand to all Web3.\r\n\r\nYeomen.ai can work with MUD or Dojo powered onchain games and financial applications in future.",
      "track": "[CLS] MUD Community-Led Session, by 0xPARC",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Autonomous World,Collective Intelligence,Intents",
      "keywords": "Analytics,Modding,AI,Ownership,Marketplace",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "20e03f55574e957b69dafacbe86318c6de92e40b6d84f08bb5db7f0ebc5514f8",
      "sources_youtubeId": "C7DaJS79ocI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:40:00.000Z",
      "slot_end": "2024-11-14T11:05:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1-3KWguxf1wrbuaxgSi8ewkempDUuj4_SzXw0fz2dbbU",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "a-dacc-vision-for-decentralized-ai",
      "sourceId": "PHPCQB",
      "title": "A d/acc vision for decentralized ai",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 627,
      "language": "en",
      "sources_swarmHash": "d938ff8687ba1682d2d5bd9b0ae5f1b7c1d6dfc76f38b46b1edb41f5e2dea6a7",
      "sources_youtubeId": "BMGnroprUnk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d78c9dbb7a90e1bc056c",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:50:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1Qb2TXCOf_qhcjYsh7BEEeZW5-zo9GB2HTcUih2-gmLw",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "downtown-stimulus-public-goods-funding-for-main-st",
      "sourceId": "VC9TDM",
      "title": "Downtown Stimulus: Public Goods Funding for Main St",
      "description": "Web3 Public Goods Funding has left web3, & successfully hit main st!  💰🏦\r\n\r\nThe downtown stimulus team raised $43k for Boulder Colorado COVID economic recovery & proved QF works in mainstream USA.  Learn about this experiment & lessons from it from Gitcoin founder Kevin Owocki.",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Quadratic Voting,Public good,Local Impact,UI/UX,mainstream,Public good,UI/UX",
      "keywords": "mainstream",
      "duration": 599,
      "language": "en",
      "sources_swarmHash": "418e76d2a15887841cd6d00d34e67a8a770a3dd778739b1baf803c368b66748e",
      "sources_youtubeId": "LmYVHyEPeSE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735da639dbb7a90e1300946",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735da639dbb7a90e1300946.vtt",
      "transcript_text": " Let's welcome Kevin. Hello, friends. What's up? I'm Kevin Awocki. I'm one of the co-founders of Gitcoin. Has anyone in the audience used Gitcoin? All right. Can't see it on the live stream, but a bunch of hands went up. All right. Can't see it on the live stream, but a bunch of hands went up. I'm going to talk about bringing the Gitcoin model of quadratic funding to Main Street. So how do we exit our little crypto bubble and start to do more public goods funding downtown? So if you've been in the Ethereum ecosystem over the last couple of years, you know that quadratic funding is a democratic way to run a public goods funding campaign. If you are an ecosystem that wants to fund your ecosystem public goods, you put out a pool of funds, and then you allow grants or projects in your ecosystem to access those funds by raising money from their constituents and receiving money from the central matching pool. But there's one weird trick with quadratic funding that makes it into a democratic power tool, and that's that the matching is based off of the amount of contributors to each project as opposed to the amount that they've given. So if you raise $100 from 100 contributors and I raise $100 from one contributor in a quadratic funding campaign, the one with the broader base of support is going to get 99% of the matching pool. And this makes quadratic funding into a power tool for funding what matters for everyday people in these communities, because the poor and the many get to make capital allocation decisions instead of the rich and the few. instead of the rich and the few. Quadratic funding has delivered $65 million worth of funding in the Ethereum ecosystem over the last six years on Gitcoin. But I wanted to prove that we could bring it out of the crypto bubble and to Main Street. And so in 2020, when COVID was ravaging the world, and in my hometown of Boulder, Colorado, 99% decrease in foot traffic for the downtown was happening. Businesses were struggling to stay afloat. And so I got together with my colleagues, Zach Herring and Katie Johnson, and we put together a $25,000 matching pool of quadratic funding in downtown Boulder, Colorado, and were able to run a quadratic funding campaign for a yoga studio, a bookstore, a coffee shop, and a couple of other projects downtown. So this is what it looked like, downtownstimulus.com, and basically the way that we positioned this was that you could support local businesses by making a contribution to these businesses using a credit card payout. So basically, we took quadratic funding, we took Gitcoin grants, we stripped out all the crypto, and we just allowed people to contribute with quadratic funding, or sorry, with US dollars, contribute with quadratic funding, sorry, with U.S. dollars and have their contributions matched by quadratic funding. So we had a $25,000 matching pool. Thank you to Vitalik Buterin and to local philanthropist Brad Feld. And we're able to raise $16,000 from 320 contributors of an average donation of about $40. And all in all, we're able to raise about $41,000 for downtown Boulder public goods. I think this is really exciting that a primitive that has been pioneered and distributed in the crypto ecosystem was able to go mainstream with a campaign like Downtown Stimulus. And I'm really excited about this meme of cosmolocalism, which is basically an approach to production and governance that provides a combination of global coordination with knowledge sharing and local manufacturing and resource use, emphasizing open source software and a relationship between the global and the local where the global services the local. Okay, so in what ways can we, in the Ethereum ecosystem and the open source ecosystem, create a global commons, create knowledge and production that is in service to the local? And this is opposed to the Web2 surveillance economy in which you have Facebook communities going into your local communities and surveilling everyone and tearing apart the social fabric because it's more profitable for Facebook shareholders that way. How can the Ethereum commons be in service of the local commons is the design space of cosmo localism and ethereum localism and i'm happy to say that downtown stimulus to me is a proof point that this is possible we can take the programmable money where we program our values into our money in the ethereum space from the ethereum space and start applying it to downtowns across the world and i'm really excited to downtowns across the world. And I'm really excited to see what kind of experiments in public goods funding, community currency, supporting the arts and supporting public goods that we can do when we do that. So I'm Kevin Iwaki, founder of Gitcoin. That's been Downtown Stimulus. Thanks so much for your time. Thank you, Kevin. Hello. Thank you, Kevin. Hello. Hello. Amazing. We have like roughly five minutes for questions. By the way, I'm also a big fan of cosmolocalism. Yeah, thanks. Anyone in the audience? The gentleman in the purple hat. Should I? Oh, wow. You're good. Good catch. Thanks. Yeah, I love your work and all that. I was going to ask, with things like community currencies and things like that where you might put conditions on your money, I sometimes think that that's obviously a restricted form of trade whereas regular fiat currencies are sort of unrestricted. So with that lens, how would you say that the community currencies can sort of out-compete freer money, if that's the right premise for you? Yeah, so the question is, how can community currencies out-compete national currencies? Yeah, like how would it have more utility if it's somewhat constrained in different ways? Yeah. Yeah, like how would it have more utility if it's somewhat constrained in some ways? Yeah. I think that community currencies are cool as an outsider, but I'm not a designer of any, so I'm not qualified to answer that question. I think Scott Morris has done a lot of work on community currencies, and I would ask him. Yeah, I guess I was riffing off the adding our values to our money. And if you put sort of constraints on it in that sense to fuel our values, I can see definitely the bigger picture that that's awesome and I want to see more of that. But then I also sort of play devil's advocate, because more so another friend does this to me where he sort of says, well, I could just do that with the US dollar or I could do that with this sphere currency. So I'm trying to sort of find the arguments, like other arguments that say this money with value restrictions is kind of more better in a more immediate form than I can currently argue. So I'm trying to find on that side. Yeah. I think that, you know, one of the things that I see a parallel between what we're doing in the crypto ecosystem and what community currencies have done for hundreds of years is that what we're doing in the crypto ecosystem is memetically local and community currencies are geographically local. Okay. So like to me, memetically local means that like Gitcoin's a public goods funding coin. Filecoin is a file storage coin. Ethereum is a computational coin in a local memetic area. And so, you know, in what ways, what we learn in community currencies can be applied to global currencies like the DAO currencies and vice versa. I think that there might be sort of a Cosmo local convergence there. Yeah. But my talk was about quadratic funding more than community currencies. I think that they're both cool, though. Thank you. Do we have time for one more question? Yes, we do. Gentleman over there. Thank you. Hi, Kevin. Hey. What do you think in your mind will be the hardest part of building some version of a downtown stimulus in another local community? Finding the initial donors or the tech, probably tech is just working something. Or making people understand how to use the system or anything like in your experience. What do you think should the focus on if you want to build something in my own local community? Yeah, great question. So we built a code base that you can use to do Fiat quadratic funding rounds. It's called simplegrants.xyz. Go to simplegrants.xyz, fork it, deploy it in your own local community. I should have put that in the talk, actually, now I think about it. But yeah, the hardest part, honestly, was raising the initial 25K. Quadratic funding campaigns in Web3, there's a lot of treasuries that are just hundreds of millions of dollars. And we draw on those to run quadratic funding rounds in QF. But the raising of funds to do quadratic funding locally is sort of hampered by, do you have access to wealthy individuals who are willing to do it? And can you crack the nut of the local government actually funding it? We actually went around the local government. We didn't even talk to the local government because their funding cycles are just quarters and quarters long. So to answer your question, I think that raising the initial amount of funding is the hardest thing that you'll do if you're going to run these campaigns. And hopefully we can make the tech a little bit easier with simplegrants.xyz. Thank you for the question. Thank you. And that's it for the time.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:50:00.000Z",
      "slot_end": "2024-11-14T11:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Lf82ct08SpegO30t849kscAqeyNa8bTNVpMQ8ljElfA",
      "resources_slides": "https://drive.google.com/file/d/1GiQe7KFzESPmZ2f8XtDUZXyNthoMw5T6/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "l1sload-precompile-read-l1-state-from-your-l2-contract",
      "sourceId": "VRXWFH",
      "title": "L1SLOAD Precompile: Read L1 State from your L2 Contract",
      "description": "We recently introduced [RIP 7728: L1SLOAD Precompile](https://github.com/ethereum/RIPs/pull/27). This is a new L2 precompile that allows dapps on L2s to read from the L1 state.\r\n\r\nIn this talk, we will explain how L1SLOAD works, and we will highlight some of the most exciting use cases that this precompile will unlock.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,Developer Infrastructure,DevEx,precompile,Cross-L2,Developer Infrastructure,DevEx",
      "keywords": "RIPs,L1SLOAD,Precompile",
      "duration": 918,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "RSIGnIm58Mo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T10:50:00.000Z",
      "slot_end": "2024-11-14T11:10:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1nkzZ5Gin2GWcgGhvYhOmVQywSYCjYFlNu3xeFIu8YLs",
      "resources_slides": "https://drive.google.com/file/d/1u5sBEvgQbpjasap_01jW_uOxR4Ltz49l/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "corruption-kyc-and-the-cost-of-compliance",
      "sourceId": "8FQ3HT",
      "title": "Corruption, KYC and the Cost of Compliance",
      "description": "Trillions of dollars in illicit financial flows slosh around our financial system today, facilitated by the most powerful centralised instiutitons. Current efforts to address IFFs are ineffective and result in harmful side effects for some of the most vulnernable in society. In this article, we investigate the causes and impact of IFFs. Despite what certain bankers and politicians might have told you, the transparency and programmability of cryptocurrencies are a solution to, not a cause of, the",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Anonymity,Censorship Resistance,Civil Resistance",
      "keywords": "Compliance,Illicit Financial Flows,KYC/AML",
      "duration": 1620,
      "language": "en",
      "sources_swarmHash": "3a055197ad020b6410ccc8cd4ade28eaee941fac5a745ac709a6c99b767d5cfd",
      "sources_youtubeId": "WiyrtXdHrXU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e0479dbb7a90e1e78f69",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1YTeRCkNqi_tWXXuL2gLaihLcpRslx1hjlAncemiP4bU",
      "resources_slides": "https://drive.google.com/file/d/1fZ6at-FEnXjAyX8qfE3znM8YwU3fIzpr/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "crypto-in-action-powering-ukraines-resilience",
      "sourceId": "7JZGQJ",
      "title": "Crypto in Action: Powering Ukraine's Resilience",
      "description": "Discover the critical role of crypto in supporting Ukraine's recovery amidst ongoing challenges. We will highlight real-world examples in energy, housing, food distribution, communication, and more, showcasing its tangible impact.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Civil Resistance,Coordination,Public good",
      "keywords": "Resilient infrastructure,Ukraine,crypto donations",
      "duration": 625,
      "language": "en",
      "sources_swarmHash": "ba4ee53fcd465dc20a1d1d779d643e677c861f73834020e32316b456cbca99ba",
      "sources_youtubeId": "WGldkJpfrZw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735d9de9dbb7a90e120b813",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:10:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/19vJbc3_xafMXjoRH6SLUAgIZjg0J4oTsoiFzXzdq3Ao",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "how-hardhat-3-will-ensure-precise-simulation-for-l2s-using-edr",
      "sourceId": "G7AHS9",
      "title": "How Hardhat 3 will ensure precise simulation for L2s using EDR",
      "description": "As the Ethereum ecosystem shifts towards L2 solutions, developers encounter new challenges in maintaining consistency and efficiency across different chains.\r\n\r\nHardhat is powered by EDR, a reusable Ethereum Development Runtime implementation to build developer tools. This talk will show how EDR's support for L2s in Hardhat 3 will streamline the development process, improve testing accuracy, and enhance the overall developer experience.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Tooling,DevEx,optimism,DevEx,Layer 2s,Tooling",
      "keywords": "EVM,Hardhat,Optimism",
      "duration": 1313,
      "language": "en",
      "sources_swarmHash": "051def2613d035b880fce6a098fdc98bd5b49ebecf4914f65c76912a6ea56741",
      "sources_youtubeId": "W7y4bYZFVJ4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e4c69dbb7a90e193d498",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735e4c69dbb7a90e193d498.vtt",
      "transcript_text": " Welcome everyone and thank you for staying until the end of the day. I am Wodan, a developer at the Nomic Foundation, and today I'll be talking about how Hardhat 3 will ensure precise simulation for L2s using EDR. There's a lot to unpack there, but first I wanted to comment on the Nomic Foundation. Maybe not everyone's familiar with it, but you're probably familiar with our work. We are a non-profit dedicated to Ethereum developers, you guys, and our most well-known product is likely Hardhat and the Hardhat VS Code extension. That's us. So an overview of what I'll be discussing today is I'll start off with a quick introduction to what EDR is. Then I'll go into variability between L2s. I'll look at some problems that currently exist when developers are developing for L2s using L1 tooling. Then I'll do a technical deep dive into how EDR actually simulates L2s accurately. This will also be interesting for any of you L2 developers out there, as I'll talk about the extensibility points that EDR will expose in the future. And finally, I'll touch upon a demo that shows how L2s work in Hardhat 3. This will be interesting for all of you Hardhat users, as it will show how the technical complexity of all of this variability is boiled down to a simple and straightforward user experience. So what is EDR? EDR or Ethereum Development Runtime in full is an reusable EVM development runtime library for tooling. It is a set of building blocks for a blockchain simulation and in particular it allows you to observe EVM and Solidity execution. So as such we are targeting smart contract development, the simulation, testing, and debugging thereof, and we're not targeting to be an execution layer node. If you're curious to learn more about EDR, earlier this year we had an EDR launch announcement when we integrated into Hardhat 2, and there's more information about other features, performance improvements that brought to Hardhat, and future roadmap for EDR as well. So what variability exists between L2s and L1? For the sake of this presentation and also for the implementation of EDR, we're assuming L2s that are EVM equivalent. This means that they have to comply with the EVM specification or the Ethereum white paper, if you will. When we can depend on roll-ups, any of the L2 transactions can have their own custom types. This means that when we're dealing with custom L2 transactions, that there is logic that's different, which is executed within the EVM. The way that we're dealing with rewards, the way we're dealing with fees can be different, and even the output that's returned can be different. For example, halt reasons when an exceptional halt occurs. When we go into a transaction and we look at the bytecode, the opcodes might also be different. It could be that an L2 doesn't support all of the opcodes of L1 or vice versa, but it can even be that the same opcode has a different type of behavior in the L2. For example, the block number opcode, how would we simulate this on an L2? Do we give a prediction of the L1 block that we expect to be included in, or do we return an L2 block number. Within the EVM, another thing that's different are precompiles. The set that's available in different L2s differs, as well as them being different from L1. Another thing that is different are hard forks. Every L2 will have a different set of breaking changes for which they'll have their own hard forks and something that we need to track as well is so-called hard fork activations. Those are the block numbers or timestamps at which a particular hard fork becomes activated and these will for example be needed when you do an ETH call at a particular block number. If we're forking a blockchain and we want to run a historic block, we need to know what hard fork should be activated at this point. When we then roll up everything into a L2 block, we also need to at the protocol level take into consideration fee calculation and we need to incorporate custom transaction receipts. When we are deploying our own chain for specific L2s, we need to consider their own pre-deployed contracts. These are incorporated in the Genesys state and mean that you can access these contracts at a predefined address. These differ per L2. And then finally, if we go up one more layer, you have RPCs, the RPC, which might have additional fields for methods. It could be that a method returns fields, but with a different behavior. And it can even be that one of those methods has entirely different logic altogether. All of these types of variability need to be incorporated and keeping track of them is a huge pain. So a big shout out to EVM diff and the L2 documentation that was very instrumental when we were implementing the OP stack for EDR. So we have an idea now of what variability exists, but what problems might occur? Here are some examples. So when we start off at the execution layer, when we're dealing with unknown transaction types from an L1 perspective, we're not sure how to actually execute them. It could be that they throw an error or it could be that they do execute but because the opcodes have different behavior, the result is different and as such the L2 execution will be incorrect within your L1 tooling. When we're then trying to mine a full block, we also run into issues. The RLP encoding for these unknown transactions is unknown, which would result in an incorrect tri-route. And it could be that the header has different header fields, which would also result in an incorrect block hash. Then when we look at the gas calculation these end up being incorrect as well. Let's have a look at the way that L2 transaction costs are structured. So on the left side you'll see something that's very familiar. We have our execution gas cost consisting of a gas price multiplied by gas used. This is the same as on L1 except that L2 gas prices will be lower. But we have something new which is the L1 data fee. This is the cost we have to pay for the roll-up or the part that our transaction is within that roll-up. This is the L1 gas price multiplied by the L1 gas used multiplied by the number of bytes of transaction data. Usually, this is compressed to reduce the amount of memory usage. But we somehow need to convey this cost to the user. L2s do this differently. It could be that they try and convert this factor of gas price multiplied by gas used to an L2 gas usage. Or it could be that they somehow try to change this to the L2 gas price. Each L2 will have their own strategy for doing this. Then when we're looking at debugging, for example, using debug trace transaction, what we're trying to do is we replay some block on the chain until we reach the transaction that we want to debug. And it could be that up until that point, we find some unknown transaction, and we could treat them as an EIP-155 transaction or a legacy transaction, and try to execute based on the best effort. But this might result in errors. So instead, we could choose to skip it. But this might have a negative side effect that if that transaction affected the state that our contract is also accessing, that we are getting a different result than we would on a test or mainnet. All of these examples that I gave have something in common. We're trying to build L2 smart contracts using L1 tools and hoping that it just works. It could be that the tests are passing but there are still subtle execution differences that give us a false sense of security. And this leaves room for security vulnerability once we deploy. So how does EDR circumvent this and accurately simulate L2s? So here we have an overview of the different building blocks that I mentioned outlined in black. In orange we have entry points from and into Hardhat and in purple we have our EVM which is the EVM dependency that we use. Everything outlined in green is parts that we previously had supported for L1 Ethereum but now we need to convert to be able to be multi-chain and also support different L2s. I've numbered it in two sections as they both have different requirements and we'll delve into those respectively now. So the part outlined in one was all Rust code. So we need to look at expansibility from a Rust perspective. For this we had several requirements. We wanted compile-time polymorphism. This would allow users of our crates or packages, if you will, to be able to use these interfaces or traits during compile time to generalize their types and functions. We also wanted to generate type errors at compile time. This would force L2 developers to resolve any issues with their typing, as opposed to the error bubbling up to hardhat users at runtime. Finally, we also wanted to ensure that their type definitions were reusable from a base chain to a L2 chain. For example, if we have an EIP2930 transaction, this is used in OP stack as well. So being able to reuse those types lightens the burden for L2 developers. The solution we used are the Rust traits and generics. Traits are a form of interface that can be used both at runtime and at compile time to constrain generics. And generics are just a way to generalize function definitions and type definitions across a type. For each of these traits or interfaces, we associated types with them that we consider to be a chain specification. Think of a transaction type, a block type, etc. And there are some constants which are used within the protocol. We distribute individual change using Rust crates for reusability. So if we look at the overview again here, and we start at the top right, we have a remote network clients which does RPC calls to a provider like Infior or Alchemy. Here we introduced an RPC spectrate that would define the RPC transaction or an RPC receipt, etc. Then when we go to REVM, here we introduced something called EVM wiring. We proposed changes to REVM and with the graceful help of Dragan Rakita, the maintainer, we were able to incorporate these large changes into REVM, which means means now within our EVM you can also run and extend different chain types. Here you would define a runnable transaction, a block within which it's executed, the hard fork, Hall's reasons, etc. Then we go up one level to something we call the executor. The executor is a wrapper around the EVM, which receives a signed transaction, passes it in, and while it's executing, we gather additional data, which we use for runtime observability. So things like traces for a stack trace, etc., that we expose to the end user. Here we introduce a type runtime spec and then when we're incorporating all of these transactions into a block within our block builder we need to consider parameterizations for the protocol level such as the base fee calculation. This has specific constants that need to be incorporated which differ between L1 and L2 And here we introduced a ETH header constants interface to define those. Then all of this logic is tied together within the node or provider. And here we introduced a provider spec with things such as a pool transaction. For example, for blob transactions, we also keep track of additional blob data, etc. And these are stored within the mempool. Then finally, we reach the RPC interface that's exposed to HardHat. Here what we do is we use a tool called NAPI, which can be used for generating TypeScript bindings from Rust code. And this allows us to basically from HardHat, which is TypeScript,ings from Rust code, and this allows us to basically from hardhat, which is TypeScript, call our Rust functions. And here we introduced a sync and API spec. Sync basically means we're trying to call our Rust code from a threaded environment, and this ensures that the access to that data is correct. And we also do a conversion from compile types to runtime polymorphic types, which we'll have a look at next. All in all, this encompasses six different traits or interfaces, and these are all the things that an L2 developer would have to implement in order for all of the building blocks that we have within EDR to be supported for their chain, and in addition for their chain to be usable within Hard Hat 3 in the future. So when we look at the part that was outlined on the left, we're dealing with TypeScript, so the requirements for extensibility are slightly different. We're dealing with runtime polymorphism. We're trying to minimize memory usage and load times. The goal here is that we shouldn't have to load all the possible L2 chains, only the ones that the user of hard hat in that particular configuration wants to use. And we want to avoid centralization of chain types. We don't want a repository where you have to add your specific chain type to an enum. Instead, we want you to just be able to plug and play your own chain types independently. For this, we created an NAPI wrapper around dynamic trade objects. Dynamic trade objects are basically virtual objects where you can access a specific instance of an object through an interface. And to distinguish chain types, we use a string identifier. And then finally, for distribution, we're using NPM packages. Here we have an overview of what that looks like on the TypeScript side. We have a, if we go from right to left, we have a provider interface that receives a generic JSON RPC request and returns a generic JSON RPC response. We have two different implementations, one for L1, one for Optimism. These would basically parse the input, make it a typed request for their respective backends, handle it, and then again convert the response to a generic string, which is returned to the user. In order to be able to construct these, we're just using a factory pattern. So if we move one column to the left, we have a provider factory, which would receive a generic configuration, again using strings, and we have two implementations that would parse those configurations and make them typed and construct a respective provider depending on L1 or Optimism. If we then move one column further to the left, we have something called a context. This is maintained from the start until the shutdown of your application and contains a registry of provider factories, which is a mapping of the string identifier to the instance of the provider factory. If we look at a usage example, you start your application and would register any of the requested providers based on a configuration and you store them in the registry. Then when a hardhat user would actually request to create a provider, let's say Optimism, we do a lookup in the registry, find the Optimism provider factory, pass on the config, and ask it to create an instance of the Optimism provider, which is returned to the user. This is the way that we have designed extensibility. For the hardhat beta beta we'll be releasing with the OP stack and l1 support in the future we'll also be releasing all of these api's and it would allow you guys to extend it for your own l2 now let's have a look at what this actually looks like in hardhat 3 that got a little bit complicated on a technical level but but from a user side it's actually quite straightforward to use. So here we see an example of a hardhat3 user config. At line 7 we have two networks defined, EDR-L1-Sypolia and EDR-Op-Sypolia. They both have the EDR type for the back-end simulation. The first one has a chain type for L1, whereas the second one has a chain type for Optimism. This is communicating to EDR which typing system to use. And when we switch to a hardhat script, we see on line 3, we're requesting the hardhat network manager to connect to the previously configured Opsipolia and we specified that for its types it needs to use Optimism. This is informing the TypeScript type system that the estimate L1 gas function needs to be available which would only exist for Optimism and not for L1 and we make a call to it through the public client and then we send an L2 transaction to L1, and we make a call to it through the public client, and then we send an L2 transaction to transfer one way, and we wait for the transaction received to be included in a block. So when we execute this, we're estimating L1 gas, get 1600, and then we see that our transaction that was sent is included in a block. If we then go back and we switch the network we are connecting to to EDR L1 Sopolia and use L1 typing, we see that we're getting a type error for estimate L1 gas as this is not available for L1 networks and the Veeam object knows this. So this is a sneak peek at some of the features that will be available with multi-ain and the kind of simple user experience that you get as a HardHat user. If you want to learn more, we've had several other talks already at DEF CON. We had a talk about the future vision of Nomic Foundation and the products that we're developing. We also had a talk which was a preview for Hardhat 3. And tomorrow we'll have a talk by the Slang team. Slang is a compiler as an API for developer tools. And this is basically useful for writing your own linters, doing formatters, etc. So if you're curious about that, go have a look at them tomorrow. Thank you. Alright, so we do have some extra time for QA. Feel free to scan the QR code if you have more questions for Walden. In the meantime, let's take this one. ARP has pre-deployed contracts that do not exist as EVM code on chain. Instead, instead is Go code embedded into Get. Any efforts to handle this? So the way that we could theoretically handle that is instead of having it as Go code, we could include this in the genesis state that is deployed when you're using an arbitrum L2 chain. Alright, so let's when you're using an Arbitrum L2 chain. All right. So let's wait for a few more seconds. We have another one here. Wenhar had that three. I think we don't have an announcement date yet, so stay tuned. We have an internal alpha going at the moment, but the beta will be sometime at the start of next year, but there's no definitive date yet. How does EDR compare to Foundry's Anvil? The hardhat simulation or the simulation component of it and the RPC that's exposed is comparable, like identical in that sense to Anvil. The performance is also very comparable. We have a slightly different way of maintaining our state, and as a result of that, if you're very often switching between different states, so going and doing EVE calls and jumping back and forth between a previous state, making some modifications and jumping back. In those use cases, we're often faster. But from a use case perspective, they're the same. The only difference that we might have is we haven't released our crates yet, which I think Foundry or Anvil has. But in the future, we'll be releasing a first version of EDR as well as crates on. Let's remain on the stage for a little while to see if we have more questions. SPEAKER 1 COOKIE MONICA DUNCAN- All right, if you have any questions, you can also meet me afterwards. And our CTO is also here who can answer any questions about Hardhat3. Alright, that would be great. Thank you guys so much and let's give a round of applause to Ruwanan.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/19L7dj6AAC2bhxtksWRYlrJuOv3Xc6aF5iQmk5DGFVbA",
      "resources_slides": "https://drive.google.com/file/d/17Al2kaNEfmbsqahJzMayjt1eMDg3CWR5/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "reclaiming-our-dollar8-billion-funding-public-goods-with-stablecoin-profits",
      "sourceId": "UCFEEN",
      "title": "Reclaiming our $8 billion: funding public goods with stablecoin profits",
      "description": "Ethereum is stuck in a dark deal with two companies. They control ~all stablecoins; facilitate 49% of DEX swaps; and can overrule all future hardforks:\r\n\r\nCircle & Tether.\r\n\r\nIn return, they reap $7.4B in stablecoin earnings (2023).\r\n\r\nBut wait—that’s the interest on OUR money! We should be in control.\r\n\r\nGiving to holders is illegal, but funding public goods isn’t.\r\n\r\nIf we coordinate, we can switch to nonprofit stablecoins and reclaim billions for eg Protocol Guild, R&D, DeFi infra, OSS—or other causes.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization Improvements,Censorship Resistance,Open Source Software,stablecoin,Censorship Resistance,Decentralization Improvements,Open Source Software",
      "keywords": "Stablecoins",
      "duration": 520,
      "language": "en",
      "sources_swarmHash": "4760ed7b4ddcd4285ecd45c32a20bb206c281acfb98eb7a3d1b45e15e7e3f847",
      "sources_youtubeId": "J2aw52g_OJI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735dbd39dbb7a90e16545d3",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1AC1UEYubPRYIH9AzVy-E905hMuR67GeAMdfpHpaGm0g",
      "resources_slides": "https://drive.google.com/file/d/1vrDVm4VXC4hVQJYZmvsFQ7ZsPWsvo7XU/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "the-long-con-pig-butchering-drainers-and-job-scams",
      "sourceId": "STMCNZ",
      "title": "The Long Con: Pig Butchering, Drainers, and Job Scams",
      "description": "I'll discuss the different types of malicious actors from low-skilled script kiddies to government-sanctioned advanced persistent threats. This presentation will include an overview on drainer groups and how sophisticated scammers string along their victims, fattening them up before extracting as much value as they can, as well as the nefarious practices these operations employ. Finally, I'll focus on the recent rise of job scams that have been targeting builders and employers alike.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security,Custody,threat,intelligence,Custody,Security",
      "keywords": "threat,intelligence",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "9b9842b85cbfb2249efb51ecd459e8c85442648743e788785f6907dc66ffa381",
      "sources_youtubeId": "XS2YF2WgIfU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1dFgaih8CwwDPKj_GGRG-nwZ_b7MobKt9l-QDbYxwOPk",
      "resources_slides": "https://drive.google.com/file/d/1ZLi23xoIspqqFivHaw5dxbXYpID30K83/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "try-it-out-in-remix",
      "sourceId": "SUEJQR",
      "title": "Try it out in Remix",
      "description": "Remix is great for your blockchain experiments for both new Web3 devs and OGs. We’ll present the new Remix Desktop - great for offline work, plus RemixAI tools and RemixZK tools, our new collection of templates, our new video guide, our new tool to make a basic DApp - great for hackathons, and more! Learn to play in Remix!",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Tooling,DevRel,Desktop,ai,Desktop,DevRel,Layer 2s,Tooling",
      "keywords": "AI",
      "duration": 1453,
      "language": "en",
      "sources_swarmHash": "ba46a6efe168366f05e9a4d908ed3909a765ef3712b9f52433c87f7275e605a0",
      "sources_youtubeId": "XRbi7AbQwSg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e09f9dbb7a90e1fef26b",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1frNEqhlzbsXj_EqKtcIYr8R8G-t4ymlj401WFG6BBYw",
      "resources_slides": "https://drive.google.com/file/d/1C4fkAelfWEUPkV4hR0uOXv_JTzWAQBwJ/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "usc-ultimate-solidity-championship",
      "sourceId": "UE8WVS",
      "title": "USC Ultimate Solidity Championship",
      "description": "A 30-minute Solidity programming competition where the winner is determined objectively, permissionlessly, and transparently after the time expires. The Ultimate Solidity Championship (USC) is an event designed to showcase the skills of the best Solidity developers in the ecosystem. Its primary goals are to highlight Solidity programming as an art form, onboard more developers, educate the community, and foster collaboration, ultimately enhancing Ethereum's long-term impact.",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Art,Hacks,Public good",
      "keywords": "Solidity,Programming,Competition",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:00:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1flrl1DVDOcGQrL2WtGO0tRQUbwP7P_Xk3IQeWVr_wIU",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "cults-truth-values-and-the-bubble",
      "sourceId": "CCVGE8",
      "title": "Cults, truth, values and the bubble",
      "description": "Our space can sometimes be challenging to navigate, with many hidden layers of truth and tribal factions. This is a quick journey about how to navigate yourself and what you care about in an increasingly complex environment. How to deal with all the political games, be human in such a fast-moving ecosystem and generally just thrive as human being in a very tech and protocol-focused world.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Governance,Decentralization,MEV,fairness,Coordination,Governance,Social",
      "keywords": "Personal,Human,Purpose",
      "duration": 523,
      "language": "en",
      "sources_swarmHash": "35519fcb0aaec0942033d5bff43ce07da73b5937a89fe29203a667f68781e906",
      "sources_youtubeId": "iuQLIvQ5E-w",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735defb9dbb7a90e1b24d41",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735defb9dbb7a90e1b24d41.vtt",
      "transcript_text": " Well, all right, that's not the actual title, but that is the actual title. So yeah, originally this presentation was intended at something more technical, but now I found it actually more interesting to bring something more humanistic here to the stage. And this title might sound confusing, and I'll try to cover a lot of ground here, and I'm very excited to do that with you together today. So a bit of a preface. So yeah, my background here is quite diverse. So I used to be a software engineer for a while. I used to work in research, and now I'm a bit more in a coordinative perspective, but I'll actually get right into it to give you a bit more of that. So values. I'll start with talking about values of individuals in the ecosystem and how I perceive different people here and where I allocate myself as well, and then give you a bit more of a landscape of different companies, different parts of the ecosystem in this value framework. Then I'll talk a bit about economical and political games, truth, cults, the bubble, and I'll try to tie it all together at the end, even though it might sound confusing now. It'll make a lot more sense when I go into it. So I like to ask myself the question a lot is, why? What's actually my motivation why I'm here? And what is the motivation why other people are here? And so to put it in a very simple framework, I realized most people can be described in this kind of triangle. So either people care a lot about decentralization in the space, people enjoy fun in the community, or people care a lot about monetary gain. And often that gets a bit disguised or it's not very clear. And so it's really nice to have very direct conversations with people about that. And for me personally, it has been a bit like that. So I started with a lot of idealism. I realized at some point there's also a lot of money to be made here, which is often not directly talked about that much. But in the end, I stayed because of the people and the community, and I'm slowly going back towards more idealism and impact. But if we look at the space overall, actually, it makes sense to extend this framework a bit and add one more intellectual challenge, which is very interesting for a lot of us here. And so if we do that, and we try to map the space a bit, so then we get to a landscape which looks a bit like this. And for me, that was pretty interesting to just allocate different parts of the ecosystem and also see like, where does it fit? Where could I personally fit with my values? And this is not the full truth, but I found it really interesting to map the landscape a bit like that. And maybe that's helpful or interesting to one of you. Yeah, continuing here towards the games we're playing, the economic games in the ecosystem. So there was a really good talk recently this week by Christoph Schlegel from Flashbots. We're actually in a market structure where there's a lot of private value, for example, private order flow. The stable equilibrium is actually a full monopoly, which is the opposite of decentralization. Do we want that? Let's look at what the market actually looks like. For example, here, the builder market. Some of you might know it's a duopoly, basically, at the moment, but in an equilibrium, actually, it would be a full-on monopoly. And why is that like that? Or, for example, looking at how order flow works. Also here, it would be basically a equilibrium of one party controlling all of order flow, but actually it looks more like this, like a very complex structure. So that means in the background, there's a lot more political games happening than we are aware of. And it's very interesting to observe those once you're a bit deeper into the space. And the question is, how do we not devour ourselves in this internal political warfare, but actually focus on the issues we care about? Open question for now. I might get back to that. So another angle to look at this is, what do we trust in this space? So here we can put it under another framework. So I feel like the space emerged a lot from losing trust in government institutions, then trusting more in protocol and code, or do we actually just trust ourselves? Because often we have this overemphasis on just wanting to trust the protocol, the code, something predictable, but maybe there's something else. And so I kind of ended up with this nice mid-width diagram. I really like those. So some of us really work hard. I used to really work hard and want to save the world, but in the end also, let's have some fun. Let's not be too serious in this space, but also really just enjoy ourselves once in a while. And so one question that arose in my reflections there is, if we just have fun, sometimes crypto feels like a big, big playground of technology. And do we then still actually care about issues? Or do we just become chill and have, yeah, just have fun, do nothing to change the world? For me, actually, it's more the way, if we can relax a bit, if we don't have to have these political warfares, for example, like like this then we can actually be more united and aligned in the ecosystem or wall and so to continue from here what are actually if we look at ourselves really reflect deeply on emotional space our needs as humans here like what do we actually care about why are we here and what makes us happy? For me, actually, it's really mostly about the people. And to be more seen as like a whole human being, to be sometimes confidently insecure and not know things, that's something which really fulfills me. And that's also a reason why things like this place here as a conference, but for me, it feels more than a conference with the cozy spaces, the smaller groups, or pop-up cities have been thriving where you actually get a feeling of community. But then also there's other dangers there. If we become such a closed ecosystem of like community really close to each other, but then there can also be, you see myself there at SuperLyn, for example, it can sometimes become almost cult-like. We're like a bubble in ourselves. So what about this vibe that the whole ecosystem feels a bit like a bubble with its own language and we don't actually know how to talk anymore with the outside world? Is that a danger as well? And so I realized in a very interesting conversation with Phil Dayan that the world is actually watching us a lot more closely than we think. For example, the New York Fed recently published a paper in August where they were looking at tornado cash sanctions. And I was astonished by the amount of detail, how they actually understand the builder landscape and the graphs I showed you before. So I think it's also time to start talking to this world and not only see them as the enemy but maybe find a more collaborative approach there. To conclude, based on all of this, we're in a fucking complex space here but sometimes there are bits too much stuck in your head because we try to build protocols which predict every single edge case but by evaluating things we can also trust more our intuition, be humans, be vulnerable, be stupid. Like, I often also have this imposter syndrome, even though we work really closely with Ethereum researchers and be, like, very deep in the ecosystem, that, like, I know nothing, but all of us sometimes feel like that. And even though we can still create things, we can dare to also be on stage once in a while like myself here and sometimes peek outside of the bubble talk to people outside of the space try to speak their language and put things in like simple words and simple frameworks like I tried to do it here today. Thank you. Thank you. Hi. Good to have you here. Any questions? I do have one. Like, what do you think is the way for us to break out of the bubble? Because it's, like, language is so different, you know. How do we start? Like, understand where other people come from. Like, start from the other person's starting point. Bring things into really simple words, simple language. Find analogies from other person's starting point. Bring things into really simple words, simple language, find analogies from other people's world, and to describe it from there, and then bring them closer in. Is that how you umpire your mom? Yeah, I'm still struggling with that, but I'm making some progress slowly. My dad is easier. Cool. Do we have any questions from the audience? Any questions? I guess we can have discussion privately after this. Cool. Happy to talk later. Thank you so much. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:10:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12xOmjbuWiCGoJo_Bx-KMT5zB8_88W6kmYHfhx1CzVcA",
      "resources_slides": "https://drive.google.com/file/d/1vSfQ-yqKGV-B-5P-B-kc9FYQU0rulYcC/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "resilience-to-global-catastrophes",
      "sourceId": "PZFHQF",
      "title": "Resilience to Global Catastrophes",
      "description": "The risk of nuclear war or an extreme pandemic is frighteningly high. Little work has been done on resilience to these catastrophes. I’ll discuss resilient food sources that can be scaled up quickly, methods of maintaining the functioning of critical sectors in an extreme pandemic, and backup methods of meeting basic needs. I’ll discuss cost effectiveness of low-cost preparations, including piloting technologies, such as resilient satellite communications, and a resilience DAO.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Climate,DAO,Effective Altruism",
      "keywords": "Resilience,global catastrophic risk",
      "duration": 700,
      "language": "en",
      "sources_swarmHash": "d9de68893eeca8426b362d031574c484cd0ee2a4ec8b2054fcb360caf33f5cf8",
      "sources_youtubeId": "hWVKnQWXui0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735da889dbb7a90e132b668",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:10:00.000Z",
      "slot_end": "2024-11-14T11:20:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1PtsLokONL6PEJ91cRee5o3KxGN3fLCjZ34KzGRksS0U",
      "resources_slides": "https://drive.google.com/file/d/1sVDimjDp8VsnyAULnG0rK6dJU63YXevs/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "state-minimized-layer-2s-and-why-ethereum-greater-evm",
      "sourceId": "VDFBMT",
      "title": "State Minimized Layer-2s and Why Ethereum > EVM",
      "description": "Ethereum is at a critical juncture in its development. Many layer-2s are of the same mentality of copy and pasting their architecture and have not innovated over key blockchain problems such as parallel execution or state growth. If Ethereum is to compete with other alternative high performance blockchains, it has to solve for state growth. This talk will explore the landscape of state minimized layer-2s and show how Ethereum will be able to go beyond the state problem with non-EVM based design.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Network State,node-requirements,Network,State",
      "keywords": "node-requirements",
      "duration": 453,
      "language": "en",
      "sources_swarmHash": "15cefb1dc3eb849cfbe6481fac471d64fa487ade696d65a54da5acf41bc079cc",
      "sources_youtubeId": "juHr9CgkFCo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d2581b0f83434d6a4c04",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d2581b0f83434d6a4c04.vtt",
      "transcript_text": " All right. Okay. So I'm going to be very, very fast. I have, like, more than 30 slides, and there's five minutes. So I'm going to have to make this work. It's fine though. A lot of this I can kind of skip. So today we're going to be doing state minimization and why Ethereum is greater than the EVM. So components of a blockchain, just to review quickly, you have this area of a blockchain node that would be, you could think of as state, state being these things that you need to both verify things and propose new blocks into a blockchain. Then you have things like execution, largely what we do with the CPU and RAM, sometimes a GPU if you're going to do GPU acceleration. Then you have data. And in this case, I'm just calling it data, which is just sort of the data you communicate over the internet to basically get everyone else in sync and, you know, kind of progress things forward and ensure that it's actually, you know, a system we can all use. So in many senses, on the data side, we have some solutions. On the execution side, we also have a lot of solutions. On the state side, though, you don't really hear about this problem very much. And you don't hear about it because a lot of people don't really have any good solutions. So that's why this talk is, you know, kind of interesting and also fun. So on the execution side, we've got all kinds of things coming out. Parallel transaxle execution is becoming more normalized. You know, you have the SVM. You have, as well, things like stylus, you know, dropping into WASM, stuff like that, as well. My project, FuelVM, we have our own virtual machine that helps with this. You know, and there's all kinds of different ways we can alleviate execution. Data has also been somewhat solved as well. So we post data now as roll-ups. We can post them over Ethereum, right, on EIP-4044. There's all kinds of sharding designs. There's all kinds of different kinds of DA layers that are coming up. So there's lots of solutions in that camp. But with state, there isn't really a lot of solutions. And so when we're talking about state, you kind of have a bunch of information that you really need to store in order if you want to progress things forward, right? So with Bitcoin, you have like the active UTXO set, the unspent UTXOs. With Ethereum, you have things like the account balances, smart contract code, smart contract state, token balances, et cetera. So there's a lot of stuff there. And Ethereum state system is quite interesting, but it's very, very, what you could say is really difficult to deal with. It's very difficult to optimize around. So you'll see there's a lot of different layers to how we're actually both taking this sort of key value store we have with Ethereum that we call like S-door, etc. when you're in contracts and how that filters all the way back up to single state roots, single succinct state roots within block headers. And it's quite enormous, the amount of state that's accrued here. So when we talk about scalability and we talk about state, a lot of people just blame the clients, they blame Geth, they blame these kinds of things like, why don't we just up the gas and screw everything, let's do it. And the thing is, Peter's already addressed this quite well, which is effectively that you can't just do this. And it's not slow because of Geth, quite frankly. It's not slow because of our clients. It's slow because we're all actually, or some people are very, very concerned about the state growth itself, right? And the requirements that puts onto things. And, you know, one way you could even look at state growth here is just like, I believe this is just the accounts on Ethereum, but you can just see that like, you have a ballooning of information. And all of these accounts have to be Merkle-ized, so it's really, really intense in terms of the amount of storage that we need to hold within Ethereum. So how can we address state growth? Basically, there's a bunch of different solutions for this. We can go on and on about it, but some ones that have been proposed before, right? State rent. There's also statelessness, which is sexy to say, but there isn't really a lot of great things underneath the hood for it. Unmerkleizing the state, so you just YOLO it like Solana does. As well, app level compression, so doing things at the application level. You can also just let it grow and maybe we just keep adding hard drives. Verkle trees, you also have some nice things here, right, where they kind of compress a certain amount of state using other techniques, but still pretty messy. And then we have another option, which I'll talk about at the end, which is bandwidth. So basically with state rent, we've all kind of heard this one before. You state, you rent it out, et cetera. Statelessness, just trying to do more things just kind of away from Ethereum state system. Unmerkleizing the state, it's not really a great approach as well. Application level techniques, you can do things more in call data, more things at the application level, but it's still not a great system in general. Here's Tully basically saying, I don't know, here's a bunch of options, but I'm not really sure. Here's Verkle trees, and again, Verkle trees, not really the best solution, because there's still a ton of state that gets accrued, right? So, alt VMs, again, Tully's system, not that great. Fuel state philosophy, we'll go into it in like two minutes, or like one minute, and then we'll go to questions. Thanks. So, fuel state philosophy is very different. We actually don't have any global state trees. We also use a system of native assets. We use UTXOs. We have as well different kinds of models to use state, but use it in the right way. We try to move a lot of the state that's typically within Merkle trees into an area which you would consider bandwidth. And in bandwidth, we actually have a lot more room to play with because everything, once you've moved it over bandwidth and kind of done the proving, then effectively you can just trim it. So there's a lot of really nice things you can do. So applications can be designed in a new technique that we have, which is called native state rehydration. And this really means that across our system, we have different techniques to basically create applications and different techniques to effectively remove state from the system and keep it very lean. These are basically, in our system, scripts, predicates, native assets, and a transaction model. I only have three seconds left, so I'm just going to skip through this. Again, UTXO model is very, very powerful in this setting, and it provides a lot of different options. And this is why in my super crazy lightning talk, I hope you guys enjoyed that. Ethereum, fuel is on Ethereum. It's a layer two. And Ethereum is much greater than the EVM with all these state problems. So thank you. Okay, cool. We have 40 seconds for one question. What about state expiry? What also keeps state in check? Yeah, so kind of, but again, you have this intense problem of a massive run-up of state. The problem with state expiry is that it's not so much that you can or can't do it. It's that basically once it expires, you would need a good system to then refill that state. And it helps, but I would say that there are better techniques where, again, you can use bandwidth. Over bandwidth, you kind of store things, and then once you've kind of consumed that or other people have consumed it, then they can bring it back only whenever they need it. You could say that's kind of like expiry, but I think expiry is too simple. You need a way to both kind of use state and then rehydrate state and then basically take it away. So in Fuel we have really nice systems for this.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:10:00.000Z",
      "slot_end": "2024-11-14T11:20:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1UJnCtYTecznVLrleCgEgafIef7JIuF9xeJmVPJ4TRHM",
      "resources_slides": "https://drive.google.com/file/d/1f7gaG44K3TWFLS4lpyOIbnbYA58VsHN3/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "zk-in-rollups-full-validity-proving-on-the-op-stack",
      "sourceId": "8J8Z7Q",
      "title": "ZK in Rollups: Full Validity Proving on the OP Stack",
      "description": "Historically, zkEVM rollups have been difficult to build, requiring deep cryptography expertise that makes customization and maintainability complicated and time-consuming. With advancements in zk, zkVMs make it easy for any developer to write ZK applications with Rust. With a zkVM, we've created seamless way to upgrade ANY existing OP Stack chain to use ZKPs in just 1 hour. These rollups get fast finality, cost-effective (<0.1 cent / tx), and full EVM equivalence.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Rollups,ZKP",
      "keywords": "",
      "duration": 901,
      "language": "en",
      "sources_swarmHash": "03b821200ebbe047eefcc6138ddffd5d683a7fb0bf2466243e8039984f8ba53e",
      "sources_youtubeId": "11b9vvKiBrY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e5c39dbb7a90e1bb8d44",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735e5c39dbb7a90e1bb8d44.vtt",
      "transcript_text": " Hello, my name is Uma, I'm one of the co-founders of Sysynkt, and today I'm going to be telling you guys about introducing full ZK validity proving into the OP stack. So let's start off with kind of motivating the problem, which is why are ZK rollups interesting and important to the Ethereum endgame? So today, the Ethereum rollup-centric roadmap is how Ethereum is going to scale. And within that, I think it's commonly acknowledged that ZK rollups are the only way that a lot of the problems we're facing today as an ecosystem will get solved. For example, we'll get fast finality, we'll get interoperability across all the rollups, unified liquidity for users, and overall, it'll help improve UX greatly. Inherently, we can kind of think of why ZK helps solve all these problems, because decentralization, with all the different Ethereum nodes re-running and re-executing transactions inherently has some overhead and ZK by giving us verifiability fixes this. So, ZK rollups have been around for a long time. Historically we've known that ZK has been very important. But until recently, ZK rollups have been really challenging. You'd have to write in specialized languages, SDKs and DSLs, to actually encode an Ethereum EVM state transition function in a ZK circuit to be able to prove it. And then, in general, there were a lot of compromises. You had type 1, type 2, type 3, where you can't use the native Ethereum storage format or sometimes even be bytecode compatible due to the limitations of ZK. So how are we solving this? Well, at Sysynkt we're building a ZK VM, not a ZK EVM, a ZK virtual machine where you can just write Rust and then use ZK. So what does that mean? As a developer, you can just take arbitrary Rust code.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:10:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1Dw9W_WUh2DLUhcVkatH257BHYs8yWdxlfLhoJXs8jnY",
      "resources_slides": "https://drive.google.com/file/d/105-JDBcWAEeDhpLVMWJLQdpvGf6kvcrh/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "dacc-closing-panel",
      "sourceId": "HTKUVS",
      "title": "d/acc closing panel",
      "description": "A one-day summit focusing on the theme of d/acc: emphasizing the values of decentralization, democracy, differential accelerated progress, and defensive tech including crypto security, public epistemics, bio defense,  neurotech/longevity, decentralized ai and physical resilience.",
      "track": "[CLS] d/acc Discovery Day: Building Towards a Resilient Utopia",
      "type": "Lightning Talk",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 1888,
      "language": "en",
      "sources_swarmHash": "a151aee7a97386e64e268734842e3cce41c70e7c054c1357989f35436f489bf4",
      "sources_youtubeId": "ux2tIERzBbQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6735e3c19dbb7a90e16a0d8e",
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:20:00.000Z",
      "slot_end": "2024-11-14T11:50:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/15jv-W1ReL9GrekRSr8kZvYKEsNZleXRAf2BtcLW2I5s",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "gas-metering-comparing-appchain-rollups-vs-general-purpose-rollups",
      "sourceId": "KXFHXJ",
      "title": "Gas Metering: Comparing Appchain Rollups vs. General Purpose Rollups",
      "description": "General purpose rollups, with all applications running in the same virtual machine, face specific constraints in their gas metering systems that appchain rollups do not.\r\n\r\nIn this lightning talk, we'll explore the differences and the design freedom in gas metering when your application isn't in an adversarial setting, avoiding potential attacks from newly deployed code. Discover the benefits and challenges of customized gas metering in appchain rollups.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gas,Appchains,Mechanism design,metering,Appchains,Gas,Mechanism design",
      "keywords": "Metering",
      "duration": 355,
      "language": "en",
      "sources_swarmHash": "b2ee4a847959a118c5bafdece36856cc369872f4faa3be0a46189efad3c358a8",
      "sources_youtubeId": "IsMxuWWsH_g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d34b1b0f83434d91b15e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:20:00.000Z",
      "slot_end": "2024-11-14T11:30:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1RCYOul1XxqYV0BU6bMqResTDK6sazsIhKVB2ctdgBKU",
      "resources_slides": "https://drive.google.com/file/d/1U5GFX636xkiBxm-tcZCDY0wiBphJbfZ8/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "tomo-dj-set",
      "sourceId": "3FTAT3",
      "title": "Tomo DJ Set",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-14T11:30:00.000Z",
      "slot_end": "2024-11-14T12:50:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1537a7C9-ILckCdyKNCQyYB-I6Kwu_xrA6i0Sk2-j9eU",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    }
  ],
  "Day 4": [
    {
      "id": "open-source-orchestra-coffee-shop-welcome",
      "sourceId": "RKELBQ",
      "title": "Open-Source Orchestra coffee shop welcome",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:00:00.000Z",
      "slot_end": "2024-11-15T03:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1DTTbLibZzh-i4lar_fk3TZfYIUaEw5RUPBHEEHYhGG0",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "anti-correlation-penalties",
      "sourceId": "DKTUMD",
      "title": "Anti-Correlation Penalties",
      "description": "Anti-Correlation Penalties is a proposal to allow the penalties for missed attestations to vary over slots, based on the number of missed attestations in the respective slots. This is great for non-correlated parties such as solo stakers and improves decentralization, fault tolerance and diversity in the validator set.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Economics,penalties",
      "keywords": "Validators,Attestations,Penalties",
      "duration": 575,
      "language": "en",
      "sources_swarmHash": "f03c15d15e239a44042b04be78d6751150069ca497d8d581ad25fb96ef34594e",
      "sources_youtubeId": "22pwgkdy2ic",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d44874749a4b892947bd",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d52974749a4b892bca29.vtt",
      "transcript_text": " Thanks, I'm excited to present some of our latest work, which is an extremely ambitious project to implement multiple concurrent proposers in Ethereum. This is early stage joint work with my serial co-author, Melesh Pai, who works with me at SMG, Alberto Cendino, who's at Miston, one of the co-authors of the Mistaseti paper, Wakim Ngu, who's at A16Z, who's written a lot of the NoMoreAttacks on ETH proof-of-stake papers, and Goldfish, and Joe Bonu, who's a cryptographer at NYU, who's written a lot about VDFs and other kind of delay cryptography. So why do we get the Avengers together? Because we want to change how Ethereum works fundamentally to be a system of execution consensus separation. So let's see if this clicker plays the video. Here's how execution consensus separation works. We have multiple proposers. We submit transactions to each of them. Each of those transactions goes into an unordered list, and we union them all together. Here, transaction two was submitted to proposer one and four. It's deduplicated. We create an unordered set, and then we apply the terministic ordering function, which is O here. In this case, in reverse priority order. So this is the basic idea of how execution consensus separation works. And the main reason we want to do it is to kill MEV. So why do we think that this will help kill MEV? Because at the end of the day, MEV is about two things, reordering and censorship. So you can choose which transactions go into the block, and you can choose which order they go in. And one of the critical primitives for building MEV-resistant applications is an auction. And it turns out that running an auction on Chain today in Ethereum's current architecture is very hard because there's a single proposer who can include bids. That proposer has an outsized amount of economic impact on what goes into the block, and they end up extracting a ton of rent. So that's why you see these numbers like $600 million a year of PBS revenue to proposers, all due to that proposer monopoly. So here's a formal definition of censorship, starting with what object could be censorship resistant. So a public bulletin board is an abstraction of a blockchain that has two operations, write and read. The write operation has two inputs, a message and a tip. Critically, the tip is very important here. You look at other definitions of censorship resistance, they don't necessarily incorporate the tip, but of course if you have a tornado cash transaction with a $100 tip, it's going to be a lot easier to include it than one with a $0.10 tip. So going into our definition, now that we know what a bulletin board is, we can say, let's describe this mapping phi, which says, given a tip, what is the minimum cost it would take a motivated adversary to censor that transaction? And that depends on the architecture of the blockchain. So since I only have five minutes, we'll skip to a theorem about this, which says currently in a blockchain like Ethereum today, which is leader-driven, we have a censorship resistance of just T, the identity function. You put in $1, you get $1 of censorship security. But if we have multiple concurrent proposers, there's multiple people who can include you, and so we get to the point where we have more censorship resistance, and in particular we can get a linear increase or even more. You can see some more details in the paper. Intuition being more people, you have to bribe them all to exclude the transaction. Getting into Braid, this is the basic architecture. It kind of looks like a DAG, except that there's no cross sub-chain votes here. All votes are on the same thread. We have multiple parallel chains running something like an LMD ghost. And then we take the union of all the transactions in all of the blocks in slot three, for example. And then we take the union of all the transactions in all of the blocks in slot three, for example. And then we apply the deterministic ordering rule. So it inherits a lot of the properties from a traditional LMD ghost. So liveness inherited from LMD ghost and eventual consistency inherited from LMD ghost because if one chain is eventually consistent, then the whole system is eventually consistent at the pace of the slowest chain. What does it mean to be eventually consistent? It means everybody agrees on what the state of the chain looks like. All the local replicas that are honest have that agreement eventually. And once you have that eventually thing, you can apply Byzantine agreement protocol, and you can say all of the honest inputs know what the chain looks like, now we can finalize and that's how basically Asper works. So this is an extension of LMD Ghost, it also works for a bunch of other protocols. I'll stop there because that's my time and take some questions. Okay, thank you very much. Can we give the mic to answer questions? Hi, have you considered or modeled the bandwidth impact of this or considered proposals where you have multiple proposers but not every block, maybe only every X block? Yeah, I mean, the goal for this was really to solve MEB, so we do want it every block. On your second question, on the first question, what are the bandwidth implications? There's, like, naively if you implement this, you get a linear increase in bandwidth because you have linear increase in blocks. You can do some things with the messages where you combine all the vote messages on each of the individual chains into a single message from the attesters. That can reduce some of the overhead, but it is obviously going to be higher overhead because you can't get something for nothing. Hello? for nothing. Hello. I just wanted to suggest that in the every block version, it allows users to make that MEV trade-off where they might have to wait a little bit longer if they want the MEV guarantee, but they can still get it in a reasonable period of time if you have every X block. Yeah, the problem is that the MEV that we're worried about is not for the user. It's not necessarily just sandwiching. We're really worried about the MEV that the protocols leak themselves, so stuff like arbitrage. And so Uniswap can't just necessarily turn off their contract. I guess maybe they could if we gave them the tools to do that. But the problem is you might have some arbitrage opportunity available, and it's available in the single proposer slot, and you don't have time to wait because the game theory says you're just going to take it right away. How does this interact with the encrypted mempool specification that's being proposed right now? Right, so I have a controversial view that private transaction submission is basically inevitable, and encrypted mempool is one way to do that. One nice thing about this property, like this proposal, is that the interface for inclusion goes from I have a set of transactions and an order that I execute them in to I only include a set of transactions. So that's a lot more compatible with encrypted mempool because, you know, I just choose either to include or not include. And the decision problem is not like this huge knapsack disgusting problem that we have today with the builders. And then another thing, like I didn't get to do it because I didn't have time, but we have a bunch of things about how do we keep the blocks sealed long enough for all of the blocks sealed long enough for all of the blocks to be released basically simultaneously. That's a critical game theoretic property for the kind of MEV properties that we want to achieve, and that's why we brought in Joe on the cryptography side. We've been working. There's tons of, there's like four different proposals, commit reveal, commit reveal force open, threshold encryption, and delay encryption, kind of in order of complexity that we're working on them all. We have time for more one question. I'll say one more thing, which is that I have a longer version of this talk later today at sequencing day at, I think, 1 p.m. on the research stage there so if you're interested in seeing more of those details about the encryption, about some more of the consensus stuff, come there and I'll share some more details. Cool, thank you very much Max.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T02:55:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Qq5x2EWSZ2rS2muLZB5exp9AEesPxIA-JBqnKej4-LQ",
      "resources_slides": "https://drive.google.com/file/d/1t0Gi9kOdr46OiOugY7KzRTISx4GeC-LI/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "building-pop-up-villages-that-accelerate-real-world-crypto-adoption",
      "sourceId": "988STX",
      "title": "Building Pop-up Villages that Accelerate Real World Crypto Adoption",
      "description": "Join us to explore the growing trend of popup villages and popup cities, starting with Zuzalu. We'll look at how these short-term communities have helped the crypto world and beyond.\r\n\r\nYou'll hear real stories from Zuzalu, ZuConnect, Edge City, Aleph / Crecimiento, The Mu, MegaZu and ZuThailand. We'll talk about how these villages can speed up new ideas, but discuss where we need to try more things.\r\n\r\nWe'll end by talking about why it's good to step outside the crypto digital world, into the physi",
      "track": "Real World Ethereum",
      "type": "Workshop",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Ethereum for Good,Social,Coordination,Ethereum for Good,Social",
      "keywords": "NA",
      "duration": 3882,
      "language": "en",
      "sources_swarmHash": "822ddb34e2793469b80389b5b20e3bd12779b9c5cad9df9f473811c19c4a77a0",
      "sources_youtubeId": "uVuE6XwnOAg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673862f91b0f83434d72e5ca",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673862f91b0f83434d72e5ca.vtt",
      "transcript_text": " All right, everyone. Thank you so much for coming. Thank you for being here. And welcome to this session on Pop-Up Cities. My name is Timur. We have an amazing set of folks here up on stage, and we want to make this a very interactive session. So we'll be passing around the mic for some feedback from the folks here. If you're tuning in on the live stream, welcome as well. It's so wonderful to have you. So just a quick overview of the agenda for today. I'm going to start with a little bit of context on the history of social gatherings, which will ground us in the context and the lineage of what's happening here. We'll talk a little bit about Zuzalu. We'll share some fun stories, and then we'll get into the relevance of this movement for crypto and crypto adoption. Then we'll talk about the design space for pop-ups and then some of the challenges and opportunities. But I'd love to start, just so you know who's up here, just a very quick set of intros. I'll kick off myself. My name is Timur. I'm one of the co-founders of EdgeCity. My background is in building startups, which I did for about eight years in New York. And then I was a partner at a VC fund for two years. I was lucky enough to go to Zuzalu and had a very kind of personally meaningful experience, which pilled me on this concept. And pretty much from that point decided to do this full time. So that's me. Nicole? Hey, I'm Nicole. I'm currently working on Zoo Thailand. But before this, I helped start Zuzalu. And yeah, that's how I met everyone here. Hi, I'm Janine Leger. I'm one of the co-founders of Age City, and I helped with getting Zuzulu off the ground. So got really excited about this movement last year and have been obsessed with communities and what can happen with social dynamics and how we can actually like progress the future in some way because of it. Hey, great to see everyone. My name is DC Posh and I'm a co-founder of Daimo, DaimoPay. And it actually, Daimo started at a pop-up village, started, like, came out of Zuzalu, which was an amazing experience. And I helped build the V0 first prototype of Zupass. We'll talk about that in a bit. And, yeah, great, glad to be here. Yeah, awesome. So we have quite a range of perspectives here, from some of the folks who really started this movement to DC, who's been one of the quickest tech development stories to come out of this space. so yeah that's that's um that's that's that there so okay i think a really good place to start here will be to talk about the lineage and the history of this movement and the point that i really want to come across here is that humans and human societies have had a long history of coming together to talk about new ideas about how society should be structured. Humans have a lot of creativity and collective agency in this process of shaping society. And it often looks like taking some space away from mainstream society to gather in a space that will allow for discussions about new ideas that can then spread and permeate through the rest of society. These movements have, through history, been critical and very important in incubating new ideas and new technology. And I think this gives us a bit of an idea about what is actually happening with pop-up cities. My personal assertion is that pop-up cities are a new surface area for this type of experimentation, especially for a lot of the ideas that we're developing in the crypto space to find actual meaningful ground for tangible in real life experimentation. A lot of the things we're doing are quite heady, they're quite abstract. And so what we've seen already in the last couple of years is that it's a very, it's kind of fertile ground for tangible applications, for how to live, how to build new tech, and how to ultimately improve society. So I went deep down this rabbit hole. I originally had maybe 20 different examples. I picked five just to give us a little bit of a sense for what's happening here. So the first one is the Enlightenment salons in 17th and 18th century France. These were mostly in Paris. They were social gatherings. They were often hosted by, mostly hosted by women. And they were kind of a safe space for new ideas about Philosophy politics and science this was France at the time was a monarchy But it was the the general prevailing sense of the Enlightenment was is there actually a new way to structure society? Right now that feels a little trivial maybe kind of obvious to us, but at the time it was really very, I mean, literally revolutionary to even be talking about ideas of moving beyond a monarchy. And the impact of these environments was huge, right? They spread ideas from the Enlightenment, kind of the classical ideas we think of about liberalism, like individual liberty, freedom, individual rights, human rights, and this is kind of the birthplace of modern democracy. And obviously, if you study the history of the French Revolution, you see that this was critical in making France a constitutional monarchy for a brief moment, and then obviously a full republic. So that's kind of one example that we can harken back to. Another one that's slightly less well-known, but I think is potentially even closer to what's happening here, is what's called the Chautauqua Movement. So this started in late 19th century US in a place called Chautauqua in upstate New York. But it very quickly spread throughout the country. So I think it's actually a really cool model for what's happening here. Essentially, it was an adult and kind of family education movement. So every summer in this town, they would bring university professors, they would bring folks and families to come together and teach each other. It was kind of like a long unconference that happened over the course of one to two months. And it had folks like Henry Ford, who's pictured here, and a bunch of the other kind of big innovators of the time who would come and spend their summers there. And this eventually spread around the country. So pretty much every city in the US had a Chautauqua in the summer. Some of them happen throughout the year. And it actually kind of approximates very closely what's happening with things like Zuzulu, things like Edge City, Zoo Thailand, where we have these one to two month temporary gatherings that are then now starting to pick up steam again. So yeah, it's just a really cool example of a small idea that became a national movement. The next one is Black Mountain College which was a experimental college it was a reaction to kind of the industrial education movement in the early 20th century it had a lot of interesting folks like John Cage Buckminster Fuller actually invented the geodesic dome while he was at Black Mountain College. It was the center of American avant-garde art and poetry. So it influenced a lot of American culture. But again, it was a permanent place. But for most people, their experience of it was that they would go there for one to two months. They would go deep on some ideas. They would collaborate with other folks who were there. And out of that came a lot of interesting philosophy, art, technology, like Buckminster's work. That's actually him pictured here. It's a little bit pixelated on the screen. But yeah, just really interesting history. It's even less than just 100 years ago. Auroville. So Auroville is an interesting example in India. It's actually a permanent new town that's still going. And it was created as an attempt to figure out a place focused on human unity and sustainable living, and especially human flourishing. There are people that live there full-time, but again, for a lot of people, their experience is ephemeral. So they come for a month or two, they stay there, they learn, they develop ideas. At the time, this kind of came out of the 60s, you know, cultural revolution happening in the us so the hippie movement a lot of talk about peace and harmony obviously the vietnam war was happening at the time um it's it's it's big there's you know 3 000 people but uh they i think they've had over a hundred thousand in terms of like folks that have been there and it's a really interesting experiment that shows that these things can have a lot of longevity over time and one oh yeah so the last one here is the Santa Fe Institute which is a little bit of a more contemporary example but I think gives a really good sense of what's possible here when people come together around a particular concept of study. So SFI is focused on the idea of complexity science. It was started by folks that were part of the Manhattan Project. And yeah, it's a space for people to tackle complex problems. It's a very multidisciplinary research space which I think is very interesting they actually incubated a new way of bringing together scientists and tech people and thinkers from a variety of different fields and finding ways to kind of create collaboration points between them huge impact again it has influenced a lot of scientific thinking in modern times, a lot of mathematics, economics, biology, and especially compute science. So, yeah. And there is one honorable mention, which is Burning Man. I wanted to include this because I think when people think of pop-up cities, this is probably the first thing that people think of. It's a literal pop-up city for 10 months of the year. There's nothing but desert in this place. And then people start building, and for a week, there are close to 100,000 people there. It has had a huge cultural impact. It has spawned really a global movement movement of burns there are now burns in most regions around the world it's obviously quite radical in the experience that people have it's quite different from people's traditional lifestyles i think so i've been going for a long time i do think it's a very interesting experiment but i do want to call out that i think it's ultimately very different to what we're trying to do and I think that's important to note because again it's like a it's it's an association that people have and that they come to quite easily but I see the burn as a chance for people to very momentarily kind of LARP or, um, yeah, like role play a completely different lifestyle to the one that they currently live. Whereas I think the thing we're trying to do with pop-up cities, and obviously there's, there's infinite variety of what people will do with this format, but, um, it's much more approximating what does, what could life actually look like if it was thoughtfully designed. Yeah, so that's that. And the question is, so where do we go from here? We, you know, our generation has a whole set of new problems. Some of these old institutions or movements were the answer to a prior set of problems. We have our own. There's a lot going on, I don't need to tell you. But we also have new tools. There are things that have been developed in the last couple of decades that have reached maturity that are now usable. Obvious examples, Internet AI and ultimately blockchains, which we believe are a very foundational tool for this kind of experimentation. And we'll get into some of that later. There are new experiments starting to happen. There's new ideas like the network state, there's Zuzulu, there's Edge City, Zoo Thailand. And so I really see these pop-up cities as a way to prototype new solutions, like I've been saying. And each experiment kind of brings us closer and closer to what a resilient, inclusive, and flourishing community could look like. Yeah, so that's my little bit. Great. Yeah, thank you. Awesome. That was amazing. And I think this is something so important for us to do generally is, like, look at history of movements. And so much of them have been pretty low-tech. And by the way, if you guys want to come over, feel free to. So much of them have been this sort of lower-tech type environments and really focused on just how do we get people together. And I think right now we've got this very interesting opportunity with like technology has progressed substantially and can actually benefit from and connect to a lot of these sort of low-tech environments to bring like low-tech and high-tech together in some magical way that makes some of these movements like actually long-lasting or spread and proliferate in ways that technology is able to help with. That didn't happen previously. So this section, we're going to have to do an exit because I did a whole update of these. Yeah, refresh. This section, we're going to try and make a little bit more interactive with all of you. We'll go through a little bit of the history of Zuzulu. I think Nicole and I haven't really sat and spoke to a lot of people about what that all looked like and why and how we were doing things. So firstly, before we sort of get to that, one thing that I wanted to profile is this was a really great slide that someone put together. Bob Haywood had put together at Edge City Lana this year, which was just last month. And he sort of was like, okay, what am I seeing, you know, these pop-up, where are they fitting in the stack? And this idea of sort of these pop-up villages being between, you know, we've got hours of time together, a few days together, five to ten days together, and that the 30-day onward mark, or at least, you know, a month, maybe it's two months, we don't quite know that right amount it's like it really is where that like irl community and culture actually start to form so there's something interesting you know when you look at the ethereum industry where if you pack up all of the different crypto conferences that we all attend we're starting to get to that amount of time there's like a real culture that has developed as part of Ethereum. And this, what we're almost trying to do in these pop-up villages is like, can we speed run culture building? And what does that actually look like? And how do we, how do you spend this period of 30 to 60 days together and really develop something different culturally? And something that I think we can go through as a group is just how each of these pop-ups in a way are creating their own culture that is slightly different and unique in some capacity. The other is just kind of where Vitalik ended up really spending his time thinking, was like, we've seen the dates and the meetups and the conferences and that structure we've iterated on a lot. But then universities and cities and towns and countries, those are also very ingrained in a specific way. And this was a two by two that, I mean a graph that Bology had originally put together and Vitalik really kind of looked at this and was like, wait a minute, there's sort of this open gap and we should maybe just be experimenting there. So a large part of this was just like cool that's a space for experimentation and that's where that kind of started and sort of the the next piece is like why like why even begin to start experimenting in that space one it's like this dissonance with governments like a lot of people aren't necessarily resonating with the countries that they're living in the network state movement sort of got that kicked off in in a in a major way along with many other things that were happening um just work environments are actually pretty stale like when you actually start to think about just corporate work environments there was sort of this open question that covert really put in the air of like, okay, people working remotely, what does this look like? As well as a shift from closed source type company environments to more open source ecosystems. And how do you actually build spaces for these open source ecosystems versus a company that's a very clear box, let's all go into this space and work in this office. The other piece is that we've seen both ourselves experiment with At Age City and what we experimented with at Zuzulu and some others are working around is just like the chronic health disease crisis is pretty real across the world. And like healthy living environments has not necessarily been a focus, but it has become a real big issue. And then when you actually look at education, it has just gotten progressively more expensive over the years and not necessarily solved more problems or made people smarter and iterate in an interesting way. And lastly, just like innovation in general is expensive. But those were sort of like starts of some of the why to stop building Sid's experiment. And then we got to where it all started. So do you want to go through a little bit of the beginning? Yeah, so when Zizzle came about, I can say it from my perspective of how I got involved. I ran into Vitalik at a little MEV hacker house, and there were like 200 people around him, but no one was talking to him. I walked up and I was like, hey, I read your recent blog on crypto cities. We had a little bit of a discussion and I invited him to a hacker house that I was hosting to which he surprisingly came. And then there we talked, riffed on everything from like Ethereum related, like account abstraction things to what his personal life was going to like how can we potentially do a crypto city so it's actually last DEVCON in Bogota where we sat down and he finally asked me like for 200 do you want to do an experiment that's 200 people for two months so like the rationale there was there's so much theory going around why don't we just try and do something in practice? And that was a very casual start. And from there, we pulled in a lot of different contributors. Milos, who represented Montenegro locally and helped us connect to, for example, the prime minister, who is a very young world leader, I think the youngest at 35 at the time, and had a background in crypto. So that's kind of why we chose Montenegro for the first experiment. We thought we would get a lot of Visa support. And it was also a place where we happened to have a venue ready to go. And we planned the whole thing in two months. So it was not perfect by any means, but scrappy in a fun way. And then from there, we brought on Vincent, who is a very curious guy. So he knew a lot of people in biotech of which Vitalik was very interested, and he helped be an inviter, and he brought in Janine and Lawrence, and they also helped contribute in, like, well, Janine ended up doing a lot of things operationally, but that's sort of like the beginning of the core team. So the core team helped drive this experiment to start, but I think the experiment itself is very co-created by a lot of different people. So we had tech teams. We'll talk about this a little bit more later, but Xerox PARC helped start ZooPass, which is a really interesting tech identity experiment. And then Althea and PSE team helped start the Zuzulu.city permissionless website. So that became a core basis for how we did permissionless culture. And yeah, a lot of other teams helped build this experiment together. So when we look at some of the original thinking, a piece of it was like, okay, can we make this cheaper than living in a San Francisco or New York? What does it actually look like to build a place that is awesome in some capacity with intellectual rigor as well as interesting people? Because ultimately at the end of the day, we all just want to be around interesting people. And there was a sense of like, okay, can we experiment in that area? So we decreased the price of and had some subsidy to make sure that that was the case. We also were like, wow, eight weeks, long time. What do we do with this? And ultimately decided to sort of like bring in all of these different contributors that Nicole was mentioning to help structure different weeks throughout the time that we were together. One learning that we had from that is weeks are, it's a great way to bring people in that are kind of interested in pop-ups generally, but the longevity of some of that learning then sort of like dissipates once that sort of like concentrated week of programming is gone. So figuring out how we can do like extensive learning in some capacity has been something we've iterated on this year but this was kind of the original start and just brought so many different people in um and this was so valuable because it brought in many different leaders and it made sort of how we started susie not about a team that did it all but about how do we empower many leaders in this community to go and do other things themselves to build and contribute and think about like what they would structure and how they would structure it so there's kind of this tension that exists now that's interesting around how much do you let different groups as we learn and figure out like oh this type of program worked or that type of program around how much do you let different groups, as we learn and figure out, like, oh, this type of program worked or that type of program worked, how much do you create structure around that versus allow people to come in and still build their own unique programming that, like, contributes to the culture of the space and the people that bring it in. This is just an early stage. Like, this was the website to begin with and sort of a start of, like, there's different projects and there's different things that we're working on and there's different ways that you can add sessions to the calendar. And this has been iterated on a ton. So everyone in this room who's, like, who here is interested in building like pop-up city tech or kind of social technology in some capacity? There is so much room for experimentation and there is so much room to build. This was the start last year and now we've got things like social layer and everything that Simplify is doing and stuff that Cursive is doing and stuff that Lemonade is doing and there's all these different projects that are now trying to build and I think it's a very interesting space that is going to have a lot more people operating pop-up villages and pop-up cities around the world and like if we can actually just get a lot of technology that helps with that coordination, it'll be super interesting. Yeah. So with that, let's get the story of Zupai started. Yeah, sure. Oh, man. So my... Okay, all right. My story with Zuzalu started just a bit after what Nicole and Janine were just talking about. And it was through some of my friends at Xerox PARC. I was talking to them about this event. So I heard about it through them. And then I was talking to them about this event. So I heard about it through them. And then I was immediately interested. OK, this sounds like something out of Neal Stephenson, right? This is a lot of like Ethereum Core. I was very fascinated with Ethereum at that time. I was working on a Bitcoin-like client that runs on Ethereum. So it was like something I had been building. And a lot of people in core R&D and people who are digital nomads, and they're moving to this post Yugoslav microstate for two months. This is an incredible thing to be happening. And then a bit into this, I also realized that there was a lot of scramble to make it happen. I was talking to Ivan Shub, a good friend of mine over at 0xPark, and he's saying, yeah, we need to build this zero-knowledge proof-based passport for Zuzalu. Okay, when does this start? Three weeks. So I ended up saying, okay, I can help ship this if I can join, if I can come to this. And then we ended up just in the, sort of like lowering ourselves down the mine shaft for those three weeks and doing this. A lot of good memories. That was really fun actually. Actually, I'm curious. So ZooPass has sort of iterated existed for Zulu number one. How many people here have used any of the zero knowledge proof stuff in ZUPASS? The answer is probably everyone here because DevCon is actually using it. Well, DevCon is using it but I think it's, DevCon is using it in like a scan the QR capacity. Exactly. For Zuzalu, we were actually doing things where, so like one of the original things, where the Z and Z, like part of it where it comes from, is, I mean, you could make zero knowledge proofs that you are a Zuzalu resident without revealing which one. And so we ended up doing some really fun experiments at the event that were related to this around anonymous polls, anonymous messaging, but not internet-wide anonymous messaging. You know everyone who's writing on there is people who you're living with. They're one of the two, three people. So, yeah, I think we'll talk about a little bit more about that in just a bit. But I think one of the ways that, you know, we're here at an Ethereum conference. I think one of the ways this really relates is that pop-up cities, pop-up villages are an amazing way to basically bootstrap critical mass for new ideas. And they're an amazing prototyping tool. And you can try things that would be very hard to get it to work on the open internet and would also be very hard to get it to work with just five people. If you need to do something with a thousand people, this is a pretty amazing way to do it. So DC is the founder of an incredible startup called Dymo. You should check it out if you haven't. But I'm curious, DC, if you're open to sharing how you had the idea for Dymo and then the kind of progressive steps of developing it at progressive pop-up cities. Sure, totally. It's funny, actually. You can connect the dots looking backwards more easily than at the time, right? So one of the things, Zsala was an amazing time. One interesting thing is that a lot of the people there, even though they're like, a lot of these people are people who are like dedicating their lives to Ethereum, right? Like that's like, they're deep in it. The amount that people are actually using Ethereum day to day was not very much. And you would see people, you know, and we sort of were, you know, there's a few of us there who are looking at this and saying like, you know, how do we just make this much easier to use? How to make this more useful. And uh, Daimo V1 came out of that, where it's like, alright, let's make something that is like, uh, you know, PayPal, Revolut, like, WePay, whatever it is, but runs on stable coin rails, runs on rollups, like, this was actually, we were kind of leading it off a little bit, you know, so skate where the puck is going. Like at the time, you know, we were saying, okay, like this is going to be like free instant transactions one to one. We were actually sponsoring like 50 cents of gas for each one, but we knew that that was going to come down. And then, you know, like a year later, 48, 44, now it's like one cent, one second transactions. It's like, okay, if this is about to happen, let's build the thing for that world. And then, you know, like what we're doing now, DymoPay, I think it's funny, part of that came out of the subsequent, you know, ZooConnect, right? We spent a couple of weeks in Turkey right before DevConnect. That was sort of the start of it and then we really figured that out and we had conviction about that later when we went to Argentina. You go to places where people use a lot of stablecoins and where people are doing, I think the most widely distributed version of what you could call real world Ethereum or real world crypto, right? It's places like Turkey, it's places like Argentina, it's we talked to a bunch of what you see in like Western crypto world where sort of like, you know, outside of the like core, like sort of intellectually curious R&D audience. There's a lot of people who just like want to buy something they wanted to go up. Right. Like for those people, though, this is like they're using the permissionless nature of the chain to be able to do things they couldn't otherwise do. I thought that was very compelling. The really hard part there was a lot of those people, you know, are not deeply into self-custody. They're like, yeah, we love Lemon, for example, in Argentina. Lemon being a, like, custodial stablecoin app that will give you, you know, certain, like, nice features that rely on it being custodial, actually, people really like. Same in Turkey, same in Argentina. And so what we're doing now is something that's like, okay, if you want to do anything on Ethereum in one transfer, even if it's from a custodial app, how do you facilitate that? Someone wants to place a polymarket bet, how do you let them do it directly from Lemon? So that's what Dymopay is. So we had our whole arc of what we're up to, closely related to things that we learned from these sort of intensive experiments. I think that's an amazing segue into the relevance of this entire movement for crypto adoption and accelerating crypto adoption, which is what this talk is about. It's such a good example of a product that was developed in the context of where people will actually use it, So inside a pop-up city. And then with progressive pop-up cities, you know, you have like four months, five months in between them, you can actually develop the product. And then the next time you go, so between Zuzalu and ZooConnect, you were able to build something for this new market where we were all going to be suddenly. And for the 400 people who were there in the pop-up city to actually use it, iterate it. They had a baklava stand. We had to use Dymo and you'd spend a dollar on some amazing baklava, which was really, really cool. And then actually be able to start to expand your reach and go out to progressively bigger and bigger markets until you get somewhere like Argentina, where it's like wow not just the people in the city but everyone here actually needs something like this now i wish i threw that picture in the slide deck of of nolan and i sitting there with the the red hats with the baklava stand that was a good time yeah and that that goes to sort of some of the recognition here of this building tech, like what does an ecosystem need versus a company, right? Like with Ethereum is an ecosystem with many different projects in it and this push towards open source and kind of like thinking what does that type of way of working need and way of living need that's different to the current constructive of, um, companies where it's like ship in a space and then send it out to, to customers. It's, we're in a spot of like iteration and, you know, getting feedback, getting people to build in one another's ecosystems getting people to build on and use the open source technology that we've got so these pop-up city environments are a space to test and iterate and share what we are doing and what you are working on in a way that we haven't necessarily had previously. And we're just seeing teams come to life with that. Like they are so excited about this idea of like being able to build a thing, have users instantly that are totally captive and excited to support and build on and connect them with other groups. And there's just sort of this network that grows, which is quite similar to what we have seen grow with the Pop-Up City network. So instead of us being a project that, you know, with Zuzulu there was this kind of open question of like, can we actually progressively decentralize but then have a core? What does that look like? Do we run Zuzulus? Do we not? There was sort of this big open question. And after Zuzalus Montenegro, which sort of how we structured it was, you know, that was chaos. It was absolute. This last sort of last minute of six weeks of planning to give you guys context there, it was like, you know, they had found a spot. But when I came onto the team, it was, we had like 10 weeks before Sousa Lou had started. My wedding was the first week of Sousa Lou. I was very excited to, you know, my plan was like, I was planning other events at the time and I was working at Gitcoin and like my, what I really cared about was bringing together folks to think about how can we fund community shared needs, and that was sort of this focus of public goods. It's like public goods are fascinating for communities. Funding people's shared needs should happen in some capacity, and I wanted to bring it IRL. Ethereum was a space that was just doing this in an interesting way online. But I very distinctly remember coming into the space and saying, like, I never really wanted to have a Twitter presence. I was like, I don't understand that world. I understand, like, the in-person and, like, tangible world, and that's what I want to build for, and it's just going to take longer. But then Zoosloo sort of popped up as this thing and I was like, this is definitely exactly aligned with what I've been trying to think about working on. At the time I was building an IRL community in Austin and then looking at how Ethereum could be supporting that type of structure. So when Vitalik and Vincent asked me to join the team, it was like, oh, this is great, but I have my wedding in 10 weeks' time. So my wedding was the first week of Zuzlu. The team really held down the fort with that, and we all just scrambled to make it happen, and I think what was beautiful about Zuzlu was how much we had to rely on the community from the beginning because we had so little planned like I remember when we started you know people were like accepted you know wanting to join for two months we were like wow someone actually wants to come we've we've got people this is amazing then we're like do we open the discord do we not we don't have any plans yet and there was sort of this push towards like build in public, build with everyone. And that building with everyone sort of led to a culture from the beginning that developed around letting many different flowers bloom and letting many different projects sort of come out of this and many different people contribute. So after Zuzulu Montenegro, everyone was like, when's the next Zuzulu, when's the next Zuzulu? And we're like, well, let's try and build something that's not Zuzulu, but it's just using the zoo to have people have attention, but doing, we called it Zoo Connect. Because it was a lot of us from the same team, everyone was like, that's Zoosaloo. And there was this idea of like, okay, well, I want to go to the Zoosaloo thing moving forward versus like allowing many different projects to come out of it. So there was this hard decision that was made around just decentralize in a way that like generally had no, no call to Zuzlu. And it was scary to so many people because it was like this thing that we had built as a community, was it going to disappear forever? Was it going to like become something else? Like there was definitely a lot of fear there for people and for us as a team as well and it was a decision that was like so beautiful because you start looking at this ecosystem and this is a year in and we've gotten here it's like so from Zuzulu Montenegro there was Zoo Connect that happened that spun off. The Moo is kind of connected to being part of the core actually. This should be a line that goes across here. This is still, someone mapped this out for us and they've sort of spun out a whole bunch of different things that also spun out Megazoo that you've got now sort of Metacamp and what's being built there. Seeing Network State, generally, like, the Network State book is kind of like a core that spun off sort of Zuzulu Montenegro. And Network School was something that, in speaking to Balaji earlier this year, he was not sure what he was going to win or how he was going to sort of build network school but because of Zuzulu and because of Edges Merelda that we did earlier this year he was like I've now got two proof points that sort of have me like wanting to test in this sort of way so there's a couple of things that have spread out from this side of it. Then when you start to look at what we said of Zuzulu as a name shouldn't be used, but like use zoo and get people to build zoo villages all over, being something we wanted to see. There's just been so many different projects that have been built and have their own beautiful culture that's like super, super unique. Some kind of different pieces of culture, like some people are diving into the health stuff. Some people are diving into like, we only want Ethereum developers, and they've got to ship, and they've got to ship the whole time. Some are trying to focus on a specific group of diaspora, like 4Cs is really focused on how do we support the chinese diaspora and like making sure that is the core one of the core values that they hold true so recognizing that like new society gets built with so many different like small nuances but having a you know north star in some way way to help rally people around has been beautiful to see. And that collection, I think, will only take many, many years to kind of figure out who's doing what and how do we connect them all together. But that was a bit of backstory over there. I can add a little bit more. I feel like a lot of things that look like happy accidents were small decisions that Vitalik made in a split second, which ended up making a lot of sense. He was always right at the end of the day. So even at the first ever Zuzulu Montenegro opening ceremony and the way that was set up, all of us were just like seated on beanbags. There were like eight people on a stage. And he already welcomed forks. He was like, I want you guys to fork this. I want you guys to take this however you want. I want to see more leaders. And the reason being, the first Zuzulu was very broad and almost by intention, and there was no centrally set vision, and that was very deliberate because we want people to take this in different ways. And that's been, I think, what's most powerful about Pop-Up Cities is what you've seen is like strong leaders people movements and people that rally around certain values have been able to use this as a conduit and as a framework to build community so we've seen that like a city has been able to build a very strong community and do repeat events we've seen Mu do that Mu is a entirely grassroots sort of fork of Zuzulu and they've done like three four events now and they always involve locals so what we've seen is like individual leaders are good at different fork of Zuzulu and they've done three, four events now and they always involve locals. So what we've seen is individual leaders are good at different things, care about different things, and because of that, they can take this in different ways. So when Vitalik decided to decentralize this, I think a lot of people were scared, but it's definitely proven to be the right choice because we've had 20 plus pop-ups just this year. And if it were just one team there's like a 0% chance we could have done 20 pop ups ourselves. So yeah 20 pop ups all Vitalik did was he started this press play on the experiment and he provides funding so he matched a quadratic funding round or two actually for both events and tech teams and that's been a nice little like pool of money that helps people kick off their own pop-ups. But for sure, it definitely is still biased towards people who are high-context who have done these before. But we almost want to welcome more people to come in. We want to grow this bigger. We want more pop-up city organizers. We want more initiatives rallying around different things. And I think the last thing here before we actually shift to more of a workshop type thing is Zuzula was really just like a seed that was planted. And see it like as a seed versus an umbrella. With so many different ideas that we'd love to continue seeing being built. And different groups building technology to support these. Building actual built environments that can host pop-ups, building operational teams that can support pop-ups in different ways, bringing in different community members to actually drive things forward. It truly was the seed that has allowed many different groups and projects and ideas to flourish. And the hope is that there's just so much more that comes out of this over the next couple of years. I think social technology is such a powerful idea and it does take both lo-fi and hi-fi environments to sort of like bring that together so on that note who has laptops because we had a fig jam planned but i'm only seeing a few laptops in the crowd so instead why don't we do a set of questions and just kind of go around and keep it a little bit more informal questions are also folks i mean i guess something brought you here this morning right unless you just stumbled into the wrong room but even then if you have thoughts or curiosities if maybe you haven't been to one of these you want to build technology for one of them, you want to build one yourself. Curious if anyone has anything to share. Yeah, I can come around to the mic as well. So everyone on the stream. Okay, you shout and then we'll repeat what you say. Yeah, that's a good idea. Yeah. So before we're building a network state in Japan, biology is our first factor. If I want to learn from you guys experience building these short-term communities, we're trying to log in. What is it called? Yatase is a new network state in Japan. Yeah, that sounds awesome. What are you guys going to be focused on? What's kind of a core idea? Our core commandment is, well, our moral innovation is that building is moral. And so the core commandment is we build. And so we're bringing together foreign and native entrepreneurs and helping them do cross-cultural learning and communication over a six-month to ten-year period. Long-term, long-term thing. Starting with one site, and then we're going wide instead of tall. So while he's doing Singapore 2.0, we're doing 100, 150 person sites. So all in Japan? All in Japan. Wow. Cool. The shape of that sounds a bit like Crescimento slash Aleph down in Argentina. I'm curious if that's something that you were inspired by, or is it different from that? Honestly, I've heard of them, but I haven't checked with them yet. So I'd love to thank you for the reference. I'll try to link up with them in the next one, please. Totally. Very nice. And they're doing something... So Crescimiento is a movement that was incubated at Edge City, Denver, in the sauna, actually, which is some lore. So that was around 10 months ago, and it very quickly spun, very quickly grew legs. Props to Juan Benet and many, many others who have been pushing that forward. Janine organized a trip to Argentina like a month later, and then they managed to do an incredible pop-up city called Aleph in August. And the only way that that actually happened was because of tapping into this whole group. Like at Edge City, we were literally in the sauna and Juan was like, I want to, I think there's an opportunity in Argentina. And I was like, you are not Argentine. And neither am I. So we need to make sure that this is something that is led by locals. And the son from the Moo was actually at Edge City Denver. And he was just hanging out. And I was like, son, I need you. You're going down to Argentina in, like, two weeks' time, and you're planning to sort of build a pop-up of sorts down there. Can you start, like, thinking about and looping in different folks? And they were incredible with just, like, quickly bringing together people to do a call. And we had a call to share about, like, the sort of opportunity in Argentina that Juan had sort of, like, identified and some people locally had kind of thought through. And we had this call, like, two weeks later with, like, ten different founders and builders and creators that presented from, like like Argentine folks that were just like amazing. And we had like 300 people on that call and we're like, wow, this is just an amazing community. Like we should go down and spend time down there. So we went down while the move was still happening. Petra was someone who was at Zuzulu and, you know, I was able to be like, Petra, can you help in some way? Who should people here meet? Petra is Argentine, right? Okay, yeah, great. And we had sort of a series of breakfasts and workshops and meals, and there were just people in the community there that were really inspired to do something that we could support get off the ground um and that was something that you know then i was like okay august timeline because this new government um in argentina doesn't have a huge window it's like four years is a really short amount of time like we need to do something bigger in august and juan had spent a lot of time um at different edge city environments where he was just like i want to like use this model to kick off some of this like to spark these initiatives and then it was just like a local team and individuals that were so passionate about the project there was was probably 70 people that were working on it. Other people internationally came in and helped as well. So it was really, again, another initiative that just spun up really quickly because of this network getting started. And the people who are now carrying it forward are within the network, but doing it in their own way. There's no top-down leadership that's driving that. Yeah, I'd be interested, you just said they like the model. What is the model? What state continues? Is it, for example, a principle like ornaments? What is the core? And do you have a playbook for different locations to get started? We've got a playbook. We've got an Edge City playbook. We have an old Zuzulu playbook. I think when we say the model, loosely, long amount of time with many people. And I actually, I've seen some people get, you know, model of like they're like it has to be 150 people to be a Zuzulu and I'm like wait what that was just what we had like that was an idea that you know more than Dunbar's number is useful but I think iteration around that is really important but I do just think like loosely it is you know long time frame around some core values. Burning Man took 30 years to, like, come up with their principles. And I think this is another thing of, like, coming up with principles too early can sometimes sort of pigeonhole you into, like, it has to be a specific way when there's still a lot of learning that we have to do. So you can take a look at the playbooks we've tried to like be more operational on those versus like this is the the principles that you need to follow yeah i think like every single pop-up has completely different values and the overall pop-up city movement is just very vaguely empowering people to do their own experiments. And that's why I think I want to see more people go crazier with it. I want people to completely break the current pop-up city model and do something new that takes off. The original format, I think people just enjoy. So I would limit to grow this to something new. I would say hacker houses, if it's smaller than like 100 people, then you're probably not a pop-up city. You're probably a hacker house. If it's shorter than a month, you're probably more of a conference residency. And if you only have like one focus, then maybe that is also considered a residency. Like the core values of the originals is we're sort of playing at intersections and playing like these new novel experiments. So yeah, the experiment itself is to do more novel experiments. Yeah, a few minutes left. Do people have any more questions, comments, roasts? Yes? Yes. Do you have some online element communities so that after you leave Edge City, that you kind of stay connected and you're sort of part of that online community So to the prior point, we're still very much in the process of experimentation around these things. We are very much IRL maxis in the sense that we feel like 99% of the possible kind of connection moments happen in person, but people have spontaneously started to, you know, gather, do little meetups around the world in the cities that they happen to share time in. So that's something that's being cultivated. We don't necessarily want to have, We don't want to fall into the same trap of some of the mistakes that we've seen some communities make of trying to lean too hard into, like, okay, everyone has to be on Discord all the time. You always have to monitor what's happening. There's a lot of cognitive load that happens there. But TLDR, we're experimenting. We're still going to try new things. Yeah, there's not like one sort of shelling point online community where all of the, even just specifically Zuzalo folks are. Yeah, actually, okay. So we have open town halls that people can join. They happen monthly now. And it's a lot of the different pop-ups coming together and having discussions. So that's like a good place. and maybe we should share that link somewhere and I think like a separate thing I actually think this is a big problem of Zulu is it's like very in illegible like and there's no real way of like engaging in long-form contribution so one thing so I'm hosting a pop-up after this called zoo Thailand and one thing we're doing with that is trying to build it entirely in public so So we are going to have an open discourse, so not discord, just like the standard forum for people to talk about pop-up cities if they would like. So that would be a potential starting point. And yeah, something that we've seen is people have tried to go online often. People have tried to go online after like people have tried to build the community between and it's there's still a lot of iteration that is needed in order to sort of like keep the momentum or actually like provide a lot of value in the in-betweens but I think that there's also the side of things happening more often in person something that I've recognized is projects that have tried to get off the ground in like edge city environments that we've had and even, you know, we didn't really have Zoo Connect I think was a good one here or some of the other pop-ups even with Crescimento where they're like, hey, can you use my tool? Like I really want this tool to be used, but actually I don't have the time to come and be there. And, like, there's a tension between wanting to see a lot of people's, like, projects out in the world and used, and a deep recognition that we've had is, like, people that are trying to iterate from afar and build for a pop-up but not being there have not succeeded super well because the value is in the iteration on the ground. So being at these spaces where you can iterate and get feedback and real feedback and are dedicated to that I think is super valuable. Hi, my name is Jason and I think today super valuable. Hi, my name is Jason. And I think today is my first time I genuinely have a touch point with Zuzalu. So I find it a very interesting experiment. I'd like to ask each and every one of you, where was the last Zuzalu? And what is one thing that caught you off guard so there was a moment where i found myself djing with grimes and that definitely caught me off guard at Zuzalu this was a very strange set of occurrences that had to happen for that to be the case but there i was on the montenegrin seaside with that happening and that kind of it's it's a funny story but it illustrates the diversity of experience that's possible in environments like this it was a diversity that led to me having a very personally meaningful experience where I felt like many sides of my personality were activated in the time that I spent in that space and I think that's kind of a core strength of pop-up cities. They really allow for this just like breadth and depth of experience. So yeah. What about you guys? So I think to this point, it's like pop-up cities. I would actually say that Doozaloo again was the seed versus the umbrella, and the umbrella is more pop-up cities. I would actually say that Doozaloo again was the seed versus the umbrella, and the umbrella is more pop-up cities. Okay. I think a moment that caught me off guard recently was we just did Edge City Lana, which was in Chiang Mai, and it was a really awesome experiment. Seeing the progression just even in the past 18 months of community, of the technology, of people that are building, of how people are understanding these spaces was truly amazing. A moment that I had that caught me off guard was an idea from last year. I was like, oh, we should have more families in these spaces because we don't want to create a spot that no one actually wants to get to the next step of life. Like we don't want Peter Pans. We want people that actually like think about building a better society and it was very much so this recognition of like if we don't have kids we will build a lot of peter pan syndrome and we will fall into the trap of like what companies today and what work environments today have almost fallen into which is like many women having to choose between work or family and it's like it doesn't have to be that way. Like, we should be able to participate in the workforce in whatever way that we can contribute versus, like, the stay-at-home mom syndrome. And maybe it's, I just have a personal fear of being a stay-at-home mom and never want to do that. But the moment that sort of, like like really caught me off guard was seeing that idea be seeded at Zuzalu of like I don't want a future of Peter Pan's, I want a future of people building something, building the future together and being at Edge City this year with many families and one moment of a mother with her two daughters being at a workout class, which was also another thing that I really wanted to push at Zuzalu was, like, making sure we did have health and, like, we're broadened longevity folks, but I was like, okay, but, like, we have to, like, stick, if we are going to say that we're a healthy place, we have to stick to that. They'd put sort of a gym, I remember this, wow, they put a gym like a 45-minute walk up a hill in a tent. And they're like, there's your gym. And I was like, no one's going to go there. You need to optimize for the laziest self when it comes to working out. And I was like, probably 2 a.m. And I was like, you know what? Screw it. We need to put the gym in the town. And they wouldn't let us because they were saying like, well, it's going to mess our little pretty marina up and like there's no way you can. So I like mapped out all of the apartments that we had, like rented out for the time. And I was like, which apartment has the largest balcony? And I was like, fine, I'll stay in this apartment so that like I have this, we can use this balcony space. And I was like, move the gym to the balcony because that was private land that they couldn't control. They weren't stoked about it, but like they had to sort of comply. So I had a gym on my balcony and it was truly like, you know, a last ditch effort at like making sure we had a space that at least people could work out. But getting to see the progression this year of having many, many workout classes, like one aspect that we want with Edge City is to focus on like building a healthy city and having, you know classes having gyms having access to you know you can be active at any given time and seeing this moment of like a mom with her two daughters and a workout instructor that we'd brought out that was that had become like this beloved relationship where the mom and daughter were attending every single morning with with this um workout instructor and she was leave the workout instructor was leaving the next day and both daughters and mom were like in absolute tears after two weeks of being able to do workout classes together in a space and i was like this is just a really cool moment where we sort of engineered like a completely different social environment to something that was normal, but it was kind of this like seeds that had become something bigger. Yeah, caught me off guard. A lot of things, but I guess like this time when Bellagio was playing the network school, there was a moment where I was just like answering all the questions and they were like, what else can we do? And I realized that all of this is just so easy, and everyone just needs to do it, and by doing it, you'll have experience. So seeing that someone like him who initiated this coming back and just asking for operational advice is very surreal. Seeing that there was 10-plus pop-ups before DevCon in Chiang Mai, that was surreal. There's just, this movement has really taken off and I really did not think it would do so, so quickly. And yeah, I don't know, very happy about that. This one's maybe a bit further afield, but one, one experience that definitely caught me off guard. So, I mean, we were, just to make it, not make it sound like this was like a big vacation, people were like pretty locked in. We were working for the most part at Zuzalu. But towards the end, there was a group of us that went wakeboarding, wake surfing. And I ended up talking to this Russian wake surfing instructor. And completely unprompted, he was telling us his stories. And he was telling us about how he did a lot of things off of stablecoins, USDT, like paid his kids tuition in USDT because he's locked out of regular banking, right? And he had escaped basically to Montenegro, you know, at the start of the conflict so that he wouldn't get embroiled in that. And that was pretty visceral. I mean, I'm in a small boat with this guy, right? Really friendly guy, and it was pretty amazing, you know, seeing people who are sort of using the, you know, permissionless nature of the system to, permissionless nature of Ethereum to basically escape authoritarianism and claim some freedom for themselves. That was really cool to see. Alright, I think we have to wrap up there, but thank you everyone so much for joining us. This was really wonderful. And... Yeah, we'll be around a little bit outside if anyone has any questions.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T04:15:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1eO1XIDc-q3KrPnErEQR2cd7nQ0SBTvIxhRw2jsJFAFw",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "challenges-and-learnings-of-utilizing-blockchain-for-cash-assistance-in-wfpun",
      "sourceId": "LCN8XD",
      "title": "Challenges and learnings of utilizing blockchain for cash assistance in WFP/UN",
      "description": "WFP has the largest blockchain projects focused on cash-based assistance: (a) It promotes a private EVM net since 2017 as a coordination mechanism to connect orgs providing humanitarian assistance (size: 4M ppl/month); (b) It is also scaling cash assistance in Afghanistan backed by public blockchain/100k people.  \r\nCome to see some of our challenges and learnings of the last years consolidated in a model and engage to shape the future of this field.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": true,
      "tags": "Best Practices,Emergency Plan,Ethereum for Good,transfer,cash,Best Practices,Emergency Plan,Ethereum for Good",
      "keywords": "humanitarian use cases,cash-based transfers",
      "duration": 1538,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "",
      "transcript_text": "",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1lKUWcEm1t3Bl4RRPCau4E1fatycz3c_Um37TvwammW4",
      "resources_slides": "https://drive.google.com/file/d/11kZAI4amvAcIXsWmOwJICfeW2WEguKAb/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "cls-ethereum-magicians-infinite-endgames-ux",
      "sourceId": "QRG8QW",
      "title": "[CLS] Ethereum Magicians Infinite Endgames: UX",
      "description": "UX has been at the forefront of Ethereum recently, as standards for Account and Chain Abstraction have been gaining significant traction in the space.\r\n\r\nJoin us (literally! This panel will be “fishbowl style”) as we discuss the challenges that we will need to figure out first, such as cross-L2 key management, asset handling and transactions; avoiding fragmentation (liquidity, network, users); coordinating standards across L2s and wallets; and more",
      "track": "[CLS] Infinite Endgames by Ethereum Magicians",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Account Abstraction,Cross-L2,UI/UX",
      "keywords": "ERC-4337",
      "duration": 4905,
      "language": "en",
      "sources_swarmHash": "1beb662626c4c3da4bfa06d36bb302142cc41c8eb388f0e4c578a43f23023264",
      "sources_youtubeId": "Tg5kew6Huzk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cce86982f234a126c7c5f",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673cce86982f234a126c7c5f.vtt",
      "transcript_text": " Hey everyone. Thank you for joining us so early in the morning. We appreciate it. We know it's been a rough night for some of you. So joining us today are Neha from Safe, Mark from Fuse Network, Derek from ZeroDev, Pedro from Wallet Connect, and Cody from Base. So we're here to talk about Ethereum user experience. You know, account abstraction is beginning a lot of attention lately, and that's because we're trying to solve something that anybody who's been using Ethereum for a long time knows is a big challenge, which is user experience. There's always this tradeoff between user experience and security and trying to juggle the two in the right way. It feels like we're on the right path, but there's still a lot of things to tackle, especially as we start looking at cross-chain. Now we have this new kind of challenge. We need L2s, we need roll-ups for scaling, but on the other hand, users need to start knowing which chain they're on and how to switch between networks and doing bridging and all kinds of things that, even as a technical user, it's sometimes daunting to do, so that's not really an option if we're talking about mainstream users. So we gather here a lot of people who are dealing with this on a daily basis to kind of talk about these topics and see where it leads us. So I'll start with kind of like an open-ended question. There are obviously many hurdles to user experience in the space. But in your eyes, what is currently either like the biggest or most important or like most pressing hurdle for mainstream adoption? Sure. Is this working? Yeah. Hey. This may be like not entirely specific to AA, but I think funding is currently a big challenge for our users at least, getting users' wallets funded in the first place. So you know, AA is great for onboarding users and it's a pretty seamless transaction experience, but I think we need to get to a space where native funding is a lot easier using things like debit cards or Apple Pay. Have it be simple to do just-in-time funding for transactions because especially as embedded wallets can fragment users' funds, I think we just need a better solution there. I'm a little biased. I think the funding isn't really necessarily a wallet problem. I think it's a blockchain problem. I mean, I know it's not a wallet panel, but that's how my brain works. And I think on the wallet side, which actually influences the user experience the most, the account management is still the biggest problem. And we kind of have fragmented ourselves because obviously it's a decentralized network and there's no single product manager overseeing everything. There's like thousands. And we need to kind of coordinate that accounts now are very fragmented and reaching truly universal accounts is going to be a problem. Mobile wallets have one version of it. MPC has one version of it. Browser wallets are the first type of wallets. And then embedded wallets come and then they do something completely different. And now accounts are just completely fragmented in different types. And all of these wallet types should just be the same instance of the same account. There should be just one universal account. Sure, you can create multiple accounts because you want a DJ and a gaming one, a social one, or whatever. But the account design needs to be truly universal across all chains, across everything. Yeah, I think if we were talking about the obstacles to adoption, I think we can talk about the adoption of crypto in general, and then the adoption of AA, which are two different things. So the adoption of crypto in general is actually, in my opinion, not primarily blocked by the lack of UX, which there is, but rather by the lack of super compelling on-chain use cases. So I think throughout the history of tech, whenever a tech is solving a great problem, it doesn't really matter how hard it is to use, people try to use it. So the fact that there are still so few people on-chain right now, I think it's primarily because there are still a very big lack of super compelling on-chain use cases. If you are talking about the obstacle of adopting AA, I think the single biggest obstacle is just the fact that today, it is impossible to use account objection on a wallet you already use such as MetaMask. So despite all the benefits of AA, no one wants to move all their assets from their ledger, from the MetaMask into a new wallet. And that's why I think everyone here is super excited about 7.2, which will finally bring the benefits of AA to the average crypto user. Works. So I think that we don't have a technology problem. We have a business problem, and specifically trying to... I think in this cycle relative to the last cycle, things became a little bit more product-led. In the last cycle, it was primarily developer-led, and I think now we see signs of actually trying to solve the business problem of getting users in growing the pie and I think the protocol is not like the main question for why should we adopt you know this technology or that technology trying to do like the perfect protocol is not necessarily a way to sell cut to customers I think we're at this technology or that technology. Trying to do the perfect protocol is not necessarily a way to sell to customers. I think we're at this point where things are complicated enough to hide them under the hood. So I think the best products out there make people's lives simpler. And unfortunately at the moment with blockchain or crypto, it's actually making people's lives harder. And I agree with Derek here. We need more solid, compelling use cases, which actually make, which actually wants, which actually people make want to use of the crypto network. And at the moment, like if somebody really wants to use crypto, they would go through the hurdles of getting the gas, they would go through the hurdles of setting up an account. And these are the people who maybe want to make a quick money, maybe, but we need more compelling use cases. Okay, thank you for that. And so, yeah, there's also, I think, it could also be said that it's kind of like a chicken and an egg problem in some sense. Yes, I agree there's not a business case, but sometimes maybe someone did create a compelling business case, and the UX was just so terrible that nobody could use it. And personally, I think that maybe the one thing that, for what Derek said, that even if the UX is terrible, people will still do it if it provides them value. So I guess that's DeFi right now, right? Because I mean, people were even trading like an Ether Delta back in 2017, even though it was really hard because they saw the value. But I want to expand a little bit on what Pedro said in terms of the account fragmentation, taking us a bit more to the UX. So that is something I'd love to hear your thoughts on because one of the things I've noticed in the space is around embedded wallets. So we made it thanks to account abstraction. Onboarding now, like Cody said, is super smooth, right? It's like you use your fingerprint, you don't know about anything about private keys, you don't have gas with Paymasters, everything really great. We got to the promised land. But now we introduce a different problem, is this kind of like wallet fragmentation, right? We're fragmenting identity. We're fragmenting liquidity. I'm just wondering what is your take on that in terms of like what do we do about that? If anything, it can be done. That bad, huh? Thank you for the mic. I'm going to go a little bit back to the compelling use cases before I talk about the counter-fragmentation. Maybe I'm just being a little too pragmatic, but even if we just cancel everything in the roadmap and we just put DeFi, like Ethereum is going to be DeFi and nothing else, that for me is already a compelling use case. I'm obviously going to be biased by my background in fintech, but we do everything like 10x better. And you need to be 10x better to justify a user to switch. But we're thinking too much from like, oh, it is 10x better as an offering but the accessibility means that you have to climb the mountain with a backpack full of rocks and it's going to be good when you get to the peak and that kind of scares people away because the reason all of these banks actually own all of finance is because you can just go downstairs and like open bank account, even though they have a terrible experience at the end. So I don't think we need compelling use cases. We already have compelling use cases, but we have to ask people to climb a mountain. And in the account of fragmentation, I think it plays a big role into that. If you're telling people that like oh your account is one thing here and another thing here and then i'm offering you the whole world of ethereum but actually it's fragmented at the wallet level then the whole composability offering of ethereum was gone and we might as well just go back to siloed apps. Because yes, we did not silo apps at the blockchain layer, but we did silo it at the wallet layer and the user lives in the wallet layer. So for them, it is siloed. There's no composability. Like we broke the composability at the wallet. So we can scale and make interoperability between the chains, but the wallets are fragmented, period. Like the user doesn't feel the composability anymore. And you feel like that's... Do you have any ideas on maybe how we can tackle this? I spend... I don't know if you guys see my Twitter account, but I write more tweets about ERCs than even about WalletConnect. Maybe I should write more about WalletConnect. But I think ERCs are the way to go like everything that you work in your job whether you're a designer or product or engineer maybe not in the other departments can be an erc like you can literally make an erc out of everything that's the beauty of it and the beauty of an erc is that as i mentioned to you like there's no product manager in the whole ethereum ecosystem everyone is their own product manager in a way. But by writing an ERC, we can share the scope of the project together, and we can write the project scope, and we can project manage together. But if everyone is writing in their little Notion docs or Apple Notes, we're not going to get there. Everyone is just kind of siloing the information and have split roadmaps. I feel like this is something that we can talk for a long time about. So I know we're talking about UX, but since you mentioned ERCs. So I think personally I feel like we also have like a ERC fragmentation problem, just like Wally fragmentation. Which is that I feel like I'm all for ERCs, I'm obviously part of some ERCs as well. But I do feel like sometimes we have the tendency to standardize too early. I think standardizing too early is just as dangerous and just as bad as standardizing too late. Because you are, what's the word, you're like ossifying a particular approach before the market, before a particular solution has even been proven on the market. So I feel like even in AA, you see things about modular smart accounts, where there's a bunch of different competing standards, because people try to standardize too early. And then the products who actually got traction later, after the first ERC was proposed, felt the need to then propose their own ERC, because they actually had a traction. So I feel like sometimes when you do ERCs ahead of traction, you might think you're solving the fragmentation problem, but you're actually creating a deeper fragmentation problem early because now you have different projects fighting between different ERCs. So personally, I actually think that we should try to find a balance between, obviously, trying to create interoperability between the solutions, but also not try to enshrine some approach too early before the solutions have actually been played out on the market. So that's on the ERCs. Then back to your original question about- Fragmentation. The fragmentation between wallets, right? So personally, I'm not too concerned about the fragmentation between embedded wallets because of two reasons. So one is that I work very deep... Our company, ZeroDev, works very closely with the leading embedded wallets players, like Preview, Dynamic, blah, blah, blah. So I know it as a fact that they are all exploring and trying to build global Wallis solutions. So by global Wallis, I mean Wallis you can take to other Devs, Wallis that will be the same across different Devs. So I think they are trying to solve the problem. That's possible by the way, thanks to a lot of the innovations in account abstraction, especially session keys. Because if you want to use the same wallets across different applications, but then still be able to provide security guarantees for the user, you need session keys to isolate the wallets between different applications. So that's been helpful. Then the second reason why I'm not worried about it is because I think I always see embedded wallets as just a temporary solution. Not temporary solution in the sense that there won't be a runs in the future, but rather in the sense that I see them as just like the first step in a user journey. So I think people they get onboarded with embedded wallets, but they would then be, at some point, be onboarded to an actual standalone wallet. So I feel like once, again, Samsung O2 comes out, and once the standalone wallets actually become smart accounts, I think we won't be worrying about the fragmentation problem as much, just because people will actually have standalone AA wallets that they can use. I also agree that... I think that there's no real fragmentation problem because the pie is not getting bigger. And also we're confusing our investors as our customers. So I think that when we talk about account abstraction, we talk about actual consumers. Right now, the way people use blockchains, they need to understand what is the opportunity cost. They need to understand a lot of technical jargon. And I don't think those people necessarily that complain about fragmentation or complain about UX are necessarily the actual end user. They're the investors, they're people that are maybe more technical, and there's a big herd that is following the technical people and the investors. And the pie over the last year didn't grow that much. So if the pie grows and new customers will come, and actual consumers, consumers don't pay for gas, and they don't also want to learn, or we don't want to create new habits. We want to give them the habits they know already. So it's a totally different type of customer that we don't have in crypto right now at a large scale yet. So I think there are like two parallels running here. One for the ERC fragmentation, one on the wallet UX, embedded wallets. So maybe if I touch on the embedded wallet side, I think the UX that it offers for Web 2 users or new users to crypto is pretty good. Like you enter an app, you don't have to worry about downloading some browser extension or having some app on your phone. You're in the environment of an app, you sign up, you have a wallet. Boom. Pretty sweet, right? Maybe for crypto native users, this is not what we expect. We want to be more in control of our funds. But people out there don't really think like that. They actually trust in the app that they use, unlike us, who are skeptical of everything. So I think I would put my bets on embedded wallets. I think they will get better, and probably one of the ways in which we can improve crypto UX. On the other parallel that's running on fragmentation, standardization is pretty, pretty good. And I think there are several standards out there which point towards the same outcome, but different philosophies. And that also creates fragmentation. So it's like an inception of standardization and fragmentation. And how do you come out of it? Like buy-ins, like you either work on these standards together or you lobby outside of these standards, which is also happening. But it kind of creates a toxic, almost a toxic or a very competitive environment in which you want your standard to win and that's quite a zero sum game in my opinion actually I want to just respond to that just one second I think that actually sometimes competition is good and it actually speaks to what Derek said about the kind of like pre-designing everything in advance sometimes this competition between two or three competing standards, I think, can breed good. I guess it's all about how it's done, right? It can be toxic. It can be positive. Sometimes people want their standard to win because they have monetary gain from it. Sometimes it's very ideological because they truly believe that this is the correct way. Doesn't mean they're right, by the way. But you're right. It the way. It's still, but you're right. It depends on how it's done. But sorry, Cody or Derek, you want to go ahead? Yeah, I think I just want to clarify my stance in that I'm not against, I'm of course not against ERCs. And I also don't fault people for creating ERCs early. I think their intentions are good. What I'm trying to say is that I feel like as a space, I feel like we should do a better job at not shaming companies who don't appear to be embracing ERCs immediately. Because sometimes I see a lot of finger-pointing whenever there's a company who's building something not on top of some open standards, and then people are like, oh, why are you not building on top of ERC, blah, blah, blah. But sometimes the reason is just because they are trying to iterate. They are trying to build the best solution for the customers first, for the users first, to try to prove out their solutions first before they make a Yasi. So I just feel like sometimes it's not so much that, the problem is not just that people are creating Yasis early, it's that there's almost like this community pressure of just shaming projects who don't appear to be super enthusiastic about the existing Yasis and not embracing them immediately. So that's my point. Yeah. No, I think I agree with Derek here. I think there's a constant balance you have to strike between trying to meet these standards, but also ship value to your users immediately. Like we only have so many resources to build things. So, yeah, we have concerns around, or I'm concerned a little bit around embracing these standards too early and instead of just building features that our users need right now. Sorry for shaming your cities. I'm super guilty. I literally do that all the time. And I don't even do it privately. I do it very publicly. Like I really push really hard and aggressively to everyone to write a standard or to use a standard. Like, I hope that wasn't like very targeted, but literally it's really important that people understand that the process of ERCs is incredibly malleable. It's not something static. It's not something that you can... And I think you touched something really interesting. Like, is an ERC an outcome or a philosophy? And I think people get too caught up that an ERC is a specific approach rather than a specific outcome. Like, some ERCs that I've worked with have changed, like, four or five versions. Like, if you go through the GitHub history, it looks nothing like it, version after version. But at least it had one number that everyone said, I am part of this ERC. I am sharing this ERC with the community. And one of the ways that has successfully happened this is people don't get too caught up of who are the offers. Like the best ERCs out there have like 10 to 20 offers. The problem is when an ERC has like two or three offers. And then these two or three offers are saying, this is my ERC. And that basically becomes like a philosophical and almost like an ego journey rather than saying, we share this outcome. And I don't think you need to force your product to use an ERC, but I think your product needs to think that it will use an ERC and needs to be part of the journey early of the ERC so that you can comment and say, I don't like how this ERC is going. Here's my feedback from my own product experience. And then you participate. And if you do that, I would add you as an offer right away just because you made a comment. Like, because now this is our ERC. And I think the way people do ERCs is the problem. It's not the ERCs themselves. There's stages, there's draft, there's review, there's final call. Like, there's a whole product management of the ERC, and some people kind of just take it a little too personally. This is my ERC. And I just want to add, don't feel shy about continuing this subject, because I feel it touches on something which is very relevant to the advancement of UX in Ethereum, which is coordination, which is something that I think we can all agree is a challenge right now. But Derek, it seems like you have something you want to say, or I can continue. I thought you raised your hand. So let's take this opportunity about ERCs, just one last thing maybe. There are a lot of ERCs in this ecosystem which are very beneficial for UX. Some of the people here are, I think most of the people here are authors or involved in those ERCs on some levels. So I want to take this opportunity if each one of you can share one ERC that they think is really interesting and important in the UX standard. I know some of you what your answers will be, but I don't know if everybody knows what your answers will be. So it's a good chance to kind of maybe tell people about them. Keep it short, right? This is not about getting on a soapbox and giving a 10-minute lecture on your ERC, but just share what you think is important right now. Yeah, I mean, I think I'll probably give a pretty obvious one, but 7702 is really important to us. We, you know... Oh, is it not on ERC? Oh, man. EIP, shoot. No, it can be in EIP as well. That's fine. All right. We're going to go with 7702. Yeah, you know, there are a lot of users right now using EOAs that obviously we want to give them the benefits of AA, so we need to get those users migrated over. I'll be the last one. You go first. All right. I have a banal example, ERC-20. We tried to kind of innovate a few years ago and use ERC-667. And I think ERC-20 is a good example of simplicity and you add complexity, you add hacks, people steal money. We had to revert back to your C20. And it also shows that not the perfect protocol wins or not maybe the nicest. It's also about simplicity and HTTP2 is trying to replace HTTP1 for like two decades already, and people settled for the good enough. So I think if Ethereum is like kind of the HTTP of money, there's some stuff that need to be bedrock, and some stuff that are a lot trial and error, and think this is like the advantage of permissionless innovation, but some things need to stay simple. So, well, lately I have a crush on 7702 because it's like migrating all EOS to smart accounts. But I think it started, UX started with 4337 for me because it enabled gasless payments, it enabled paymasters, bundlers, batch transactions, basically putting smart in accounts. I'm not super nice at remembering random numbers, but there's an ERC that gets wallet capabilities. Thank you. I like that one. That's from Wallet Connect. But I really like the concept. It makes developers' lives easier. So when we talk about UX, we should not only think about the users, but we should also think about developers, the product people. And that hits the nail on its head. So yeah. What's the name of the ERC with the plug-ins? Six. Six. 7579. 7579. Yeah, yeah. 7579 and 6900. Those are the two. 6900. Yeah, yeah. I think, I don't remember the exact number, but I think there are starting to be attempts. I think actually also one of them is by Wally Connects to solve a very you know, I think actually also, like one of them is by WalletConnect to solve a very important problem, which is the fact that as chain abstracted wallets becomes a thing, it's hard for the debt to correctly read the balance of the wallet, right? So if my wallet has 100 USDC on base. Seven, eight, one, one. Okay. Seven on base, okay, 7811 apparently, yeah. And then, you know, so if my wallet has 100 USDC on base, but then I'm connecting to a polygon DAP, today there's no way for the DAP to know the fact that my wallet can actually pay the money, even though it doesn't appear to have money on Polygon, right? So I think there are a number of different approaches to solving this problem. So I'm generally excited about ERCs in that they try to solve the problem. I'm glad that everyone said a bunch of ERCs that I was going to say, so I can say the last one in my list, which is 7715, which allows wallets to grant permissions for the app to actually act on the behalf of the wallets with certain policies that are enforced by the smart contract. Yeah, so session, you mean like session keys in a sense? Yeah, yeah, yeah, but more like smart sessions sessions because it doesn't necessarily have to be keys. Yeah, okay. Smart sessions, sorry. So going back to what Derek said about the... Yeah, there's a question, but let's not derail it necessarily. So going back to what Derek said about the fragmentation of asset, that's also a very interesting topic as we get to chain abstraction. I'm going to move us away a little bit from ERCs. So when we're talking about chain abstraction and this kind of like fragmentation of assets, so we've seen some interesting solutions come out of it, usually using the word magic. There's magic, there's magic spend, and that's an interesting approach. What are your thoughts on, you know, in general, about how do we solve this problem? Like what you said, Eric, that you go on one chain and you want to know how much balance you have on another chain, stuff like that. Sorry, so the question is how do we solve which problem? So the fragmentation of assets. I'm starting to shift a little bit more to talk about cross-chain. Yeah, I think it's a very active research area right now. I think a lot of teams, I think Zero Dev, one of them, is very actively exploring the approaches. But generally speaking, the idea of a chain abstracted smart account is basically combining two technologies. Smart accounts and intent. So smart accounts and intents, right? So we are starting to have very fast intents-based slash liquidity-based bridges, such as across, relay, blah, blah, blah, that can... So it's like basically, fundamentally, what this chain abstracted, what is that doing? I mean, not often, but a lot of them, what they're doing is they're essentially just bridging your assets through a liquidity bridge and then making the transaction, right? And because the liquidity bridge is so fast, your transaction time goes from maybe two seconds if it was on the same chain, to maybe four seconds if it's cross-chain, but it's still very much within the realm of what users are comfortable with. So there's a really high level, I guess, intro to how chain abstraction smart accounts work. But there are many, many different approaches that I can dive into if we have time. So in this panel, I feel like there's a big focus on wallets, obviously, because of everyone who's involved. But chain abstraction actually comes with two problems. One is the authorization layer, which you just described on the intents. And I think there's been some good work in there. But I think it's fundamentally blocked with the actual asset fragmentation. So the liquidity layer is really hard to optimize because even if you have the best intent structure in place and every wallet supports intents, tokens are completely fragmented. And that's another source of fragmentation that needs to be solved. But I would say that everyone in this panel actually is not default for that because it's out of scope of everyone else's project. Token fragmentation is something that needs to be solved at the bridge, blockchain, and token issuer layer, and that actually would make intents substantially much easier to design because every design with the tents is really really hard to do with like all of the token fragmentation and whether the token is bridge or is a native token and like if it's evm or non-evm it becomes really really hard and that's something that is completely blocking so when we think about the counter traction we really have to think single chain because the moment it becomes multi-chain there's multiple problems to solve one of them being intense and account management across chain but the token fragmentation is quite a heavy problem just to clarify like when we say token fragmentation you're talking about like you're talking about the fact that for for example forC there's like native USDC, USDC, is that what you're talking about? There's actually two problems to it, right? So when you think about the token, what do you guys think? A ticker. Like ticker is the thing that defines a token. So let's say USDC. How many USDCs are there? There should be only one, right? As a user, I only care about USDC. You don't go and think like how many types of euros or dollars or pounds exist. Like there's only one version of it. That's not true in blockchain. Like USDC, even though they did this massive upgrade for USDC v2, where they tried to make it as native as possible across multiple chains, it's still not truly native. This is like probably the most interoperable token in this space, and it's still not native everywhere. So that means that in the tokens which is not native, there's like bridged versions of it. And for every bridged version of it, you could have multiple USDC versions in the same chain. So you don't even know if it has the same address, if it has the same liquidity, if it has the same risk assumptions, because it's bridged through different issuers, and this issuer bridge fragmentation causes a token fragmentation. So I think that one of the things that we're ignoring is, like, the reason L2s exist right now is because people want cheaper gas. But I think different types of customers will use different L2s and there will be different types of transactions and different types of tokens. And I think some people want to move a million dollars across continents and they need the trust guarantees of an L1 and they need to be the most secure way to transfer value. And some other people want to pay $5 for a coffee and they want to be able to maybe reverse this transaction and they need instant clearing. They don't want to put anything on chain because what's the point of announcing a global settlement network that they bought coffee and so I feel like there's like a maximalist approach in in crypto where everything needs to be like I think we we had this debate in 2017 with private blockchains you know it was this false dichotomy let's put everything on the blockchain let's put nothing they needs to be decentralized nothing needs to be decentralized, nothing needs to be decentralized. And right now we're trying to find the sweet spot in the middle, but really I don't see anyone talking about, I don't know, off-chain transactions, really. We're not even at this stage. I think we're at the layer two, and what I'm talking about is layer three, four, five, and finance is built by layers. So we're not even at the point where we actually need to address those issues yet. So that's my take. Like fragmentation is not really like a big problem until we have all those use cases trying to work on the same L1, trying to settle to the same L1. I think you're right that the motivation for the fragmentation was like cheaper gas and more accessibility for these tokens. But the actual problem was caused early on when we're designing everything around the blockchain and not around the asset. Because if you designed assets to be like a primitive of the blockchain then they would actually be treated differently like for example with ethereum we have a very different use case it's the native currency in pretty much all evms so you do not have this problem with token fragmentation as much because it's treated as a special asset because tokens are treated just like any other smart contract. They end up inheriting all the problems that smart contracts have in fragmenting across chains. So if tokens are treated as a special class in the blockchain, you can still benefit from all the cheap or private versions of the token, but still have one canonical version of the token. And that actually is what caused the problem, even though the motivation for fragmentation was cheaper gas, privacy, and all of that. I just want to add something else to the mix when we're talking about this fragmentation. There's also an additional fragmentation that comes about with smart contract wallets, which unfortunately, this wasn't a problem with EOAs due to their simplicity, which is the key fragmentation as well. Now we have additional issues with the fact that you have to deploy the smart contract on each network, and if a network you haven't deployed it on, your key is in a sense could be still valid for it without your knowledge. If you replace your key now, you have to replace it even on networks that haven't been created yet and it's kind of like a funky situation. I'd love to hear your thoughts also on that in addition. Do you think that's also something that we'll solve with additional layers or actually we have to go back to layer one and put everything there or a different L2 or what do you think about the key identity fragmentation as well alongside everything else we've been talking about. Yeah, I'll go real quick. Yeah, I think this is a big issue that we're seeing because users, the way a lot of smart wallets are built right now is just through a passkey that is inherent across all chains because when the account's deployed, it's the owner on the account. But as you want to add new signers and whatever risk profile the user may have, they want to secure their account. We need to make sure that these signers are valid across all chains, which is a hard problem to solve. One thing that I'm excited about is minimal keystore rollups that Vitalik has talked about too, where we can sync these changes across chains. I don't think we're there yet implementation-wise, but I think there's a pathway to get there. Yeah, I think keystores are generally recognized to be the end game to solving their problem. So I think, actually, been chatting with a few teams at DEF CON who are working on key stores. My impression is that I think people are optimistic that by Q2 next year, we'll have production-ready key stores. So I think the one that BASE is working on is actually one of the most promising solutions right now. So once you have a key store, there won't be a problem anymore. Yeah, I think key store roll-ups is definitely the answer. But each day, a new L2 is announced, and somewhere out there, somebody's day a new L2 is announced and somewhere out there somebody is still announcing an L2. And I think the one that brings it all together or converges everything will probably be a key store rollup. Hopefully we don't mess it up again by changing the philosophy or something like that with key store rollups. But divided identity, I'm not sure if it's still 100% going to be solved with key store rollupsups but divided identity I'm not sure if it's still 100% going to be solved with key store roll-ups but definitely an effort in that direction and something I'd say fear very actively looking into one thing that I can add is that trying to imagine how a billion people using blockchains will look like, and trying to kind of reverse from that, like reverse engineer from that, it's really a difficult exercise. And I do think that L2s in the future would be sort of the equivalent of spinning up a server. You know, you can buy or you can rent. So if you don't want to have your own dedicated infrastructure, then you can just use a public cloud. And for open banking, it's a real big challenge to build public banking clouds. So I think that, you know, a billion people using blockchains probably means there's going to be a lot more. You know, all the jobs we see today is Web2. Product is a Web2 concept. SEO promoters didn't exist before Web2. Community managers didn't exist before Web2. Community managers didn't exist before Web2. Web2 created a lot of new jobs that we didn't really imagine even 20 years ago. So I think that the problems that we think are the problems are not necessarily what we need to solve to get to to billion people. So that's my take. It's a bit abstract. Yeah, I just want to maybe add a few words about key stores. I think there are actually two problems with key stores that are relatively unsolved. So one is the economics of key stores. Someone needs to be running the key store operator, and since the key store rollup is gonna be a ZK rollup, the cost of running it is not gonna be trivial. So I think, I still haven't met a team that is building key store rollup, but also has a good answer as to how they're going to make that, to make keeping that key store rollup running economically sustainable. You know, so I'm just curious if like anyone, even in the audience, you know, who has some thoughts about it. I mean, it's very hard to spin up a new blockchain and even an L2. So we're basically kind of just bootstrapping as EVMs, but let's be honest. Do we really need an EVM for a keystore rollup? We don't. Like, we could make, like, the whole cost of a keystore rollup, like, 10 times or 100 times cheaper if it's, like, a specialized machine that, like, virtual machine that literally just does keystore mappings and management. Like, that is the solution. But as you said, like, we have to work with what we have, and we'll use EVMs, and we'll cost 100 times more than it's necessary. But then one day we'll realize, oh, we have this roll-up that runs three smart contracts. Just encode the three smart contracts into the actual machine code. Go as close to the machine as possible and don't do Turing-complete smart contracts. It's all unnecessary. So you're saying basically hopefully like 10 years from now this is just like an inherent cost when anyone is paying for a transaction they're in a way subsidizing the key management of everybody or something like that? Which I tend to agree by the way because that is something I tried to tackle back in like 2018 2019 with Porus and it just didn't make sense. Who's going to pay for this? The user won't pay for managing their keys, the dApps won't. Who should? So you're saying the network should? No, like I'm saying that the keyster rollup, first of all, we're kind of like exaggerating the cost of a keyster rollup. You only need to touch it every time you need to update it. Like if you add the new device or if you add the new chain, that's when you actually need to touch the Keystore rollup. It's not like you need to touch it at every transaction. But the point is exactly that since there are so few actual key updates operations, the sequencer, the operator of the rollup is not gonna make enough revenue to offset the cost of running the- But we just said we're going to do 1 billion users. That's a lot of Keystore updates. That's like a business in itself. But even if it's not updates, just maintaining the data, keeping it high availability for everybody. And there's even the security element around that. You don't want to just... It needs to be on the network, which is... There's a financial motivation to keep it secure because that's the best way to secure right now. Fireblocks was a blockchain. Fireblocks was a blockchain? You got to... Look at him, so happy. He's so happy now. I love that question. I hope Fireblocks likes it. Yeah, for the audience back home, he said he loves that question. Okay, so do you guys want to keep talking? I think that guy wants to say something. Oh, sorry, yes. You want to... Oh, okay. I actually wanted to make a weird plug pitch announcement thing. You were speaking earlier about EIPs and standards, and I always like to emphasize belligerently that EIPs are not standards even if they're final and implemented. Only node level EIPs become standards because they get hard-forked in. Anything above is opt-in. It's never standardized. EIPs are just specifications that went through a process and got published. So UX ideas, user stories, mock-ups, flows can be EIPs. They can go through the document process. ERCs. Sorry, they can be ERCs. Well, can be EIPs. They can go through the document process. ERCs. Sorry, they could be ER... Well, ERCs is sunset. Anyways, they could be published as documents by the process. So you can use Ethereum Magicians to put out user stories, multiple solutions to one problem, definitions of a problem. These are all things that you can do in the open to design in the open or to rally developers around an idea. Something becomes a standard when there are multiple competing companies or even entire ecosystems using the same design. But it being a final ERC doesn't make it a standard. It just makes it a design that was published. So as the guy on ETH Magicians who reads every single thread about UX, just put UX in the title. I will see it. I will give feedback to anything about UX. Like, please design in public, everyone. Zero sub-economics isn't the best way to do design. So you're actually raising an interesting point. The person controlling the question from the audience can scroll down, please. I saw a question before that was actually relevant. It's kind of like a counter-argument to what you're saying. I want to hear your thoughts on that. Can you scroll down, please? So can someone actually vote on that question so it goes to the top? Can you? Oh, yeah. Here it is. scroll a little bit up just a little bit up too much uh no go go back up you had the question it was just like you scroll too much down a little bit more oh there it is so um i guess it starts there so you're someone asked like what's your thought on product like product-led protocol development is erC's convention the reason we have not so many apps? Why no ERC equivalents in Web 2? Product market fit is already so hard. Why focus on standardizing pre-product market fit? I will say that the equivalent in Web 2, I guess, is RFCs. Yeah, but RFCs are also usually something. You don't just spin out an RFC for every little thought that people have, right? It's like usually... Yeah, but it's like, no, but I'm saying it's an interesting question because you might even say, you know, in a way, maybe it dilutes the meaning of an ERC or an EAP. If every random thought in one person's head, is this the way we want to truly collaborate right from every little thought that a person has? I think that's an interesting question. It's a really good question. It's actually three questions. But I think that one of the interesting things about RFPs is that they were monetized by big corporations. There was a lot of HTTP standards. And nobody's doing that with ERCs. No, that's the whole idea. So basically what happened with OAuth, for instance, is that it was created by this public standards body and then Facebook monetized it. And it's literally how it came to be used. OIDC. I think it means OIDC more. Facebook totally conquered OIDC. Yeah, yeah. OAuth or OATC. I think it means OIDC more like Facebook totally conquered OIDC yeah, yeah, OAuth or OATC yeah, yeah I think that Web3 has a lot more rapid a lot more open a lot more democratic approach",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T04:15:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1O8Er1O0dSSedqAxQY9z1pjTRU-Hr46AyiOlBS6Dnvq0",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "cuevm-gpu-accelerated-evm-for-security-and-beyond",
      "sourceId": "PQBKHF",
      "title": "CuEVM: GPU-Accelerated EVM for Security and Beyond",
      "description": "In this talk, we present CuEVM, an EVM executor implemented in CUDA for running a massive number of transactions in parallel. Its primary application is to accelerate fuzzing by testing transactions in multiple sandbox EVMs on GPUs. Additionally, we have integrated it into Goevmlab to support a broader range of use cases. We will discuss the design choices, challenges, results, and future plans to leverage CuEVM beyond fuzzing.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Scalability,Security,Fuzzing,EVM,parallel,Fuzzing,Scalability,Security",
      "keywords": "Parallel,EVM",
      "duration": 1555,
      "language": "en",
      "sources_swarmHash": "2bcf10010103aa3b0336342e4c65e40a351248cac02448aac16b3d5892cb8161",
      "sources_youtubeId": "yhsy0RAkz0Q",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d06874749a4b891b95de",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d06874749a4b891b95de.vtt",
      "transcript_text": " Morning everyone. Okay, it's my pleasure to start the session today. And yeah, my name is Minh and I'm from the parallelizing on the stuff and today I will introduce about our work on parallelizing EVM execution. So yeah, let's go into the details. So this is a very high level overview of what this talk is about. Basically, we have GPUs, very powerful accelerators, invented decades ago, mainly for gaming, but then it found its use cases in other domains as well. People have used it for general purpose computing, scientific computing, and recently AI picked up the trend. That's what made NVIDIA become the largest company in the world by market cap. So yeah, on our blockchain domain, we found a use case in mining. And you have seen people use it for mining Ethereum for a long time before the merge, but then after the merge, a lot of compute power on all of those GPUs. Some of them come back to become AI training inference, but some of them find that new use case in GK. So you have seen quite a few talks in this DEF CON about accelerating prover, generating proof faster using GPU. But this talk is about something different. So we want to find, okay, those GPUs, accelerator power, what can we use? What else can we use it for to help the Ethereum ecosystem. So we think about one more use case beyond GK to run the transactions in parallel. So nowadays, most of the transactions you can see is not just simple transfer from one account to another. So the transactions now are more complex smart contract executions. And those are quite compute intensive sometimes. That is also one of the reasons for people to set up like block limit and block scale limit so that the computation is bounded. So, you can verify, run all the transactions in block time. But we don't intend to use this work to run your transaction in mainnet. We found another use case in fuzzing to secure smart contract. The use case we found from our work when we built our in-house fuzzing tool to find bugs in smart contract we got the chance to run and test and also compare with state-of-the-art work in academia and also in industry and we found that there's a very high demand to run transaction and test and you get some feedback you get some result so you can finally the main goal is to find the bugs in smart contract to secure your application and GPU is just the main like just the the the best like accelerator for you to run this kind of thing in this use case because it has the massive parallel programming model you can run like thousands of threads and how about we map those thousands of threads into thousands of EVM instances running parallel so running transactions in parallel first we focus on the niche fuzzing use case, but then along the way when we implement our project, we were approached by different teams, startups, they all building cool stuff, and some of them use some of our source code to fork out and have their own private repo with their own amazing use case in the ecosystem. But this talk will be mainly about fuzzing. I will have one slide about the other use case in the end. So what is fuzzing? Just a very simple high-level understanding. So fuzzing, the main purpose is to find bugs in software. It's one of the popular techniques in software engineering. And recently, there's a trend for people to apply it to find bugs in your smart contracts as well. So this is a very high-level depiction of gray box fuzzing. Basically, you have a tool that generates a lot of inputs. Those inputs have more coverage and more complex than your unit test usually you use. So it's more complex and eventually it's an iterative process. You generate input, you execute it, and then you get feedback. So those feedback can help the further to like generate better input Eventually you can find a input that can trigger the bugs in your smart contract or in other software so the input at so if you map it into Ethereum transaction to test my contracts the inputs are the state and transaction to interact with your DApp, your smart contract. And then the feedback can be traces, branching data, or the updated state after you run the transaction. And those are up to the first tool design and implementation. And yeah. So one example on the right is how they instrument the EVM to catch bugs. I just show very simple example. Some of them are not like the add overflow. It's not very, like it's not exist anymore after Solidity 8 because they inject by code to check it before. They try to revert it before the overflow happened. So for example, if you want to catch overflow, you just instrument the EVM every time you see up-code add, you check the operand, if it's like over the 2 to the power of 2, 5, 6, then it's overflow. Or, another interesting up-code is jump-I, all of your if-else condition in your smart contracts or required statements, it will be encoded into this XAMPP I and those will be the helpful hints for the fuzzing tool to further generate better input. I show one example of the run after this. So this is fuz thing on conventional iterative process on your CPU. Then we try to map it into CPU execution. So you see, our fuzzer can generate thousands of input. Instead of iteratively one by one, now you can generate a bulk of them, like a batch of thousands of them. And then you can offload the execution of thousands of them like a batch of thousands of them and then you can offload the execution of thousands of them into GPU. So this is the main idea, the main proposal that our project is trying to achieve. So after we offload the execution to GPU, we can map one or few threads in GPU to run this transaction, simulate the EVM, and then you get the feedback back to the version on the CPU and try to update it in the node state strategy, generate better input. And yeah, that's about it. You have to work, you have to have some communication between the accelerator and the CPU side, and they all work together to find bugs in your smart contract. Okay, so we offload the EVM execution to GPU, and the EVMM execution basically you execute one transaction and then you're given the world state so this is a very high level of what is running inside. So you see nowadays like transaction interacting with smart contracts are quite complex. They have dynamic behavior. It's not just one single AVM call you They have dynamic behavior. It's not just one single EVM call, you run and you exit. It's actually every time you have a call context, and then each of them have the volatile machine states like stack memory, and they have their own size. And it's evolved over the running at the runtime. We don't know beforehand. So this is also one of the challenge when we try to implement on GPU we cannot just pre-allocate does for some of you know that stack size maximum stack size and maximum column depth if you pre-allocate everything is this gigabyte so and we don't need to do that at the beginning so that is one of the challenges. And when you have call context, and then you can create a new call context, or you can revert back or exit and update the parent state, depending on the result of the child call. So these are the high level. And when we map to the GPU threads, actually, we only consider each of the current context. So, when you enter a context, you have your own stack memory, volatile machine states, and then it will enter the execution loop. Basically, you just keep fetching new opcode and increase program counter, executing the logic. And then eventually, this context either return back, reverse exception, or create a new context. And they keep looping. And we have the decision of how many GPU threads that we can map to execute one EVM instance. And imagine we have to run thousands of them. So the decision is actually dependent on one of the libraries we use. We use a CGPN library from NVIDIA Lab to reduce the complexity of development at the moment. So they have the limitation like how many threads we can configure to compute 1U into 5.6 arithmetic operation. So imagine it's like some big library that you can use out there. And also because they have some limitations, so we currently have a plan to develop our own library for this. They are developed for very general kind of use cases. You can configure the bit width, but then for our use case, you only need to use 256 Uint. So yeah, we can optimize a bit more on this. So this is how you map the EVM to CPU thread. And after we implement mapping using those libraries, and you need to ensure it's correct. And for this one, when we developed this one, we didn't know much about the tools in the ecosystem. So thanks for the advice from Ethereum Foundation, we actually need to implement and compare with other EVMs as well. So we integrate with GoEVM Lab, which is the tool developed by core developers of Ethereum, and that is available there. And we make sure our trades are compatible with all the EVMs as well, then we compare line by line. So we use EIB 3155, make sure it's output correct. And then we compare, and then we use ATX tests to compare. And then currently we pass on the tests in functional folders, the important one. And the one I highlighted here are the ones that are most time-consuming to us, recombined contracts and zero-knowledge. They are just a few recombined contracts, but the logic and the time we spend on them is quite a lot. So, yeah. EVM, of course, is simpler than this. And some of the things we can use open source, but some of them we have to develop on our own, especially easy pairing. Actually, we copied from the Python library from Ethereum, so we write it in CUDA based on the logic there. So after all the tests, the rare corner cases remain, and mainly gas difference. So if your phasing use case is not reliant, the logic is not reliant on gas termination or some of the logic, then you can actually use it correctly. So we are still trying to fix all of the remaining gas difference. So after we are satisfied with the correctness, then we think about optimization. So because time is limited, our developing time, also the time of this talk, so I just pick one of the optim try to optimize the normal transaction that you usually see out there in the real block, in the real transaction. What are the popular, of course, what are the things that are critical to performance, what are the things that you have to execute all the time, so we optimize them first. Basically, we check the statistics. And then we found, OK, it's quite intuitive. It's quite obvious that the stack operations is almost everywhere. You have to optimize the stack first. And then after that, we also check the stats of the stack size. What is the normal stack size that usually transactions use? And what's the stack size, what is the normal stack size that usually transactions use, and what's the memory size, and the program counter, it goes up to 8,000. So we found that, okay, we cannot cache the program counter, it's too much. Sorry, cache the bytecode, 8,000 bytes is too much. But then stack size 20-something, we can allocate the fixed size stack on fast memory. That's what we did. So we should pre-allocate that one first. Very fast stack on share mem. And then after your stack is bigger than that, you can use a slower memory as well. So most of the transactions you use this, you don't use a lot, you don't use a lot you don't use last stack that if we run very fast that's one of the main optimization that we're trying to do we try to optimize for conventional common transaction you can find out there there. Okay, so current implementation, we are happy with one beta release currently. So basically, we support Shanghai. We started with Shanghai, but we couldn't catch up with new EVM hard fork. It's quite a lot of EIVM implementations. So we stopped at Shanghai first, and then executable, we output the JSON trace, and then we have two versions, GPU, GPU, and we also have the share library, so you can use it in two modes. One is you use this share library, it's open source now, and then you can use it to try on Google collaboration. The Google collab, you can use GPU there. So yeah, you can use it in two modes. You can write your Python code to interact with it. And also you can use executable. Yeah. So performance is still depending on the workload. So it requires more comprehensive benchmarking. But yeah, we compare with our own CPU version. It's faster. But one thing is that our CPU version is actually slower than the current EVM implementation out there. So it's not like directly Apple to Apple comparison. And yeah, we are still improving the performance. So this is one of the sample run in fuzzing in real, in action. So you can try Google Colab, the link in our GitHub label. And then one example is a toy example in smart contracts with some of the very obvious bug there and some of the input guarding if conditions, so to make sure that you do some real fuzzing, you make sure you get the correct input. You can run it and eventually it will output the line, the source code line, and the input that trigger the bug, like you see, input equal 400,000 or something, yeah, 40,000, so input four, five, six, seven and trigger the bug. It can show you the source code, the line of bug, this one running, the execution running in GPU. So besides detecting bugs, we also have feedback. So these are feedback feedback branching feedback. One example is this function test branch. It's like there's a condition for input to be 1 million, and you see the current input and the distance before you can break that branch. Those are the gray box fuzzer technique that we implement in here. Those are quite common in gray box fuzzing tools out there. And our current release, the bottleneck, still remains in the way we prepare transactions and we get back the serialized data to update the state of our fuzzing tool. So it's still slow. But we have experimental branch where we remove all of the complex logic. And of course this branch does not conform with the yellow paper anymore. But then it will reach very high throughput. It can reach 60K transactions per second and improving. And we saw some teams have private repo, and some of them clone or fork from our repo with private implementation and optimization. They can reach 100,000 transactions per second as well. But we are open source, and we're trying to make sure it's still confirmed with the Ethereum standard. And beyond fuzzing, this is some of the interesting use case where we talk with other teams, other teams doing cool stuff out there, but they are doing the layer 2 and also fuzzing. But the main thing when we were thinking about parallelized transactions for Ethereum execution client, is it possible? It sounds cool, but then it's not very practical at the moment because you need more transactions. Currently, how many? Hundreds? You need thousands to have, to exploit that parallelism in GPU and also the transaction they are all different they are not very similar so it's not easy to actually speed up on GPU and also to achieve this kind of thing you need more client support like the GPU, you cannot just get that terabyte of workstations. It's not possible in the minute. It's too big to run. And also because of memory content, and also the client needs to ensure it's safe and correct to run those transaction parallel, which it's not. Because transaction, you need to run sequentially one by one after the other, and eventually to get the final state. This one runs in parallel. You have to make sure that all the nodes running it reach the same deterministic output so they can reach consensus. Otherwise, it's not possible at all. So that's what we think at the moment. So we still can find other use cases besides trying to speed up Ethereum execution client. We see other layer 2 teams also can try to have their own execution engine. Also, transaction simulation platform. I saw people use it to simulate swap, for example. And imagine you have a lot of similar transaction testing simulations. And this one is a use case for it. Yeah. You can simulate millions of swaps in a short time, serving a lot of users. Okay, so the last two slides, I want to talk about our team and collaborators. So yeah, working with me, also we have a small team of three researchers and engineers. Working with me is Chung Lee and we are from the Singapore Blockchain Innovation Program, NUS. Also working with me was Dan. And we worked in Singapore before, and he went back to Romania to become professor. And, yeah, we want to thank Frederick and Ethereum Foundation for the advices and the grant support, especially the advices to make sure it's correct and how to use all of the new tools in the ecosystem that we were not really familiar with. Thank you. Yeah, so that's all about this one. And you can find out on the GitHub. And yeah, just feel free to create PR open issues, these questions, and reach out to me. I will try to help. Thank you, Minh. And thank you for this amazing presentation. issues, these questions, and reach out to me. I will try to help. Thank you, Minh. And thank you for this amazing presentation. So I have a few questions. Let's go through them. Let's take the first one. How much work would it take to integrate this in an open source fuzzer like Echidna? You can use it. Currently, we released the shared library. You can use it right away. But the thing is you need to modify your fuzz. Our example toy fuzz, we have that bottleneck to repair thousands of transactions before you send to GPU. That one is quite a big bottleneck. You run it on GPU, it's fast, but then you prepare for it, and you run, you collect the result. It's still quite bad. So currently, we need time to improve the interaction between the library and your fuzzing tool. So it works, but you need to work on piping. Yeah, the interaction, yeah. Okay, on plumbing. All right. Can QVM plug into a simulated node tool like Anvil or EVE Tester? Yeah, so in the end, we need to make sure it's possible, but we need to make sure it's compatible for API calls. So basically, we provide that library. If you have your own adapter, your requirements, so you just send us the state and the transaction you want to run, but make sure the state is not so big. We can manageable, like transfer it to CPU and keep it there. Sorry, GPU memory and keep it there. And it's possible to transfer the state transaction, run and return it back the result, whatever result format that you want. Fantastic. Thank you want. Fantastic. Thank you. All right. Third question. Would fuzzers have to change to adapt to your GPU acceleration? It seems like this is a different version of the same question. Let's mark it as answered. All right. Fourth question. How does the GPU thread get the state from one executing S load? Well, that's specific. Okay. Do you want to take the next one? We have the data structure that resides on global memory. For the state and memory, we have to keep it in the slow memory. We don't do any caching. That one, we keep the structure pointing to the state and we search through that. Basically, if the state is big, so this is one implementation detail. We didn't implement very fast searching in the states. Basically, we have to go through on the array of accounts. So if the state is so big, currently it's quite slow. But yeah, that's how it works. So everything on global memory, we have to search through them. Fantastic. Thank you. All right. We still have 10 seconds, so maybe a more playful question. Have you considered calling it CuteVM other than 3VM? I don't know. But, you know, suggestions, you know? Yeah, yeah. We may change it to this theme. Wonderful. I mean, thank you for your time and all the work you obviously put in your slide. Looking forward to your work. Yeah, thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1abSiS9Ilz8g4Nc9doFglzH8ruOPatELbzUm3z4IqpRE",
      "resources_slides": "https://drive.google.com/file/d/1h5eEqI2PO5I3vi-Oy7ZPE5InVRBtFOuA/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "defcon-at-devcon-a-table-top-experience",
      "sourceId": "P9DRXY",
      "title": "Defcon at Devcon: A table top experience",
      "description": "It's 3am and your phone is blowing up—Telegram, Signal, Discord, X—all are saying your project just got rekt. Your team is panicking and begging you to sign off on a quick protocol upgrade. What do you do?\r\n\r\nJoin our workshop to get hands-on with crisis management in web3. Learn to handle attacks, keep cool under pressure, and manage your stakeholders. By the end, you'll turn this crisis into manageable challenges, protect your project, and keep building.",
      "track": "Security",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Best Practices,Hacks,Event monitoring,threat,intelligence,Best Practices,Event monitoring,Hacks",
      "keywords": "Tabletop,Incident Response,Threat Intelligence",
      "duration": 7193,
      "language": "en",
      "sources_swarmHash": "c08211a236fcdd3c9d7cbd794a66f4f2876cef2110fe3e59d6b3d4e31cdc8d09",
      "sources_youtubeId": "JfmMhs_3MYc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6738636120d1f9ac48bd15b5",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T04:45:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1s2NkLIuneQtBUvfLLlkFlOE3IWetDHM6-4OAQMItN-0",
      "resources_slides": "https://drive.google.com/file/d/19gf4SRrr0gnNrZC2edSzmlB3w2quGwEM/view",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "discovery-the-tool-at-the-core-of-l2beat",
      "sourceId": "G9ESC7",
      "title": "Discovery - the tool at the core of L2BEAT",
      "description": "Hands on workshop about how to use an L2BEAT tool called discovery for mapping, researching and monitoring all the contracts involved in a project. We'll start by introducing the problem that discovery tries to solve and after that we'll get into trying to understand the architecture of a real world project by using the avenues this tool gives us. After this session the participant should feel empowered to use discovery to deepen his knowledge about all on-chain deployments.",
      "track": "Developer Experience",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developper",
      "featured": false,
      "doNotRecord": false,
      "tags": "Architecture,Tooling,DevEx,Event monitoring,research,DevEx,Event monitoring,Tooling",
      "keywords": "Holistic monitoring,Architecture research",
      "duration": 4792,
      "language": "en",
      "sources_swarmHash": "2446734d4de4327fb1de791a429823052d3a3669c0f5d37607ec93e2160ffec5",
      "sources_youtubeId": "azowA66W5UY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6738656320d1f9ac48c07177",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6738656320d1f9ac48c07177.vtt",
      "transcript_text": " Hi, hello. So I'm Mateusz, or you can call me Matt, like the short for Matthew. I'm from L2Beat. I'm half of the tooling team in the L2Beat. And my workshop is on the discovery. It's the tool we use, we build, to help us research projects, and it turned out that it is really useful at solving all of our problems. I mean, not all, but like majority of problems I'm going to later talk about. So, a funny story is that I almost lost the demo part of this presentation. And the only surviving copy of it was like some random Vim buffer. So I just wanted to point it out. Okay. So there is a lot of projects on L2Beat. You know, the amount of projects is growing. And we internally expect there to be at least 1,000 L2s. So currently you can see that there is 51 rollups and 66 validium and optimiums. So around let's call it 110 projects of only L2s. We also track bridges. Not to the same standard, but for all of those old tools, we have a minimum bar of data we want to show, and that would be the risk rosette. We want the risk rosette to be obviously correct, and the standard for this data is actually quite high. We want to show data that is correct correct so you don't lose your funds, you don't make a decision based on our data that is incorrect. So how to even maintain all of this, right? So just like a quick math lesson, right? So imagine that we have 110 projects, and every project has a single update every two months, right? So it's actually quite sparse. We see projects that are updating way more often. And even if every project had a single update every two months, we would have an 84% chance of seeing at least a single update every two months, we would have an 84% chance of seeing at least a single update in a day. So it's basically guaranteed that every single day something new will change on chain. We need to be able to detect it, see it, see what's changed and act upon it. And also, like, even if we assume that an update happens every six months, the chance is still 45%, right? So it's like a coin toss, basically. And these chances are basically always working against us. The more and more projects are going to be added, the higher those chances are basically to be 100%. So what is our needs, right? So like I said, there is a high chance of an update happening. And we need to be able to fastly see that it happened. And we need to be fast in reacting to it. If, for example, our optimism, right, any of the top docs has an update and we take like three weeks to process that update or to even see that an update happened, then we are showing incorrect data for three weeks, which is actually a quite long amount of time when it comes to Web 3.0. When an update happens, we need to know what changed. So it's not enough to see that something has changed. We also need to know what has changed, right? So did any of the risks, assumption that we had before, are now different? So also, we need to be able to look inside the project and compare it to the previous version. And as always, we want to automate as much as possible. You cannot get to 110 projects listed on a website and do all of them manually. It is possible with human effort, but we don't have the amount of resources to do it. So right now we have four researchers and we maintain all of the 110 projects with only four people and we also manage to do stuff that's additional. So discovery essentially, I think, is at the core of solving all of those problems. Of course, discovery is not a panacea. It does not solve all of the problems by itself. But of the problems that I listed, discovery is always at the core of the solution. So the mental model of discovery is like this. I like to think about discovery this way. And also I like understanding things from the low level. So let's try to understand the inputs and outputs of discovery. The input, of course, is the Ethereum state. We want to see what is the state of each project on a given, let's say, block number. So the input to discovery is that Ethereum state. Of course, we are not sending, like, a snapshot.zip to discovery. We are doing something like... I mean, we use RPC providers, which is essentially having the entire Ethereum state at your fingertips. You can do any call you would like. And to facilitate the querying the data from the state, we have a config JSON, which instructs the discovery program, which data we are interested in and how to process it. And after passing those two inputs to discovery, an output is generated, which is a discovery JSON file. Of course, it's all a simplification. The more, like, the actual mental model, which I don't even think is correct, and there's missing some parts. It's more something like this. So that's what I mean when I say discovery is at the core, right? All roads lead back to discovery. I'll try to, during this workshop, I'll try to touch upon all of the things that are listed here. So you'll be able to understand at least how this flowchart happened. So since I myself am a visual person, I want to create, I want to show you a demo of something nice to look at. So this is something we have been working on, which will enable us to move from command line to a graphical user interface. So right now we are calling it Discovery UI. And let's see, like like all of the projects that are listed here are projects that we are tracking with Discovery. So I have picked Zora for the project we are going to be using during this workshop. So let's see what kind of data we can expect to see in this tool for Zora. So of course there is like a lot of things to take in. But let's start from the left and keep going right. There are on the left, the thing is that they look like files, but they are actually contracts. They are kind of inspired by the file view in Visual Studio. And you can see, maybe, I don't know, the contrast is not the best, but I hope you'll see that. We have two contracts, which are the initial contracts. And by initial, I mean the contracts from which we are going to be starting and all of the contracts that are on the left have been found based on those two initial contracts. So it is, as you can see, like it is quite useful to be able to find two contracts used in a project and then basically find all other contracts used by that project and also this updates automatically so if anything new happens, new is added or is removed, they are automatically updated. Each of those contracts has some values, right? So each contract has an address, a name, a description given by a researcher. And let's focus on the fields for now. So fields are state variables variables that we have found in the ABI which are either public variables or functions which don't take any arguments and just return something so we are trying to build the state of the contract through these. There is also one more part which will be important later during the demo. I mean during the workshop part. Which is that we try to build arrays from functions which take a single argument which is of type integer. So we just assume that this function is like get something by index and we try to get all of those things. And you can see that there are actually addresses inside of those values. And those addresses lead to other addresses. And this is how discovery works. It gets all of those state variables, and if it finds an address, it assumes that this address is also connected and just keeps discovering on those addresses. And the third view here is like a graph view. So all of those projects here are used in Zora. And the way that it works is, for example, let's focus on the security council. Maybe that's not the best. But the system config, right? So you can see that there are some state variables and they point to other contracts. For this view, it is really easy to understand how the project is built, what contracts reference each other, and how the data flow inside the project happens. Of course, this is not the default view. Like the view allows you to select them, move them around, you can color these, whatever your heart desires to make it easier for you to understand. We have two layout algorithms since it's a graph view. You can use the D3. We called it slow because it is not that fast. It uses force simulation to lay out the graph. We also have more like a hierarchical view which just lays the graph from left to right. And the third thing is the code. So all contracts have source code. And let's go back to the L1 standard bridge example. And we want to show the code to the researchers because to understand what a contract actually does, you need to look at the source code. And you might be weirded out by the fact that there are only two files. I'm going to touch upon this a little bit later because it is actually quite important why there's only two files and not like more. So you can just view the code in here. The part that I want to also show is that we see that it is important to be able to switch between views, right? So I can click L2 output Oracle on the left in the list view and it is selected in the values and the nodes panel. Vice versa, I can select something in the values and it is selected in the list and the nodes view and I can select something in the nodes view, and it is selected on every single audit review. So this is something which is really graphical and nice to show. And it basically is only the look inside the Discover JSON. This is just a nice way to visualize what's inside the Discover JSON. But it doesn't touch upon the way of how we even got that Discover JSON. So it is something we are working towards. It's not yet ready. You can only view things. It's basically read only at this time. But in the future, we hope to make Discovery this, so you'll be able to do your research in a nice graphical environment and do anything you need. So yeah, let's get back to, yeah, go ahead. So you were showing the source code, but the source code is not on-chain, right? Yeah, so the question is, how do I get the source code since it's not on chain? So I kind of skipped one assumption. It's that by the Ethereum state, we also kind of consider the EtherScan source code database. Even if you go to L2Beat and look into the products that we list, if a product does not verify their source code on Etherscan, we give them a big red warning. We expect you to verify your source code to show transparency to the users and to the researchers so they can do their stuff. It is something that I omitted, but we do use Etherscan or Etherscan derivatives like BlockScout or stuff like that to get the source code, and we heavily rely on the source code because if we didn't get the source code, we wouldn't be able to call anything because we don't know what the ADBI is, right? So, yeah. If you have any questions, do please shut them up during the workshop. But I'll get back to the presentation. Yeah. So if you run into something where you can't get access to the source, you don't try any kind of decompilation? We don't try that. If you don't try decompile? No. I mean, there was one case where we knew what the source was, it just wasn't verified, so we like forcefully, like we verified the source code for the project because they didn't want to do it for some reason, so we just did it. And we are not trying to decompile the bytecode in any way. I mean, if it's something that we can't get the project to verify and we need to look inside, we might decompile it, but Discovery does not try to do anything like that. It just assumes the happy case where the source is on ETHESCAN or whatever explorer the chain is using and just goes from there. But yeah, if we hit snag, like there is no source, we just either accept it as we cannot look inside, or we talk with the team to verify the source code. Okay, if there are no more questions, I'm going to keep moving forward. So, going back to the mental model, because I think it is really important, like, if you leave this place with a single thing, it's just that discovery is just a program. It has some input, it has some output, right? So the output is this discover.json, and the input is the config.json.c. And the way you can think about discovery, if it makes thinking about it easier for you, is that it's like a scraper for a website. So scraping a website looks like you put some address of a website, it downloads the content of that website, it tries to find any links, and it follows those links recursively. Discovery essentially does the same thing, but it doesn't download websites, it downloads the state of contracts and doesn't follow links but follows addresses to other contracts. And of course, discovery is not just like a simple thing that is like a black box. There are things happening inside that we are going to be talking about a little bit. So discovery is able to detect whether a contract is a proxy. It does the source code flattening. I'm going to be talking about it later, like I said. It does template matching. It is something that also we are going to be touching upon during the demo part. It has custom handlers, which it executes. We are also going to be doing that during the demo. And it has typecasting. I left typecasting out because it is really, it will take like 20 minutes or 25 minutes to explain all of it. So I just left it out. If we have time or you are interested, you can hit me up after the workshop and I'll be glad to talk about it with you. And of course there is like an engine that orchestrates everything. But yeah, it's like there are things inside that black box. So I have a demo prepared. There's a QR code. If you want to follow along with me, please do. And if you get stuck, I'll be able to help you gladly. I'll be going over the same thing personally. So if you just want to sit and just listen, no problem. The website has instructions so you can do it now or later, whatever. And also I really hope that the website works for you because it's self-hosted. So if it doesn't work, try disabling your VPN, try a different country. I tried it like five times. It worked without any VPNs. So I'm going to give you like a minute to get to the website if you want to follow along. And yeah. All right. Okay. I expect everybody to be on the website right now. If you didn't manage to scan the QR code or type it through, just ask your neighbor for a nice deed so they will be able to show you the URL. So like I said, I will be going over the same thing. Personally, I have it on my iPad so I will be going over all of the things that are written in here and also I will be like adding some additional context for it. And also, this is the important part. I forked discovery from the L2B repository just for the purposes of this demonstration. If you actually want to use discovery yourself, please use the original repository because I'm not going to be maintaining this fork and it's going to get really stale really quick. And why did I fork it? It's just because we are not, no, it's an internal tool. We will try to make it more available to the public. It's just, it is in a rough stage right now. But it is still cool to show what this tool is able to do. OK. So is this visible? Like the contrast is okay? Okay. So I have downloaded the, the only thing that I have done is just npm install. And if you, if it worked, you should be able to just do npx discovery. And something along the lines of this should appear. So there are two subcommands. You don't really need to worry about them. So there is the single discovery. It's just for convenience. If you really need to discover something like a single address, you put it there. But if you want to build like a project, you'll be writing a config JSON either way. So there is also invert. Invert is like we used it to build those graphs you saw. We used mermaid before we got the protocol bit I was showing. So don't worry about those. The only thing that you're interested in is this cover. So the most boring part of doing anything fun is setting it up and configuring it. Unfortunately, I'll have to leave the part of configuration to you. You need to configure two things. You need to configure the Etherscan API key. So, you can actually call Etherscan. And you need to configure an RPC URL. And the sad part is that only few RPCs work that well with discovery. I'll say that the free tier of Alchemy worked for me without problems. So if you have Alchemy, do use it. There is like an asterisk where RPCs that only support block ranges of 10,000 when you're querying for logs do not work with discovery. For simplicity, we are essentially expecting you to have an RPC where the block range for log querying is basically infinite. So you will have to set this all up. I will just copy my end from the previous tries of that. So I have an end and you should probably do the same. You can always exploit the same variables if you want. So now we can get to the actual fun part. And before we actually configure anything, you know, we need an initial address. The address in the website is already there. But like this is also a good question. Like how do you actually get a hold of any address that belongs to a project? And I'm going to show you a simple example that you can find addresses belonging to a project. So we are going to be doing it this way. So like I said, I chose Zora for this presentation. So let's just go to the best website and find Zora. And how I do it is just go to their website of any project and try to bridge something. And also it is important there are multiple bridges for Zora, for example. But what you want is the official bridge. I know it's a touchy subject, but in L2B, we assume that the official bridge is part of the rollup. I don't want to get into it, but make sure you have the official canonical bridge to use. And we just want to bridge the smallest amount of if. do please don't bridge anything. We just want to get to the part where it says sign and don't sign, please. We just want to get to the part where it shows us the address we are going to be interacting with. So let me just bridge some if. Yeah, whatever. Let me just bridge some ease. Yeah, whatever. Okay, so this is the address of the contract we are interested in. And we can just copy it and store it for later. So now how do we configure the discovery to start at that address? So we need to create a folder structure that discovery is able to understand. So this folder structure looks like this. So just... I'll type it out here. So Discovery is the folder where all of the projects that you will use live, basically. And they're like configurations, results, flat files, source code, anything that pertains to a particular project lives in Discovery. And the actual project is like that. So anything related to Zora lives in there. But there's also one more level, which is Ethereum. This is the actual chain you're going to be doing Discovery on. Because we have the ability to do discovery on multiple chains, like I showed, maybe I'll",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T04:45:00.000Z",
      "slot_roomId": "classroom-c",
      "resources_presentation": "https://docs.google.com/presentation/d/1T24SoFUkubwO-ppCiYWJoisNwayKtozmAgEJYNPvVho",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-c",
        "name": "Classroom C",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/DeL2AeS4Bmw",
        "youtubeStreamUrl_2": "https://youtube.com/embed/qBY00sRrthM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/daSwJ4zzIwk",
        "youtubeStreamUrl_4": "https://youtube.com/embed/HZnOhi2MDMs",
        "translationUrl": "https://stm.live/Classroom-C"
      }
    },
    {
      "id": "folding-starks-with-the-mova-folding-scheme",
      "sourceId": "J78CHZ",
      "title": "Folding STARKs with the Mova folding scheme",
      "description": "We will present a new folding scheme that is 5 to 10 times more efficient than Nova, and 2.5 to 4 times more efficient than Hypernova. We will then explain how to use the scheme so as to construct a folding scheme for STARK proofs.",
      "track": "Applied Cryptography",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Zero-Knowledge,STARK,post-quantum,STARK,Zero-Knowledge,ZKP",
      "keywords": "Folding,Post-Quantum",
      "duration": 1385,
      "language": "en",
      "sources_swarmHash": "075645407ecaef5d20b741c3c2a15c9d02b76d18802bd15d67e8612f19039826",
      "sources_youtubeId": "psSr045sdso",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673833811b0f83434dc2c6ef",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673833811b0f83434dc2c6ef.vtt",
      "transcript_text": " This Friday morning. So yeah, I will talk about how to folding in the context of Starks. So let's start with the basics. Wait one second. All right. So what is folding? So in folding we have a relation R, relation of interest, which consists of pairs X, W. X is an instance or a statement of a problem, and W is a witness or a solution to the problem. And a folding scheme for this relation R is an interactive protocol between a prover and a verifier, where the prover and the verifier have two instances, X1 and X2, and the prover also has two witnesses, W1, W2, and they are valid witnesses for the instances. And then the prover and the verifier interact, and at the end of this interaction they output a new instance witness pair, XC CW3 with this key property. So this output instance witness pair is valid so it is in the relation. If it is there, if it is in the relation then the original two instance witness pairs are in the relation as well except with negligible probability. So this is the key property of a folding scheme. So basically what is going on here is that we have two tasks. We have to prove one instance witness pair and another one. And we have applied an interactive argument that reduce these two tasks to a single task. So if this folding step is very cheap, then you are basically gaining, you are gaining work, right? You have to do less work because now you only have to prove one instance witness pair. Okay, so commitments play a crucial role in this type of schemes, at least in the modern ones. And in reality, in practice, all instances include a is the same as the instance witness pair, but it's a different instance, it's a different instance, it's a different instance from other ones. And in reality, in practice, all instances include a commitment to the witnesses. So in reality, in practice, we have the instance witness pairs look like this. So the instance is a true instance, XI prime, and then it also includes a commitment to the witness. Okay? So a folding scheme looks like this. We have two instance witness pair where the instance contains a commitment to the witness. You fold, you get a new instance witness pair. Okay, and the commitment most of the time is homomorphic, and this is a crucial point. And homomorphic means that the commitment to the sum of two vectors is the sum of the commitments to the vectors. Okay, so if you've never seen folding, this is everything you need to know to follow this talk. Let's look at how folding looks from 5,000 kilometers away. So as I said, in folding you have two instance witness pair of this form. There's a prover and a verifier. The prover knows the instance and the witnesses. The verifier knows the instances. And now the exchange messages doesn't really matter what's going on here for the purposes of this talk. And at the end, the verifier sends a uniformly sampled challenge. And then the prover and verifier output new instance witness pair. The witness is only known to the prover. And crucially, the new instance and the new witness is a linear combination of the initial two using the last challenge sent by the verifier. So we have these formulas in here, x3 equals x1 plus alpha x2 and so on. And the verifier computes the... So the verifier needs to get the whole instance, right? At the end of folding. So it's easy for the verifier to get X3 because it knows X1 and X2. But how does the verifier get the commitment to W3? Well, here you can use the homomorphic property of the commitment scheme. The verifier knows the commitments to W1 and W2. And because of the linear, the homomorphic property of the commitment scheme. The verifier knows the commitments to W1 and W2 and because of the linear property of the commitment scheme it can obtain a commitment to W3 without any interaction with the prover just by performing this linear combination of the commitments. So this is how folding looks from 5,000 kilometers away. Many folding schemes look like this. OK, let's discuss commitments in folding schemes in more depth. So usually, the commitment scheme is a Pedersen commitment or a KZG commitment. So it's an elliptic curve-based commitment, or some variation of Pedersen and KZG. For example, in nova, hypernova, protostar, protogalaxy, et cetera, this is the commitment of choice. the of the of the of the of the of the of the of the of the of the the vector has large entries, so you have to prove statements about elliptic curves. So you have to prove statements about elliptic curves. You have to prove that the vector has large entries. So you have to prove that the vector has large entries. So you have to prove that the vector has large entries. So you have to prove that the vector has large entries. then you have to prove that the folding is done correctly and so on. So you have to prove statements about elliptic curve operations, which are quite a headache. And yeah, because of this setting, you are kind of bound to using a KCG-based NARC to prove a folded instance, right? Because you are using large fields, you have elliptic curve-based commitments, and so on. So, let's say we want to use a starg to prove a folded instance. So, we want to use a starg to prove a DCG based NARC to prove a folded instance, right? Because you are using large fields, you have elliptic curve based commitments and so on. But maybe you want to use a different proof system, right? So, that would be, that's an inconvenience. Okay. So, yeah, let's say we want to use to prove our folded instance win spurs. So by Stark, I loosely mean a snark that uses codes and Merkle tree-based commitments. For example, the Stark protocol from Starkware, Plonky 2 on 3, Boojum, RIS0, and so on. These protocols are configured on small fields, and they are getting smaller and smaller. For example, Goldilocks for Plancky 2, Baby Bear for Plancky 3 and M31 for CircleStark. This is the attractiveness of small fields is that, well, the attractiveness of these fields is that you get smaller arithmetization due to special properties of the primes of these fields is that you get smaller arithmetization due to spatial properties of the primes of these fields. You also get cheaper computations because field elements never get very large. If you invert a 32-bit field element, the inverse has at most 32 bits. But there are some problems in the context of folding. The first problem is that Merkel trees are not homomorphic. So if you want to do folding, you don't have the homomorphic commitment right away. And since we are working on small fields, we cannot rely on elliptic curves here. And also, depending on how you are going to do folding, even if Merkle trees were homomorphic, it might not be worth it in the context of Starks, and I will explain why now. So let's look at the cost breakdown of creating Stark proofs. And yeah, let's look first at how a fry-based Stark works. So yeah, all these proving systems I mentioned before work as follows. So first the prover computes the trace or the circuit values, the values in the wires of the circuit. Then it encodes the trace using a narrow correcting code polynomials and for this it interpolates the columns of the trace and then it evaluates the polynomials on a larger domain using inverse FFTs and FFTs. This is called computing the low degree extension of the trace. Then the prover commits to this low degree extension using a Merkle tree. And finally there's some quotient, the prover takes some quotients and applies fry and so on. Okay, what's the cost breakdown of all these steps? I'm citing Elie van Sasson from a talk at SBC this summer. So the trace generation, in the case of stool, costs 46% of the total proof cost. Computing the low-degree extension costs 28%, Merkle trees cost about 13%, and the rest also cost 13%. So, yeah. And if we are in the context of folding, if we are thinking of using folding, it means that somehow we want to use recursion. So we are probably not using Merkle trees full of K-chucks or SHA-256 or classic hashes. We are probably using Merkle trees that include some type of algebraic hash, in which case the third step would be much more expensive. Yeah, so here's an important observation. Even if the Merkel trees were homomorphic, you don't want to do the folding with the commitments to the low-degree extension of the trees. Because you would, at each folding step, you would perform this step, this step, and this step, and you would save this part in here. And you would still need to do folding. So in the absolute best-case scenario, the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of the value of We don't want to commit to the low degree extension with Merkle trees. We are just going to commit to the trace in the most cheap way possible. Yeah, so this is a key difference with approaches like accumulation with automomorphism or the ARC protocol from these researchers from papers from this year. Okay, so let's recap what we want to do. We want to commit to the trace instead of the low-degree extension. We want the scheme to be compatible with Starks, and this means that we have to work over a small field. We want the folded instance to be provable in a reasonable manner with a stark. And yeah, so since we want an instance to be provable with a stark, here we think of instances as being errors or plonkish instances. And we look at this as CCSs. A CCS is basically an R1CS constraint, but generalized with more terms. Okay, so here is the general framework of what we could try to do if we want to fold STARKs and afterward I will talk about actual instantiations. So let's say we have two instance witness pairs, two errors. The framework is as follows. The prover would commit to the trace, not to the low degree extension of the trace, so the trace, the witness, with a homomorphic commitment scheme. Okay, so now we have these two instance witness pair and then somehow we would fold in a way that the folded instance is still in an error or somewhat similar to an error. And now say that we want to prove a folded instance. So we have done folding, and now we want to prove the folded instance with this pair. So what would the prover and the verifier do? The prover computes the low-degree extension. So we want to use starts, right, to prove the folded instance. And for start, if you are using start, you have to encode the witness because you are going to use error-correcting codes. So the prover computes the low-degree extension of the folded witness, W3, and commits to it with a Merkle tree, as if it was just using the start protocol. the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the instance witness pair is in the relation with his favorite start. Okay, so we have to keep in mind that we'll have to do this step in here when we want to prove a folded instance witness pair. So this will inform how we choose the commitment scheme to not make this parting here too expensive. Okay, so let's instantiate the framework now. We need a commitment scheme that is homomorphic, that is compatible with a stark field, so it somehow works over small fields. And the candidate here is the ITAI commitment scheme, getting inspiration from the So we have a lot of data, but we have a lot of data. So we have a lot of data. So we have a lot of data. So we have a lot of data. So we have a lot of data. So we have a lot of data. So we have a lot of data. If we look at error or Plunkish instances as CCSs, so let's say R1Css or relaxed R1Css, then the first thing that comes to mind is Nova, because Nova folds relaxed R1Css into relaxed R1Css. So this is our candidate, Nova. However, there's no Nova-type folding scheme that works over lattices. So, we have a very good model for this. So, we have a very good model for this. However, there's no NOVA type folding scheme that works over lattices. When it comes to folding over lattices, the only work available right now is lattice fold. lattice but there's no nova analog over lattices so one of our main results is designing such a such as a folding scheme okay so a bit on lattices and and lattice fold so lattice fold and many lattice based schemes work over so-called cyclotomic rings which are cyclotomic rings are basically, they look like field extensions, right? You take a ring of polynomials over a field, a finite field, and quotient it by an ideal generated by a polynomial. However, here, the quotient polynomial may not be irreducible. And in this case, this means that if the polynomial is not irreducible, it means that this ring splits as a direct product of several field extensions. If F is irreducible, then here there's just one factor and you have a Galois extension, the standard field extension. But if F is not irreducible, then you have this direct product. But in any case, these rings are quite nice because they are just direct products of field extensions. So you could, in principle, and we show that you can configure R in a nice way, and at the same time choosing F to be a start prime field. So we can choose F to be Goldil the first one, the first one is the first one, the first one is the first one, the second one is the first one, the third one is the first one, the fourth one is the first one, the fifth one is the first one, the And the parameters of this commitment scheme are as follows. It's just a matrix sampled uniformly at random from the ring with ring elements. It commits to vectors of length m of ring elements. And the commitment is simply the matrix vector multiplication, a times the vector. So let's discuss the efficiency of this commitment scheme first. So say, and this is an example configuration we can deal with. We can choose the base field to be the Goldilocks field, which has 64 bits, and we can configure R so that R splits as eight factors, and each factor is a degree three extension of the Goldilocks field. Here there's an important remark on how to work with cyclotomic rings, and this is that because of this isomorphism in here, this is the number theoretic transform, because of this isomorphism, you can potentially store eight trace cells in a single ring element. So if you want to store eight trace elements, and each element is in the field, you can put each element in one of these components, and then these eight elements become just one ring element. So a vector of size 2 to the n with ring elements can store 2 to the n plus 3 trace cells. And this is quite relevant for performance. And this is some benchmarks of our implementation of the ITAI commitment scheme using this configuration. So say we want to commit to a vector of size 2 to the 16 field ring elements, and this means 2 to the 19 field elements. So this costs 65 milliseconds. This should be the commitment time I don't know why it says field and this is for comparison what you would get if you commit if you do Merkle tree commitments to the vectors so if you commit to the to the 16 field elements the code with a Blake function so classic classic hash this is 14 milliseconds but as I said this is just to do a 16 field elements, this is to do the 19 field elements. Also, if you are using Merkle trees, you probably have encoded the witness, which means that here you have an extra one in the exponent, which is not giving you anything when you want to encode the witness. Maybe the overhead is even larger if the rate of the code is less than one half. Yeah, so if we compare 2019 and 2019, we get similar performance. And if we put algebraic hatches in the Merkle trees, which we probably have to if we are in the context of folding, because it means we are interested in using some kind of recursion, then the difference, of course, is dramatic. How much time do I have? Five minutes? Okay. So yeah, we are almost... there's one or two slides left. So recall, I take commitment is just matrix vector multiplication of ring elements. There's a big issue with this commitment scheme. I said what is good about it, and now what's bad about it. This commitment scheme is only binding when the vector you're vector is the largest coefficient of an entry of the vector when you look at the elements of the ring as polynomials. And yeah, remember that the folded witness in our folding schemes are a linear combination of the two initial witnesses. So there's two issues here. Because of the linear combination of the two initial witnesses. So there's two issues here because of this caveat. One is that even if W1 and W2 have small norm, because W3 is a random linear combination of W1 and W2, it is possible that W3 has large norm. In general, W3 might have arbitrarily large norm, in which case the commitment to W3 with the I-type commitment would not be binding. And there's another issue that is, even if one does not happen, even if W3 has low norm for some reason, when you prove knowledge soundness of the folding scheme, you need to make sure that the extractor gets witnesses of small norm. Because we, because of this caveat, we only commit to witnesses of small norm, so we end up requiring that the prover commits to witnesses of small norm. So when you prove knowledge soundness, you need to show that the extractor only gets witnesses of small norm, and this is quite difficult to enforce. Okay, so since there's five minutes less than I had planned I will skip how LatticeFold and how we address these two issues and just jump to the Q&A part. Thank you. . . Thank you very much. We have one question. Why M31 is WIP and because of the ? The thing is that there's a lot of factors coming into play when configuring these rings. We have to configure them in a way that when they split as field extensions, they have at least 128 bits to ensure soundness during sound check. And at the same time, we have to ensure that there's a subset of small norm elements that is large enough for the soundness in another part of the protocol. And this places a lot of constraints on how you can choose the cyclotomic polynomial. Yes, but the composition of P minus one is completely different. The way you can choose the, the way you can configure the ring is highly constrained on the factorization of p minus one, on how it factors. Do we have any other questions?",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/190Nsmxqio3tQ_4Rk6RPoyEf0-2DbZVoOuYNvY9It1YM",
      "resources_slides": "https://drive.google.com/file/d/1N38NGMTpDb1V9lLFZWKepuFeBU5UKsZI/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "how-to-do-something-to-some-state-in-some-contract",
      "sourceId": "HECBJV",
      "title": "How to do something to some state in some contract",
      "description": "Smart contracts are changing. \r\n\r\nSo far, they needed every transaction to be public in order for nodes to agree. Zero-Knowledge came in to change things a bit: you can actually make your transaction client-side and just broadcast a proof.\r\n\r\nIn this workshop, we will use Noir and write a simple Aztec and/or Ethereum contract that allows for most of the execution and state to remain private.",
      "track": "Applied Cryptography",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Privacy,Decentralization,Cryptography,Mobile,proving,Cryptography,Decentralization,DevEx,Privacy",
      "keywords": "ZKDSL,DevOps,Mobile Proving",
      "duration": 4706,
      "language": "en",
      "sources_swarmHash": "bb0f866e09f66552c94ef7529b2d57baa5498b11f4c4838e64ea3795d3696642",
      "sources_youtubeId": "QD9ijtKRsWM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6738674520d1f9ac48c64e89",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T04:15:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1V-PhhZNdNFgdu0_mbGXOQJjINihO5JLwJV7DDAJh4nc",
      "resources_slides": "https://drive.google.com/file/d/1PP9P8J0c_MVSkIu5cBKpRMXpobIHJQdt/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "l2-daos-biggest-challenges-we-face-to-make-l2s-sustainable-long-term",
      "sourceId": "BF8EWR",
      "title": "L2 DAOs - biggest challenges we face to make L2s sustainable long term",
      "description": "Today L2 DAOs are mostly focused on growth and supporting their ecosystem builders. But long-term they will be responsible for the management and maintenance of their chains from all perspectives - ecosystem growth, software development, security, chain economic parameters management, and others. In this talk, I will explore what DAOs need to figure out and fix before they will be able to take this responsibility in the coming years and why we should be addressing those issues already today.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Governance,processes,Coordination,DAO,Governance",
      "keywords": "structures,processes",
      "duration": 1500,
      "language": "en",
      "sources_swarmHash": "59a57914473f149d366580f8bc98b834d15b56a2bf4f9eb3b68020593761e8c2",
      "sources_youtubeId": "rG3Zkuo08SM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673d8fd117a97b4f4d216e34",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1vQuKk5kYywWP8c4RZ3Xv_lV6TMmiWy4s6jRMdeFV9MU",
      "resources_slides": "https://drive.google.com/file/d/170c4q2SoSO8VZ9kclQSueu-A3qZZF60P/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "state-of-the-ens",
      "sourceId": "VBSW3N",
      "title": "State of the ENS",
      "description": "Jeff Lau, co-founder of ENS, gives an update on the state of ENS, and our progress with migrating over to layer 2. ENS's approach to layer 2 aims to preserve users' ability to choose where their names are stored and administered, while massively reducing transaction costs and increasing scalability for the vast majority of users. Embracing its status as a public good, we want to make ENS the most useful to the largest number of people possible.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Identity,Public good,usability,Identity,Protocol Design,Public good",
      "keywords": "Usability",
      "duration": 1573,
      "language": "en",
      "sources_swarmHash": "4f4d5561be4b6ad259c73d440b96399b09651ae3087ffac98f44090ee6ba0c20",
      "sources_youtubeId": "Lycp5FW-4x4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736e62c1b0f83434d0b798d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1z_YHSVofOJSq48tqbAiqN423gAZrzi5rzZMND8BcHDw",
      "resources_slides": "https://drive.google.com/file/d/14BWDzyHK6IbPK6J9dKnv1OTWYzm7Uro5/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "transforming-systems-lessons-from-taiwans-movements",
      "sourceId": "B9EDKY",
      "title": "Transforming Systems: Lessons from Taiwan's Movements",
      "description": "I will talk about the most recent struggles of open source communities in Taiwan, g0v specifically, how da0 has been trying to help in the past year or so, the conclusions we had and what is still missing. g0v has been running bi-monthly hackathons for 10 years now, which has been the key foundation for the community. April this year they stopped due to lack of funding support, we use this as a point of reference and how a web3 oriented subgroup like da0 could have done better, and the future.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Civil Resistance,Coordination,Public good",
      "keywords": "Ecosystem,Funding,Mainstream",
      "duration": 924,
      "language": "en",
      "sources_swarmHash": "2b9dfee773cde09da38f0a590cb1a7dbf70bfd70506a4361cbd19409fbeea5ef",
      "sources_youtubeId": "2cuicY646Jo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736ef6274749a4b891853e7",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736ef6274749a4b891853e7.vtt",
      "transcript_text": " Thuy Nguyen Reviewer Reviewer 1 Can you guys hear me? OK. Hi guys, my name is Noah. So today I'm going to talk about the strategies of making changes and our reflections from Taiwan. So, 10 years ago, this community called GovZero started in Taiwan. It started from having a bunch of hackers trying to open up government data. And later in the years, it become a beacon of digital democracy globally in the world, a leader of digital democracy in the world. And in recent years, the momentum has slowed down a little bit. So two years ago, me and my partner Vivian here basically started this initiative called DAO Zero. And the mission was to supercharge Gov Zero, to supercharge the digital democracy movement. And we did a bunch of things. This includes the decentralized ID pilots in Taiwan, these include various retroactive funding experiments, these include a lot of research funding the Commerce Taipei, Dao Taipei, Plurality Taipei, a bunch of different things. But at a certain point, I started asking myself, what has really changed? Were we really able to help? At a certain point, despite the relative success of all the experiments, this year in April, GovZero basically posted this picture saying that, okay, so we're running out of funding, please donate to GovZero so it can keep running its bimonthly hackathons continuously, which the bimonthly hackathons for 10 years, it has never paused once. So this is a signal that maybe despite all the experiments that we've done, maybe we need to go deeper on how we can make systemic change in the future. So if we are to initiate something new, if we really want to change system to the better, positively, sustainably, what are the strategies we are to adopt, what are the potential tools",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:45:00.000Z",
      "slot_end": "2024-11-15T03:05:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1mKMsPFBtVYtAcJOczCaTR2Ssw6fiQ86zw-Jz3zyGmFk",
      "resources_slides": "https://drive.google.com/file/d/1lxSdq1o6dZ8LQL69aRwqftKqmVQDNKNW/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "braid-implementing-multiple-concurrent-proposers",
      "sourceId": "WQUSDT",
      "title": "BRAID: Implementing Multiple Concurrent Proposers",
      "description": "BRAID is a consensus specification for implementing concurrent leaders in ethereum from parallel chains. The talk will cover the design of braid. Technical challenges of alternative designs for multi proposer and, if time permits, other topics of interest in execution consensus seperation.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Consensus,Censorship Resistance,proposer,concurrency,multiple,Censorship Resistance,Consensus,Core Protocol",
      "keywords": "Multiple,Concurrent,Proposers",
      "duration": 539,
      "language": "en",
      "sources_swarmHash": "147f0690dc6f9df2e01e336b41e1768c3a6da84e3faa8510fcb0748380db427f",
      "sources_youtubeId": "afmlcyB3h0I",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d52974749a4b892bca29",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d52974749a4b892bca29.vtt",
      "transcript_text": " Thanks, I'm excited to present some of our latest work, which is an extremely ambitious project to implement multiple concurrent proposers in Ethereum. concurrent proposers in Ethereum. This is early stage joint work with my serial co-author, Melesh Pai, who works with me at SMG, Alberto Cendino, who's at Miston, one of the co-authors of the Mistaseti paper, Wakim Ngu, who's at A16Z, who's written a lot of the NoMoreAttacks on ETH proof-of-stake papers, and Goldfish, and Joe Bonu, who's a cryptographer at NYU, who's written a lot about VDFs and other kind of delay cryptography. So why do we get the Avengers together? Because we want to change how Ethereum works fundamentally to be a system of execution consensus separation. So let's see if this clicker plays the video. Here's how execution consensus separation works. We have multiple proposers. We submit transactions to each of them. Each of those transactions goes into an unordered list, and we union them all together. Here, transaction two was submitted to proposer one and four. It's deduplicated. We create an unordered set, and then we apply the terministic ordering function, which is O here. In this case, in reverse priority order. So this is the basic idea of how execution consensus separation works. And the main reason we want to do it is to kill MEV. So why do we think that this will help kill MEV? Because at the end of the day, MEV is about two things, reordering and censorship. So you can choose which transactions go into the block, and you can choose which order they go in. And one of the critical primitives for building MEV-resistant applications is an auction. And it turns out that running an auction on Chain today in Ethereum's current architecture is very hard because there's a single proposer who can include bids. That proposer has an outsized amount of economic impact on what goes into the block, and they end up extracting a ton of rent. So that's why you see these numbers like $600 million a year of PBS revenue to proposers, all due to that proposer monopoly. So here's a formal definition of censorship, starting with what object could be censorship resistant. So a public bulletin board is an abstraction of a blockchain that has two operations, write and read. The write operation has two inputs, a message and a tip. Critically, the tip is very important here. You look at other definitions of censorship resistance, they don't necessarily incorporate the tip, but of course if you have a tornado cash transaction with a $100 tip, it's going to be a lot easier to include it than one with a $0.10 tip. So going into our definition, now that we know what a bulletin board is, we can say, let's describe this mapping phi, which says, given a tip, what is the minimum cost it would take a motivated adversary to censor that transaction? And that depends on the architecture of the blockchain. So since I only have five minutes, we'll skip to a theorem about this, which says currently in a blockchain like Ethereum today, which is leader-driven, we have a censorship resistance of just T, the identity function. You put in $1, you get $1 of censorship security. But if we have multiple concurrent proposers, there's multiple people who can include you, and so we get to the point where we have more censorship resistance, and in particular we can get a linear increase or even more. You can see some more details in the paper. Intuition being more people, you have to bribe them all to exclude the transaction. Getting into Braid, this is the basic architecture. It kind of looks like a DAG, except that there's no cross sub-chain votes here. All votes are on the same thread. We have multiple parallel chains running something like an LMD ghost. And then we take the union of all the transactions in all of the blocks in slot three, for example. And then we take the union of all the transactions in all of the blocks in slot three, for example. And then we apply the deterministic ordering rule. So it inherits a lot of the properties from a traditional LMD ghost. So liveness inherited from LMD ghost and eventual consistency inherited from LMD ghost because if one chain is eventually consistent, then the whole system is eventually consistent at the pace of the slowest chain. What does it mean to be eventually consistent? It means everybody agrees on what the state of the chain looks like. All the local replicas that are honest have that agreement eventually. And once you have that eventually thing, you can apply Byzantine agreement protocol, and you can say all of the honest inputs know what the chain looks like, now we can finalize and that's how basically Asper works. So this is an extension of LMD Ghost, it also works for a bunch of other protocols. I'll stop there because that's my time and take some questions. Okay, thank you very much. Can we give the mic to answer questions? Hi, have you considered or modeled the bandwidth impact of this or considered proposals where you have multiple proposers but not every block, maybe only every X block? Yeah, I mean, the goal for this was really to solve MEB, so we do want it every block. On your second question, on the first question, what are the bandwidth implications? There's, like, naively if you implement this, you get a linear increase in bandwidth because you have linear increase in blocks. You can do some things with the messages where you combine all the vote messages on each of the individual chains into a single message from the attesters. That can reduce some of the overhead, but it is obviously going to be higher overhead because you can't get something for nothing. Hello? for nothing. Hello. I just wanted to suggest that in the every block version, it allows users to make that MEV trade-off where they might have to wait a little bit longer if they want the MEV guarantee, but they can still get it in a reasonable period of time if you have every X block. Yeah, the problem is that the MEV that we're worried about is not for the user. It's not necessarily just sandwiching. We're really worried about the MEV that the protocols leak themselves, so stuff like arbitrage. And so Uniswap can't just necessarily turn off their contract. I guess maybe they could if we gave them the tools to do that. But the problem is you might have some arbitrage opportunity available, and it's available in the single proposer slot, and you don't have time to wait because the game theory says you're just going to take it right away. How does this interact with the encrypted mempool specification that's being proposed right now? Right, so I have a controversial view that private transaction submission is basically inevitable, and encrypted mempool is one way to do that. One nice thing about this property, like this proposal, is that the interface for inclusion goes from I have a set of transactions and an order that I execute them in to I only include a set of transactions. So that's a lot more compatible with encrypted mempool because, you know, I just choose either to include or not include. And the decision problem is not like this huge knapsack disgusting problem that we have today with the builders. And then another thing, like I didn't get to do it because I didn't have time, but we have a bunch of things about how do we keep the blocks sealed long enough for all of the blocks sealed long enough for all of the blocks to be released basically simultaneously. That's a critical game theoretic property for the kind of MEV properties that we want to achieve, and that's why we brought in Joe on the cryptography side. We've been working. There's tons of, there's like four different proposals, commit reveal, commit reveal force open, threshold encryption, and delay encryption, kind of in order of complexity that we're working on them all. We have time for more one question. I'll say one more thing, which is that I have a longer version of this talk later today at sequencing day at, I think, 1 p.m. on the research stage there so if you're interested in seeing more of those details about the encryption, about some more of the consensus stuff, come there and I'll share some more details. Cool, thank you very much Max.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:55:00.000Z",
      "slot_end": "2024-11-15T03:05:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1tXKl-KfPkdejsNKMAx-FYPo2FsJi7vaUxz5WyIszR84",
      "resources_slides": "https://drive.google.com/file/d/1Zdi5EvYO78kR-P5seRx9f_x6Kfh9RK6a/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "is-multi-block-mev-a-thing-insights-from-2-years-of-mev-boost-data",
      "sourceId": "E3JADX",
      "title": "Is multi-block MEV a thing? Insights from 2 years of MEV Boost Data",
      "description": "Multi-block MEV describes MEV that arises from one party controlling several consecutive slots. Currently, it is discussed as a potential blocker for several prominent mechanism designs. We analyzed two years of MEV boost data covering more than 5 million slots to investigate historical patterns of it. Amongst other findings we see less multi-slot sequences occur than randomly feasible however that payments for longer sequences are higher than average.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Economics,Tokenomics,MEV,data,analysis,Economics,MEV,Tokenomics",
      "keywords": "Multi-block MEV,Data Analysis",
      "duration": 1085,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "KcEZHQiopTg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f4321b0f83434d3ca90a",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736f4321b0f83434d3ca90a.vtt",
      "transcript_text": " I Did already Friday morning quite a few people make it to the MEV stage seems like they're quite some MEV ultras Looking forward to it. I'll be doing quick intro on multi block MEV just quick lightning talk to dive right in. Quick recap, what is multi-block MEV? Actually, multi-block MEV means one block builder has multiple slots in a row and can extract more MEV by having the slots in a sequence than by having individual slots. This arises due to the fact that MEV accrues Exponentially if we look at the data we can see that over time it slightly exponentially grows for example one strategy could be to Manipulate prices a block builder can for example buy a certain token in slot one, then only include buy transactions on one side and not include the sell transactions, and then in his last slot capture the MEV of selling the token first. This is one potential strategy of manipulating the price. There are other ways to do it. We can do liquidation attacks of also forcing the price to go below a certain threshold and thereby liquidating assets and capturing the liquidation gains. So I won't go too deep into it, but a whole set of strategies. What did we do? We looked at data since the merge, 4.3 million data points of MEV boost payments, and did, on the other side, a Monte Carlo simulation of daily market shares of builders to see what would be like a statistically normal distribution of multi-slots to have a baseline. And based on this, what we saw is, interestingly, we have less multi-slots to have a baseline and based on this what we saw is interestingly we have less multi-slot sequences than expected. We see especially on the chart on the right we see the blue bars being the expected distribution of multi-slots. We see less of them also interestingly what we saw, however, longest sequence was 25 slots in a row by the same builder, Beaver Build. And for the same validator and same builder, it was 11 slots in a row March of this year. So what we see just by statistics, there are long sequences, which would allow for multi-slot MEV strategies. So in the next step, what we looked at was the payments, are long sequences which would allow for multi-slot MEV strategies. So in the next step what we looked at was the payments, the MEV boost payments for longer sequences. So this is basically for a sequence of length 5 for example, what was the average payment. What we see interestingly it increases for longer. So it seems to be there is a value in longer sequences. However, why this is the case, so far we can only speculate. One hypothesis could be that MefBoost, it's a continuous first price auction, so it basically, from an economics perspective, works like a second price auction from the value where we end and so it could be actually driven not by the highest bidder but by the second highest bidder who has an increasing value for the blocks based on private order flow, MEV captured in private order flow so it could be the case that actually the intrinsic value of the second highest bidder increases and that drives these increasing values. So what we also did is looked for the payments per slot. So in longer sequences, what was the bid for the slot at a specific point there? We saw so far there is an increase but only a very very slight increase so this led us to the conclusion that so far we don't see dedicated multi-slot strategies. We also looked at the top 10 builders to see if there's a certain pattern for the top 10 builders if there's there are builders that are deviating from the mean. We didn't see any very outstanding data. In the case of few builders that have more slots, more multi-slot sequences, or that are particularly strongly correlating with the same validators. However, from our perspective, that's probably based on things like latency and co-location and not based on other types of strategies that are run. Last thing that we looked at was autocorrelation of math boost payments to see if we can use historical math boost payments to predict future ones. There what we see is based on different correlation metrics that it strongly goes down after the second or third slot so it means there's a very very low auto correlation so a low predictability so we don't have times of high or low MEV but there's a fast reversion to the mean overall. Yes, there was the quick run-through. If the topic in general is interesting, there's a post on ETH Research. I'm going into the details, also linked to the Jupyter notebook with all the data, and I'm also doing a workshop today in the afternoon on execution tickets, agent-based simulation of execution tickets, so whoever is interested, feel free to join. Otherwise, happy to take on questions. I think I have around one and a half minutes or one minute left for questions. Feel free to scan the QR code or ask them directly. So we already have two questions. We want more. You can just scan the QR code and put it. And one quick question, Pascal. Where is your talk so that the audience is aware of the room that the talk is in? The talk is in classroom B, so just down the aisle. So I'll read out the question for those who are joining virtually. Have you studied Arbitrum Time Boost? Do you think DAO should be capturing MEV revenue? To be honest, I haven't looked deeply into Arbitrum Time Boost. I think there are people in the audience that have much more knowledge of this, so I can't really give their in-depth take. Unlike Proposer, Builder is not guaranteed to win PBS auction multiple times in a row. Would you advocate for a random Proposer allocation, which does not allow the same Proposer in a row? In general, I think it makes sense. However, I think there is. So, I mean, that's a bit of an outlook already on the simulation. What we saw is that the secondary market usually reduces decentralization, which means there should be some component of just-in-time auction. However, I think with the current MF boost setup, what we have is the problem of in-flight variation. So I think a combination of both could be interesting. But I think some component of randomization does make sense to prevent multi-slot MEV strategies. Thank you so much. That was a great talk and a good start to the day. To MEV Day, I like the name that you gave. This is MEV Room, you know? Thank you. Thank you. So we have our next talk coming up really quick with Felix Leupold. The talk is on do we really even need PBS and how we can solve MEV at the app versus infrastructure layer. This is going to be a full day of hardcore research, so really glad to see all of you here and engaging and asking questions. Make sure to scan the QR code if you have any questions for our speaker. And with no further ado, let's welcome Felix on stage. Hi, I will have to speed run this talk. so if you want a more relaxed version of it, you can scan the QR code or go to tinyurl.com slash DAPCon, where I gave this talk in a 25-minute version. Let's start with the problem that I have with proposer-builder separation, why I think it's not a good idea. There are three main arguments in my point of view. First, the trust and centralization problems that it brings onto the Ethereum base layer for all types of transactions, not even the ones that actually contain MEV. Ethereum today is proud to have more than one million validators proposing and validating the chain, and we are one of the most decentralized chains, or the most decentralized chains in the world. But at the end of the day, if you look at it, there's only two very well-known entities, Beaver and Titan, that propose more than 90% of the block's contents. And how valuable is it really to have a vast validator network when the most important part, the content of the block, is decided by two entities. Moreover, PBS adds this trusted component to the supply chain, the MEV relay that builders have to put their faith in. And even if we trust the research that's ongoing, that's trying to propose ways to enshrine the MEV relay into the base layer and get rid of that trusted component, I think there's a couple of issues with that. First, it adds complexity to the core protocol, one thing that we're trying to avoid. And second of all, it's not even clear that if implemented, it will find adoption because MEV is a very latency-intense game, and any decentralized peer-to-peer gossip-based solution is always going to have latency disadvantages and also feature disparity. Builders today want to have bid cancellation as an example. They want to make a bid and then cancel it if they change their mind. And such a feature is simply not possible in an enshrined PBS version because once you gossip the message, you cannot take it back. And let's not forget that PBS actually maximizes the harm that is inflicted on users. In order to win the PBS auction, you have to create the block that is extracting the most value via sandwiching or other types of MEV attack, and thus we've created a system that really favors the worst possible outcome for the user. Okay, enough of a rant. What do we do about it? First, let's maybe take a step back and try to find out why does MEV exist in the first place. And here, really, the fundamental reason is that in today's system, every token trades at many different prices within the same block. Here I brought you an example block from a couple of days ago in which ETHUSD trades at least seven different times with seven different prices. And so you'll see 6-6 arbitrageurs that are able to access, for instance, the Uniswap pool at the previous block's outdated price. And then the rest of the block will trade at the new kind of fair equilibrium price. You will see liquidators that are able to get bad collateral at a fixed liquidation penalty, 15-20% below fair market price, and the rest of the block trading at a different price. And last but not least, you have sandwichers that frontrun a user to buy a token at price A, the user then priced it at price B, and the sandwicher sells it at price C. All of these strategies have one thing in common. The same asset trades in the same block at different prices. And the reason for this is that we basically took our very well best known standard first come first served mechanisms that work in traditional continuous time markets and deployed them on blockchain. And this was the key mistake. Ethereum doesn't have continuous time. There's only new information being released every 12 seconds, and then it's released in one go. And the ordering of the information within the block is up to a unilateral party, the proposer, to decide how to order things there. And so it's unsafe to deploy any kind of mechanism that works on continuous time on a discrete time blockchain. So what's next? How can we do to fix this? Well, as you might guess, we need to come to a point where trading the same asset in the same block should lead to the same execution regardless of trade ordering. Or put differently, we need to get to one price per token per block. This design requires to be able to batch trades together so it doesn't work on raw Ethereum transactions. Instead, we need trade intents or just signed limit orders and batch those together in a multidimensional auction which can later be cleared at a single price. For details on how this looks, I encourage you to look at Cow Protocol, the trading mechanism that powers CowSwap, which pioneered and implements this mechanism. It's processing many billions of dollars in transaction volume every month and is basically proof that this mechanism can work on-chain. Cow Protocol provides MEV protection for all the cases that I mentioned on two slides ago. We protect swappers by having buyers and sellers that trade the same asset receive the same price regardless of their ordering. And we can even match them together in what is known as coincidence of wants. By making AMMs part of the batch and treating them like swappers, LPs no longer trade at outdated prices, but instead get the fair new equilibrium clearing price. And liquidations are just stop-loss orders, which you can also add to the total liquidity pool and make sure that collateral recovery happens at the most efficient and fair price. I want to leave you with this number, which is a DUNE query that you can check for yourself that shows that more than 97% of MEV today is trading-related, and that is sandwiches, sex-sex arbitrage, backrunning, liquidations. And those can be vastly reduced, if not completely eliminated at the application layer by not using pseudo-continuous time mechanisms. So I'd like you to join me on my mission to convince this space that we should fix the mechanism rather than making the chain more complex. And with that, thank you very much. Thanks for that great talk. If any of you have questions, you need to scan the QR code and ask the questions remotely. I'm going to read out the question for those joining virtually. Do you have an estimation of reduction in MEV if only DEX used is cow? Yeah, so right now, CowSwap focuses on swap MEV protection. I didn't break the number 97% into swap AMM resistance and liquidations. So right now, it would probably be less than 97%. But the mechanism itself, not the product as it exists today, but the mechanism itself would be able to reduce MEV by that number that I showed. The other question is, do you think PBS causes centralization or is it correlation? I think PBS causes centralization mainly because it's a winner-takes-all latency game. I wouldn't say it's correlation. There's also a reason I couldn't get to that slide because I didn't have enough time for why I think cow protocol or an application specific, um, auction is not going to cause the same centralization or even if it would not affect kind of all types of transactions. So if I want to make a transfer, I'm not kind of subject to the censoring that happens today in the, in the builder market. Um, but yeah, for this, I would refer to the longer talk. How would you compare cow with what Sorella is doing to fix MEV on the application layer via Uniswap hooks? Yeah, I mean, cow is working out there and live. I think the mechanism is relatively similar. And yeah, I think that's the gist of it. You can deal with the last question. How does cow deal with multi-block MEV? It doesn't, because basically the batch itself has its own kind of heartbeat, its own time. And so if you were to try to do multi-batch MEV, I think you're running the risk that whoever solver will win the batch is going to execute the trade before you. So there's no real incentive for you to...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T02:55:00.000Z",
      "slot_end": "2024-11-15T03:05:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1spOihF0kLB_BzD62uWufsHORVgg_JGXZoISZsJris6M",
      "resources_slides": "https://drive.google.com/file/d/1qigMecB5gPgxQ_SUfpKx0lRZ1jfQ0OTc/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "cls-programmable-cryptography",
      "sourceId": "UTCRP8",
      "title": "[CLS] Programmable Cryptography",
      "description": "The Programmable Cryptography CLS hosts a series of talks exploring how advanced cryptography can reshape digital infrastructure beyond blockchain and trust infrastructure. \r\nSCHEDULE: 10:00–10:20 AM, Justin Glibert / 10:20–10:45 AM, Vitalik Buterin / 10:45–11:10 AM, Albert Ni / 11:10–11:35 AM, Barry Whitehat / 11:35 AM–12:00 PM gubsheep",
      "track": "[CLS] Programmable / Frogrammable Cryptography, by 0xPARC",
      "type": "Mixed Formats",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": 5855,
      "language": "en",
      "sources_swarmHash": "99b2e9a8f4aded87244db1641fb73a4b9376d47b848ded4ef376334a9cd7254a",
      "sources_youtubeId": "S6ixhGBnvKc",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673d8dd017a97b4f4de70e1c",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:00:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1KpnGjqycfpLNFKUjuTryELdVgZfiVhV0qOcH-f6orS0",
      "resources_slides": "https://drive.google.com/file/d/1FPkrcYulC9u3mlG4Q7QMg_VSPOZazDBZ/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "dj-i34r7h",
      "sourceId": "QTHGFE",
      "title": "DJ @i34r7h",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:00:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1f64FrhWEvOeHjw8XlNFarHOwwNkBaofQKZdOavm-Zq4",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "does-ethereum-really-need-pbs-solving-mev-at-the-app-vs-the-infrastructure-layer",
      "sourceId": "TNKFPP",
      "title": "Does Ethereum Really Need PBS? Solving MEV at the app vs the infrastructure layer",
      "description": "In this talk, we will give a brief history of MEV (Maximal Extractable Value) and its influence on enshrining PBS (Proposer Builder Separation) into Ethereum. We will explore the Ethereum community’s evolving perspectives on PBS while looking at successful outcomes, unexpected consequences, and alternate solutions. \r\n\r\nUltimately, the talk will provocatively ask: does Ethereum really need PBS at all?",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "redistribution",
      "keywords": "Intents,MEV,PBS,Redistribution",
      "duration": 1475,
      "language": "en",
      "sources_swarmHash": "81761a34dfd0fb923c0c06c5a197e014fbd7fbeb3e32c50fc0fb7f8b110f9696",
      "sources_youtubeId": "4-QVxNAeIsw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f4bf1b0f83434d47cb02",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6735aeab9dbb7a90e1bde09b.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\"Presenter\": Hello, welcome back. So, today I will be talking about Phantom Zone. But before I dive deep into Phantom Zone and talk about the rest of the things, I would walk you through the motivation behind Phantom Zone. So, if you guys are familiar with globally mutually trusted third party, I would want to introduce you to this idea of globally mutually trusted third party. What is this globally mutually trusted third party? Well, it provides you three guarantees. The first thing that it says is that whatever information you send to this third party, it will always keep that private. It would not leak it to anyone. The second guarantee that it provides is that all the information that it has collected over time from different people, you know, we have been sending this globally mutually trusted third party all our information let's say for a year. All the information that it has collected for all these years, it will always keep it private and would not allow anyone to poke inside its memory. And the third guarantee that it provides, which makes this particular party very magical, is that it can compute any arbitrary function we want it to compute, as long as we provide enough authorization to be able to compute that function. And it will only output the necessary outputs. Usually, I sort of refer to this mutually trusted third party as a mutually shared computer. And if you guys are familiar with something called the God Protocol, this is the God Protocol. This is a picture from an example back in 1987. So first observation to make is that if you really want these three guarantees to be, if you really want this party to be globally mutually trusted, we want this party to be able to prove these three guarantees to any individual without requiring any additional interaction, which is why we require cryptography. to prove these three guarantees to any individual without requiring any additional interaction which is why we require cryptography we cannot just make it on a certain legal arguments or something like that we require cryptography for building this the building this globally mutually trusted shared computer and we started to build phantom zone to eventually build the god protocol but to stick within the realms of practicality, we could only build an abridged version of it. So for the rest of the talk, I will be talking about, A, what is Phantom Zone? Why it is an abridged version of this God Protocol? And the second important thing that I'll be talking about is, how can we push the frontiers to eventually build the God Protocol? Okay, so Phantom Zone, the abridged version. The key idea in Phantom Zone is something called multi-party fully homomorphic encryption. And for me to describe you multi-party fully homomorphic encryption, I have to eventually describe you what is single-party encryption. In single-party encryption, you have a single client, this guy over here. They hold a secret. They keep the secret private to them. They can encrypt their information, which is A here, with their secret and produce an FHC ciphertext. And then they can send this FHC ciphertext to any server. And the can evaluate and any arbitrary function on their private input which is a and produce an output ciphertext and that the and the client can receive the output ciphertext and decrypt it. So this is single-party FIG coming to multi-party FIG. Well the key idea in multi-party FIG is that you split this secret which is held private by a single client in single-party FIG. Well the key idea in multi-party FIG is that you split this secret which is held private by a single client in single party FIG among many clients. So you have S0, S1, S2 as secret shots split among these three people over here. The first step in multi-party FIG is something called collective public key generation. So all these three parties come together and they generate the collective public key. And then all these three parties, using the collective public key, encrypt their private inputs and produce FHC ciphertext. And then they send their FHC ciphertext to the server. Server executes a marble refunction on the FHC ciphertext and produce an FHC output. The key thing to notice here is that all these parties would have to produce a decryption share to eventually decrypt the output ciphertext here. So they produce the decryption share using the secret shards and then they send it to each other and then only they're able to decrypt the output ciphertext. Because in this case, the secret was split among all these parties. So why is Phantom Zone an abridged version? Well, because Phantom Zone, assuming that in the future we're able to add publicly-verifiable FHE to a Phantom Zone can only guarantee the three guarantees that I talk about in the God Protocol to only the holders of the secret shots. It cannot guarantee these three guarantees to everyone around the globe. Which is why Phantom Zone is just an abridged version of it. Okay. So you might wonder, how do we build towards the God protocol? How do we even do it? Well, what I would like to say at the moment is I would have loved to say that after a lot of research and a lot of five years of research, we have figured out the solution to build the God protocol. But no, there are no enlightening thoughts here. And there's one obvious answer to eventually building the God protocol, which is program of sophistication. What's program of sophistication? Well, to simply describe the program of sophistication, let's just assume that you have function f, right? What you can do with program obfuscation is you take this function f and perform some transformations on this function f and produce an obfuscated circuit. You can give this obfuscated circuit to someone else and program obfuscation guarantees that the only thing that you can learn from that obfuscated circuit is the input to output map and nothing else. Now you might be wondering why is this useful? Because if the function is trivial, then you can easily learn it from the input to output map. Program obfuscation becomes very interesting when you sort of like obfuscate a program that is a cryptographic function. For example, let's just say that I take a function that decrypts any ciphertext that is encrypted to my public key. So I take a function and this function has my secret key and it decrypts any ciphertext that was encrypted to me using my public key. And I perform certain transformations using program obfuscation to this function and produce an obfuscated circuit. I give this obfuscated circuit to someone else. What they can do is that they can decrypt any ciphertext that was encrypted to me using this obfuscated circuit. But they can never, ever learn what the secret is inside that circuit. They can never learn my secret key. And these are the class of functions where program obfuscation becomes useful. And I'll tie it to building the God protocol later in the slides. So now, assume that we can only build program obfuscation for some limited class of functions, not for general class of functions, but limited class of functions. I'll tell you one way of building the got protocol using program ob application. Step one, modify the FHT scheme that we're using before to become publicly verifiable. What do I mean by that? Well a publicly verifiable FHT schemes does those things. It evaluates the FHT function which you know a a normal FHE scheme does. In addition to evaluating the function, it also produces a proof of correct evaluation so that anyone can verify this proof with the output ciphertext and be assured that the server that sort of executed this FHE function executed it correctly, and which I usually refer to as proof pi of correct evaluation. Step two, replace the collective key generation operation that we did in the multi-party FHE with a trusted setup. In the trusted setup, you have arbitrary number of people here. They perform some MPC protocol to produce FHE keys. The two types of FHE keys which are very important. Public key and the bootstrapping key. Bootstrapping key is usually used for some sort of FHE operations that you can completely black box. The key thing here is that no one knows the ideal secret key because we're doing a trusted setup in MPC to generate these two keys. The third step is modify the trusted setup to also output an obfuscated conditional decryption oracle. Okay, that's a mouthful. I sort of like go into it one level deeper. What is an obfuscated conditional decryptor? This particular conditional decryptor is an obfuscated program of the following functionality. What it does is that takes an output ciphertext and a proof of correct evaluation of FIG circuit. It verifies whether the proof is valid and decrypts the output ciphertext if and only if the proof is valid. And this sort of like tells you why did we assume in the first place that program obfuscation may be feasible only for like limited class of functions because to build the GOT protocol like to build the got protocol using the FHERoute, we only need program obfuscation to be practical for this obfuscated conditional decryptor. So we modify the tracer setup to also output this obfuscated conditional decryptor, and that's it. And another thing to note is that this conditional decryptor also has the secret key, the ideal secret key that no one knows embedded inside it. Okay. So the end-to-end flow is, you do MPC to generate three things. Public key, bootstrapping key, and the offscored conditional decryptor, which I now realize is somewhat of a mouthful. I should have chosen some other term. Anyways, the second flow is, now anyone can encrypt their private inputs using the public key that is the output of the MPC protocol. So you have multiple ciphertexts here. And then they can send it to the FHC server. FHC server evaluates the FHC function, outputs the encrypted output. In addition, it produces a proof because the FIG server is evaluating a publicly verifiable FIG scheme. And then we plug in the proof as well as the output to the off-scaled conditional decryptor and the conditional decryptor would only decrypt the encrypted output if and only if the proof is valid. So this is one way of building the God protocol using publicly verifiable FHE and program obfuscation for obfuscated conditional decryptor. So there's one way, which I've just shown you, but we need new ideas to push the frontiers and to finally build the program obfuscation or and to finally build program obfuscation or indistinguishably obfuscation, if you're familiar with that. Here, I've showed you just one way. But if you are able to come up with new ideas, then probably we can make program obfuscation more practical for general circuits, not just for limited class of functions that we used before. And probably, we can directly build the God protocol from program obfuscation. So while I was exploring this field of program obfuscation and I.O., one key observation that I made was that it's really hard to get efficient program of specification from standard assumptions and we would inevitably require exotic assumptions. And I'll tell you what are standard assumptions and what are exotic assumptions. Well a standard assumption is an assumption that has been there for a while, for example D log, discrete log problem. There also exists additional incentive for people to break these standard assumptions. And exotic assumptions are somewhat newer assumptions. Like, they have been only there for like five years, or not even five, it was like two to three years. What we can do as a community to, you know, realizing that we might inevitably need newer assumptions to build practical program amplification is we can start examining these newer assumptions, start breaking them, start testing them. Or we can build applications using this assumption so that we can incentivize people to break them and tell us whether they're broken or not. And then eventually, in a few years, we would have candidate assumptions that are newer assumptions, but they have become then standard using which we can build practical program sophistication. And taking a first step towards this, we are launching a bounty program to break one of the candidate assumptions, which is called program obfuscation by local mixing. The way I think about this particular assumption is that they're taking more computational complexity approach than taking the traditional approach of using algebraic structures to build program obfuscation. The goal of the bounty is that we provided an obfuscated circuit with roughly 240,000 gates, which was obfuscated from an original circuit with roughly 1,000 gates. And you had to find the original circuit. You can learn more about the bounty at OfficeTopia.io. If you know what OfficeTopia is, OfficeTopia means that we're living in a world where authentication is practical, and the bounty amount is 10K. And this bounty is launched in collaboration with Ethereum Foundation and Zerix Spark. Okay. So before I break, and I think that I have a bunch of time, okay, before I break, and I think that I have a bunch of time. Okay, before I break, I would want to make one conjecture. And the conjecture goes as follows. I think the God protocol is the convergence of cryptography. Probably building the God protocol would require certain sort of like FHE. That is just one route, but like publicly viable FHE and other things like MPC for just setup and so on and so forth. But once you build the got protocol, I think it encompasses everything. It gives us everything that we have been wanting for for a while. It gives us witness encryption. It gives us zero knowledge proofs via signatures. It gives us MPC, multi-party computation. It gives us FE, functional encryption, all of these things that we've been demanding for a while. And this is also one of the major reasons that we should start investigating much more seriously how to get practical program application and finally build the God protocol. And that's it. Thank you. All right, thank you for All right. Thank you for the talk. We do have some questions rolling in. Yeah, let's go through some of the questions. Let's start with the first one. Can we implement threshold ECDSA with Phantom Zone? At the moment, yes, because you can express everything. Like, theoretically, yes, but it would be very impractical to implement ECDSA with PhantomZone at the moment because ECDSA is like you're doing elliptical operations, which is a lot of operations. As far as I understand, threshold ECDSA is possible. It takes two days to generate one single signature. All right, so next question. Can you tell us a little bit more about the definition of obfuscation as a virtual black box? That's the first question over here. Isn't the definition of obfuscation as a virtual black box impossible? I am not posing obfuscation as a virtual black box. I did not mean to say obfuscation is a virtual black box. By the way, the impossible result of a virtual black box is only for certain very restricted class of programs. It's not for general class of programs. Eventually you can aim for virtual black box with certain caveats. But again saying that my definition of sophistication is not virtual black box. All right and what can be done today with Phantom Zone? At the moment as I said Phantom Zone is an abridged version of the Scott protocol. It does not even have publicly verified FHE scheme, so it does not give you all the three guarantees. The only guarantee that it gives you is that it will execute the function that you ask it to execute while private information can be coming from multiple people. It'll keep the information private, but you'll have to trust it for it. So you'll have to trust this particular server to always keep the information private, but you'll have to trust it for it. So you'll have to trust this particular server to always keep the information private and not send it to anyone else. Perfect. And we do have one last question. Oh, cool. More questions rolling in. Can obfuscating programs undermine open source transparency and make it harder to verify the absence of malicious code? I see. Make it harder to verify absence of malicious code? I see. Make it harder to verify absence of malicious code. Well, that is assuming that the entire program is obfuscated. When I say obfuscation, we require obfuscation for certain parts of the program, which can interact with a public program and a private program which is obfuscated. I understand that obfuscation can be used for many malicious purposes as well, like for example, you know, like, there are several reasons why people might be interested in obfuscation, but we can, as a community, make sure that there's interaction between the public interfaces and the private interfaces which are obfuscated. All right. And why do you call the publicly verifiable FHE circuit obfuscated? Doesn't the require solidity verifier or something which is public? No, I think once I give you obfuscated circuit, there are certain guarantees that you can learn from the obfuscated circuit itself, that it does not reveal anything, as long as you've done the obfuscation correctly. Alright, and do you have evidence that the conditional decryption functionality is possible using I.O.? Yes. There are theoretical results and we're trying to make it practical as well. All right. Can you give one example each on how I.O. can replace ZK, MPC, FHE? Okay. So for ZK, what you can do is like you can embed a secret key inside this off-secreted circuit, the God protocol, and a zero-knowledge proof is just a signature from this God protocol. Whatever secret exists inside this particular server or this God protocol, or this FHC circuit, off-secreted circuit, a signature by that thing becomes a zero-knowledge proof. So you do not require zero- zero knowledge on the client side anymore. For MPC, again, it's a globally mutually trusted third party. All of us encrypt our private inputs with the public key corresponding to the secret key that lives inside this off-site circuit. And we send our private inputs to this. It decrypts that, performs some function, and produces the output. So that's one way of replacing MPC, and the same applies for FG. Cool. We can stay here for maybe another 10 seconds if there are any new questions rolling in. All right, cool.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:05:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1y6tAISW_K9exOHiT-8JDt3qSFgyDYP0v5Zkc3T7JIdw",
      "resources_slides": "https://drive.google.com/file/d/1ajfbPhbuj_WS7D8L0JFilnwZ1mPLSntT/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "solo-staking-in-the-dark-forest-a-survival-guide",
      "sourceId": "REJ3SW",
      "title": "Solo staking in the dark forest: a survival guide",
      "description": "Solo stakers are key to keeping the Ethereum ecosystem geographically decentralized and censorship resistant. But PBS leaves solo stakers extremely vulnerable to a variety of narrowly targeted DDOS attacks, made possible by public information on the p2p network. This talk will explain why privacy matters on the p2p layer, provide an overview of the attacks solo stakers would face in PBS, and demonstrate some of these in a sandbox environment.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking,Privacy,Security,MEV,metadata,MEV,Privacy,Security",
      "keywords": "Metadata",
      "duration": 582,
      "language": "en",
      "sources_swarmHash": "2d1f8fe35ffd0fab6e8af4f1e2723e4a8f364a230dce06f962976bdd2be70268",
      "sources_youtubeId": "ZHXWCH6N9tQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d5be74749a4b892d57fc",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d5be74749a4b892d57fc.vtt",
      "transcript_text": " Hello, hello. Yeah, I guess it's my turn. Yeah, surprise title. Actually, I'm going to talk about why Dark Forest is even more dangerous to solo stakers with an attack that strikes at the heart of the Ethereum validation. So together, actually, we're going to play as an attacker. And we're going to find out a validator or a solo staker that actually uses MEV boost, which is next to their consensus client. In order to find them, we actually are going to do the exercise together. But once we find them, what we're going to do is we're going to disrupt the block production of the solo staker and stop their rewards and manipulate the Randall value which is responsible for assigning new block proposers. So my name is Q, I'm the decentralized technology architect at Hopper. At Hopper we build privacy preserving infrastructure. So we'll actually use a sandbox environment for demo purpose, but there's nothing actually preventing this attack happening in real life right now. Our sandbox actually has three consensus clients with a total of 192 validators in a kurtosis network with also one MEV relay and one block builder. For now, those validators are just a list of anonymous public keys, but not for long. So while this test has been running, let me explain how we actually conduct this attack. So first, we actually need to de-anonymize validators to identify our target. So validators actually need to attest block productions, and those attestations are propagated into the peer-to-peer network in GossipSub. We can actually just silently observe those attestations to create a probabilistic correlation between the validator's public key to the IP address of the consensus layer client. And we actually have this, oh, the test has actually been running. We just let it run for a little while. Block has been proposed. Just leave it there. So now is the moment to actually explain the other information that we need to conduct this attack. We actually need a little bit of help from the trusted map relay. Maybe we're just observing the map, or we're just colluding with the map relay, or we're just the relay ourselves. MapBoost users actually need to trust the relay to deliver, well, basically trust them on delivering correct content of the block on time. However, they need to trust more than that. So that's the moment we look into the database of the relay and look at one of the new data table that we created, which is called metadata. Here, we collect all the metadata of each HTTP request from connected validators and block builders. So it actually gives us a strong link between the validator public key to the MapBoost IP address, which sits right next to the consensus layer client. And this is just because validators, they have to, they must register with every relay that's connected to. Okay, here, now we have the exercise going on. As an attacker, we just have a dashboard with all the information we need. Right here is the attestation that we have with the occurrence of the attestation that we accumulate with, like, the more attestation we have, the stronger link we actually build between the public key of the peer IDs. And then we know the IP address from the peer ID pretty easily, and then plug the information of the MapBoost ID, MapBoost IP address on top of that. And next question is who to attack. So here we actually just pick slot 246 and 247, but we actually have a wider choice because validators, they are, like the proposers are announced two epochs ahead of time. Right? To launch this attack, we use traditional DDoS technique, introducing the ICAP flood and SYN flood. But just to make our attack a bit more clear, we actually use a memory stress test just directly on this instance to obtain the same result. And here we can see that block 246 and 247, they are skipped. And now it's time to check the rundown value of the slot 246 and 247. They're actually the same. So congratulations, fellow attackers. We managed to stop the block production and also control the Randall value. Yep. So if I can skip to the last slide. Yeah, so here we just demo the attack as an attacker gene. I welcome everybody who are interested in this topic to further discuss the implication and improvements on how to avoid this attack. We're going to have a report released soon on the Etherchurch forum, as well as a discussion that's going to be held at 1.30 later at the blue discussion corner. To close my talk, I would like to give a heartfelt thank you to Ethereum Foundation that gave us a grant to conduct this research. Thank you very much. Thank you very much. Thank you very much, Q, and sorry for missing your name. Now, are there any questions? Come on, don't be shy. So just to clarify, so we were able to skip the solo sticker slot here? Is that the attack, or was it just the random manipulation? So we actually managed to make the solo sticker not being able to propose at the slot that it was given. So by doing so, we indirectly actually manipulate the rendout. And like, is there any, what do you think are the financial implications of this? Do the big staking pools be doing this to maximize their rewards? Actually, good question. Because there was one of the further discussion that we had around this experiment is that we believe that the current definition of MEV, which is a single, we always look at just one, whatever transaction that's been included into one block. This narrow definition, which is also the definition written on the Ethereum, like the page of Ethereum Foundation, is a bit too narrow. So by kicking out the production of one block, we can effectively have this skip slot MEV. Just an example, like some people would like to have this multi-block MEV, right? This is one of the attack of having that to achieve this multi-block MEV. And also, it just basically, it also creates a threat on the resilience of the network, right? If you can easily identify a block proposer and then just kick them out at the place where it's supposed to produce a block, then does it mean that, especially for solo stakers who have very poor, let's say, who doesn't really have access to strong network protection, does it mean that those percentage of solo stakers of Ethereum network, they are at risk. Last question, please. How difficult is it to obtain the IP address of a certain validator and what is the role of MevBoost in this kind of process? Yeah, thanks. It is very easy, actually. Right now, you can just modify a little bit of your, I don't know, Lighthouse client. Then you can collect attestations by just dumping them into a database. Very simple data analytics, and then you can have this correlation. So the longer you run that, the stronger link that you have it. Even though there's some technical details about how we're actually going to tackle the aggregated attestations. But yeah, you know, you're going to figure it out. And we also know the pattern of gossips up. So combining these two, the analytics is actually pretty accurate. And the role of the map relay is that because solo stakers, let's say they already invest into a hardware, right, to, and also put into some stake to run their own staking setup, their goal is, one of their main goal is to make sure that they have economic returns. And very likely they're going to introduce whatever that helps them to build the most profitable block. And here right now we have, thanks to your research, Tony, is that we know that there are more than 90% of the validators, they are using MapRelay or MapBoost architecture. And Relay, which is an obvious, very obvious single point of failure, no, let's say a single entity, like a central entity",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:05:00.000Z",
      "slot_end": "2024-11-15T03:15:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1d-GmGcNLmt1uMkzzdpBPgSsDGcejG31g_wfOtXcVIvg",
      "resources_slides": "https://drive.google.com/file/d/1zQOsDWjaz5U_rR_njTavH_W8LwNu-EDH/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "road-to-effective-public-goods-funding-through-quantitative-cross-comparative-analysis-of-grants-programs",
      "sourceId": "NHERZE",
      "title": "Road to Effective Public Goods Funding through Quantitative Cross-Comparative Analysis of Grants Programs",
      "description": "I aim to achieve effective public goods funding by comparing grants models. Grants programs are key in the crypto ecosystem, but comparative studies are rare. Our study compares Uniswap, dYdX, Optimism, Gitcoin, and more, categorizing them into \"top-down,\" \"bottom-up,\" and \"QF (algorithmic)\" types. Findings suggest bottom-up and QF types distribute funds more evenly with smaller variability and grant amounts, while top-down types show greater variability with larger grants for fewer grantees.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,DAO,Governance,Regenative Ethereum,Public good,funding,public,goods,Coordination,DAO,Governance,Public good,Regenative Ethereum",
      "keywords": "Grants Program,Public Goods Funding",
      "duration": 488,
      "language": "en",
      "sources_swarmHash": "53bd35c89f9ae07e4dc208c1d9b3b8fcc5c864d3afe99e7362260a3dbf804082",
      "sources_youtubeId": "YyoQSc4iDPk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f10974749a4b891f39c8",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:20:00.000Z",
      "slot_end": "2024-11-15T03:30:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1el9pBQpo_PXoaMz4cdOtMT4cXnCNpLdicORmmniTBK4",
      "resources_slides": "https://drive.google.com/file/d/1uKI_zsMoBbvB-UFJsjuJ_Oe-HF9Naa8r/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "can-zero-knowledge-and-blockchains-help-humanity-survive",
      "sourceId": "ZCHNUF",
      "title": "Can zero-knowledge and blockchains help humanity survive?",
      "description": "While someone is building one more NFT marketplace rn... I claim that blockchains, privacy, and verifiability can disrupt, heal, and modify the world around us for real: spacetech, agrotech, biotech, AI/ML, neurotech, and many other domains have a fit for our beloved technology.\r\n\r\nBased on dozens of interviews with ppl from other industries, I want to share the most unexpected, exciting, and urgent use cases e.g. satellite coordination, post-nuclear war recovery, and LLMs presumption of innocence",
      "track": "Real World Ethereum",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zero-Knowledge,Use Cases,Product-market fit,privacy-preserving,blockchain,Product-market fit,Use Cases,Zero-Knowledge",
      "keywords": "Future of humanity,Ethereum prosperity,privacy-preserving blockchain",
      "duration": 435,
      "language": "en",
      "sources_swarmHash": "67c0f882d9ca5f67cbe58f937ca1b5c7f732821bf92f57c27f865121523e5e79",
      "sources_youtubeId": "PyziarEoUa4",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673836c71b0f83434de919a3",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673836c71b0f83434de919a3.vtt",
      "transcript_text": " speaker who will talk about how their knowledge and blockchains will help help humanity survive. Please welcome Lisa Akelsrod. Hello everyone. This is a lightning talk meaning we don't really have any time for jokes. sorry for that. We have to start being very serious from the very beginning. So the question I'm going to talk about today, the problem is that we are very small. Like as a blockchain industry, I mean. On this picture you can see how the rest of the world is trying to see what is blockchain. If you want some more specific numbers if we compare our industry to other tech verticals like space tech or biotech we are something like 10 to 40 times smaller. Frankly speaking we are like a small dot. But I think we need to be large. If we want to our industry to prosper and take a meaningful place in the world. And I think we need to learn from mushrooms how to be large. What's the difference between blockchain and mushrooms? Blockchain is a three-mushroom network. And mushrooms are a three-trillion-mushroom network. Three-trillion-mushroom network is a really cool thing. Whenever someone sees it, they want to jump in because if there are 3 trillion mushrooms who joined, it seems to be worth it. But three mushroom networks seems to be a bit a doubtful activity. So we need to become mushrooms. That is to say, it is on us as a blockchain industry to find use cases where blockchain and ZK are a perfect fit for huge world's problem. And furthermore, it's on us to communicate it to right people in right industries and in right countries around the world. Because I can guarantee you, they will not figure it out on their own. First because they don't think about blockchain, then because they don't know about zero knowledge. So today I'm going to, just as an example, give you three real-world use cases that are crucial for humanity's survival. And I'm going to use the formula of ZK for verifiable computations and blockchain for coordination. Think, for example, about the space. Think, for example, about satellite coordination. There are many example, about the space. Think, for example, about satellite coordination. There are many countries operating in the space, and many of them are not friends. But still they have to coordinate their activity in the space. How can China prove the US that their satellite won't clash into some US space facility? What they do today, they ask, you promise? Yes, I promise. But what if they use zero knowledge proofs for proof of proof of proof of hitting the specific coordinate instead of just promising? But how should they coordinate these tons of zero knowledge proofs by hundreds of parties for thousands of satellites 24 to 7 without disclosing any sensitive information? They can use privacy preserving programmable blockchain. I think about agriculture. In many countries, agriculture is run by self-piloting machines like computers with software running on it. If tractors, cultivators and balers are out of operation for the whole country, its economy stops and the country is in deep economic crisis. How can a small human force thousands of machines with different programs running on top to report them what's on their mind using zero knowledge proofs? But what about a network of thousands of machines and thousands humans transmitting zero knowledge proofs in arbitrary directions 24 to 7? With privacy preserving programmable blockchain. I think about nuclear missiles I know this is not the favorite topic for most people but just for 30 seconds if a country a has a concern that the country be launched in nuclear missile towards it is president has several minutes less than five to call their president ask did you do it but and if the president didn't answer they must launch their all their missiles as a prevention. But can we prove that something has not happened? Can one prove that they have not launched a nuclear missile? If they did, can they prove that this particular country is not a destination? I don't have 100% answer for this one, especially if we can prove that something has not happened, but I think it might be doable, and I think it's worth thinking about it. So, can zero-knowledge help humanity survive? Yes, sure. Can blockchain help humanity survive? This is a bit trickier, but yes, but it must be privacy preserving programmable blockchain no privacy no blockchain adoption that simple that's it thank you Thank you for interesting talk. I also strongly believe that blockchains and their pro-knowledge will help humanity to reorganize in the future. Thank you again. We have some time. Okay, then let's try to kind of stay together because the next lightning talk will be started in two minutes. Thank you. May I answer the fourth one? Yeah, if you want. Yeah, about proving to the partner that someone is not cheating. If you are a computer, I think, and she's a computer, I think it is very easy. If one of you is not a computer, it is tricky because when it comes to bridging data from the physical world into the digital world. From my perspective, this is still a very tricky thing. And it wasn't resolved so far. There was a paper in 2013 written by some Princeton guys about utilizing zero-knowledge proofs for monitoring the state of nuclear facility. And they assumed that there is some nice solution to like an a nice solution for a Oracle from physical world to the digital world but up to now there is no such solution as far as I know and that's why all the cases I described they are digital and native from the very beginning so something exists like a piece of code from the very beginning like self-piloting machine or like satellite",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T03:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1lN3h53WUwFynoQ5vR7IcSTB74eoYrpCJWu2YVtIjyTw",
      "resources_slides": "https://drive.google.com/file/d/1IdbON4m-O_WxcoEdRgsVJHvn1lQ_BKGO/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "fuzzing-zero-knowledge-infrastructure",
      "sourceId": "QYWS83",
      "title": "Fuzzing Zero-Knowledge Infrastructure",
      "description": "Zero-knowledge (ZK) infrastructure is highly complex and highly critical for the correct operation of L2 chains; that is, a single bug can result in massive financial and reputational damage. To find such potential million-dollar bugs before they are exploited, we have developed a novel fuzzing technique that can find logic flaws that impact liveness or safety of ZK infrastructure. Our fuzzer has already found 16 such issues in four ZK systems, namely Circom, Corset, Gnark, and Noir.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "ZKP,Zero-Knowledge,Security,Fuzzing,Testing,metamorphic,Fuzzing,Security,Zero-Knowledge",
      "keywords": "Metamorphic,Testing",
      "duration": 1352,
      "language": "en",
      "sources_swarmHash": "bcfdc7603511f26b65425f9d655cd88325107f27d65cce5a4f2e3ada344d1489",
      "sources_youtubeId": "c0lmPEE6qDM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736c85374749a4b89b3ae2c",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1C0qMB9Xtv-bWWVg8T0URvn0L0LP0y88aiS1n8-LmL1U",
      "resources_slides": "https://drive.google.com/file/d/173Qm3wPWejv68uN44W31lJVp2sFSdTzy/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "how-model-checking-can-help-build-trust-in-the-design-of-distributed-protocols-like-single-slot-finality",
      "sourceId": "89M7ME",
      "title": "How model checking can help build trust in the design of distributed protocols like Single Slot Finality",
      "description": "Ethereum is a lively place for developing distributed protocols. Getting a distributed protocol right is a notoriously difficult task. When it comes to developing the Ethereum CL, the community follows two pragmatic approaches: Writing pen & paper proofs and writing executable specs in Python. We show how model checking can confirm our intuition about the behavior of consensus protocols or disprove it. We do so by applying our method to one of the recently proposed Single Slot Finality protocols",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Protocol Design,Formal Verification,apalache,Consensus,Formal Verification,Protocol Design",
      "keywords": "model checking,TLA+,Apalache",
      "duration": 379,
      "language": "en",
      "sources_swarmHash": "1bddf56010a39f73da7d59c09b393729d459d8abbbfad177e9a41ce45d6fc3fd",
      "sources_youtubeId": "9IqwdXnVnsE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d63474749a4b892f230d",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d63474749a4b892f230d.vtt",
      "transcript_text": " Hello, thank you. So my name is Igor Konov. Here are the slides. Happy to be here. Actually, this is work done by myself, Tan Haidran, who is also here in the room, Jure Kukovic, Thomas Pani, who is also here, and Roberto Saltini. And we are happy to have done this work supported by Ethereum Foundation. So if you look at consensus, Ethereum consensus, you probably see a lot of algorithms starting as Casper, Gasper. Now we have single-slot finality. And recently on the block, we have three slot finality. You can check the recent report by Francesco, Roberto, Tanhai, and Luca. And if you look into these things, you'll see that there are a lot of definitions. I cannot explain all of you, all these definitions to you. You'll see that there are chain blocks slots checkpoints justified checkpoints finalized checkpoints votes by validators ffg votes that connect these checkpoints and so on so forth so these are not simple algorithms so in our work we don't uh we don't have uh basically time budget to to do everything so we focus on accountable safety which means roughly speaking that if you have a fork in consensus you should be able to identify at least one-third of the validators that are basically that produce this fork and they should be slashed. So we focus on accountable safety here. How would we be able to check that these protocols, namely three-slot finality, satisfy accountable safety? Well, in science fiction solution, when it's all good, we will take the code in Python, we will take the executable spec in Python, produce some examples maybe to convince ourselves that it kind of works, but also we would also produce an automatic proof of accountable safety. Unfortunately that's a bit of science fiction nowadays, there are no over-the-shelf solutions that would take executable Python spec and reason about such complex algorithms as consensus. So what we have been doing in this project, we actually were writing specifications in temporal logic of actions. That's a language invented by Les Lampert some time ago for reason about concurrent and distributed systems. And we did produce specifications by hand because there are no tools that would be able to do that, although we have been thinking how we could automate that. So basically the first specification we wrote was just too complex for the model checkers. We gradually produced abstractions using this specification and essentially produced like four levels of abstractions here so the model checker could handle the complexity of the algorithm at the end. We used the model checker Apalache, which is offloading the verification task to the SMT solver D3. And in addition to that, as things were a bit slow, we also wrote a specification in Alloy, which is also a well-known model checker that is backed by a sat solver and in addition to that we wrote smt constraints in cvc5 using the theory of finite sets and cardinalities so we kind of did a lot of experiments here to check accountable safety under different uh using different tools so as as I told you, model checking could help you. How can it help you? The first thing it can help you is actually you can query for interesting states. If you have a large protocol, it's not easy to produce examples. And that's what model checking is good about. For instance, here, I'm just writing an invariant saying there are no two conflicting blocks. Basically, there is no fork. And challenge the model checker writing an invariant saying there are no two conflicting blocks, basically there is no fork, and challenge the model checker with this false invariant. Then the model checker comes back in several minutes and shows me an example. So actually these tools work as a good communication tool for protocol designers. Here's an example of such an execution. You don't have to read it, it's just long, but it's machine readable and it's an actual execution in the in this specification so the second thing where these tools can help you is to show some properties not for all kinds of values but for small scopes for small parameters for instance here we have experiments for five blocks seven checkpointspoints, 24 votes. And as you can see, when we increase the parameter space, the tools slow down dramatically. However, we have some evidence that these properties hold true, at least for the small parameters. And that's, again, fast feedback that you can get without proving things in heavy tools. So to come to the summary of our work, we believe that model checking actually helps in ensuring correctness of protocols. We still need humans in the loops, unfortunately. We still need us, basically, to construct these abstractions and specifications. Tune in for the upcoming technical report. We are going to publish all of it and you'll see it. And thank you Ethereum Foundation for giving us a grant. Thanks a lot. Thank you, Igor. Does anybody have any questions? Okay. Okay. I think it was very clear. Nobody has anything. You have two quick and come back again at 10. .",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T03:40:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Xd-R_4o4lETYbwbQd-AVQI0TPre950m6puMNTO8psWk",
      "resources_slides": "https://drive.google.com/file/d/1ioS-bm94TRo0-wW3To7f9ZE4aKNmDRzH/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "mood-rebalancing-singing-bowls-handpan",
      "sourceId": "SVAHJU",
      "title": "Mood Rebalancing (Singing Bowls + Handpan)",
      "description": "By Most Handpan X Ice\r\nThis session helps you feel emotionally centered and peaceful.\r\n- Bring balance to your emotions with singing bowls and handpan. \r\n- Using an emotion wheel, you’ll explore and understand your feelings, a key step to managing them. \r\n\r\nNov 15 10:30 - 11:15",
      "track": "Entertainment",
      "type": "Mixed Formats",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T04:15:00.000Z",
      "slot_roomId": "decompression-room",
      "resources_presentation": "https://docs.google.com/presentation/d/1STERW4iF8WxYtoPJQKN2mZr5qwM1yuH_XYRcXEVM1pw",
      "resources_slides": "",
      "slot_room": {
        "id": "decompression-room",
        "name": "Decompression Room",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "onchain-is-the-next-online",
      "sourceId": "CXZ7UT",
      "title": "Onchain is the next online",
      "description": "The goal is to bring the world into a global onchain economy that increases innovation, creativity, and freedom — and that's only possible on a decentralized platform that’s super easy to use. In this talk, Jesse Pollak, Creator of Base, can share his insights on why building for simplicity is so important for the Ethereum ecosystem, and what he’s learned from building the fastest-growing L2.",
      "track": "Usability",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Account Abstraction,Paymaster,creators,Account Abstraction,Layer 2s",
      "keywords": "Account Abstraction,Layer 2s,UX,Wallets,Developer Tools",
      "duration": 1552,
      "language": "en",
      "sources_swarmHash": "978b7fc60439de724ea5e51af3ba22045297010a8db8f3a42e0913f9fdbcc179",
      "sources_youtubeId": "olXwQyMrDqQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736e6c61b0f83434d10c862",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736e62c1b0f83434d0b798d.vtt",
      "transcript_text": " Tanya Cushman Reviewer Reviewer's Name Good morning, everyone. How's everyone today? Good! My name is Jeff Lau, and I'm a co-founder of ENS. Traditionally, this talk is done by Nick Johnson, but sadly, he did not make it here today out to Thailand, so instead, I will be talking to you instead. The last time I did the E&S, State of E&S talk was in 2018 at Prague. Who here was at DEF CON 4 Prague? Wow, wow. That's actually more than I thought. At that time, five years ago, we were exploring the permanent registrar and migrating from the Vickrey auction to the commit reveal registration of today. Since last time, so much has changed, and I'll be talking about some of those things today. So who here has heard of ENS? Right, that's pretty much everyone. That's good. That's good. Vitalik said this in 2024. ENS is our most successful non-financial application on Ethereum. And I'll go into a little bit of what ENS is for the very few people out there who don't know what it is. So ENS is for the very few people out there who don't know what it is. So ENS is a user experience protocol. And that means we put users first. And that has multiple meanings. We endeavor to create the best user experience to make the blockchain easier to navigate. We want our users to be in control of their identity, creating true, trustless ownership. And the protocol is built to be incredibly control of their identity, creating true, trustless ownership. And the protocol is built to be credibly neutral and permissionless, so anyone, anywhere can use ENS. ENS's humble beginning started with trying to solve one of the innate weaknesses of blockchain. Everything is a long number in hexadecimal, and it's completely unreadable. So really, how are we supposed to scale to billions of users if the first thing they do when they create a wallet is see this? ENS helps you turn that into something humans understand. A name. The simplicity of a name is familiar to all of us. As soon as we are born, we are given a name, and it forms part of our identity. Whether that large number, sometimes called a hash, represents your identity on Ethereum, a decentralized website, or something else, a name is an order of magnitude more meaningful to people. And ENS serves humans, not machines. ENS has several core principles that make our protocol user-centric. It's decentralized, meaning no single person or entity controls the levers and buttons that govern the protocol. It's permissionless, so anyone can register a name and create an identity without having to state who you are or where you're from. And lastly, the protocol itself is built to be credibly neutral. This means we're doing our best to not have codified, privileged actors in the system. Okay, definitions out of the way. Let's talk about the years so far, and we've had a really great one. If you put all of the ENS registration durations end-to-end, you'd have to start 9 million years ago. In the late Miocene epoch, Earth was a wild realm of change. Sabertooth tigers stalk sprawling grasslands while early elephants thunder across open savannas. And despite an abundance of large, sharp teeth, nobody was trying to sell Dentacoin to anyone. Some more seriously though, it illustrates we're brewing pretty damn well. Here's the basic stats. Nearly 2 million .eth names. 900,000 users, of which nearly 200,000 have set up decentralized profiles for themselves, which you can view in Ethereum follow protocol. But ENS was not only about .eth. A truer picture of ENS's popularity has to take into account the enormous number of names using one of our many, many partner integrations. And many of these are completely trustless and cost-free. Linear, Base, and Uni all have released subdomain registrars this year. Linear has completely trustless on Linear L2 using state proofs on L1 for CCIP read verification. Base has them also stored in L2 but uses signature verification. And uni.eth has them completely off chain and verified using a signature. This really shows the progressive decentralization of subdomains that can start simple and gradually be made trustless using L2s and a state proof with CCIP read. And lastly, the £1,000 Gorilla Coinbase. As of today, Coinbase reports they've issued over 12 million CB.ID names to their users. This marks the largest deployment of ENS names to end users since we launched and really demonstrating the value of rolling out ENS infrastructure to absolutely everyone. And that has really been the theme of the past year or more, bringing ENS to more people, more affordably, and with less know-how required. CCIP read and off-chain names have been the foundation of that and have led to a number of hugely successful integrations. Almost 16 million active names, more than 14 million more than .eth names that exist, around 10 million users, and over 400,000 decentralized profiles. .eth only paints a small picture of the success of ENS. But if you include all of our many partner integrations, you'll really see how wide-ranging ENS truly is. But ENS is not only about subdomains. Our success is how widely integrated we are, allowing names to be used out in the wild. Integrations are one of the most important success metrics for ENS. Imagine owning a Visa credit card, but no one accepts Visa. This is the problem we need to tackle by bringing large integrations that allow the use of ENS within their existing users, allowing ENS name resolution to exist outside of our echo chamber. And this year has been a fantastic year for integrations. And I'd like to tell you about a few of them today. But first, while not an integration specifically, I'd like to quickly highlight Gasless DNS Sec, one of our biggest technological advances in ENS to date. We've supported DNS top-level domains inside ENS almost since day one. But importing one came with a substantial fee. Gasless DNS set combines our trustless DNS import functionality with the gateway architecture of CCIP read and makes it possible to make nearly any DNS name function inside ENS without transactions or gas fees whatsoever. And one of the first to use the power of gas as DNSSEC was GoDaddy. GoDaddy added built-in support for every name registered through them, removing the usability barrier of understanding how to set up DNSSEC, and you can set up your DNS name to work in ENS using GoDaddy in just a few clicks. GoDaddy has over 80 million names registered with them, 20 million customers, and just this one integration on its own vastly improves the network effect of ENS. In collaboration with 3DNS, Dotbox is the first truly blockchain-native, fully ENS-enabled DNS TLD. Dotbox registrations are all recorded on Ethereum, and every Dotbox name is automatically ENS-enabled. It's not just the main related companies that are integrating us, though. Bitwise, a traditional finance company who released their Ethereum ETF product this year, has created ENS subnames for each of the addresses that hold the ETH back in the ETF. This is a great use case, and it hopefully makes tracking custody of unchanged assets more transparent and more auditable. Payment providers have started to adopt ENS too, most notably PayPal and Venmo. They rolled out their ENS support for their crypto wallets to all US users early this year. Venmo has 90 million US users and PayPal has over 400 million accounts globally. And this really shows how ENS is really super wide-ranging and going to millions and millions of users. And lastly, can you really say you're a successful protocol until Google integrates you? Today you can search your .eth name on Google and it will automatically show you your balance and your resolved address. Google is the gateway to the internet, and ENS has become important enough to be integrated into the most used search engine. So what do these large integrations bring to ENS? Well, subdomains means more users, more people using ENS for the first time, and broadening the social network ENS creates. Large integrations like PayPal, Venmo, and Google means there are more places for ENS to be used, making ENS fundamentally more useful. In the same thing, we've also been pushing forward L2 UX with some EIPs in the pipelines, the first of which is ERC-7785, that will enshrine ENS to be the namespace to allow discovery of new L2s. In the future, we hope to push further EIPs to help the fractionalization of L2 addresses and address this problem head-on. ENS works on these problems because, of course, selfishly it helps ENS proliferate. But selfishly, it also helps the entire Ethereum ecosystem as well. Born out of Ethereum, we as ENS have never forgotten our roots, all the values that have come out of them. Ethereum is not just in our name, but in the DNA of E&S itself. For seven years, primary names have only existed on L1, which limits their use to people who are willing to pay Ethereum gas fees. Being able to set a primary name on any chain, effectively for free, will be a huge unlock for the Web3 UX. And this is the first example of a more for free, will be a huge unlock for the Web3UX. And this is the first example of a more flexible contract that will be seen to be broadly in ENS v2, which was announced in May, and marks the first time that any ENS contract will be officially deployed to L2. And I'm proud to announce that primary names are coming to L2 and are available on testnets today. Reverse registrars, the smart contracts that power L2 primary names, are live today on a handful of testnets and should be coming to mainnet by the end of this year. And to be clear, there's nothing for developers, non-developers to do this time, but if you're a developer, you can go to testnets today and play around. Now, things have really changed in the last seven years. I mean, look at our old 2017 logo. L2s are the norm now, and smart contract accounts are commonplace. Gas prices are fluctuating, and it's not sustainable to stay on L1 anymore. In May, we talked about moving to L2, driven by several key reasons. The first is reducing the overall cost of interacting with ENS. The second is reducing the number of transactions in all possible situations. And the last is we want to let all users transact from the chain of their choice without the need for on and off ramps that can make L2 so painful. With these UX goals in mind, we are aiming to make a true improvement to ENS UX without the usual trade-offs. With that, on Monday at Friends Day, we announced our plans to build our own chain, which we're calling Namechain. Namechain is built on a few core things that are really important to us ENS. Today, ZK EVMs are the only way to get transaction finality in a reasonable amount of time. And because ENS is one of the only applications that requires reading state from L1, we need the fastest finality we can to ensure that everything can be read from other L2s. ZK EVMs use ZK proofs to prove the correctness of a state transition on an L2 and commit this to L1. The ENS resolution process can then use CCIP read to verify the data from name chain. Contrasting this to an optimistic rollup, the industry standard would require us to wait seven days before our name can be in use. The second is, of course, the stack must be complete open source, allowing it to be easily audited, edited, and forked if required. And lastly, we care about the credit neutrality and decentralization of ENS, and maintaining that is a priority. We're exploring whether shared sequencing or base sequencing makes sense for us at NameChain. As for decentralization, it is non-negotiable for us. We're going to be building L2 to L2 bridging right into the protocol. So you can do things like commit whilst you bridge and pay from your preferred L2. This will allow you to start your ENS journey from any L2, lowering the barrier to entry when you buy your first name. And this is one of the primary reasons we want to launch our own chain versus deploying to another public chain. Being able to control the entire stack from protocol to governance means that this roll-up is here to serve ENS and naming. This is super important to us as we at ENS are committed to being L2 agnostic. Name chain is the L2 we're building. But we're not just building an L2. ENS v2 is a complete ground-up redesign of ENS. And only one component of that is having a dedicated L2. It's designed to be more flexible, more accessible, and more affordable. So we continue the momentum started with CCPI read and off-chain names and expand its utility to every user and application that needs it. So how are we going to do that? Well, first, we've redefined how names are registered and recorded with a completely redesigned registry. Second, .eth names will be recorded and by default hosted on our dedicated L2 name chain. And finally, a user-driven migration with backwards compatibility. And let's explore these in a bit more detail. .eth on name chain means that all registration renewals will no longer live on Ethereum L1, but instead on name chain. Storing it on name chain means that all registration renewals will no longer live on Ethereum L1, but instead on name chain. Storing it on name chain means that registration renewal gas will be significantly lower. Although Ethereum has its conveniences, it's not a long-term solution for gas prices on L1. Having registration on name chain means we can ensure registration gas costs are as close to zero as possible. However, even though .ethnames will move to name chain, name resolution largely remains untouched and begins from Ethereum itself. Let me reiterate that. ENS resolution, reading from ENS names, remains anchored on L1. And that is a fundamental difference between ENS and other protocols moving to L2. We do not fractionalize our protocol. We connect it together and remain as one. And this is how we remain L2 agnostic. L2 support was a consideration for the beginning of the design process of V2. And that means we can also deploy the V2 contracts to L2 networks. Combined with CCIP read, this makes it possible to seamlessly stitch together a coherent namespace from multiple distinct networks and L2s. This architecture reinforces how we are staying L2 agnostic, beginning everything from L1, but only moving .eth to name chain while supporting ENS on other L2s. Okay, let's talk about the complete technical rewrite of supporting ENS on other L2s. OK, let's talk about the complete technical rewrite of the ENS registry. Registries are contracts that store data about that name. The current registry stores the owner and resolver records, and currently all names are stored in a single flat registry. This design is straightforward, but also has a couple of significant drawbacks, one of which that it's difficult to find custom rules for name issuance and ownership. Another is that because each name exists independently in the registry, when a name is transferred or deleted, all subnames remain unchanged. We're solving both of these problems while adding more flexibility to the system as a whole by introducing a new registry design in ENS v2. In v2, each name optionally has its own registry which deals exclusively with its subdomains. There's a root registry which contains all the top-level names. Each top-level name has a registry containing all the second-level names such as nick.eth and so forth. Using a hierarchy of registries has a number of advantages. Anyone who owns a name can now supply the registry implementation of their choice, giving full control on how subnames are issued and controlled. This allows all the functionality of the name wrapper and more, whilst preserving the flexibility for name owners to set their own rules, ownership, and even resolution. When a name changes hands, or if an owner wants to start from scratch, they can easily replace the registry from the name entirely, erasing all existing subnames, allowing a very easy, fuss-free way of clearing up the hierarchy of your name. Likewise, because each registry exists independently, we can even insert registries at multiple places in the ENS hierarchy. When you're ready to move from a subname to an ethname of your own, you can bring everything with you while still ensuring your old name functions as it did before. In line with ENS's core principles, moving to v2 is an entirely user-driven and opt-in process. All registrations will be transferred to the new L2 chain, because by its nature, a registry has to be stored all in one location. But the names themselves, their records, and subnames, however, can continue to exist on Ethereum or any other L2 or off-chain storage solution. After launch, users can migrate their names to v2 with a single transaction. This will move the name from the legacy flat ENS registry to the new hierarchical one. At the same time, they can choose to either leave the name on L1 or migrate it to name chain. And names that aren't migrated will continue to function indefinitely. The new ENS contracts are programmed to automatically fall back to looking up names in the version 1 registry. This means that for names that are owned by immutable contracts or for users that can't or won't upgrade, functionality of their name is safeguarded forever. It's been seven years since the inception of ENS. Google, Venmo, PayPal, GoDaddy. Wow, we couldn't have imagined seven years ago that we would get so far. And ENS is Ethereum-aligned and always will be. That's why we always enjoy coming and giving the state of ENS to DEF CON every single year since DEF CON 1 or 2. And L2 reverse registrars on testnet today, it's something we've been working on for a little while now, and we're really trying to bring the interop into Ethereum, and that's really been the theme for this DEF CON. And ENS is expanding to name chain. But remaining on L1 whilst remaining L2 agnostic. And ENS v2 will improve the UX of ENS across the board for years to come. And lastly, this talk was too long to mention some of the amazing ENS DAO service providers this year. We're not the only ones building ENS. And it would be impossible to expand to a billion users without help. To name a few, first, the Ethereum Follow protocol is bringing the social profiles of ENS to the masses. NameStone is powering many of the subdomain integrations and Unruggable for building the generalized gateways that are powering CCIP read. And of course, my ENS Labs team for building everything that we've seen today and supporting us today and for years to come. And finally, the adventure is not over. If you're looking for a position at ENS, we're always looking for good people. So if you'd like to work for ENS Labs, please check enslabs.org for job postings. Thank you and enjoy your last day at DEF CON. Thank you so much, Jeff, for that. I think that was fantastic. So as you can see here, if you've already asked your questions, that is great. What you can do is you can also upvote a question. So if you really, really want to have it answered, it can move up to the top of the question. So we're going to start off with the first one. What about privacy? Using ENS is a major privacy leak and can dox a user. Sure. I mean, this is a question we get a lot for us at ENS is a usability protocol on top of Ethereum and L2s. And Ethereum and L2s have this issue regardless of ENS. So obviously you need to practice good OPSEC and make sure your address is not linked and do the things that you do to protect yourself with addresses. Having a name on it doesn't additionally create less privacy. So you obviously need to create that good OPSEC no matter whether you're using ENS or not. All right. And then we've got Google integration supports .eth domains, but not the corresponding subdomains. Why is that? I mean, if anyone from Google here can do this, it would be fantastic if they could. Obviously .eth is our flagship product. Google really respects that and wants to do some integrations and starting with .eth. But hopefully in the future, we can really push them to do self-domains as well. And if anyone has connections at Google, please talk to us and we'll try and get that in. Okay, remember you can still add some more questions if you feel like none of the ones that you have are on here. We've got, can builders use name chain for other stuff? Yeah, so as I said, name chain will be on a ZK EVM most likely and that means it's EVM compatible, so it's going to be a public chain that anyone can use. We're not going to be permissioning the chain so you can deploy your own resolvers. All right. Then at the top here we've got, what are some of the problems that stand in our way to increase mainstream adoption of ENS? I mean, I think this applies to all of Ethereum and all of blockchain itself. It's really about getting... improving our wallet user experience, having names and all of these things to replace addresses. And once you do that, you can get more blockchain and Ethereum support across the board, in your cafes, in your paying for things day to day. And then it'll be very easy to kind of improve the mainstream adoption of ENS. I believe already ENS is quite widely supported across blockchain. So it's really about expanding the entire ecosystem itself so ENS can serve a larger ecosystem. All right. Can Namechain be Keystore roll-up as well? It is something we talked about, and I don't think it's something we can rule out. I think a Keystore roll-up would be absolutely great. But obviously there are technical difficulties to the Keystore roll rollup as well. So if we can solve them for other chains, I'm sure we can do it for Namechain as well. All right, we got one that just got upvoted quite hard. How will the ENS token be integrated with Namechain? That is something up for debate right now. I believe the token is currently on L1, and for now it will stay on L1. Obviously, this is discussions about where we would put our DAO and governance contracts, but yeah, that's still up for discussion. And then a few more questions here. How does ENS compare with unstoppable domains? What do you guys think? How does ENS compare with unstoppable domains? I think we're doing pretty well. I feel like that one was definitely a trap. The next question, when will the ENS manager app support EFP? Oh, we'll have to talk about that, Brantley. All right. What has been the greatest challenge for ENS? The greatest challenge for ENS? Oh, there are many. I think it probably starts many, many years ago, and when people didn't really believe that we were a project worth supporting, but I think we've overcome that now, and it's really like a non-starter if your application doesn't support ENS names. So I think just overcoming that just shows we're here to stay, and we're here to help all of you builders out here.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1-gQZPtDYukgyGQgCLVng3phznkejfM-uJlR1MDiF-MQ",
      "resources_slides": "https://drive.google.com/file/d/1AvEAd1lBJUlv5pYg3fJwNBUomazD-31r/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "superliquid-mechanisms-for-decentralized-stablecoins",
      "sourceId": "SLNQ8K",
      "title": "Superliquid Mechanisms for Decentralized Stablecoins",
      "description": "USDC and USDT outpace decentralized stablecoins in large part due to their liquidity. This talk covers the theory, data, and risks of stablecoin liquidity innovations. This will include mint/redemption mechanism design, liquidity pool design, rehypothecation, and protocol-owned liquidity. The analysis will distill how the flexibility of decentralized stablecoin issuance mechanisms can safely be used to their advantage over centralized stablecoins, which Gyroscope v2 is putting into practice.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Mechanism design,Economics,AMMs,defi,AMMs,Economics,Mechanism design",
      "keywords": "Stablecoins,DeFi",
      "duration": 1533,
      "language": "en",
      "sources_swarmHash": "59211204d3c04b626b444be0436bb8a47a78e7244db6c5f669e22c0bd4079e86",
      "sources_youtubeId": "TdAa95XDFSw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673d906517a97b4f4d386947",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Uq2Z7r9A4ctbRuT4PbYzFJRFe2xqpvo_AnrVxHcMjiU",
      "resources_slides": "https://drive.google.com/file/d/1IUb5eSKmm_AG1EKxnCuTLAUXodcL6xHE/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "the-future-of-web3-grants-learnings-from-studying-30-programs",
      "sourceId": "F9YCZY",
      "title": "The Future of Web3 Grants: Learnings from Studying 30+ Programs",
      "description": "This presentation will cover learnings from studying almost 3 dozen grant programs across multiple chains and ecosystems. I will present an overview of the state of grants across Ethereum as well as Bitcoin, Cardano, Solana, and other chains. I will present on the most pressing challenges for grant operators, feedback from grantees on their experiences, and will present a potential path forward in terms of collective priorities that can help all programs improve.",
      "track": "Coordination",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "capital",
      "keywords": "Grant,Allocation,Capital",
      "duration": 437,
      "language": "en",
      "sources_swarmHash": "53db20665d6eb076c87dcfb4bd938bed30aa84c796cc95e39f06bdc9eff7351c",
      "sources_youtubeId": "Vp6ju5k3w3A",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f1cc74749a4b8923840a",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736f1cc74749a4b8923840a.vtt",
      "transcript_text": " Hello and good morning, everyone. I'm Eugene Leventhal. I'm currently working on governance at scroll and I'm a research director at MediGov. Today I'll talk about some of the research that we've been working on over the last two years at MetaGov in trying to also get a qualitative understanding of the nature of grant programs in our space. And so we can better interface with grant programs beyond Web3 as well. So today, I know we don't have a lot of time, so I'm going to run through some quick context, some takeaways, and kind of what I'm thinking might be happening as we go into 2025 and beyond. And so to start with some context, you know, as part of this research, as mentioned, we've covered a bunch of different grant programs. This is just a sampling of some of them. So you'll see it's, you know, a lot of the major programs in the Ethereum world, as well as some other L1 ecosystems as well. So we're lucky that we're getting a view well beyond any kind of single set of programs and a lot of these folks have very different priorities so it is really interesting to capture that. And this year as well we expanded to not just cover programs but we also got into covering grant tooling as well. And as you'll see here we aren't covering any of the grant stacks themselves, we kind of chose to focus on tools around the ecosystem that can make it possible for grant operators to perform better, for grantees to better understand how they're performing relative to others, and so that we can all just have a better sense of the data infrastructure around the space. And so I wish I could just point to a slide with a 100% confidence of this is the amount of grants shipped. And that's actually its own challenge is because a lot of the data is fractured and siloed. Not all programs are on chain. Even the ones that are need a little more sense making. There aren't perfect databases. You know, from the programs that we've covered, we've seen over $1.2 billion USD go out. We haven't covered the formal RPGF programs and the Arbitrum and Tyco and some of the other very large programs out there. So, you know, there are multiple efforts, including Open Source Observer and DAO IP5 from MetaGov and DAO Star Star where we are trying to improve this landscape. So I hope at DevCon8, wherever we'll be in the world, whether me, Shinya, or anyone else working on these problems, someone will be able to show off this wonderful new database of clarified information. Let's get into the actual takeaways from studying these programs. One thing is on the mechanism side. We see that basically, you know, perspective grants, I'm giving you money to go try a new thing, and as of more recently, I'm putting in at least some milestones in there. That's kind of the norm right now. And in the last, you know, six to 12 months, we've definitely seen more and more experimentation. I feel like thanks to Optimism, the retro programs are definitely getting the most attention. But if anyone caught Rafa from zk-sync presenting in the GovHub yesterday, you saw like 15 different little experiments running. There are a lot of cool mechanisms being created. I think Ohwaki in his book charted around 50 plus potential mechanisms to utilize. And we're going to definitely see a lot more experimentation in the coming months and years. Also looking backwards, recognizing that we all want to better understand our impact, but what we actually mean by impact and how we go about studying it has been a huge challenge. And so at the very least, as a space, there's that recognition that we want to understand the impact. We don't feel like we really understand the value of all the grants shipped, but we don't know how to do it. And related to that is also thinking about, well, great, you gave someone a grant. Was that actually enough to help them succeed? Right? A lot of grantees actually need more than just money to ensure that they're able to accomplish their desired outcomes and impact. And as mentioned, we're seeing more tooling and infra. And as we start thinking about the future, you know, just the tools themselves are great, but they're not always enough. We need more documentation of what's happening. We need more both qualitative and quantitative analyses. We want to see more experiments happening. And I personally think in this next year, we're going to see a strong embracement of pluralistic grant programs. So a single funding entity might have five to ten separate programs running with various levels of coordination between these programs. single funding entity might have five to ten separate programs running with various levels of coordination between these programs. And, you know, I think we will see kind of some data and operational coordination across programs, but then letting each program kind of run their own thing. In the context of impact, and if anyone had a chance to drop by the grant hub yesterday, you know, we had a whole day focused on impact because, again, there is this recognition. And so some groups are getting open to embracing this program evaluation model where we better understand what are the outcomes, right, the milestones you commit to, what outcomes do those produce, and over time, how does that play out into impact? But we need more accountability of the grant programs, of the grantees, and throughout. So definitely looking for more improvement and adoption of some of these tools and use cases. With that, unfortunately, the time is up for today, so I'll just shift over to questions, and if anyone wants to geek out on grant programs later, please always feel free to find me. Thank you so much, Eugene. We see that there are no questions yet. So please scan the QR code to submit your questions. But while I have Eugene on stage, I'm really curious, are there any key takeaways from grant programs run by other chains that you think the grant programs in the Ethereum ecosystem can really learn a lot from? Yeah, I think the first thing, and this isn't exclusive to Web3 individually, you know, we have already run some events where we've had traditional foundations and re-granting non-profits show up. I think it's just the recognition that impact measurement is hard for organizations that have been doing this for a century. So we shouldn't feel bad that it's hard for us. And especially going back to one of the questions that I saw Shinya got, right? A lot of grant programs might only have one person running the entire program full time. That's not enough to both build really good systems, build relationships and figure out your impact. So if you actually are trying to deeply commit to doing grants, you have to be willing to commit to the operational overhead that inherently comes with doing it well. And I see the questions pouring in now. So look at the first upvoted one, tips and tricks to find good grants. Yeah, so I think for the most part, the days of free money are over. You know, for those who have been applying, you could just like show up, vaguely express interest, and then magically money appears in your wallet. At this point, you definitely need to focus on having a clear vision of what are you trying to accomplish and what does that mean for the ecosystem that you're applying to. Don't focus on spray and pray applying to every program you can. Pick one, maybe two or three that you could deeply commit with and work with and talk to them about, well, how does this help you? How can this be a two way relationship? Thank you so much, Eugene. Unfortunately, that's all the time we have. So let's put our hands together for him.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T03:40:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1kRi6qfFHeK8txYMq58KLUaOTV4stHccKNP0m-WyZWWg",
      "resources_slides": "https://drive.google.com/file/d/1C0x3d-0uCFsO6Fi8OyoxrZyKvdMWX9Nl/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "the-hunt-for-impactful-use-cases-from-the-crypto-for-good-fund-what-15-blockchain-pilots-revealed-in-emerging-markets",
      "sourceId": "TV3QRD",
      "title": "The Hunt for Impactful Use Cases from the Crypto For Good Fund: What 15 Blockchain Pilots Revealed in Emerging Markets",
      "description": "* This talk will provide a snapshot of the some of most impactful real world uses of web3 in emerging markets covering the additionality added by blockchain. \r\n* Additionally, the talk will deep-dive into the insights and results of 3 web3 pilots funded by Mercy Corps Ventures in Africa & Latin America, showcasing how web3 is addressing the needs of financially underserved and climate vulnerable populations.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Use Cases,RWA,Ethereum for Good,latin,america,Ethereum for Good,RWA,Use Cases",
      "keywords": "Emerging Markets,Africa,Latin America",
      "duration": null,
      "language": "en",
      "sources_swarmHash": "43e1e9395b8434ec533fbc43720e541e8b9bbd1ba59495a15958edbbc71e3873",
      "sources_youtubeId": "180uJuutaYQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1vwkrczNxrHXLNfycNjtYzjJo4jXX3Z2RUJ7NWPh4OMQ",
      "resources_slides": "https://drive.google.com/file/d/1M_H16XPRyv40wQjr4Nz7RTEixXFZ95vf/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "the-tension-between-mev-and-censorship-resistance-gadgets",
      "sourceId": "G3MBF7",
      "title": "The tension between MEV and Censorship Resistance Gadgets",
      "description": "Although fairly unrelated at first glance, MEV is currently *the* bottleneck for a censorship-resistant Ethereum. This talk will first explore why MEV and censorship resistance are fundamentally counterforces. Then, we will dive into how MEV constrains the design space of censorship-resistant gadgets like Inclusion Lists and Concurrent Block Producers. What does the future of censorship resistance look like for Ethereum?",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Ethereum Roadmap,Censorship Resistance,Design,MEV,protocol,Censorship Resistance,Ethereum Roadmap,MEV",
      "keywords": "Inclusion Lists,Protocol Design",
      "duration": 1463,
      "language": "en",
      "sources_swarmHash": "d9d78ece0ddf69b8a9645577faf0b8079b898bf69e9ec1ff922fed3e2860bd2d",
      "sources_youtubeId": "4OJ1eCtEAVs",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f4bb1b0f83434d475cef",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:30:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1q6BQXCGubElt47T2cCMmisWZixsWRezzeO8I3FiONPU",
      "resources_slides": "https://drive.google.com/file/d/1i5SzPIVMPh8S46n_z8uvZ66Vo5JCmXBs/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "dare-to-be-solo-staking",
      "sourceId": "ZMSNHW",
      "title": "Dare to be Solo Staking",
      "description": "I have been solo staking on my home computer since the very first day of the beacon chain. It's been a challenging journey, and I anticipate it will remain so in the coming years. This talk will delve into the time, financial, and technical commitments required for solo staking. It aims to provide a practical overview of the solo staker experience from an Ethereum enthusiast's perspective. I will highlight what is keeping us from a wider solo staking community.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Staking,Home staking,Best Practices,Public good,Best Practices,Home staking,Public good",
      "keywords": "staking",
      "duration": 549,
      "language": "en",
      "sources_swarmHash": "e303a2a1ebd6ba053493f2c8a2e7ee2fa9e9782406379462d20600be1549f1f9",
      "sources_youtubeId": "w3dZMvSOYzg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d6cf74749a4b89309278",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d6cf74749a4b89309278.vtt",
      "transcript_text": " There should be solo staking because it's important because the networks need your help. So I've been solo staking for four years. This is one of my validator 145. I got very lucky this week because not only I am in DEF CON enjoying the community, but also I mined the block. I proposed the block on Tuesday. If you're looking for me, I have the same model everywhere. And I have a personal history with staking because my firstborn was born actually the day we did the merge. So I had to take the wife to the hospital and also monitor my node, like, is it going to be okay? And the core devs did an amazing job, and the community did an amazing job. Everything went right, and also the surgeon did an amazing job. The kid is super healthy. Anyway, a little throwback to when Ethereum France was born. We were hosting meetups every month. We're now hosting ECC as well as meetup and so on. We're translating technical papers in French. If you're looking for a good tutorial on how to stake in French, go to ethereumfrance.com. And before that, we were hosting mining meetup. It was keeping Ethereum weird. We were convincing people to run their mining node, to run a node, to discover smart contracting and so on. And now we have a good mission, is to make people stake at home. Because it ain't much, but it's honest work. It's because we have individual stakers at home that we are keeping Ethereum from being a glorified centralized database. The censorship resistance starts with you all in this room. If you care about Ethereum, you should stake. Ethereum is an unstoppable machine because we have a lot of nodes. But don't look up too much. It's hard to say from the network topology on how many nodes exactly there is going on. If you look at Ethereum nodes or if you look at Etherscan and different scanners like, yeah, well, 6K, give or take. If you look at the beacon chain and the different staking node alive, validating node on the beacon chain, you're like, well, give or take maybe 11,000. Yeah, so that's the branch, like between 6K and 11K probably. And there's a lot of validators that have multiple times 32 ETH on their nodes. So there's a lot of room for solo staker. We need to get this number down. Like, go, go out and solo stake. And I'm here to convince you to solo stake. So there are different stages of staking. Who here is staking? Woo! A lot of solo stakers. Like, go out there. You already know this presentation. And talk to your fellow Ethereum people. Get them to solo stake. Get them to solo stake until the state of illumination, home staking, where it can be home staking. You can stake your ETH on a centralized exchange, sure. You can use a managed staking provider. You can be a liquid staker. You can be a remote staker. And you can even be a home staker. You enter the fellowship of the home staking. But not everybody is home staking, unfortunately. And the best data on why people are not solo staking are probably coming from Vitalik's polls. You have cool numbers, like running a node is too hard. I don't have 32 ETH. I need to take ETH instantly. I'm not a solo staker, but I'm staking still. I want DeFi yield and so on. Well, okay. If you want auto DeFi yield, I cannot blame you. Getting richer is better than getting poorer. But still, if you're solo staking, you're going to be getting a lot of airdrops. So this is not a good reason to not solo stake. If you have less than 32 ETH, that's understandable. It's expensive now to get 32 ETH, but we have an EIP coming up to let you stake less than 32 ETH. Not being able to take out ETH instantly is a good reason as well, but still, we can be better than that. We can be able to withdraw the ETH faster. There are EIP coming up for that. If you are concerned about private key risk, come on. You've been handling your private key if you're sitting in this room. And there are good things to manage your private key as well. It's getting better and better. So running a validator is too hard? Well, if you're picking the most common way to solo stake, you have GIF and Lighthouse. If you're running on Ubuntu, that's like seven lines of code. So it's not that hard.stand tutorial on how to do it. Ethereum France has one in French, so check it out. But even more, there is thousands of giga-friendly chat to help you on solo staking. Here are a few of them. Eve Staker, Heroglyph, there is even a community hub here. Eve Pillar, Stereom, they are here as well. The Dappnode guys, like everybody is here to help. We want you to solo stake. So go out there and talk to your friends about solo staking. If you're solo staking, you're also helping up the diversity of the network, which is making us even more resilient. You have five flavors of execution nodes and five layers of consensus nodes. So you get to bring the diversity to the network and more resilience to the network. Okay, maybe you're still not convinced and you're not ready to make the move. Well, get yourself a Dapp node. It's $1,000 or something and get yourself one Gnosis Chain token and you'll be able to solo stake with your Gnosis Chain token. And I couldn't finish this presentation without an unsolicited note picks. This is my node. It's running and kicking. And this is also my Gnosis Chain node, a little DAP node. You can do it. If I can do it, you can do it. And if you want to chat about solo staking, I'll be hanging around next to the red room in the 114 room. And you can ask me anything about solo staking. And we have two minutes for questions. So fire up. Yeah. I wish I had this 32-if to stake. Now, but you can also stake with less than 32-if. I don't know. Not solo staking pool, but you have staking pools that you can participate in, and there is an EIP coming up to let you stake one-if. I think that's the objective. Yeah, it would be definitely interesting. Are there any questions for Jerome? But if there is no question, you can talk to me outside if you're too shy to ask. Of course. Come on, it's a room full of solo stakers, I know. Well, it's okay. You know how hard it is. Yeah, you. Won't I get slashed if I solo stake? You won't get slashed if you're solo staking and just running one execution node and one validating node. As long as you follow the principle of I'm running a node on the beacon chain and I'm running one node on the execution layer, you won't be slashed. It won't happen. The slashing happens when you are trying to do weird things. Like I'm trying to have replication of my execution node, I'm running multiple validating nodes around the thing. It can end up with you being slashed, but if you're just solo staking with one machine, one node for the execution layer, one node for the validating layer, there's no way you're getting slashed. Absolutely not. It's not a good reason not to solo stake. Anyone else? And what about power outages or like... Oh, yeah. So that happens. Indeed. If you look at my statistics on node 145, validating node 145, it happened to me a couple of times that my power went down and I was inactive for a few days. Or it also happened to me that I didn't take the whole SSD, lots of memory thing seriously. So I got like five days of outage. So when you're not validating, you are not earning. And you are losing what you're not earning. So typically, if you're earning like 0.001 ETH per day, you are losing 0.001 ETH per day. So if you've been out for five days, which can happen, you have to be in for 10 days to compensate the thing that you lost. Well, what you don't earn, you lost it. Thanks. But that you lost. Well, what you don't earn, you lost it. Thanks. But that's okay. In the meantime, you are solo staking, and you are making more than what you usually get on the mining pools. Okay, we have time for one last question. Can it run on old hardware? It can run on old hardware. The main limitation is that you need an SSD. Like you need a really good hard drive, like a SSD hard drive and four terabytes. That's the main blockers that you need a lot of memory and super fast memory. So your motherboard needs to be able to handle SSD, of course. And now in terms of processor and RAM, it's not that expensive. it's not that expensive. It's not that much. So if you have an old desktop from like 2018, that's fine. As long as it can take an SSD, that's fine. And you can see at that node, it's a very efficient node, and it's a super small nuke, so it's fine. Okay, thank you very much, Jerome. Thank you, guys. Enjoy DEF CON.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:40:00.000Z",
      "slot_end": "2024-11-15T03:50:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1YmHz7J7_ErPzoGv9lX-paIBhxZrCFEk_NqqiRA-wNk8",
      "resources_slides": "https://drive.google.com/file/d/1DgoUV3fxjTFzkGdsm845p2f8Fpae5jTb/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "memecraft-effectively-communicating-crypto-concepts",
      "sourceId": "FAKRPS",
      "title": "Memecraft: Effectively Communicating Crypto Concepts",
      "description": "Memes have been crucial to the proliferation of various concepts and ideas within the crypto space (ultrasound money, (3,3), regen/degen, QF) which has led to real capital being allocated toward impactful outcomes. The downside to some of this memeing however has been misleading narratives and misunderstandings. How do we leverage memetic power for education and tacit understanding of complex concepts?\r\n\r\nThe workshop will include 1) Scene Setting 2) Structured Discussion and a 3) Group Activity.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Public good,Marketing,User Research,memes,Marketing,Public good,User Research",
      "keywords": "memes",
      "duration": 1535,
      "language": "en",
      "sources_swarmHash": "9136a3b8cc8e010316494d6e453d22cb6c424cebbd28d13c9d36695708dd3aa5",
      "sources_youtubeId": "BfnDgX9uy5E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673dbf5a17a97b4f4d3ce491",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:40:00.000Z",
      "slot_end": "2024-11-15T04:10:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1WKMS7RU7L0T4jR34wKgLFODsY4ligbUzbHahkZWhf6I",
      "resources_slides": "https://drive.google.com/file/d/1IYAO7yGaKhPZhvBfGT9u1ZTZCYaZg95r/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "wtf-are-based-rollups-and-preconfs",
      "sourceId": "UG79AE",
      "title": "Wtf are based rollups and preconfs?",
      "description": "The rollup-centric roadmap is critical for scaling Ethereum but has introduced fragmentation of users, developers, and liquidity. But don't worry, based rollups are here to save the day! But wtf is a “based rollup”? And wtf are these “pre-confs” that usually get talked about together?\r\n\r\nThe focus of this talk is to demystify these concepts and try and get more people engaged in the based rollup ecosystem, which has the potential to heal Ethereum’s fragmentation problem.",
      "track": "Layer 2",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "Validator Experience,Layer 2s,Rollups,sequencer,preconfs,pre-confirmations,Layer 2s,Rollups,Validator Experience",
      "keywords": "Based Rollup,Preconfirmations,Sequencing",
      "duration": 462,
      "language": "en",
      "sources_swarmHash": "a0f742f4e79679bf245603fb9d0b0337c6bb2cbbe6a4e47ea5a1b04b9dc8bed6",
      "sources_youtubeId": "j4wLhmXaZn8",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673838b41b0f83434df58cff",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673838b41b0f83434df58cff.vtt",
      "transcript_text": " All right. Thanks for coming, everyone. I know it's kind of early. So this is a lightning talk on based roll-ups and pre-comps. And as you know, these lightning talks are extremely quick. So this isn't going to answer all of the questions. It's really going to try and lay the foundation and motivation for why different teams are working on this. So I would want to start with like, what is the big motivation here? And the big goal of these base rollups really is to help solve this fragmentation issue we're starting to see in the L2 space and to restore some value capture back to the base layer. So how did we get here? Well, Ethereum is always in this tricky position. The goalposts always move. Gas was too expensive. We created this roll-up centric roadmap. We succeeded in offloading all of this execution. Things got cheaper. TPS increased. But ETH is dead. So we really want to try and address some of these problems head-on. And the big focus of this talk is around fragmentation. So currently these L2s aren't interoperable with each other. They fragment liquidity, they fragment users, they also fragment developers. You have to pick a winning ecosystem to deploy on or deploy across many, which starts to spread your resources. And what we're really seeing is this kind of convergence on what I'm calling intra-op. You have interoperability within your ecosystem but not across these ecosystems. So how do we fix this fragmentation problem? Well, one easy solution is that we just agree on one entity to sequence all of these rollups. And that sounds pretty centralizing. So can we do this in a way that preserves a lot of the values that we care about? So enter based rollups. The idea here, this is a quote from Justin's paper. The TLDR here is it's a based rollup when it's sequenced by Ethereum validators. So in this picture, on the left-hand side here, we have centralized sequencing. The idea is you have these unordered transactions. A centralized sequencer's job is to order them for the rollup. These little squiggly things are the rollups at the bottom here. Okay. As we move to the right, we're increasing in decentralization, and we're unlocking interoperability. So with shared sequencing, you have multiple parties that are all agreeing, according to some leader election mechanism, on who has the ability to sequence all of the roll-ups. And as we move all the way to the right, we enter this based sequencing mode. The idea is that the transactions for these L2s will be sequenced directly by Ethereum validators. And how does this help? How does this unlock interoperability? The idea is that we have these write locks over L2 state. When an Ethereum validator is going to propose a block, they have a write lock over the entire L1 block and all of the L2 blocks that are going to be included. And when we have a bunch of rollups that are all agreeing to be sequenced by this validator, it unlocks this ability for you to start passing messages across these rollups that are all agreeing to be sequenced by this validator, it unlocks this ability for you to start passing messages across these rollups. We don't need these bridges. We're able to do these more seamlessly. So this has limitations. One of the big issues with based rollups is that they have really 12 second block times. A lot of users wanna come to L2s because they care about that snappy UX, those instant transactions. We can always reduce the L1 block times, but that's a very long, arduous process that has a lot of unknowns and centralization vectors. So, pre-confs, this is another one of these new terms, that stands for pre-confirmations. A pre-conf is a commitment made by these validators to users about doing something related to block proposals. So this could mean I'm giving a guarantee to a user that I'll include their transaction when it's my turn to propose a block, or I can even give a stronger guarantee, like this will be the state after executing your transaction. And if I break my promise as the pre-confer, then I can get slashed on various means. So to kind of wrap this up, like how does this all come together? So the user over here would be able to send their roll-up transactions to be sequenced by an Ethereum validator. They, in response, give back this pre-confirmation signature, which is like this receipt for the users, guaranteeing that their transaction will be included or it'll be executed inside of the rollup. And if the validator does break this promise, they can be slashed by submitting evidence to the slashing contract. And what does this enable? Well, it solves a lot of these UX problems. And when we start to enter this execution pre-comps, we really make it to a place where we can actually outperform these Alt-L1s by giving these very instant transactions back to users. And this all comes without modifications to the base layer. So hopefully this maybe piqued people's interest on this topic, but of course, in a five-minute lightning talk, there's still many, many things to be explored. So thank you all for joining. Okay, yeah, we have a few questions. Right, yeah, so the first question here, how does this notion scale if they need to validate all of the L2 transactions? So this is a great question. So I think there's kind of two worlds here. Like one is sequencing itself doesn't imply execution, so it doesn't have to take on all of the load. But realistically, there's been a lot of work to get the data out there. And, you know, I think that's a big part of the reason why we're doing this. And, you know, I think that's a big part of the reason why we're doing this. So, I think that's a big part of the reason why we're doing this. And, you know, I think that's a big part of the reason why we're doing this. for me, these pre-conf networks will likely require consensus. Is this the biggest drawback? So definitely over the past year, it started from this very dark forest, unknown, and over time, we've started to untangle it. And some of the bigger questions are now just around pricing. But really, you don't need an actual consensus protocol to build this. You're able to just broadcast actual consensus protocol to build this. You're able to just broadcast these messages directly to the users. And if the user doesn't get their pre-confirmation, they're able to go and slash. And maybe one last question. Why is Spire better than Puffer? Why is Spire better than Puffer? Well, we're all here building based rollups. So, yeah. I understand. So everyone has their own vision for the best approach. Thank you very much. So thank you. Please give a round of applause to our speaker. Thank you. Thank you. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:40:00.000Z",
      "slot_end": "2024-11-15T03:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1XBmbnq_59WsG85OTcNpUu6A8prP6pC2w2YjOs_3x7-Y",
      "resources_slides": "https://drive.google.com/file/d/16ia-BPY-_Aijs5eSGwaF9obnGQmhp4iH/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "demystifying-solo-staking-struggles",
      "sourceId": "9KV8UQ",
      "title": "Demystifying solo staking struggles",
      "description": "Is solo staking easy or hard? What are stakers struggling with? I will go over the main issues facing solo stakers and what can be done about it. I will talk about the importance of solo stakers.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Best Practices,Home staking,Staking",
      "keywords": "beginners",
      "duration": 554,
      "language": "en",
      "sources_swarmHash": "3d144e5a76361e6fc5c09fda245ccedab865abf52497afcee7f791dba178d7aa",
      "sources_youtubeId": "Q-sMeimSWJU",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d76774749a4b89323f1c",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d86274749a4b893436c0.vtt",
      "transcript_text": " . Hello, how are you? Thank you for coming. A quick about us. We help users to run if you know in low-power devices such as arm boards like a Rock 5B or Orange Pie. And basically this is what we do. And so let's talk about running nodes. This was our first message that we sent to Reddit in 2016. It was about running nodes in Raspberry P3. It worked great. And turns out that eight years later, you can still run a full node, an archived node, or a staking node on these kind of devices, a similar device such as, as I said, the ROG 5 or the Orange Pi 5. So why? Why do we care? Why the community cares? Why the researchers care? Why the dev cares about running nodes on these devices, on commodity hardware. Well, first of all, the core purpose of the blockchain is to trust no one, to verify yourself. So you need a node to do that, right? And second, quite important, this helps individuals running nodes, helps the decentralization of the network. So how? Well, if thousands of individuals run nodes at home, you have anyone can interact with the Ethereum blockchain, anyone, no third parties, you don't depend on anything. Nodes are distributed over the world. A decentralized network should be consistently operational, pretty much 99% of uptime for Ethereum. The network can be used by anyone, anywhere. And additionally, if the low staking barriers are low, solo stakers can participate in securing the network. So what happens if there are no users, no individuals running nodes? So let's take the example of Solana, for example. The Solana's approach is about specialized nodes. As you can see, the requirements are data center-like. I mean, 256 gigabytes of RAM, four disks, a huge bandwidth for network. So nobody owns this kind of resources at home. So you cannot run a network at home. So you need to go to a data center and pay for cloud services or whatever. So these are two different approaches, home node runners versus specializes nodes. So the question is about decentralization. Is a network without home runners decentralized? Well, if you see the Nakamoto coefficient, which is basically, it's not enough. It is not. Because for us it's imperative to be able to verify the chain. Okay? And that means run your own nodes. So you see two different visions. One specializes in nodes. One home nodes, home stakers. So to maybe settle this, let me finish with two thoughts. This is a quote from Vitalik this Tuesday, actually. And if you have users that are verifying the chain, even two-thirds of the stakers acting together are not able to change the rules on people without everything breaking. The Ethereum rules can only change throughout the hard fork that is agreed upon through wide community consensus. I think that says it all. And the last one is about what we said about the core purpose of the blockchain. When we talk about truly decentralization, that means any user can run their own node at home and not rely on a third party or service provider for verifying the chain. So you don't have to depend on a third party. That's all. So my last message is run your own nodes for you, for the network, and that's all. Thank you. Thank you very much, Diego. Are there any questions for Diego? If not, I have one. Okay. Diego. How would you say, like, I have been trying to run on Ethereum, but I have not been successful. Why do you think there is so much trouble running? Well, actually with our solutions today, you need some technical skills. This is, yeah, this is a low barrier that we are trying to improve. But today, yeah, sometimes it's a little tricky. But you have other choices, you have other hardware that is easy to stop and other tools that can be very useful. But I think that anyone that wants to run a node, it's fairly easy with some tools. Definitely. Anybody? Don't be afraid of the mic. Okay, if there are no more questions, we will resume here at 10.50, so we now have a 40-minute break. Just to remember you. Thank you, Diego.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:50:00.000Z",
      "slot_end": "2024-11-15T04:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/12_jK1k9PlkGv-cHbW_ySIi8eDOSs8LavI_tRw_CJE10",
      "resources_slides": "https://drive.google.com/file/d/1gs2a9CoJvoTKSpAlvhqGza_MZ_TXJu1B/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "next-generation-based-rollups-a-practical-approach-to-unifying-ethereum",
      "sourceId": "GHVK8E",
      "title": "Next Generation Based Rollups: A Practical Approach to Unifying Ethereum",
      "description": "I plan to speak on the concept of based sequencing (based rollups). I want to not only introduce the concept but also explain recent developments (what I like to call next generation based rollups). This includes based preconfirmations, fast->realtime proving, customizable composability, practical synchronous composability, among others. I will introduce I also plan to provide a brief summary to my Bankless Summit talk on ETH value accrual in the presence of based rollups.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Fragmentation,Frameworks,Layer 2s",
      "keywords": "based rollups,sequencing,composability",
      "duration": 1378,
      "language": "en",
      "sources_swarmHash": "ec4e2a052bdc5fc188946c250ffacdba021eaa61b2efc79b21737c18ae7d1f3a",
      "sources_youtubeId": "Ier_f5V4_ow",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673839411b0f83434dfbaecb",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673839411b0f83434dfbaecb.vtt",
      "transcript_text": " . Hello, DevCon. All right, so we're going to talk about next generation base rollups today. There are some base rollups that are next generation. There are some base rollups that are last generation. You guys just learned about some. I'm going to tell you more about how you can become a next generation base rollup, kind of what the definition is, and then a little bit more about how I see the future of based rollups evolving. Cool, so this first section we're just going to go through some of the traditional based rollup research, talk about the the based rollups that have been, you know, explored in the past, some of the designs have been proposed, and then we're gonna understand some of their flaws, some of their weaknesses. So one of the very first designs that is used by Tycho today on mainnets is total anarchy based sequencing. This is like kind of letter of the law, exactly what Justin Drake proposed in last year, right? And it's very simple, very basic, right? It's called total anarchy because it uses a election system that is permissionless and allows anyone, that's the anarchy bit, to propose a block. So one key point here is that the sequencer is not elected beforehand. You don't know who the sequencer will be beforehand. You might know the Ethereum proposer because you can look at the Ethereum look ahead, we know 32 in advance, but the sequencer of the base roll-up might not be the proposer. In Tyco's case, it is Tyco Labs for the majority of blocks. One other thing to note is that layer 2 block proposing is completely permissionless, right? There's no permissions involved. The way that this ends up playing out in practice is that we have this is we have a bunch of proposers in Tycho's system that give their blocks to the builder, that would be Titan, Wintermute, et cetera. And they would take these blocks, order them in the layer one block, each block proposal is a layer one transaction, and then send this completed block to the proposer through a relay and through the MevBoost auction, right? And then eventually the proposer would propose this to Ethereum once they've selected the block with the highest bid. So it's up to the proposer and the builder to choose which block created by these Layer 2 sequencers or proposers actually gets onto the chain. Now, one design that aims to solve a few of the problems of the previous design is vanilla-based sequencing. So Limechain, George Spasov put out this excellent bit of research that explained how we can actually solve a few of the foundational problems with total anarchy-based sequencing, the TECO model. So the first thing that we try with vanilla-based sequencing is a simple sequencer election, right? That gives us the ability to choose a sequencer based on some rule, have a primary rule, and then we have a fallback as well. So the fallback in vanilla-based sequencing case is just total anarchy. So there's still some total anarchy here. And the primary sequencing is based on some election mechanism that is not clearly defined and kind of left to implementation in the initial research. So the other thing that vanilla-based sequencing includes is kind of very basic primitive support for delegation, which is allowing the Ethereum proposer to give the rights to their base drawout block to an external builder without directly going through MevBoost. It's like external to the MevBoost auction. Vanilla-based sequencing also included basic support for pre-confirmations, which gives the, you know, we heard about it last talk, but it gives the proposer the ability to promise things about the block that they will eventually propose. It does require some sort of delegation or some increased hardware constraints on the proposer. And so the design for pre-confirmations has kind of wide-reaching implications for centralization vectors for stake and also for pre-confirmations in real time. Now, the other thing that vanilla-based sequencing introduced that really caught my eye was revenue generation from a percent of congestion fees. And you'll hear Justin Drake talk about this all the time. You'll hear me talk about this. Base rollups can capture revenue from congestion trivially, right? These base fees, if you're doing EIP-1559 EVM, right, they are represented on-chain and are kind of understandable from within the EVM, within the state transition function. So you can view things about the fees, you can take these and you can do things with them, right? You can direct them to a treasury, you give some of them to the proposer, or you could return them back to users. So kind of the problems with these that are actually really big problems, like Tyco on mainnet live is extremely inefficient. All forks of Tyco without major changes will continue to be extremely inefficient. One other problem with kind of total anarchy and vanilla-based sequencing is that there's kind of no built-in composability. Any composability is on a layer above the sequencing. And that's really unfortunate because whenever you have to build kind of another layer, you introduce complexity and centralization vectors of some sort. In the case of base rollups, that's quite possibly around pre-confirmations, which means we could have centralized Ethereum stake, which we will want to avoid. The other thing is MEV revenue. So lots of people still have this misconception that base rollups leak MEV revenue, and that's true for traditional base rollups, because they do not use an auction of any sort to kind of get an Oracle into MEV. So if we look into layer one MEV research, we'll see things like execution tickets and execution auctions that are all designed to get some Oracle price essentially of the MEV in a block. Now the other thing to note about based relapse like I mentioned before is inefficiency. So Tycho has a about a $10,000 a day cost to do their sequencing. Before Blobs, they had about $100,000 a day. This is a lot of money, clearly, and the kind of thing that you need a massive amount of users and network effects to actually be sustainable. And that's something we obviously want to avoid. But there is a solution. For some of these Tyco forks, we've been looking at doing slower block time. So Tyco now has a block time of 33 seconds instead of 12 seconds, which is of course Ethereum block time. That is, you know, that doesn't allow their cost to come down by about a third, which is good for them. But it's also really bad for users. You get a worse user experience. So the way we can patch over this is with pre-confirmations. Just throw some pre-confs on top. The downside of that is that without doing your pre-confirmation designs in a clever way, you're going to end up with limited benefits. So your users are going to get very limited availability of pre-confirmations and of course limited composability. So unless you're using a shared pre-confirmation protocol, you're not going to have composability. So we start breaking a lot of things when we add pre-confirmations. The other thing that, you know, specifically Tycho's case, you lose composability with layer one because pre-confirmations happen, you know, over a course of an entire epoch, perhaps, and during that time, layer one state could change a ton. And so the kind of current layer one proposer is very different than who's actually sequencing the rollup. That's not shared sequencing. That's not based. But we can do better. Traditional designs are not good enough. We have, over the past two years, we have new research. We have new teams. We have a lot more data about base rollups because we have one live, and we have a lot better technology. So one of the big things that we'll be talking about in the next section is ZK, using snarks to save costs and improve efficiency. So let's kind of take a step back, let's think from first principles with all the data, with all the research, and with all the teams that we have now, what are the things that we really want to get out of base sequencing? The first thing is, in my mind, the most important, which is synchronous atomic composability. And this is a buzzword for sure. What I think that means is atomic cross-chain contract calls that are sufficient enough for Flash Loans. And the reason for that is because flash loans are a foundational tool for doing atomic arbitrage between domains. So once you have cross-chain contract calls, you can do arbitrage. You can do flash loans for arbitrage. And this gives you the ability to essentially arbitrage prices on different domains, giving users kind of the best prices across chains. You can also use that for doing like a swap that uses multiple chains liquidity. If I'm on Arbitrum, I could directly use Ethereum Layer 1 liquidity without waiting for a service provider or some collateral. Now, that's pretty straightforward. The other thing to look at is a seamless user experience and developer experience. So anyone who's ever tried making an asynchronous application will know that it's ten times easier, I'm not joking, to build a synchronous application. If it's asynchronous you have to deal with the case where something goes wrong and you have a desync. Now the other thing is of course network effects, right? Once we have the efficient synchronous composability between chains, we have the ability to build network effects on one chain and use that on another. And because Ethereum already has massive network effects, that means base rollups can use that, those network effects, those liquidity users, protocols, et cetera, to directly improve the user and developer experience on their rollup. Now, the other thing that we want, of course, all the time is faster things and cheaper things. On the faster side, we're looking at good pre-confirmations that don't break things and that are fulfilled 100% of the time. Frequent block proposing. The more we talk to Ethereum, the more we can compose with Ethereum. Custom frontends. So this is kind of changing the Uniswap style swap interface to something that actually represents what a pre-confirmation might look like. So that's a green check mark, some confetti of some sort. It doesn't seem like a lot technically, but it's actually really important to improving the user experience materially. One thing is getting rid of the approvals and then swaps. You can get rid of that completely with pre-confirmations. Another thing is of course cheaper things, which is better execution. So just writing your smart contracts better and not making the mistakes that we've made in the past when we were rushing to get code out. Aggregate everything. There's a slide on that, so I'll wait. And of course, efficiency of all shapes and sizes. Now, we want to do all of these things, and both of these things, I guess, without sacrificing decentralization and censorship resistance, liveness, and sustainability. So this is where you can get faster and cheaper. If you go to all layer one, you can do it on a centralized sequence or layer two. What you can't do is faster and cheaper with decentralization, censorship resistance, liveness, and sustainability without some form of base sequencing is my opinion. So we know what we want, it's not that complicated, we want fast things, we want cheap things, and we want everything to feel like one chain for developers and for users. Getting there. Next generation base sequencing. This is the subject of these three companies up here, this is kind of the industry that we're in, RISE, Spire Labs, and Tycho Gwyneth. And we're taking a very practical approach to unifying Ethereum. So we're trying to avoid the kind of discussions where we get into the ivory tower semantics and actually build tangible, useful products that are good for users and developers. So let's get into the specifics. This is an intermediate talk, so we're going to get a little bit technical here. We're going to share literally everything. So when it comes to things we share, like if the answer is no, then you're probably lying. We have deposits, proposing blobs, proving network effects, assets, contract calls, off-chain infrastructure, security, economies of scale, et cetera, et cetera, et cetera. So shared deposits, that's AgLayer. These are teams that are working to reduce costs of posting ZK proofs to mainnet by aggregating ZK proofs. Also has the excellent benefit that you can do interrupt between layer twos without ever touching layer one. So you save a lot of costs with that as well. Shared proposing saves a lot of costs and enables atomic composability, again, between base rollups if we can share blobs So if you're a base rollup and you're frequently checkpointing or posting to aetherium you're gonna be purchasing a blob for da in most cases and Well, they're pretty big and most base rollups do not use 100% of the blob space in a single blob. So there's a lot of empty space And there's a whole bunch of teams, Spire's kind of leading some of the research on this side, to figure out how we can have multiple rollups use the same blob. So we build some crypto economic system to make everything a little more efficient and a lot cheaper for base rollups. We also look at things like shared proving to save costs, shared network effects. So we want the canonical ledger of last resort for every asset to be on layer one. We want fungibility between these. That's something we get with shared deposits, but it's a big priority in and of itself. Contract calls, of course. We want to do contract calls. We want to batch those. We want to make sure that we have a good balance between the two. So we want to have a good balance between the two. that's something we get with shared deposits, but it's a big priority in and of itself. Contract calls, of course, we want to do contract calls, we want to batch those, you know, that's pretty obvious, security, that makes sense, and then of course economies of scale in everything we do. So one of these kind of tools is pre-confirmations. We have a, over here in number one, we have a centralized sequencer that gives out pre-confirmations. This is what every layer two does today except for Tycho. And number two, we have some proposed models for based pre-confirmations where the layer one proposer, I mean, you know, the guy running on a Raspberry Pi with a dial-up, is the sequencer and they're the one directly giving out pre-confirmations and directly interacting with the people who are requesting pre-confirmations. So obviously a pretty huge bandwidth and compute resources cost. And number three is the design being explored today, which is some form of delegation. That's that purple line to a gateway who actually gives out pre-confirmations to users. And then they create some builder constraints, some set of rules that builders must build blocks that comply to, right? So the gateway is acting somewhat like a relay in this case, but with a little bit more constraint around the builders. So we still have an auction taking place, very similar to Metaboost. And of course the builder gives the block to our Ethereum proposer who goes and gives that out to layer one. Now the other thing that we want to do is MEV retention, censorship resistance being included here, and of course doing this all with pre-confs, which turns out to be a really difficult problem, but we have some excellent research from the Ethereum layer one about execution tickets. So we've kind of extended this to layer two on base relapse. We have these things that look very similar to execution tickets. We distribute these in an auction, and then you must burn a ticket to propose a layer 2 block, although we do some of the burning and kind of registration early in the epoch and off-chain optimistically so that it's not an expensive thing to do on-chain. And of course with this, we can throw in some no delay forced inclusion. So every rollup today has a drift where you can deposit, make a deposit transaction on layer one, and then you might wait a few layer one blocks before that's included in layer two. In a base rollups case, you can pretty straightforwardly build no delay forced inclusion right into a base rollup, which gives you excellent central persistence equivalent to that of Ethereum. Now, one other important thing about next-generation based relops is checkpointing. So whenever we, TychoCos is proposing by the way, but whenever we want to kind of put our sequence onto layer one we need to make a checkpoint here. And this is little red flag on my diagrams but what we're actually doing is making kind of a point of atomicity. A layer one EVM transaction is atomic. You can't revert part of it. And because of this, you can do cross-chain contract calls based on that atomicity. You can also checkpoint together. That's this bottom left diagram where we share lots of checkpoints in one, which saves a ton of costs. The other thing we can do, of course, is other things while we checkpoint. bottom left diagram where we share lots of checkpoints in one which saves a ton of costs the other thing we can do of course is other things while we checkpoint so we can use layer one liquidity during our checkpoint make this available on layer twos we can also you know save the states of things on layer one we could change the state of a layer one contract to simulate layer one execution but actually doing the execution on layer two. Cool. So one important kind of piece of technology, and probably the most important for the base roll-up teams to compete on and be the first to, is validity-proving and fast validity-proving and shared deposits. So there's a whole bunch of ZK teams working on this. Succinct and RISC-0 are two examples. And the goal is to take fast-float d-proofs for a whole bunch of layer twos, combine these, aggregate these into one fast-ZK proof, and then put this onto Ethereum in a shared deposits bridge contract that has funds for lots and lots of roll-ups. Now, this has security kind of concerns. You have to trust your ZK, so we probably want multi-proving. And it's also, today, really slow. So the biggest competitive advantage of Bayes-Scalops in the future will be how fast is your proving. So this is the magic of Bayes sequencing that we've been working on at Spire to some extent, of course. And that is using a sensory resistance committee to sacrifice liveness for sensory resistance. You can read more about this in our light paper. And then forward-based sequencing, which is using a sequence posted in a layer one slot to affect a future layer one sequence. This enables you to do based sequencing and traditional shared sequencing at the same time, which is obviously super powerful. So if anyone from Espresso is in the room take a look at this Cool. So after we've done all this, what do we get? Harmony and more specifically deconstructing economies of scale to enable parallel innovation while maintaining network effects I've said this in all of my talks so far this week and it's so important, right? We don't want economies of scale to introduce centralization risks, and we want to promote parallel innovation as much as possible. The other thing, one of our important goals for the user and developer side is a monolithic experience, right? Nothing monolithic in practice. We want to have the ability for users and developers to feel like they're on one chain just to get that clearly better user experience. And, of course, doing all this while establishing infinite expressivity, giving builders the direct ability to customize whatever they want about their roll-up, including execution environments, gas fees, et cetera. Infinite gardens, break walls. Next generation-based roll-ups actually solve problems. Stay based. Oh, here's a quote from Heraclitus Drake. Stay based for one man's liquidity is another man's liquidity. Thank you. Okay, thank you a lot. So we have a few questions. So, first question, what can Tyco do to become a next generation based rollup? Well, first of all, upgrade their bridge contract so it can share proposals and deposits and ZK proofs with other rollups and then add actually good pre-confirmations. Yeah, they have a lot to do. Yeah, same question. As someone new to the space, I find it hard to factor out the marketing. How does your approach compare to Espresso? Sounds similar at least. That's a same question. As someone new to the space, I find it hard to factor out the marketing. How does your approach compare to Espresso? Sounds similar, at least. That's a good question. So Espresso is building a shared sequencer, and they're building a form of what they call based Espresso, which is not actually based sequencing. They just call it that. It's a marketing thing. And the goal of Espresso is to unify layer twos. The goal of next-generation base relapse is to unify Ethereum and layer twos, so we're involving Ethereum in the equation. But to get there, you can do shared sequencing. You can even involve the layer one sequencer, but that doesn't mean anything unless you do this checkpointing, unless you do this atomic synchronous composability. So base espresso will not allow you to do cross-chain contract calls. Okay, thank you. the same thing. So, I think that's the main thing. I think that's the main thing. So, I think that's the main thing. So, I think that's the main thing. Thank you. Do we have any other questions? We still have time. So, this is something that every base rollup must do, which is reorg with Ethereum. So, in the case of an Ethereum reorg, then your base rollup must always reorg. It doesn't have to change anything, but it will reorg. And this is something that is very hard to fix in protocol. We need an Ethereum upgrade to reduce the risk of reorg or decrease time to finality. But there are a lot of other protocol things we can do. If you get lots of proposers opted in, in the look ahead, you can just ask them not to reorg, have them put up some collateral, and you'll be able to reduce the risk of reorgs. You can also paste over it on the user experience side. You can have a solver or somebody take a duration risk. The user never has to experience a reorg. And then Spyro's also working on an RPC that's custom, kind of retrofit, to deal with common reorgs. One thing I would add is that base drops will reorg as much as Ethereum does now. So I don't know how much you use Ethereum, but if you do a lot of DeFi or trading, you've still probably never experienced a reorg that has affected your life. So I wouldn't expect that it's a massive user experience problem. Yeah, we would have to reorg. Okay, thanks. And then, sorry, just another question. I thought cross-chain contract one was very interesting, but I still can't imagine how it works across, let's say, a bunch of different chains that all operate at a different speed. So could you maybe provide some practical examples with a bit more details? Come talk to me afterwards, because that's a longer discussion. So all base rollups have a batching time of 12 seconds and a block time of theoretically less. This means that the composability you do between them can only happen at batching time because that's the only time we have atomicity and synchronicity. We can compose every 12 seconds, we can do a cross-chain contract call every 12 seconds, and then within that boundary We can't do any composability that is enforced by cryptography. We can still do intents or other kinds of interop.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T03:50:00.000Z",
      "slot_end": "2024-11-15T04:20:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ftf3rfy0W2vOu0uKzcm-Qyqhd_eURotVsS5HzTB9jFw",
      "resources_slides": "https://drive.google.com/file/d/1NC_7AWntfiYe_4I6veFUfkNF24gUZAYo/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "building-a-builder-community-in-africa-from-ground-up-the-hurdles-to-navigate-the-silo-mindset-and-overcoming-it-with-clear-conviction",
      "sourceId": "XZ7E7A",
      "title": "Building a builder community in Africa from ground up: The hurdles to navigate, the silo mindset and overcoming it with clear conviction.",
      "description": "Join us to explore the growing African tech talent pool. We'll discuss how initiatives like AyaHQ, web3bridge, and web3clubs have empowered developers through education, hackathons, residency programs etc. Learn about the impact of their work, the challenges they face, and how these initiatives—among others—are helping shape the future of Ethereum in Africa.",
      "track": "Real World Ethereum",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": true,
      "tags": "reputation,trusted,builder,Collective Intelligence,Decentralization Improvements,Solarpunk",
      "keywords": "Reputation,Trust,Builders",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:00:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1MVujY9JzCpdqRtOCYp2LMS83iZxI8agQ_40Y6msZg4I",
      "resources_slides": "https://drive.google.com/file/d/1hLKk8KJE7-KWeU2KIbeEpvYFD39Ktbd9/view",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "degen-incentives-for-regen-outcomes",
      "sourceId": "MNWVFW",
      "title": "Degen incentives for Regen outcomes",
      "description": "Degens (financial speculators) and Regens (blockchain altruists) don't like each other. But there's a lot that can be achieved if both tribes work together. In this panel we'll explore those projects that embrace both energies to find balance in the force and unlock Ethereum’s potential.",
      "track": "Coordination",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Ethereum for Good,Regenerative Applications,Product-market fit,regen,degens,Ethereum for Good,Product-market fit,Regenerative Applications",
      "keywords": "Ethereum Culture,Ethereum Community,degens🤝regens",
      "duration": 3320,
      "language": "en",
      "sources_swarmHash": "8d01b82c832acdeb522e5dea2f2aeab4635c8f3e95d66f4cbb1d5b1ed7b2d28e",
      "sources_youtubeId": "m8Hq-JFVfXQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673d90c417a97b4f4d488d86",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:00:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "stage-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1VL2_zkuomzUJ59v6VkJCzFGACRwHLUks6cXhys2kzmA",
      "resources_slides": "https://drive.google.com/file/d/1VKr9CqjlWrX8Y_Nm5DNmSPWZm64LSn5O/view",
      "slot_room": {
        "id": "stage-1",
        "name": "Stage 1",
        "description": "Fans",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://streameth.org/embed?stage=6721008424af22d0ca96ce72",
        "youtubeStreamUrl_2": "https://youtube.com/embed/lkCg4vfNKws",
        "youtubeStreamUrl_3": "https://youtube.com/embed/M4Kxw4NtB_A",
        "youtubeStreamUrl_4": "https://youtube.com/embed/KPY8MJtPP4g",
        "translationUrl": "https://stm.live/Stage-1"
      }
    },
    {
      "id": "empowering-users-how-ethereums-low-node-requirements-promote-true-decentralization-over-solana",
      "sourceId": "QAJNMK",
      "title": "Empowering Users: How Ethereum’s Low Node Requirements Promote True Decentralization Over Solana",
      "description": "Nine years after Ethereum's launch, you can still run a node at home on commodity hardware, even low-powered devices like $185 ARM64 boards.\r\n\r\nWhy is this so important? Wouldn't Solana's approach, using more powerful hardware for higher speed and throughput, be better? We'll explore why home nodes matter for decentralization, credible neutrality, and global accessibility.\r\n\r\nWe'll also compare node requirements vs the Nakamoto coefficient as metrics for measuring decentralization.",
      "track": "Core Protocol",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Stakers/Validators",
      "featured": false,
      "doNotRecord": false,
      "tags": "Decentralization,Home staking",
      "keywords": "",
      "duration": 436,
      "language": "en",
      "sources_swarmHash": "e392f868233af92802f11b232aab83133a4eca653b22dc1d8af805c6e35f1a75",
      "sources_youtubeId": "Nu7zHhcaRWY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d86274749a4b893436c0",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:00:00.000Z",
      "slot_end": "2024-11-15T04:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/149MDCwjImcWRfdIwZw6lfpbIkNtiT4AFD60ebK9hnNQ",
      "resources_slides": "https://drive.google.com/file/d/1RIE_HihMHMceJ7W1to2mArnrXLDaNAzi/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "ethereum-needs-native-l2",
      "sourceId": "9RNWDX",
      "title": "Ethereum needs native L2",
      "description": "Right now, L2beat tracks 116 L2s. However, they represent a wide range of trust assumptions, which makes assets—or more abstractly, messages—from these L2s non-fungible and thus significantly hampers interoperability. We are advocating for Ethereum to deploy a large number of native L2s, developed and governed by Ethereum's open-source developers. These L2s would be highly interoperable with L1, fulfilling Ethereum's early promise to provide sharding using L2 technology.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,Ethereum Roadmap,Scalability",
      "keywords": "interoperability",
      "duration": 1619,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "BWsz_ulng6Y",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736e7311b0f83434d194833.vtt",
      "transcript_text": " The controversial presentation and I kind of expect to make well kind of everyone angry because I'm criticizing to some extent the existing L2s and to some extent the existing Ethereum core roadmap. But I think it needs to be done. So in this role, I'm founder of Gnosis, but in this capacity I'm just speaking as someone who has built for over 10 years now on Ethereum. So we started building on Ethereum when it was still a testnet, deployed one of the first apps a couple of weeks after Ethereum was released, and have continuously been builders in the space. So, let me make a few comments on the previous presentation. I have the absolutely highest respect for Jesse, for what BASE is doing, what Coinbase is doing in general, but I do think the claim to say we are bringing people to Ethereum Coinbase is doing in general. But I do think the claim to say we are bringing people to Ethereum, we are bringing the next billion people to Ethereum, is wrong. We are bringing the next billion people to base. And that is a significant difference. So just a few examples. So Jesse was also talking about the 30% fee that centralized platforms can charge from you, of course, referring to Apple, that is something, for example, Base can absolutely do as well. So if you are an app on Base, it's absolutely in their hands to control how much fees you're paying for a transaction. And of course, they can start charging a 30% cut. And this is typically something that doesn't happen early on. That happens much later. Once a platform is established, a lot of kind of lock-in effects happens. And it often even doesn't happen with the first generation. So ultimately, Coinbase is a regular company and yeah, they are great people. I absolutely, again, have highest respect for Jesse and Brian. But they are employees at the company and at some point they might leave. They might retire. They might do something else. And then another set of people comes. And ultimately, it's a company controlled by shareholders. There's a fiduciary duty to maximize shareholder value. So, oh, we have this asset base. Oh, we can spike up fees. We can take more fees from those applications. And in a way, it's even our duty to do it because we have to maximize value for our shareholders. So my claim is bringing people to those L2 ecosystems is quite different from being on Ethereum. The claim, of course, is always rollups inherit the security of Ethereum. And we see labels like it's secured by Ethereum, you kind of get what you get that you would get from Ethereum. But the reality is absolutely none of those that top L2s actually, yeah, kind of truly inherits the security of Ethereum. So if you look into detail, funds can be stolen here, funds can be stolen here, funds can be stolen here, funds can be stolen here, funds can be stolen. And then on top of that, now we are saying, okay, let's do chain abstraction. So let's make it invisible to the user on what chain they actually are. But in a world where the underlying security is quite, well, different and opaque, chain abstraction actually means, in that case, risk abstraction. There was a nice talk two days ago. So if you're abstracting the chain, you're abstracting the risk. And that's, well, that's dangerous. So at some point, funds might be lost. And then you can no longer abstract the chain. And you need to tell the user, oh, sorry, your funds are lost. But there are even more risks that even L2 beat does not cover. Yesterday there was a great presentation by James showing and again just referring to the presentation showing that on any chain with a centralized sequencer that's again for example BASIC, arbitrage optimism, if you have funds as a user on a with a centralized sequencer that's again, for example, based up from optimism. If you have funds as a user on a money market like Aave and Compound, the sequencer can indefinitely and at zero cost prevent you from accessing your funds, from withdrawing your funds. And you might think, yeah, but you can force include a transaction via L1. No, that won't help. So roughly the idea is you create this withdraw request, and because the sequencer ultimately can control the exact transaction ordering, they can alter the state of that money market so they can just, for a sub-second, borrow too many funds out of that money market so that your request to get funds out or your own funds out fails and then kind of directly after you repay, so they pay zero interest because it's the same time, it's in the same time stamp. So just kind of those kind of things, yeah, in my view, again, truly show if you build on one of those two systems, you're far from inheriting or getting the security that Ethereum promises. So what does Ethereum security actually mean? It means things, it's a culture, it's a process, it's much more than code. So Ethereum security means public core developer calls, public discussions, multiple client implementations, rigorous testing, highest level of promise of backwards compatibility. And kind of those things cannot be derived or exported automatically to an L2. Just because you kind of deploy some code on Ethereum, that process and that culture cannot be derived. Of course, there can be efforts to kind of promote that culture. And to some extent, Ethereum is doing that. And kind of Talik is going around and saying, yeah, if you don't do those things, then I won't call you an L2 anymore. So, yeah, that culture can try to promote this culture. And L2 is, of course, doing a great job in kind of trying to export that culture. But again, it cannot be derived with code. So looking at another data point, again, the claim assets are secured by Ethereum, but the reality is only, or it's the part that is actually of assets that is actually coming from Ethereum is getting less or the fraction is getting smaller and smaller. So most assets, again, this is for example base, but it's very similar on other chains and on smaller chains, it's even much worse. Most assets are actually not coming from Ethereum. So this is here the canonical, the, oh yeah, so back here in time, the canonical bridge was still the dominant. So that is kind of the one, the assets that are actually kind of secured by Ethereum. Now the dominant part are native assets. So those are assets issued on the L2 or on that other chain directly. And they do not inherit, there's not a concept of exiting those to Ethereum. And there are reasons for that. So if you use the canonical bridge and you want to derive that security and you currently want to say, let's say, send a message from Arbitrum to Optimism and get a message back, that transfer takes two weeks. So if you actually use the canonical bridge, it would take you two weeks to do that so a pigeon is faster in transporting a message for most parts of the world so yes, no one is using that mechanism instead we are using new kind of bridges or external bridges that don't inherit the security of Ethereum so if most assets are not natively bridged, and sequencing is also not done by Ethereum, so assets not bridged means from Ethereum not secured by Ethereum, sequencing not done by Ethereum, then the role of Ethereum kind of is reduced to being kind of this checkpointing systems. So here again rollups could choose to how they build blocks and they can either choose to use their own sequencing which means they can do super fast confirmations, they capture their own MEV. But the disadvantage is that they can only not synchronously read into Ethereum. And that is the rational choice to do if you optimize for connectiveness to other chains or for yeah for for TreadFi or you can go kind of what's called based where you let Ethereum do the block building and the sequencing and here you are really optimizing for connectiveness to Ethereum but that's only rational if it's more important for you to be as closely connected to the kind of economic zone that Ethereum is. But the reality is only 1% of value is choosing to do that. So here's my proposal for Ethereum to fix that and to address those issues. And the proposal is that Ethereum itself should develop and deploy ZK proven EVMs, rollups, and deploy 128 equal instances of that that are highly interoperable. And that are truly kind of, if you build on those, you truly build on Ethereum. So what would that mean, being built by Ethereum? It would mean that you don't even think about introducing a multistick. That would be unthinkable that Ethereum deploys something and it has some multistick as the upgrade mechanism. It would mean that we at least have, well, we want to have multi-clients, so we would have at least two independent implementations of, let's say, the proof systems. That's the most important part here. Rigorous testing, thousands of eyes that look at the stuff and actually care about all the details that are often pulled under the rug. So the idea is, yes, we can still have L2s, a wide range of designs. Within that we have based L2s, so L2s that use Ethereum for block production and for sequencing slash block production. But even a subset of those would be native L2s. So truly built by Ethereum, governed by Ethereum. And again, we can here see some things. So reads into L2 are synchronous here. But of course, also the economic perspective is important. So only in the native roll-up, essentially all value of that roll-up is captured by Ethereum itself. Now, one question could be, is that actually sharding what I'm proposing here? No, not exactly. So sharding would be a design where you would have, in a way, multiple L2 instances. And that was kind of still the promise a couple of years ago that we would have 1024 shards. So essentially instances of Ethereum L1. And they would all be kind of live on the same layer. What I'm proposing is not that. But instead, but still something that comes, tries to come as close as possible. Have the L1 and then have those 128 equivalent L2s. But still in this hierarchy. And looking more at it, yeah, some things. So on an L1, you can do synchronous reads and writes, and that's what we all love about Ethereum, this composability. The composability within this larger system would still be pretty strong. So from an L2, the idea is you can still do synchronous reads into Ethereum. So meaning you can execute some code on the L2. You can actually read a contract on L1 in that execution. So let's say in price oracle or anything that lives on Ethereum. And you can kind of even read the function, continue with that result in the L2 process and do things. So kind of the L2 can do things dependent on the state of the L1 so it has immediate transparent access. The other way around, from the L1, you cannot read immediately the state of the L2s. That is asynchronous reads. But writes are actually synchronous. So if you can very well do a transaction on L1 that in the same transaction, so atomically also creates a write into the L2 and affect the state, you will just not in the context of L1 be able to immediately get kind of the result of that state change. So again, the read will be asynchronous. Same in communication between L2s. So you can synchronously make a transaction that affects the state of two L2s, but again, you cannot read from one L2 to the other. So the general idea is stuff that everyone should be able to read, a kind of contentious state might live on the L1, and on all the L2s, they can access the state synchronously, but there can be much more non-contentious state. So all the things I have proposed so far would be possible today on Ethereum without any kind of upgrades or changes to Ethereum. It would essentially just take the Ethereum community to decide to build that. Of course, if we do that, we can actually make those L2s even more powerful. So two things I'm proposing here that can be done. If we make those L2 native, we can make them also native to the economics of Ethereum. So Ethereum, of course, let's say issues ESERV to reward validators to participate in the consensus. In the same way, it could also kind of redirect or direct this issuance towards proving the correctness of all those L2s. Another one is that I'm suggesting that those L2s should have distinct namespaces, so meaning an address should be clearly attributable to one L2 or to one kind of in a way chain, either L1 and L2. And in my view, that is a big problem that right now you have an address and the same address on various chains can mean very different things. So, for example, if you have a safe, then actually the same address can exist on multiple chains, but can have completely different things. So for example, if you have a safe, then actually the same address can exist on multiple chains, but it can have completely different owners. So what I am suggesting here is that we shouldn't have this address collusion. So each L2 kind of uses an additional salt to have its own address space. And if we have that, we could allow sending a message from the L2 with its unique address into the L1 and actually have the message sender on the transaction be that unique address that only exists on the L1. So that allows you to do things like, let's say, you hold a token on L1 by an address or a contract that only lives on an L2. But yeah, some technical details. So here's kind of the spiral I expect or I see already happening if we don't go this route. So if we don't go this route, it means the economic zone that is Ethereum and Ethereum block building becomes less relevant. And therefore it already makes it less attractive to use Ethereum as a block builder or as a sequencer. And again, we already see this today. 99% of the economic value of L2s choose not to be based and not to use Ethereum as a block builder. At the same time, more and more native assets are not coming from Ethereum and are not really secured by Ethereum. Either they are natively issued or they use external bridges. So for those, the economic security of Ethereum also matters less and less. So in total, that means that the relationship between roll-ups and Ethereum really becomes weaker and weaker. And yeah, I'm going so far and say it just becomes a meme. So now, so essentially in my view, that's the crossroad we are in front of. With native roll-ups, we can kind of continue to have Ethereum the most relevant or economic zone. I would claim that should be the goal, to be the most relevant economic zone even in the world. That's a place where prices are... There's also the other perspective. Some believe Ethereum should be a meme or is a meme. And they promote this idea of, yeah, let's say Ether is money. And their perspective might be, if we do native roll-ups, we kind of damage the meme. We damage our collective maybe religion or kind of, and because we create tension within this nice family that we all are. So we disenfranchise those roll-ups so therefore the ETH meme gets weaker and all that economic stuff doesn't really matter. It's just about saying ETH is money and spreading ETH and therefore it's better to not do native roll rollups. All right. So final slide. For whom is it to decide what route we go? Absolutely. For all of us that are here, for everyone who is Ethereum and we are all Ethereum it's not just let's say some core developers or the Ethereum foundation essentially it is yeah Ethereum and anyone can essentially affect changes to Ethereum thank you very much applause applause applause thank you very much. Thank you so much. And while we're still on that note, we have some questions from everyone here. And the first one is, there are 128 identical rollups. Which one should I deploy my DAP on? Yeah. So, I mean, the first important part would be that right now, if you wanted to deploy somewhere right now, well, you either use L1 Ethereum or you have to choose one of those many ecosystems, be it Superchain or AgLayer or something. And this would give you the opportunity to actually just say, okay, I want to be on Ethereum. Now, if you have to choose a particular one, then again, reads and writes within a particular L2 are synchronous. So if there are other applications you want to regularly interact with, then yes, it would be wise to choose that one. If you're fine to say okay, I just need to access, I want this close access to the L1, they are all equally close to the L1, then you really should just choose the rollup that is used the least because that will be the cheapest. Why not implement this on Gnosis? Yeah, I mean, maybe, but I think really, I mean, well, Ethereum is the big thing. So Ethereum has that credible chance to be this most important economic zone. As much as I would like Gnosis to become that, we are not there. I mean, Ethereum is 100 times bigger. And again, I mean, saying that again, so there are all those ecosystems, and Gnosis is one of them, that also tries to do valuable things. And again, I think that's totally valid. I'm just saying, I'm not saying if you come to Gnosis, you'll come to Ethereum. No, you come to Gnosis. And we try to do valuable things there and in the same way I would say other ecosystems again, they are great ecosystems and they bring great value but you should understand that you are building on that ecosystem and not necessarily on Ethereum. Alright, why 128? Why not start with 4 or 16? Yeah, I mean, the idea is really to make it clear that building on Ethereum or building here is long-term viable and that, of addresses costs so I do think, well I absolutely also want to share the vision to bring a billion people on chain and for that Ethereum needs to be much more ambitious and I think 100x increase in effective block space is really what should be aimed for on a timeline for let's say two years, I would say it could be realistic for that and not the 4X or something like that. Alright, the next one wasn't necessarily a question, it was more a compliment. They just agree completely and they're thanking you for stating the hard truth. But then we've got, do you consider this as rugging existing roll-ups? Yeah, I mean, well, rugging, yeah. So I think existing roll-ups, well, already have big time advantage. They are live. They are already built. So that will take at least, again, I think, let's say another two years. But then, yes, if existing roll-ups will not offer anything other than kind of just provide EVM block space, then they will have a strong competition. But I do absolutely think that all those existing roll-ups are great people, innovative people and I think they will be able to kind of evolve and create something or essentially have additional innovation on top and if they do that then yeah they can just be much more kind of basic EVM block space as we know and love it since 10 years. That's what I'm proposing here, to just create much more of that. And then there can be those L2s that are more innovative, that do additional things, that do cutting edge, maybe built-in privacy, maybe some form of other sequencing that is much faster. There are all kind of ways to innovate. So I think, yeah, again, both absolutely can live. All right. Unfortunately, we are out of time. Thank you, everyone, for your questions. And thank you once again, Martin, for your phenomenal talk.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:00:00.000Z",
      "slot_end": "2024-11-15T04:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1kNj2hbZYPECNuJmWk7WXk0CzL745n9QV5DwtBh6rF6A",
      "resources_slides": "https://drive.google.com/file/d/17IT9Dzi9Fwk0w29j6oQ23UQVnm6yyqDF/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "what-dont-we-know-understanding-security-vulnerabilities-in-snarks",
      "sourceId": "NL3A7T",
      "title": "What don't we know? Understanding Security Vulnerabilities in SNARKs",
      "description": "Zero-knowledge proofs (ZKPs) have evolved from being a theoretical concept providing privacy and verifiability to having practical, real-world implementations, with SNARKs (Succinct Non-Interactive Argument of Knowledge) emerging as one of the most significant innovations. Prior work has mainly focused on designing more efficient SNARK systems and providing security proofs for them. Many think of SNARKs as \"just math,\" implying that what is proven to be correct and secure is correct in practice.",
      "track": "Security",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Security",
      "keywords": "ZKPs,Security",
      "duration": 1540,
      "language": "en",
      "sources_swarmHash": "73f82e4075b6c3ec2a21aeda21e6795a208a239ac164b2168f95d285ab44d739",
      "sources_youtubeId": "njXVouCOBQY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d27174749a4b8926b8b4",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d27174749a4b8926b8b4.vtt",
      "transcript_text": " Hello, thanks for the introduction. So today I'm going to talk about vulnerabilities, mainly in the implementation of Snarks or ZK-Snarks, and also on what can go wrong when we deploy SNARKs in production. This is joint work with collaborators from TUM, the Ethereum Foundation, ZK Security, the Scroll Foundation and also Imperial College London. Okay, so let's start. Okay, what is the state of ZK-AP applications today? We have ZK-K rollups that have become very popular in the last two years. They have more than 5 billion USD in TVL in them. We have Zcash, which is a payment system. It was deployed, I think, the first version around 2015, 2016. We have many ZK applications both for infrastructure such as ZK bridges but also for private payments, for using wallets without having to use seed phrases like ZK login. We have private programmable L1s and L2s like Mina, Aleo, Azdec, and also we have some off-chain applications. And although all of those systems have been deployed, we haven't seen any major exploits like the DAO exploit we had in smart contracts. And although we haven't seen any exploit, there have been bugs in the systems deployed in production. So Zcash had a vulnerability. Sorry for the pictures, might be a bit small, but I will go through them. So Zcash had a vulnerability for, I think, more than a year in it. People from within the Zcash, found it, and patched it. Then one of the most popular mixers had a vulnerability that if someone exploited, it could have basically drained the smart contracts of that protocol. One of the most popular ZK rollups had a major vulnerability that someone could again potentially could have exploited and get everything out of that roll-up. And also I would say that in audits in ZKE protocols, even in top-notch protocols, if you compare it with top-notch smart contract protocols, the ratio of critical vulnerabilities, it's even higher. So there are many vulnerabilities. And people for many years suggested that ZKPs are very difficult and very hard, and not many people actually understand them. And also they have suggested that to exploit a ZK protocol, it's much more difficult to exploit, for example, a smart contract vulnerability. I would say the first one is not true anymore, right? Because if you see the number of presentations in ZK in DEF CON this year and compared with three years ago, we have an exponential increase. And also, although it might be true that some ZK vulnerabilities is difficult to exploit, I would say that some of them are pretty simple. And for example, here we have a circumcircuit. It's a very old circuit, but anyone who has written a circumcircuit could probably understand what's going on here. And still there are such vulnerabilities in ZK protocols that I think are pretty easy to exploit. So there is a huge risk if there are still vulnerabilities in deployed protocols to be able to exploit them at some point. Okay, so let's start with explaining what are the properties of a ZKE protocol. We have knowledge tenderness, which basically means that a dishonest prover cannot convince a verifier of an invalid statement except with a negligible probability. We have perfect completeness, which means that if you have a valid statement, a prover will always be able to convince an honest verifier of the correctness of that statement. And also we have zero knowledge, which means that the proof pi that we produce with the zero knowledge proof does not reveal anything about the witness we are proving. So what is our threat model in a ZK world, right? We have three adversaries. So what is our threat model in the ZK world, right? We have three adversaries. We have the network adversary who observes the system and its public values but cannot interact with the system. We have the adversarial user, which basically is able to submit some inputs for proof generation in an honest and non-malicious prover. And finally, we have the adversarial prover, which is the most common thread model, and it's our thread model when we actually need the ZK property, right? But I would argue that even if we don't need it, if we want to have a fully permissionless system, then that's the adversary we have. And it has the ability to produce proofs and has the ability basically to do everything to try to trick the verifier. To give you an example of what I mean with the second category, because it might be a bit confusing, consider ZK rollups at the moment, right? Where we have a single centralized trusted L2 node that is both the sequencer and the prover so users can only submit the transactions there and then that centralized node will produce a proof so in that case we have an adversarial user basically and what can be the impact of ZK vulnerabilities. So, we might be able to try to break sadness, which means that a prover can convince a verifier of a false statement, and that could result in basically, for example, in ZK roll-up, to get all the funds out of it. We can break completeness, which means that a verifier cannot verify proofs, or basically that the prover might generate verify proofs, or basically that the prover might generate invalid proofs, right? And for example, such a vulnerability could basically have a high impact in the liveness of ZQ rollups. And we might also break zero knowledge, which means we have some information leakage. Okay, so what we did is we analyzed 141 bugs and vulnerabilities from audit reports, from vulnerability disclosures, and from bug trackers. And our goal was to split those vulnerabilities in layers and understand what can go wrong in each layer and also create a taxonomy of vulnerabilities. So let's start with that figure. So in the real work, non-SNARK work, we'll have a relation, a specification, some idea that we want to actually create a ZKP about. And we might have some public and private inputs. So the first step is to manually encode that specification, that idea, in a circuit and get the circuit implementation. So we figure out that in that level, it's where most of the vulnerabilities happen. And the main reason, in our understanding, is because it's confusing for most developers to write circuits because they have to think both about computation and also about constraints and they might do very aggressive optimizations there and they might try to apply some tricks and that typically leads to vulnerabilities. So we identified three main vulnerabilities. Other constraint vulnerabilities which means that you forgot some constraints or some of your variables are partially constrained. And that typically leads to sadness vulnerabilities, which is the worst vulnerability that can happen in a ZK system. Then we have over-constrained vulnerabilities, which is the exact opposite. That most typically will lead to completeness issues. And we also have computation or hint errors, which is just on the computation part. And accordingly, you might have messed up constraints, but the root cause was in the computation part. So we did a complete root cause analysis and I will share with you a QR code for our paper to look into examples and to look on how you can fix some of those vulnerabilities, etc. But very briefly here, we have categorized them in three main root cause classes. First is that when developing circuits circuits we have a different programming model and that could lead to many vulnerabilities. Secondly we observed that the root cause of vulnerabilities were optimizations and also having cryptography at the outer layer and in very low level DSLs that could introduce many vulnerabilities and also common errors like in any software, like specification issues or API misuses, etc. So, the next layer is the front-end, which is basically composed from two components, a compiler and a witness generator. The compiler will take the circuit and will try to produce an intermediate representation that it's on what our proof system works on top, for example R1CS, and then the witness generator will take the circuit, will take the public and the private variables we have, and it will produce a witness. And the next one is a backend. The backend is composed of three main functions, setup, proving, and verification, and things can go wrong in all those functions. So the vulnerabilities we identified here in the frontend is incorrect constraint compilation and errors in witness generation and in the previous presentation we saw how things can go bad there and it's very critical to actually trust and to be able to have correct implementations of front-ends and in the back-end the situation is quite similar. From our data we found out that unsafe verifier is a very common issue and can lead to major vulnerabilities. Let's go to the next one. The next one and the last one is the integration layer, which is basically you can think of one and the last one is the integration layer, which is basically, you can think of it in the blockchain space as the JavaScript that is responsible to run your prover client side and create the proof, and also the smart contract that consumes that proof and calls the verifier you have implemented or it was produced automatically and try to do some things. And we had some very interesting vulnerabilities in that layer. I want to focus in the first one, which is passing unchecked data. And what does that mean? Sometimes, as already said, we might try to do some optimizations in the circuits. And for example, one thing that is pretty common is for people to say, OK, in that circuit, let's have some implicit assumptions that our inputs are in a specific range. And then delegate that check to the actual code that will call the verifier. So in that example, we forgot to do such a check, and that could lead to major vulnerabilities then in our infrastructure. And in the last year or so, there has been a major change in some architectures, where instead of circuits, we have ZKVM circuits right so the developers now only care about writing some program typical in the high-level language like Rust and then compile that program and giving it as an input to ZKVM. Still, circuit bugs can happen in the ZKVM itself, and I would say a subtle new threat here is traditional compilation errors that might happen to the Rust compiler, for example. That could lead to have invalid proofs. So that's something that people should take into consideration when using ZKVM cells. So another way to see what we currently described is in a hierarchical way. And here I have an example of all the stack when we use the circum-programming languages and SNAC-JS with GROT16 Azure-proof system. I have two new layers here. One is full arithmetic elliptic curves, which have nothing to do with ZK, but when we construct and implement a proof system, we have to have such a very efficient library and things can go wrong there. And also things can go wrong in the hardware, in the operating system, in the blockchain we are using, right? So you should always be very, basically think about what you are going to use and apply all traditional best security practices we know from other fields. And one last thing is that in the proof system, there could be errors there, there could be errors in the initial description, in the papers of proof systems. So if something goes wrong there, it doesn't matter if you have formally verified the circumcircuits, if you have the best backend or frontend, it could be exploitable. And that basically it's true for any layer. So if your frontend or the backend it's vulnerable, then even if you have formally verify your circuits, they could be exploitable. Okay, so we did that analysis and now I want to present some of the results. So we categorize the bugs in all those layers and also based on their impacts. And we can see that circuits was the number one threat in the whole infrastructure of using ZKPs. And also most of the vulnerabilities can result in soundness issues. So what can we do? Fortunately, there has been a lot of development and a lot of research of creating security tools for ZK circuits, specifically for ZK EVMs or ZK EVMs and also in the last month two new papers were published and tools, Circus which was presented in the previous talk and also MTZK which is great because such novel tools can detect infrastructure bugs in circuits in ZPs, but I would say there are still a lot of work that needs to be done. For example, most of the circuit tools, they target a specific DSL, and also they typically target a specific vulnerability class. And then we have some tools like static analysis tools like circumspect, which might have tons of false positives. And then we have some really nice tools and very novel tools like PyCos that try to formally verify and find any under-constrained issues in the circuits but unfortunately those tools do not scale that well. So there's a lot of space to do to have innovation and try to build better tools and here I have a list of security tools. You can scan that QR code and it's basically GitHub repository. If I don't have any of the tools that you are know of, please add them. And yeah, we need to do a better job here. And one also major issue I see in this space is that we don't have good tools for writing tests. And most of the code base that user knowledge proves is that we don't have good tools for writing tests. And most of the code base that user knowledge proves are unfortunately not that good in having complete test suites and try to understand in the testing part both soundness and completeness issues. Okay, so in conclusion, why do we have bugs? One of the reasons is that because ZKPs are not just mods. There are implementations and many things can go wrong in those implementations. Why else? This is a quote from Ron Rivest in a completely different context, but I really like it. And I would say that in the ZK space, unfortunately, we have given to the poor developer enough rope with which to hunt himself. Circuit languages are typically very low level, so they don't have good abstractions for developers to write safe code. We expose a lot of cryptography to the outer layers, and also there is a lot of complexity and a different threat model than what developers are used to. And there is a lack of specification throughout the whole infrastructure and the whole stack for using GKPs, so we need to write more specifications. So what can we do? Basically, we have to negate everything from the previous slide. We need more layering resources, which I think we are doing a great job in that as a community. We need to write specifications and get used to write specifications because if you have complete specification, then we know exactly what text we should put in each layer and what vulnerabilities can happen in each layer and that's how we can help developers but also auditors into doing a better and what vulnerabilities can happen in its layer. And that's how we can help developers, but also auditors into doing a better job in trying to find vulnerabilities in those systems. We need easier and more secure programming languages, which I think it's kind of where we are heading to. For example, Noir is a great language that it's much more safer than writing circuits in Siricom or Hello2. But in some cases, people will still need to write circuits in Hello2 or Siricom because they need to do some specific optimizations or they need to deploy to specific blockchains, for example. And then we need better testing and security tooling from simple frameworks to write unit tests, to do property-based testing, to formal verification. And not just formal verification. So that's it. I have there a link with our paper where you can find many examples and how to try to avoid some of those pitfalls. And we also have a blog post that we publish many blogs about ZK security in general. So, thanks a lot. Thank you, Stefanos. This was enlightening. All right, people. As usual, you can ask your questions here. We're going to go through them in order. And let's take the first one. Several times in your slides, you referred to witnesses. What are witnesses? Are those private inputs? So a witness, I would say, it's composed from both the private inputs, the public inputs, and all the intermediate steps and the outputs for our circuit. So I will say it's a trace that then we create a proof about that trace. Thank you. All right, next question. I've put a bunch of those, so do ask serious questions, please. We have a bit of time. What is your favorite bug ever? What is the most interesting bug you've ever found? That's a very good question. I think I can't pick one, but I would say typically the simple bugs, right? For example, the bug I have in one of the first slides, that could have led to basically draining one of the major mixers we have in the space. But also bugs that have to do with using cryptography in the circuits. And typically due to some optimizations or some logic errors in those circuits, there could be like pretty interesting exploits that someone can do. Pretty cool, thank you. Alright, the next one. You're doing research, looking for bugs, you're paying your bills and buying your foods by finding bugs. Can we consider you have a bug-based diet? We can consider that, yeah. I hope that in some future world there won't be that many bugs and maybe I will have a better diet. But unfortunately at the moment we have tons of bugs. Fantastic, thank you. What are your thoughts on TEE? Okay, that's the question of DevCon, I feel. Everyone asking that question. I would say you have different security assumptions when you use TEs, right? I think it can work along with ZKEs, but they can't replace ZKEs. You have a much weaker threat model when you are working with TEs. So, yeah, people should use them when they have to use them, but also don't trust them like a black box that will do everything for you and you are secure if you use a TE. Wonderful, thank you. What can we do to make more secure languages like Noir faster compared to Sercom, particularly with respect to gas cost? How do we make it more efficient? So gas cost, I will say that it's independent kind of what programming language you're using. It's more about like what proof system you are using, right? And if that proof system has very efficient verification, that's the main factor. But also more general in the circuit layer. I would say that indeed someone, if you don't like really use unsafe in Noir, which then breaks the whole purpose of using Noir, you can write more optimized circuits at that point in Siricom. But I would hope that we will have major advances in compilers for ZKPs, and then we can have compiler optimizations that are very strong, like in any other field, and rely on those optimizations to get pretty optimized circuits. But if we do that, then we need very, very solid testing for our compilers to detect any issues in those optimizations. Thank you. You mentioned in your slide that, you know, sometimes we give too much rope to the users to hang themselves with. I think the design in ZK circuits is difficult, but also using them is not very commonplace, right? A lot of users are not used to using this kind of systems and what goes in, what goes out, what you can do with them, what is safe behavior to do that. You mentioned learning resources. Do you think there's something to do with users also to explain to them what are the benefits and what should be done? Or is it entirely on the app developer? Yeah, yeah. That's a great question. I think as researchers, it's our responsibility to create learning resources that are easy to follow by almost everyone. So I think we are doing kind of a good job there. For example, at ZK Security, we published a book on Hello2. And basically, many teams in that space develop pretty nice learning resources. And what I really like is that they also have a section about security vulnerabilities and what you should look at when you use a specific DSL. So, yeah, I think we are doing a great job on that. And in a few years, it will be even better. Fantastic. Stefanos, thank you. We're over time. So thank you for your talk. Thank you. Thanks a lot.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:00:00.000Z",
      "slot_end": "2024-11-15T04:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1b-4F9L2PRDflpHb2iAzeGwsuH6cvqfh3FMJsnOPZOtc",
      "resources_slides": "https://drive.google.com/file/d/1A8f_scWEPOUSkiJ5gVpj0s-RXLBtPn2P/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "how-to-coordinate-an-epistemic-revolution",
      "sourceId": "DNJMER",
      "title": "How to coordinate an epistemic revolution",
      "description": "Amid widespread misinformation, division, and fractured consensus, we face an epistemic crisis. This talk unifies learning and governance theory, organizational design, social consensus tools, AI, and prediction markets. We will explore how DAOs and Ethereum can serve as decentralized platforms for collective intelligence and planetary-scale problem-solving, guiding us toward an epistemic revolution at this critical time.",
      "track": "Coordination",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Consensus,Quadratic Voting,Collective Intelligence,citizens,assembly,Collective Intelligence,Consensus,Quadratic Voting",
      "keywords": "Semantic Governance,Identity,Citizens Assembly",
      "duration": 1524,
      "language": "en",
      "sources_swarmHash": "8af47220dd27fe4b3dd7d5ceaf2f7a995e25ae4ead83ba92484fdd4f428d8f94",
      "sources_youtubeId": "7PjZyQsscQE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f77574749a4b8998ffb0",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:10:00.000Z",
      "slot_end": "2024-11-15T04:40:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1sq5KPHZmGsWxfhQtVwIBL6Wm8XGy-QAB5wFPQck9lO4",
      "resources_slides": "https://drive.google.com/file/d/1S9B7KM8koAEtRLuu7YlVtbMqWu9rHjKN/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "beyond-recursive-proving-for-starknet",
      "sourceId": "3CNMYL",
      "title": "Beyond recursive proving for Starknet",
      "description": "Recursive proving is very cool tech enabling very large proofs or combining many different statements into a single proof. Beyond recursive proving, statements can be combined in interesting ways to further reduce system overheads such as data availability compression and layer1 state updates as well as various privacy concepts. In this session we'll discuss some of these technologies and how they are being applied in Starknet to achieve various user and system benefits.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Zk Rollups,starknet,recursion",
      "keywords": "Proving,recursion",
      "duration": 1434,
      "language": "en",
      "sources_swarmHash": "24638b90dbf36120d55951ead1e1f18f411f2fd4041d5d760b2300cc9242907b",
      "sources_youtubeId": "SdWkt9B5W8E",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67383a891b0f83434d0bb1ae",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67383b971b0f83434d1dbd83.vtt",
      "transcript_text": " Hello everyone. Today I'm gonna talk about base pre-confirmation with Mr. Meb Boost. I'm Lino Shitaini from Nethermind Research. Okay, so most rollups, let's talk about most rollups today. So most rollups today build their blocks using centralized sequencer. So users send their transaction to centralized sequencer, the L2 transactions. The centralized sequ sequencer, the L2 transactions. The centralized sequencer will sequence the L2 transactions, eventually construct these L2 blocks, submit it to the L1 block building pipeline, and then the L1 block will include these L2 blocks. And centralized sequencer are nice. They can provide pre-confirmations. And pre-confirmations, what are pre-confirmations? Basically, users, for example, user submits their transaction C to the centralized sequencer. The centralized sequencer will return a pre-conf, which will say, I will promise to include your transaction C at the third position of the L2 block after transaction A and transaction B. And then some time passes, and then the centralized sequencer will actually submit this L2 block. But the thing here is this preconf will happen way ahead of time before the L2 block is actually submitted to the L1. So it provides good UX. So centralized sequencer are nice, they can provide facts to UX via preconfs, but however they're centralized. So big question here is how do we decentralize? So one idea here is what if we move the sequencer role entirely? And this brings us to based rollups. The based rollups idea is to use the L1 block building pipeline to sequence not only the L1 transactions, but also the L2 transactions. So the user submits their transactions, not only L1, but also the L2, and then the L1 block building pipeline will sequence not only the L1, but also the L2 blocks. So let's expand this L1 block building pipeline part. So this is like what the L1 block building pipeline looks like right now. The user will submit their transaction, and then there are going to be builders who are going to collect these L1 transactions, and then they're gonna be builders who are gonna collect these L1 transactions and then they're gonna construct blocks. And then there are gonna be many builders and then there are gonna be many blocks and then each are gonna submit a bid to the proposer. The proposer is gonna select the most profitable block and then propose it. So the idea of base rollups is that let's use this same pipeline that was used for the L1 which is used in blue here. Also for the L1, which is used in blue here, also for the L2 transactions, which is using the red arrow here. And base follow-ups are great. They inherit the L1 censorship resistance and liveness, because there is no external entity other than the, and we only have the L1 actors that we already have right now. And it also enables L1 composability, because these L1 builders, they will have full sequencing rights over the L1 and the L2. So they can act basically as a shared sequencer between the L1 and the L2, providing composability guarantees. But there's a problem, which is base rollups are slow, because there is no pre-confirmations here. So to the address of this problem, there is base pre-confirmations here. To the address problem, there is base pre-confirmations. Base pre-confirmations idea is this. Let's let the L1 proposer opt in to provide pre-confirmation during their slot. The user can directly send their L2 transactions to the L1 proposer. The L1 proposer can directly respond with a pre-confirmation. And then later on, the L1 block building pipeline runs, and then the proposer will ensure that these L2 blocks are included in the L1 blocks that they propose. So one observation here is that providing pre-confirmation requires sophistication. Because like the proposer here, you have to have some API or some like RPC endpoint to provide the pre-confirmations and you also need to run the L2 full nodes to actually like sequence these L2 blocks. So they would probably want to delegate and they will want to delegate to what we call gateways. So the idea of gateways is this. Proposer can delegate their pre-conf duties to gateways ahead of their slot. So the proposer can say, hey, this is my gateway. Please send your pre-conf request, not to me, but to this gateway. And the gateway will be responsible for responding with pre-confirmations. And then eventually the gateway will sequence L2 blocks, submit it to proposer, proposer will include it in their L1 blocks. So gateways are nice, but they have some problems, which is first, the inheritance of L1 censorship resistance and liveness is degraded. Because now you have this gateway entity that didn't exist in the L1 blockbinding pipeline that is now sequencing the L2 transactions. And now L1 composability is also made more complicated because you no longer have an entity that's sequencing both the L1 and the L2. Because the gateway is sequencing the L2, the builder is sequencing the L1. So the question here is, can we introduce base pre-conformation while retaining the good properties of base rollups? And here comes our proposal, which is called Mr. MevBoost. So it's for multi-round MevBoost. And the idea is this. So let's split the slots into multiple sub-slots, aka rounds, and in each round let's run a MevBoost auction to pre-confirm a partial block. So this whole L1 block pipeline that's sequencing both L1 and L2, let's just run this whole thing many times during the slot. So instead of running it once in the 12 second duration, run it every three seconds, for example. This pipeline is nice because there is no additional entity added to the pipeline. So there is no gateway here. And as a result, we inherit the L1 sensor resistance and liveness much better. Because there is no additional choke point to the system. And also enables L1 composability. Because the L1 builder now is sequencing both L1 and L2. Or they're pre-confirming both L1 and L2 at the same time. That it is, thank you very much. Yeah, thank you a lot. So just a quick reminder to the audience, you can ask questions by scanning the QR code. Do we have any questions from the audience, you can ask questions by scanning the QR code. Do we have any questions from the audience? Yeah, I was wondering myself if you can explain how long you've been working on this idea and if you published any papers already. Yes, there's an E3 search post titled",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:20:00.000Z",
      "slot_end": "2024-11-15T04:50:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1iFe5uo4vVJtKR4EYxLvteyWIaeQLzKWuZBOgGlIlGLk",
      "resources_slides": "https://drive.google.com/file/d/1A7SwgF9gYFDS241aTDN29m29fvkvLXBl/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "can-we-fix-mev",
      "sourceId": "Z3BPXH",
      "title": "Can we fix MEV?",
      "description": "MEV is problematic today. The MEV supply chain puts centralizing pressure on Ethereum. There’s also an allocation problem; proposers (not apps or users) earn nearly all MEV, though they’re merely protocol agents. Numerous proposed solutions address this (ePBS, EAs, ETs, FOCIL, BRAID...), each with tradeoffs and assumptions about whether MEV is intrinsic to blockchains or extrinsic & preventable. Research is challenging to enter w/o continuous engagement. I’ll provide an overview of the research.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "PBS,Consensus Mechanisms,MEV,execution,tickets,Consensus Mechanisms,MEV,PBS",
      "keywords": "Execution Tickets,Execution Auctions,BRAID",
      "duration": 1568,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "69IZIIsanvM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "",
      "transcript_text": "",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:30:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1uiCga8JgSVcEJbIHMSRcQYytS1nwlZiz6mLdidoUbr8",
      "resources_slides": "https://drive.google.com/file/d/1MxUF3ReK7gjSPhfqvCqzh4I5uQ-YXwAr/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "cls-ethereum-magicians-infinite-endgames-ethereum-execution",
      "sourceId": "S8NCDC",
      "title": "[CLS] Ethereum Magicians Infinite Endgames: Ethereum Execution",
      "description": "A fishbowl-style discussion with core Ethereum contributors and community to publicly discuss the \"endgame\" of execution on Ethereum. We will discuss what the evolution of Ethereum’s execution layer will look like, L1 vs. L2, settlement vs. execution, enshrined rollups, consensus changes vs. client performance improvements, etc.",
      "track": "[CLS] Infinite Endgames by Ethereum Magicians",
      "type": "Workshop",
      "expertise": "Expert",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,In-protocol Account Abstraction,Rollups",
      "keywords": "",
      "duration": 4849,
      "language": "en",
      "sources_swarmHash": "47a4a6de64e06f0a2c8389f00f3bff04b17d0251efd7c0c3693f74522d80399b",
      "sources_youtubeId": "o8n-MCAnxso",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673cceb9982f234a126ca721",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:30:00.000Z",
      "slot_end": "2024-11-15T06:00:00.000Z",
      "slot_roomId": "breakout-1",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ovum9wCpn-eOO_GaydQ7myGTVXFB4g6lDX0Btv4ApMI",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-1",
        "name": "Breakout 1",
        "description": "",
        "info": "1",
        "capacity": 100,
        "youtubeStreamUrl_1": "https://youtube.com/embed/hybNMSQGyZ8",
        "youtubeStreamUrl_2": "https://youtube.com/embed/CH13V8pRzSk",
        "youtubeStreamUrl_3": "https://youtube.com/embed/uIsCaqDRQqI",
        "youtubeStreamUrl_4": "https://youtube.com/embed/63w7kHh737w",
        "translationUrl": "https://stm.live/Breakout-1"
      }
    },
    {
      "id": "polynomial-commitment-schemes-for-zero-knowledge-proof-systems-a-hands-on-workshop",
      "sourceId": "QAQAUX",
      "title": "Polynomial Commitment Schemes for Zero-Knowledge Proof Systems: A Hands-on Workshop",
      "description": "In this workshop, we will compare three distinct classes of Polynomial Commitment Schemes employed in various zero-knowledge proof systems: pairings-based (e.g., KZG), discrete logarithm-based (e.g., IPA), and hash function-based (e.g., FRI). We will explore their mathematical constructions, properties, and trade-offs. Participants will engage in hands-on proof-of-concept implementations, gaining practical experience of these advanced cryptographic protocols.",
      "track": "Applied Cryptography",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": true,
      "tags": "Zk Rollups,Zero-Knowledge,Cryptography,implementation,Cryptography,Zero-Knowledge,Zk Rollups",
      "keywords": "cryptographic primitives,implementation",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:30:00.000Z",
      "slot_end": "2024-11-15T06:00:00.000Z",
      "slot_roomId": "classroom-d",
      "resources_presentation": "https://docs.google.com/presentation/d/1L15TG4XE9h8o3WvPj5ksj6cdCnNYdYuY1dI9gWq3GEg",
      "resources_slides": "https://drive.google.com/file/d/1yM92AclsMJ7nFpBrBtWJ7ZWnI9YvTVbx/view",
      "slot_room": {
        "id": "classroom-d",
        "name": "Classroom D",
        "description": "",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Kv4uKJZXLOk",
        "youtubeStreamUrl_2": "https://youtube.com/embed/vIVV-8e83ZM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/aR9mpxHDsvw",
        "youtubeStreamUrl_4": "https://youtube.com/embed/O6rHUjKg6gI",
        "translationUrl": "https://stm.live/Classroom-D"
      }
    },
    {
      "id": "rip-7755-empowering-cross-chain-interactions",
      "sourceId": "787TJ7",
      "title": "RIP-7755: Empowering Cross-Chain Interactions",
      "description": "Cross-chain interactions are becoming essential as Ethereum Layer 2 solutions multiply. RIP-7755 changes the game by trustlessly bridging the gap between L2 chains, allowing new use cases that rely solely on Ethereum and its rollups. In this workshop, we’ll explore RIP-7755 by building a cross-chain NFT minting app, focusing on nested storage proof implementation details to eliminate trust assumptions.",
      "track": "Layer 2",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cross-L2,Rollups",
      "keywords": "Interop",
      "duration": 5524,
      "language": "en",
      "sources_swarmHash": "f335f509aad994029fa3bd29d0c69456d45499bee29aea62b1cd0877fa13e0c3",
      "sources_youtubeId": "yw-lgjdg7FY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673869701b0f83434dee5eaa",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/673869701b0f83434dee5eaa.vtt",
      "transcript_text": " . Okay. I think we're good to get started. Hello. Welcome to the workshop for RIP 7755. This is a proposed standard for empowering low-level cross-chain calls with minimal trust assumptions. My name is Jack Chuma. I'm a senior software engineer working in R&D on the base team at Coinbase. And I'm so excited to share with you all a little bit about this project. So a couple of goals for today. First and foremost, I want to promote a deeper understanding of RIP 7755, what exactly it is and how it works. Second, considerations for adding new chain support in the future, as I foresee that being one of the main opportunities for open source contribution. Number three, one of the main features of the standard is that it minimizes trust assumptions, and that's done via a mechanism called what we're calling nested storage proofs. So I'd like to do a deep dive there and promote a deeper understanding. And then lastly, integration details. So if you're an app developer and you would like to be able to facilitate some kind of cross-chain call between L2s and the Ethereum ecosystem, how would that work if you're integrating with the standard? So to give you some context and a high-level purpose on where we're coming from here, I included a screenshot here from L2Beat sometime last week. And what it's showing is activity in Ethereum L2s. And more specifically, how it's surged by over 500% in just the last year alone. And that's being spread out over many different networks. So that's why I chose this screenshot specifically. It shows a handful of networks. This is just a small subset of how many L2s there are already. And that's only just going to continue expanding. And this has been great for scaling Ethereum. But it has caused fragmentation in the ecosystem, where if you're a user and you want to interact with an app that's deployed to a specific chain that you maybe don't have funds on, there are certain hoops that you need to jump through to get funds to the correct location to be able to interact with that application, and that hurts the user experience. It's a critical problem that has to be solved, and so to solve that problem, we believe that there should be a standard for communication between chains that checks the following three boxes. Is public and decentralized in the spirit of Web3, relies solely on validation data that is trustlessly available on-chain, so minimal trust assumptions, and has built-in flexibility to support any arbitrary message. These three bullet points were the three North Stars that we kept in mind as we developed the RIP-7755 proposal, and we'll dive in now. So because Ethereum L2s post some sort of state representation to a shared execution environment, they are uniquely positioned to solve this problem with minimal trust assumptions. And this is done via a mechanism called nested storage proofs, as I previously mentioned. This allows us to prove state about one L2 from another L2, even though they don't have a direct line of communication. To understand storage proofs, I think it makes sense to do a quick refresher on Merkle trees. Of course, this is nothing new, but a required prereq to understand storage proofs and how they work under the hood. So as a quick recap here, a Merkle tree is basically a tree data structure where each node is a hash of its direct descendants. And so if you wanted to represent, like in this diagram here, I don't know if you can see my mouse, yeah. We have these four data blocks, A, B, C, and D. And if you wanted to convert that into a Merkle tree, each one gets hashed, respectively, to create a leaf node for the tree. You group the nodes into groups of two, concatenate them, hash them together. That creates their parent node. And you do that recursively until you reach the root node of the tree. and effectively what you've done is generated a unique identifier for the entire data set that is just a single hash. So this has a couple of interesting properties, one being that if any of the data blocks changes in any way whatsoever, the root hash is going to change completely. And then this also, like this property, allows us to efficiently prove inclusion or exclusion of data within the larger data set. So in this example, if we wanted to prove that data block A, it was in this larger data set of A, B, C, and D, we first would need a verifier to have trustless access to the root hash that is represented in this root node up here. If that's in place, all we'd have to supply to the verifier is data block A, and then this node of hash of B, and then this node hash of HCHD. And that data alone is all the verifier would need to recreate the root hash at the top of the tree. So the verifier would then hash A to create this hash A node, combine hash A with hash B, hash that together to create this hash H hb node and then do that again for this this final level to recreate the root node and if that is equivalent to this the stored root node that the verifier already had then that's a successful proof so how does that apply to ethereum storage basically all of ethereum's state is represented as a modified form of that merkle tree data structure called a merkle Patricia try. And the exact details in terms of the differences between the two data structures are out of scope for this talk, but it's basically a handful of optimizations for Ethereum-specific use cases. All we really need to know or care about for this application is that that Merkle-proof paradigm applies here. So for every block in Ethereum, there's a handful of block headers, one of which being a state root. This is the root hash for a Merkle-Patricia try for all of Ethereum state, where the values in that try are Ethereum accounts. These accounts could be EOAs, so like any off-chain wallet, like Coinbase Wallet or Metamask, or it could be smart contract accounts. These accounts, as they're represented in the try, are represented by an array of four pieces of metadata about the account, and that's what I have listed out here. So you have the account nonce, balance, storage route, and code hash. So for smart contract accounts, it's highly likely that they're managing some type of state. And if they are, that state is stored in that contract storage. And that contract storage is also represented as a Merkle Patricia try under the hood. And you guessed it, the root of that try is the storage root that is stored with the account. So if we have access to a state root for a network, we can supply a path down that state try to a specific account within the network. And then using that account's storage root, which can be extracted from the account's metadata, we can supply another path from the storage root to a specific location in that account's storage. And that's basically the high-level concept of how a storage proof works. So how does that apply to cross-chain messaging, though? Because that's basically the high-level concept of how a storage proof works. So how does that apply to cross-chain messaging, though? Because that's just proving state within a specific network. So to explain that, I have this diagram here. It's very simplified, obviously. But this is meant to represent two roll-ups in the Ethereum ecosystem that are both sharing state with a shared L1. So this L1 at the bottom would be like Ethereum mainnet, and then chain A and chain B are two L2 networks. What this diagram is depicting is bidirectional communication between the two layer 2 chains and the shared layer 1. For this downward arrow direction in both chains, an Ethereum L2 chain wouldn't be an Ethereum L2 chain if it wasn't sharing state with L1 in some context. So that's what this is representing here. It's requiring that both chains are sharing some representation of their state with what I'm calling a roll-up contract on layer one. This could be the state root directly, or it could be some other representation of state, the only requirement being that it has to be verifiably linkable to its state root, at least for the way that RIP 7755 is working thus far. And then in the other direction, we need trustless access to a layer one state representation within the L2 chains as well. So that's what these upward arrows pointing to the beacon roots Oracle contracts are. This is made possible by an improvement proposal that's live today in many networks called EIP-4788, which trustlessly exposes the most recent 8,191 beacon routes for the Ethereum consensus client within the L2 execution environment. It should be noted a beacon route is not the same as execution client state route, but it is verifiably linkable to the L1 execution client state root via a very similar process. So from chain A, if we were trying to prove something about the state of chain B, this diagram represents everything that needs to be true for that to work. So if chain A starts with a trustless access to a beacon root. It can supply like a Merkle Patricia try based proof to verify the L1 execution client state route. And then if we have a verified L1 execution client state route, that exact storage proof process that I just went through applies. So we could then prove anything about the state of layer one from chain A. In this context, the first step would be to go from the state route to like a path to an account within that state try, and the account here would be chain B rollup contract. And then using chain B rollup contract storage route, we can supply another path to a specific location in that contract. What's interesting here is if the value that you're verifying inside of Chain B roll-up contract is itself a state representation for Chain B, you can then recursively follow the same steps again to prove something about Chain B's state try. So then starting from Chain B's state route, you can supply a path to a specific account within Chain B, and then again a path from that account's storage route to a specific storage location. And that effectively allows us to prove verifiably a location and storage in an account on chain B from chain A, even though there's no direct line of communication. So that's cool, but how does that help us with cross-chain calls? This diagram is an overall architecture for how RIP-7755 is set up to work, and that should help answer this question here. So as you can see, we have two chains represented, an origin and a destination, and then we have both on-chain and off-chain components here. Every supported chain is going to have some sort of inbox and outbox contract that we're calling RIP 7755 inbox and outbox, but I'll just stick with inbox and outbox for the rest of the talk. The outbox contract is basically the entry point into the standard. So if a user wants to request a cross-chain call, they submit a request to the outbox contract. If the request settles properly, it'll emit an event that some off-chain actor that we're calling fulfillers should be listening for. And if there's sufficient incentive to respond to that request, then the fulfiller will. So that brings us back to a way to incentivize fulfillers to respond to the request. Another key piece of logic that happens here is the user will also lock some kind of reward bounty for the Fulfiller to respond to the request. If the reward bounty is sufficiently like Incentivized to the fufiller, then it will respond. So the fufiller then assuming it's a sufficient incentive Will submit the requested call to the destination chain over Here. That routes through an RIP 7755 inbox contract which will perform a handful of validation steps mainly confirming that the request is arriving to the correct chain and the correct location at the correct chain at that. There's also this custom like optional validation step called a pre-check contract where this could be absolutely anything as long as it adheres to a a pre-check contract, where this could be absolutely anything, as long as it adheres to a specific pre-check contract interface that the standard requires. And all this is meant to do is allow the user to encode any kind of arbitrary fulfillment condition that should be true in order for the fulfillment to work out. But like I said, again, it's totally optional, so if it's not being used, this step will be skipped. If all the validation steps are sufficiently checked out properly, then the requested calls will be routed from there. So this could be a batch of any low-level arbitrary call that goes to a handful of addresses with encoded call data and any native currency value that may be included. If all of those are successful, the main purpose of this inbox contract is to then store a receipt of successful execution. And this execution receipt gets stored in a deterministic location within this contract storage that is derivable from the origin chain without knowing anything about the state of the destination chain. So that's important, and we'll come back to it in a second. After the call has been successfully submitted, the fulfiller then comes back to the origin chain to say, hey, I did the job. Now can I have my payment? And what's unique about this standard is that the payment will only be released if the fulfiller can cryptographically prove that they did actually submit the request to the destination chain. And that's done via that nested storage proof concept that we just walked through. So from the origin chain, the origin chain would have to be able to verify a specific storage location in the inbox contract on destination chain. and because of the fact that the outbox contract can derive exactly where that location is supposed to be, then the outbox contract has everything it needs to be able to verify the successful fulfillment of the call. So only if this nested storage proof checks out will the outbox contract release the reward to the fulfiller. And that would close the loop on the full process for how RIP-7755 is working. Yeah, so for today's workshop, I have an example project for us to go through together. So if anyone's interested in coding along, there is a starter repository on my GitHub that I can show in a second. If you're not interested in following along, I'll be doing it up here as well, so we can go from there. So I'm going to start by walking through all of the contracts and services that are present in the starter project. This is going to be a self-contained system that is mocking a multi-roll-up ecosystem that will run locally on your machine. So once you clone the repository, you should have everything needed to run the entire app end-to-end locally. So after a brief walkthrough of all the services and how they're working together to make that work, we'll implement and test a nested storage-approved validation contract, as that is where the bulk of the effort is going to have to be applied to add new chain support in the future. I also just think it's really cool. And then once that is working properly, we'll integrate with an off-chain client application. So for this demo, it's just a simple NFT mint application where the NFT owner wants to be able to support users who don't necessarily have funds on the chain that the NFT contract lives on. So if they don't, it would be an RIP 7755 request to send the cross-chain call to still mint the NFT. So once that is all set up, we should be good to run the app end-to-end. Before we jump in, I'll leave you, well, I guess I'll leave this presentation piece with that. It's still very early in the research phase that a lot of these details are subject to change, but we have proven the concept on live networks of this nested storage proof and how that can be used to trustlessly verify cross-chain calls. So this seems to hold a lot of potential. It's something I'm very excited about, something the base team is very excited about. We do have an open source proof of concept repository on the base org github that I fully invite anyone and everyone to contribute to if you have interesting ideas. So with that, we should be good to dive into code, but as a quick gut check, are there any questions before doing so? And there will be time for questions at the end, too, and after this talk. Okay. So I didn't know the best way to share the link, but my GitHub is my name, Jack Chuma, and I have this DevCon 2024 RIP 7755 workshop project. If anyone's interested in coding along, you can clone this repository and follow along with me. I already have it cloned, so we can start with a brief walkthrough here. So once you have the project, what you'll notice is there's two main directories. We have contracts and services. So this is on-chain and off-chain components from that architecture diagram. We can start by going through the contracts. I'll start with the NFT contract because this is very simple. But this is what our demo client is going to be using. The main piece here is this mint function. The only reason I'm covering it is because this is going to be needed to set up the integration when we get there. But there's nothing too interesting happening here. Next up we have a rollups directory and this is used to mock the multi rollup system locally. I didn't want to have to rely on good internet connection for this to work so we got a mock system running locally. This rollup contract is what would be deployed to a mock L1 and this is what will be storing a state representation for the mock L2s. In this context, that state representation is a hash of the L2 block timestamp and the L2 state root. And then on the L2 side of things, we have this beacon Oracle contract, which is meant to mock the EIP-4788 interface to query one of the beacon roots that are being stored in the L2 execution environment. For this example, it's a simplified example, and this is directly storing the L1 execution client state route. So we are cutting out a step of verification going from beacon route to state route, but I think it still gets the message across. And then we have all of our RIP 7755 contracts. So as a brief walk through for what an actual request looks like, we have this RIP 7755 structs file. And this is exactly what a request would look like. So all the fields are we start with a requester. So this is the pretty self-explanatory, the address submitting the request. We have a batch of calls where each call is a low level description of the exact address that you'd like to send the call to, encoded call data, and then any native currency value that should be included with that call. And then we have a specified prover contract. The reason this is here is because there's no standard way for L2 chains to post their state representation to L1. And because of that, the exact implementation details for verifying state about a destination chain will vary depending on what that destination chain is. So right now we have this set up with the proving logic abstracted in two separate contracts. This very likely will change in the near future. If we baked that into the outbox contract, that would require multiple outbox contracts to be deployed to each chain. So it's a trade-off. But for right now, this is set up to be one outbox, one inbox, and then an array of prover contracts deployed to each chain that the user would have to specify which one should be used to verify fulfillment. And this contract is what we're going to be implementing in a few minutes. Then we have destination chain ID, which is pretty self-explanatory. Inbox contract is the address of the RIP 7755 inbox contract on the destination chain. L2 Oracle address, that is the address of the RIP 7755 inbox contract on the destination chain. L2 Oracle address, that is the address of the roll-up contract that would get deployed to L1 for the destination chain. So this is the user specifying where the prover contract should be looking for the state representation for the destination chain when it's verifying that the call was submitted. So the user is specifying the address where that should be located submitted. So the user is specifying the address where that should be located as well as the storage key within that address. And then we have a reward address. This could be an ERC-20 address or as specified by ERC-7528, there's a special address value that can be used to depict native currency. Then there's reward amount, which is the amount of the reward asset that should be locked within the request. It should be noted that reward amount should cover all of the value that's included in these calls, plus whatever the gas cost would be for submitting the call to the destination chain, plus an extra tip for the fulfiller. And that extra tip is what acts as the incentive for the fulfiller to respond to the request. Then we have finality delay seconds. This is the gap after the call is successfully fulfilled on the destination chain. That has to pass before the fulfiller is allowed to claim the reward. This is basically like destination chain reorg protection for the user. The higher this delay is, the more likely the destination Chain won't be reorged and ensure that the call was Actually submitted and will stay submitted. And we have a nonce value for ensuring that every request is Unique as the way we're identifying these requests is by Hashing this entire structure. An expiry field for when the Request should expire. This is relevant for the user being able to reclaim the reward if for whatever reason the call was never submitted to the destination chain. There should be a mechanism for the user to recover those funds, and that's what this expiry timestamp is. If the call's not submitted before the expiry timestamp, then that's when the user can reclaim. And then we have these last two fields for the pre-check contract. So a pre-check contract address and an arbitrary bytes array of encoded data for that pre-check. This is for that optional, like, arbitrary fulfillment condition that should be true. If this is the zero address, that is depicting that we're not going to use it. And we're not going to be doing a pre-check step in today's demo, but I figure it's worth covering that it's there anyways. So the next step, we'll do a quick walkthrough of the inbox and outbox contracts. So the outbox contract being the entry point to the system. We have one main function that we really care about here, request a cross-chain call. That's where the user would submit the request and where the event would be admitted that the fulfillers are listening for then we have a claim reward function which is where the fulfiller comes to claim their reward after successfully fulfilling the request and that calls into the prover contract here on this line and so this contract is again what we're about to implement it is expected to revert if the proof fails so this contract is again what we're about to implement. It's expected to revert if the proof fails. So there's no return value or anything here. And then lastly, the cancel request function. So this is like after the expiry timestamp, if the reward has not been claimed yet, then the user gets to reclaim it. On the destination chain side, we have the inbox contract. The main piece of information we care about here is this fulfillment info struct. So this is that execution receipt that should be created in storage, and that will be the target of the nested storage proof validation. The whole point of the storage proof there is to prove that this struct exists in storage for the specific request. And this is storing the timestamp at which the request was submitted, is to prove that this struct exists in storage for the specific request. And this is storing the timestamp at which the request was submitted, as well as the filler address that should be able to claim the reward back on the source chain. And that gets created during this fulfill function. So we have all our validation steps up here. And then we route the calls here. And if everything's successful, we're left with the created fulfillment infostruct. So this contract is fairly simple. Something a little bit more interesting, we have this state validator library. And if you in the future are working on setting up a new prover contract for a new destination chain that's not currently supported by the proposal, you likely would be utilizing the state validator library. The whole point of this is to abstract a lot of the complexity involved with storage proofs away from the developer. No need to reinvent the wheel every time. So the way that this is set up, there are two main functions that we care about. There's validate state, and then there's validate account storage. Validate state is for if you're starting from the beacon route on L1, and you're trying to verify the L1 execution client state route against that beacon route. That's what validate state would do. And then from there, using the verified state route, it would then prove storage location for an account within that state. Because our example today is not using beacon routes, we don't need to use this function, but I figured I'd briefly cover it. The function we care about is this validate account storage. So this takes in an account, which is a specific account within the network, a state route for the network, and then a handful of these account proof parameters. And using these account proof parameters, which are a specified storage key, an expected storage value, and then an account proof and a storage proof, the account proof can be thought of as the path down the state try from the state root to the specific account we care about, and then the storage proof would be thought of as a path from that account's storage root to the storage location that exists at storage key and should be storing storage value. So it should be noted that all of the values in the state try are keyed by the hash of the Ethereum address of that account. So that's what we're doing here. We're deriving the account key, and then using that, we can do a merkle try.get to return an encoded account. An encoded account is basically an encoded array of the account metadata that I went through in the slides earlier that have the nonce, the balance, the storage route, et cetera. So from that, we can extract the storage route, and then using the storage route, we can verify that storage location using the storage proof that exists in the account proof params struct that I just went through. So at a high level, that's how that's working. There's a directory in here called Provers. This is what we're about to implement, so we'll be coming back to that in a second. Just real quick before we dive into the implementation details there, I want to do just a quick summary of what off-chain services are running here. The demo is the app that is facilitating the NFT mints, so we'll be implementing some details in this directory. But then these other two directories are surrounding services that are needed for the full system to run locally. The sinker is in charge of sharing state representations bidirectionally between the mock L1 and the mock L2s. And then the fulfiller is the off-chain agent that's listening for requests and will validate the request and ensure that the incentive is enough to compensate them for their time. And we'll respond accordingly and submit the request. And then we'll be generating a full nested storage proof that gets validated against the contract we're about to implement. And if that checks out, we'll be able to see the fulfiller claiming its rewards in real time. So with all of that being said, we have enough here to dive into beginning to implement a prover contract. I have a handful of imports just set up in here already just to save the time from typing them out. You'll notice the first import is an iProver interface. So we can start here taking a look. And what it defines is a single function. And this is all the approver contract needs because this is the function that the reward claim function from the outbox contract is going to hit. So we can start by literally just copying this entire thing into our prover contract to initiate the implementation of that function. So I will replace this comment with that function declaration and add empty curly braces here. So, oh, and then we can extend that interface. This is iProver. To take a quick skim through the comments here that are explaining what validate proof should even be doing. It validates storage proofs and verifies fulfillment. Okay, makes sense. It should revert if the storage proof is invalid. Also makes sense. It should revert if fulfillment info is not found at inbox contract storage key on the specified inbox contract. That is kind of interesting here. It should be noted that the storage key, like I was saying before, is derivable from a network that doesn't actually have context of the destination chain. And because of that, this is being done in the outbox contract before this function is hit. So this is not coming from the off-chain fulfiller. We can trust this value. Lastly, it should revert if the fulfillment info timestamp is less than the finality delay seconds amount of time from the current destination chain block timestamp. So this is that destination chain reorg protection that I was mentioning earlier. We need to ensure that the finality delay seconds is not currently still in progress. Then we have a note about that the implementation should vary by destination L2. This is due to the lack of standardization around how L2s post their state representations to L1, like I was saying. And then a quick summary of the input parameters that we have to work with here. So inbox contract storage key, I just kind of mentioned, is the storage location in the inbox contract on the destination chain where we expect the execution receipt to be. Next up is the fulfillment info struct. So this is that exact execution receipt that should be existing at inbox contract storage key on the destination chain's inbox contract. Then we have the initial request that came from the user. And then we have an arbitrarily encoded proof data bytes array. This is because of the fact that, like I was saying before, the lack of standardization around the state posting, there could be subtle differences in the exact data that is needed for the prover to verify that state. So there's no enforced structure to this data at the outbox level. This is being implemented here within the prover. So we'll start to set that up in just a moment. Okay, so with all of that gone through, we have enough here to start to set this up, so we can think about what the steps are for validating a nested storage proof. For starters, we'll want to enforce a structure to proof data, so we can decode this into some defined structs that we'll define in storage up here. So decode proof data. Next with the decoded proof data, we can use some supplied data to trustlessly access the beacon route from L1. So I'll want to query L1 state representation. In a real network, this likely would be, or if the network is supporting EIP 4788, this would be a beacon route. For today's example, it's the state route directly. So I'll just make a brief comment explaining that. In real network, likely beacon root in two days demo state root. Okay, so then step three, once we have a state representation for L1, using L1 state root, we'll verify storage location on L1, USING L1 STATE ROUTE, WE'LL VERIFY STORAGE LOCATION ON L1 AND THAT STORAGE LOCATION WILL BE THE DESTINATION CHAINED ROLLUP CONTRACT. SO VER. STEP FOUR, WE'LL NEED TO VERIFIABLY LINK A DESTINATION CHAIN STATE ROUTE TO THAT, OH, YEAH, LET'S SEE, USING L1 STATE ROUTE VERIFY STORAGE LOCATION, THIS SHOULD BE DST CHAIN STATE REP. WE'LL NEED TO USE THAT VERIFIED VALUE TO LINK A STATE ROUTE FOR We will need to use that verified value to link a state root for the destination chain. So we can do that as a step here. Verify. We link to chain state rep. After that step, we should have a verified state route for the destination chain, so then we can essentially repeat step three again. So this would be using L2 state route. Verify execution receipt in inbox contract on DST chain. And then lastly the only step that we haven't covered is the this revert statement that's saying if finality delay seconds is still in progress this function should be reverting. So we can check that as our final step. Whoops. Step six. Revert if finality delay seconds in progress. Okay. So if we can successfully set up these six steps, then we should be good to go to verify these nested storage proofs. If we take a look up here, you might have noticed this contract is expecting to be deployed with an address in the constructor. This address is for the beacon roots Oracle contract. In a live network, you wouldn't have to do this because theIP 4788 specifies a deterministic precompile address that you could just hard code into your contract storage. But for this to work in both tests and deployed to our local network, I have it being deployed with the address specified here. So as our first step, we can store this as an immutable, and call it beacon Roots oracle. Something like that. Roots. And then assign that within the Constructor. And then if we start to think about how these steps are working, the first step being that we want to decode proof data into some specified structure that we're going to define, we can start by defining a struct, RIP 7755 proof. And the compiler is going to be mad about the struct being empty, but we'll come back to that in a second. We can set up this first step with that in place by decoding the proof data into a local variable that we can call proof that is adhering to this RIP 7755 proof struct. So if we copy this, paste it here, this will be in memory, is equal to ABI.decode proof data, and then pass in the name of the struct as the second argument here. This will decode the proof data bytes into whatever structure we define at the top of the file here. So for step two, we want to then query the L1 state representation. This is going to come from that beacon Oracle contract from the rollups directory that I covered briefly. So if we pull that up to take a look at the storage layout here of how exactly we should query that, this has a fallback function in here, and that's to mimic the interface that would be used to query a beacon root from the real beacon root Oracle contract on a live network. And all this takes is an encoded block timestamp. So that's actually the first piece of data that we need to add into our RIP 7755 proof struct. Is we need to know the L1 block timestamp that we're going to be using for the proof. So we can add that in as a Uint256. Call it L1 timestamp. And then using that, we can set up a static call into the beacon Oracle contract. If you remember, we have the address stored as the immutable variable here. So we can take this, copy that, and under step two, paste that. That'll be, yeah, beacon roots Oracle. You do dot static call, which is like a low-level call, but kind of like a view function where it's expected to not mutate any state in the destination address that you're calling. And then we're passing in the encoded block timestamp for the L1 chain, which we just added to proof, so this should be available via abi.encode with proof.L1 timestamp passed in. Whoops. This static call will return a tuple where the first value is a Boolean, so we can call that bool success. And the second value is bytes array in memory. So bytes, memory, data. With any low-level call in an EVM chain, you need to confirm that success comes back as true. Because if something weird happens with this address or the static call fails for some unexpected reason, and success comes back false, we need to ensure that we revert the transaction here. So for that case, we can add a custom error at the top of the file for if the static call fails. We can call it error. Beacon roots Oracle call failed. Copy that. And then underneath this line, we can do if not success, revert with that custom error. So if we get past this if statement, we have a returned Data bytes array where the data is representing an encoded Version of the state route for l1. After this if statement, we can decode data into a bytes 32 State route. This would be bytes 32 l1 State root. It's equal to another abi.D code. With data Passed in and then the data type is bytes 32. Okay. So that Should do it for step two. So at this point we have an L1 state route that we can then use in a storage proof to verify something about the state of L1. And that's exactly what step three is laying out here. So using L1 state route we want to verify the storage location in the destination chain's roll up contract ON L1. THIS IS GOING TO BE THE STATE REPRESENTATION FOR THE DESTINATION CHAIN. IN ORDER TO GET MORE CONTEXT FOR HOW THAT SHOULD WORK, LET'S TAKE A LOOK AT THE ROLLUP CONTRACT BECAUSE THAT'S WHERE THE DESTINATION CHAIN WILL BE POSTING ITS STATE REPRESENTATION. hosting its state representation. So that will be inside of this rollup contract. What's kind of interesting here is in a lot of live networks, the exact storage location that the state representation is going to exist is not necessarily known at the time of request. In that case, the request just knows the storage slot, maybe where the data structure is located. So like in this example, the mapping storing output routes exist at storage slot one. So in this context, the request specifying the destination chain's storage location for their roll-up contract on L1 would just be specifying storage slot one, and then we'd have to take that and derive the location of the value for the mapping based off of the destination chain's L2 block timestamp. So that brings us to the next piece of data that we need for this proof. It's going to be the block timestamp for the destination chain. So we can add that as another Uint256 instead of the proof. And this can be L2 block timestamp. By the way, for anyone following along, try to stick to the exact names I have here for the fulfiller proof to work properly. The logic by itself should work fine either way, but in order for this to be compatible with the surrounding system, we have to use the correct names. So if we have the L2 block timestamp here, then we have enough to derive the storage location of the output route associated with that block timestamp inside of the roll-up contract. So how do we do that? If we take a look down here, so for step three, we're going to Be using the l1 state route. This is the first example of an Actual storage proof that we're going to use. This is where we would maybe want to take a look at that State validator library again because this has a bunch of Utility functions for facilitating storage proofs Directly. Nam namely being this account proof parameter struct. This is going to come in handy. And then that second function I mentioned for validating account storage. So we'll actually be using this function here. This takes in an account, a state root, and then proof parameters that should be provided by the fulfiller. So we can start to set that up now, starting with state validator. And then dot validate account storage. My autocomplete added the names of the variables here. So for the account, what's the account here? This is the roll-up contract for the destination chain. And if you remember from the structs file that I walked through at the beginning, one of the fields that is specified by the requester is this L2 Oracle address. And this is exactly the account that we care about for this first storage proof. So we can pass that in here. This is going to come from request, which is being passed in. So this will be request.L2 Oracle for the account. The state route is the L1 state route that we just decoded here. So we can use this for state route. And then the account proof parameters, this is this account proof parameters struct inside of the state validator. This is going to be supplied by the off-chain fulfiller. So this is the next piece of information we need in our proof struct. So if we copy this, add it as a third argument in the proof struct or third field. Oops. This will be state validator dot account proof parameters. And we're going to call this one DSTL2 state root proof ramps. So then we can copy that variable name. And this will be the last argument passed into our validate account storage function. So this will be proof dot and then that copied field name. If you take a look at the validate account storage function here, you can see that it returns a Boolean value. So we need to ensure that the Boolean value returned is true. So we can capture that in a local variable here. We can call it bool is valid L1 state is equal to the state validator. Validate account storage. And then for when it's not a valid L1 state, we can define a custom error up here. This will be error invalid L1 state. Copy that. And then if not is valid L1 state, we'll revert with that custom error. And there we go. All right. So what we have happening here is after this step, we should have a verified storage location in L1. To jog your memory on the params that are being passed in by the fulfiller, there is something kind of fishy happening with the way we currently have it set up. These account proof parameters specify an exact storage location and expected value. So if this checks out and returns true, it means we've successfully validated that location. But what if that location's the wrong location? Like what if it's not the state representation for the destination chain? That would be a problem. So this is where we'd want to override the storage key. We could either derive what it should be and confirm that it's equivalent, or we could override it. For this example, we'll override it, but maybe there's a subtle gas optimization one way over the other. But for starters, we'll want to create a helper function for deriving what the L1 storage key should be. So we can create that down here as a private helper function. Derive L1 storage key. This will be a private peer returns bytes memory. Because as you see up here the storage key is a byte string not a bytes 32 or anything. So yeah. So this is the storage key of where the state representation should be in the roll-up contract on L1. So we can take a look at this roll-up contract again to see how that storage layout is set up. We have a mapping located at the first storage slot, which likely is going to be the inside of the structs file. This likely is going to be the L2 Oracle storage key. So I would expect this to be the bytes 32 representation of the number 1. And then we can take that and hash that with the L2 block timestamp, which under the hood is what Solidity is doing to generate the storage location for the value that should be keyed by the block time stamp. That's exactly what we can recreate here. In order to do that, we need a couple of pieces of information Here. One of them is the l2 oracle Storage key which comes from the request. We can pass in the request here. Copy this whole thing. Pass that in. And then the other piece of information is the L2 block time stamp, which if you'll remember we added to the proof struct up here. So we can pass that in as well. So if we copy this declaration, we can paste that here as a second input argument. And then, so what does this return? So this is going to return a derived value for the storage key where the L2's output route should exist. That is going to use, so we'll return an ABI.encode pact. With the block timestamp passed in. So this is going to be proof.L2 block timestamp. And then the storage key location. So then it's going to be request.L2 Oracle storage key. So this concatenates them together into a single bytes array. We now need to hash this to generate the storage key. This concatenates them together into a single bytes array. We need to hash this to generate the storage key. Wrap that whole thing in a catch act 256 hash function. And then one final step here because catch act 256 returns a bytes 32 and we need this to return bytes memory. We have to wrap this in one more abi. Encode. And that should be all that we need to derive the storage Location for the l1 storage key. Now we can close these and use This function to overwrite the storage key that gets passed Into this validation step. So this will be proof.dst L2 state root proof params dot storage key is equal to derive L1 storage key where we pass in the two input parameters of request and proof. Okay, sweet. So at this point we now have a verified value in the rollup contract for the destination chain on L1. We confirmed that it is in the correct location so we can trust that it's the proper state representation. And then we can move on from there. So the next step here is to verifiably link the destination chain state route to the destination chain state representation. If the destination chain is directly posting their state route, this is unnecessary. We just need to make sure that we have a verified state route here. So in this example, because we're not directly posting that, it's a hash of block timestamp and state root. We need to recreate what this output root should be. So we can re-derive that with bytes 32 for a local variable. This will be derived L2 output Root is equal to cat check 256. And what gets passed in here? We're basically just recreating this line over here in the roll up contract. So this is going to be an ABI.encode pact. And inside of the ABI.encode pact, we want to pass in the block time stamp and a state route. But at this point, we don't have a state route to use. So that would be the perfect, now is the perfect time to add that as the next field in our RIP 7755 proof struct. We would expect the fulfiller in this case to supply what the destination chain's state root is, and then we can use that to re-derive the state representation that was verified using the first storage proof, and if they're equivalent, then we can trust the passed in L2 state root. So this will be a bytes 32. L2 state root. And we need to pass these Arguments in in the same order they're passed in over here. So we'd start with the L2 block time stamp. This will be proof.L2 block time stamp. And then we want to pass in the state root. So proof.L2 state root. Okay. So now at this point we have a re-derived output root for the destination chain. In the case that this doesn't equal the value we just verified, we'll create another custom error for that to revert in that case. So we can call that an error invalid L2 state root. Copy that. And then we need to compare this to the storage value that we verified in this step. So that'll be if derived L2 output root does not equal proof.dst L2 state root proof params.storage value. Then we revert with that custom error. And this is yelling at me because the storage value is a bytes string and this is a bytes 32. Because we would be expecting them to be equivalent we can wrap this in a bytes 32 for the type safe properties of solidity. One other thing I'm noticing here, this is kind of just like a personal preference for me, but the state validator library up here, because we're using it on a specific account, for solidity purposes, we can actually bind that library to the account to just improve the legibility of this line, make it a little bit more succinct. So I'm going to do that, but that's totally just a personal preference. So we would add a line at the top of the prover contract that says using state validator for address. And then what that allows us to do is to copy this request.L2 oracle and remove it from the function call and then paste it here instead of state validator. And then this is doing the same thing, but it's just a little bit shorter. OK. So at this point, we have a verified destination chain state route. Now we can use that to basically redo step three, but now the account we care about is the inbox contract on the destination chain. So what that's going to look like is a bool is valid L2 state is equal to, and then to jog your memory again on the structure of a request, we actually have the inbox contract being defined by the user when they submit the request. So this address is going to be the address that we're verifying state against. This will be request.inboxcontract. Verify, what's it called again? Validate account storage. And in here we need to pass in a state root and another instance of the proof params. So the state root here is passed in from the proof struct and at this point we've verified that this can be trusted. So we'll use that value. So this will be proof.L2 state root. And then we need to add another instance of those proof params This time for the inbox contract on the destination chain. So we can duplicate this line and instead of calling it dst L2 state proof params we can call it dst l2 account proof Params. And copy that field name down Here for the second storage proof. We can pass that in as the second argument. So this looks like proof dot that destination account proof params field. And then for the case again where is valid L2 state comes back false, we need a custom error to throw as a reversion in that case. So we can define that up here. Error invalid L2 state. Copy that. And this will be so if not is valid L2 state, revert with that custom error. Cool. And then so if you remember from the first storage proof, we had an issue trusting the storage key that gets passed in from the fulfiller. You can assume that we have the same issue in the second storage proof, and we do. Luckily, it's a little bit easier to solve on this side of things because, like I said at the beginning of the walkthrough here, the outbox contract rederives where that storage key location should be already, and that gets passed in. So we can just reassign the storage key value with this passed in inbox contract storage key. We'll do that above step five. So this will be proof.dstl2 account proof params this time dot storage key is equal to inbox contract storage key. And what that's going to do is ensure that we're verifying against the correct storage location where the execution receipt should exist on the inbox contract on the destination chain. So at this point, we have a fully verifiable proof. The last step is we just have to confirm that finality delay seconds is not still in progress. We have the receipt being passed in here, so we can use that for this check. And for when it is still in progress, we can add a custom error at the top. This will be our last error. So this will be error, finality, delay seconds in progress. Copy that. And then down here at the bottom of this function, if fulfillment info dot timestamp plus request dot finality delay seconds is greater than, if you'll remember for the proof, we defined the L2 block timestamp up here. This is exactly what we need to use for this timestamp-based protection. So we can do proof.L2 block timestamp. If the timestamp at which it was submitted plus the configured finality delay seconds is greater than the time that we're using for this proof, then we can't accept this proof because finality delay seconds is still in progress. So that's what this line is doing. So we revert with that new custom error, finality delay seconds in progress. And then cool. That's all of our steps. But there is one final piece of data connection that we're forgetting here. And that is stemming because of, well, we confirmed a specific storage location in the inbox contract on the destination chain. And then we confirmed a passed in receipt satisfies that finality delay seconds requirement, but we did not confirm that that receipt that we're using for this final check is the same value as the one that we just verified. So we need to make sure that the storage value for the second storage proof is equal to the encoded execution receipt that we're using for this timestamp check in the last step. And so that represents the final piece that we still have to set up to secure this thing. We can set up the encoding of the fulfillment InfoStruct as a separate private helper function. So this will be, this will be, what do we want to call that? Encode. Fulfillment info. Okay. So for encoding fulfillment info, we need to pass in the struct that we're using for the validation up here. And inside of this, it's a simple ABI.encode packed. So return ABI.encode packed. With fulfillment info.filler and then fulfill info.timestamp passed in. This is because of the struct packing rules and solidity storage we have to custom encode this struct here. If I show you the definition of the struct in the inbox contract again, we see that it has two fields, timestamp and filler. The timestamp is a Uint 96 and the filler is an address, so this can get packed into a single Uint256 slot. But the way that it gets packed or the ordering that it gets packed is in the order of the defined fields, and it packs them into the lowest value bits first. So the timestamp actually ends up on the right side of the storage slot, and then the filler is on the left. So in order to recreate that alignment, we have to use an ABI.encode pact here where we pass in the fields in reverse order instead of just doing ABI.encode and passing in the entire struct. We would have it in reverse order in that case. So using this function now, we can override the storage value for this final storage proof. So this will be proof.dstl2accountproofparams.storagevalue is equal to encode fulfillment info. And we pass in fulfillment info. Excellent. So that should be our first pass at a full implementation for validating one of these nested storage proofs. We can now see if this is compiling. So if we CD into the contracts directory, can run a forge, well I like to format it properly first. And then we can do a forge build to check if it's compiling. It is. So now to check this, I have a test file in here with mock data from a working system. It's commented out just to prevent compiler issues with the initial structure of the project. We can uncomment this and run a forge test to make sure we did implement that properly. It will recompile this test file and then okay, cool, it's passing so this is using a previously generated proof um that adheres to the structure that we just set up in that rip 7755 proof struct and um it uses is being proven against a specific storage route or a specific state route that is being assigned here and this commit beacon route function in the beacon Oracle contract. So because this is passing, it's a good sign that our implementation is working properly. So that wraps up the on-chain implementation piece. At this point, we should be good to take a step into the off-chain world and look at the integration for this NFT minting site. So I'll close the contracts directory for now, although we'll be back here shortly to reference different function signatures. Inside of the services directory, we care about this demo directory. So if we take a look in here, a brief walkthrough, it's literally just like a back end server script to run. It's going to just tell you what it's doing. It will display your current NFT balance for the NFT contract that's going to get deployed. We're calling it deployed on a mock arbitrum chain and then the user is going to be minting it from mock base. So it will be displaying your NFT balance on mock arbitrum, your current ETH balance on mock base, what the price is of the NFT, which I think I have hard coded as one ETH in the deployment script. This is using local anvil nodes so using easy numbers is pretty easy. And then lastly, it'll prompt you for like once you hit enter, it'll trigger minting the NFT, and then we can watch the fulfiller in action generating its proof after the fact, which gets verified against that prover contract we just set up. There's not too much for us to change here. This is just, I just wanted to give you a rundown of what exactly it is that's happening. What we care about is inside of the SRC directory, there's a client file called clients.service. This is all in TypeScript, if anyone's familiar. There's a function called rip7755mint. That is completely empty. So this is what the developer needs here. We have to set this up to be an RIP 7755 request to be able to integrate with the cross-chain call and close the loop for this minting process. So we can start here by setting up what this request is going to look like. To do that, it would help to have the structs file up here as a reference. I'll open that up. Because actually before we dive into that, as a refresher in the outbox contract, the function that we care about as the entry point is this request cross-chain call, which accepts one input argument and it's a cross chain request. So really what we need to build here is a cross chain request with all the correct fields. So that's what I want to show you guys. So to start with this implementation, we can get an outline of what this request is going to look like as well as the low-level call that we need it to do to facilitate. So we can start with const calls. It's a batch, but it's just one call, but it still needs to be set up as an array because it's meant to be able to support a batch of calls. And then we'll have a const request, which will be an empty object. We can start by just getting the field names in here. I'm just going to assign them all as empty values to start, just to motor through this. And then we can go through each one individually to make sure that we're setting it up properly and explain it all as we go. So we need to add prover contracts. And again, I'm just copying everything to try to prevent some silly typo mistake. So just bear with me for a second. Pass in the inbox contract. L2 Oracle. L2 Oracle storage key, reward asset, reward amount, initialize at zero, finality delay seconds, initialize at zero, come back through this in a second, nonce is zero, expiry zero, and then the final two pre-check related fields, pre-check contract and pre-checked fields, pre-checked contract and pre-checked data. And then for the individual call that we want to set up here, this just takes in three parameters. There's two, data and value. So we can take two, empty string,, and then value. Okay, so we have our structure outlined here. Now we need to make sure we set up each of these fields properly, and it helps to have an understanding of exactly what each field is, which we now all should. In order to help with this, I have inside of this common directory, there's a constants file that has a couple of chain configs in here, one for mock arbitrum and one for mock base. So this is going to be very helpful for where we can pull the addresses from. Assuming everyone deploys the contracts using the deployment scripts that are provided, these addresses are, I mean, they're deterministic, so if they're deployed in the right order, then these addresses should all be correct, which is why they're pre-populated in this file, even though the contracts are not currently deployed on our local network. So, okay, let's start filling these out. For calls, the destination address here, again, what exactly is it that we're trying to do? We want to mint an NFT. So the to address is going to be the NFT address, which we have up here as mock arbitrum NFT address. So at the top of this client service file, we can import that. So let's see. Import mock arbitrum NFT address from dot dot slash common slash constants. And then we can copy that into our to field. Data is going to be the encoded call data for the mint function. I'm going to leave that for the last step. We'll come back to that in a second. Value, as you see here, this function is being called with an address. This is going to be the user address receiving the NFT. And then the mint price, which is the mint price of the NFT denominated in way. So we can just pass in this mint price as the value directly. Yeah, so this will be mint price. And then now as we start to fill out the request, the requester is pretty self-explanatory. Like I said, the address of the requester is being passed in, so we can use that. Calls is going to be this calls array. Prover contract is what we just implemented. This will be the contract address on the source chain, which in this context is mock base. Of the prover contract that has been implemented specifically to validate request or validate state about the destination chain which in this context is mock arbitrum. If we look over here, there's only one defined in each but this is set up in a way that we can define multiple prover contracts for each chain. This is pretty straight forward. We'll start with, well, first we have to import the chain configs here. So if we go back up to the top of the file, we can import that from the same file that we just imported the nft address. And then down here in our mint function, we can destructure the mock arbitrum and mock base configs from that outer object that's containing the chain configs. So this would look like const curly braces with mock arbitrum and mock base is equal to chain configs. And then now we can access all of these config fields directly on mock arbitrum and mock base. So for the prover contract, because this is the base side of the equation here, this will be mock base dot prover contracts dot prover. The destination chain ID is going to be mock arbitrum's chain ID, so we can go mock arbitrum's chain ID. So we can go mock arbitrum.chain ID. Inbox contract is going to be the address of the RIP 7755 inbox contract on mock arbitrum. So this will be mock arbitrum.contracts.inbox. L2 Oracle is the roll-up contract on L1 for mock arbitrum here. So this will be mock arbitrum.L2 Oracle is the rollup contract on L1 for mock arbitrum here. So this will be mock arbitrum.L2 Oracle. Same thing with the storage key or the L2 Oracle storage key. This is for the L2 Oracle contract for mock arbitrum. So we'll grab that from the same place. It will be mock arbitrum.l2 oracle storage key. And then for a reward asset, for this example, we're just going to use native currency because we're sending native currency for the mint function, but it doesn't necessarily have to be one-to-one like that. We could be passing in some ERC20 and then expect the fulfillers to do the necessary conversions off-chain to make sure that the ERC20 is enough value to account for the native currency that's being passed in here, but it's a little bit cleaner for the demo here to just keep it one-to-one like that. So that's what we're going to do. The ERC that I mentioned earlier that had a specific asset that represents native currency is hard-coded here. So that's what this like OXE address is. And this is another exported constant from this constants file. So we can copy that and add it to the import statement. And then pass that in for reward address. Reward amount, again, to remind you, is meant to cover all of the value from the calls plus whatever the destination chain gas cost is plus a tip for the fulfiller. The exact mechanism to calculate what that surplus should be can be pretty complicated. But for the sake of today's demo, doing it in a simplified context, if we add an extra 2% to the request, that should be more than enough for it to be profitable for the fulfiller in our local network. So we'll do it that way. So this will be reward amount is going to be the mint price, which is already in way, to remind you. We'll add a 2% buffer. So we can do that with big ints in TypeScript by multiplying it by something like 102n for a big int and then dividing it by 100. This results in the mint price having 2% added to it. All right. So then for finality delay seconds, in a live network, this will likely be something on the order of days to a week to ensure protection against reorgs on the destination chain. Because this is a self-contained system that is going to run locally on your machine, we have the flexibility and the freedom to make this really short. And because I want this to be a responsive demo and for us to see everything happening in real time, we'll make it just 10 seconds. So what this is going to result in is like a 10 to 15 second delay after the request is submitted before we'll see the fulfiller actually generate the proof and claim the reward. NONs doesn't matter because that's going to get overwritten in the outbox contract. It's just like a canonically incrementing NONs for every request. Expiry doesn't really matter too much for this demo. If you see over here in the constants file, we have a one week constant. We can just add one week to the current block timestamp. We just need like whatever this value is, it has to be greater than the finality delay seconds in the future from like now, plus some extra cancellation buffer period, which is hard-coded as a constant in the outbox contract storage as a full day. So for the demo, just to get a valid value in here, one week should be more than enough. So this will be date.now. So in JavaScript, if you're not familiar, it has a global date object that if you do date.now, will return the Unix time stamp but in milliseconds and in solidity it's in seconds so we have to divide that by a thousand but solidity doesn't like decimals so then we have to floor that to the nearest integer and then from here we can add the one week constant so we'll add that as another import from this constant file and then use it down here for the expiry time stamp. We'll add one week. Okay. And then the last two fields are this pre-check contract and pre-check data. Which like I said earlier is an optional like fulfillment condition that the requester wants to be true. Because it's optional we're not going to be using it today. But yeah, so in order to not use it we have to pass in the zero address and there's a helpful constant from the web pre library I'm using theme that is just called the zero address. So we can import that at the top and then use that as the pre-check contract address in here. And then last but not least, pre-check data. In order for it to pass the off-chain validation that Veeam does, we just have to add an OX. But this can be anything. It just has to be some kind of arbitrary byte string. Because we're not using this step, it doesn't really matter. Okay. So that just about covers all of the requests. The only thing we haven't done yet is encoded the call data for the mint function on the nft contract. So to reference the nft contract, let me pull that up. It's just a simple mint function that takes one input argument, which is the two addresses that should be receiving the nft. There's another helpful function from Veeam called encode function data. That we can use for encoding the call data here. We can define this as another local variable above the calls constant. So this will be const encoded call data is equal to encode function data. And this takes in, actually before we define that, we'll just add this in as the value for data, encoded call data. And then now for encode function data, this accepts one input parameter, which is an object with a couple of fields that are necessary here. The first one being the ABI for this NFT contract, which is actually already being imported as NFT ABI for some of the other helper functions that are defined in this class. So we can just use that. So this will be ABI colon NFT ABI. I don't know why I closed that contract. We still need it. Next up, we need to define the function name that we're encoding data for. So that's just simply mint. So if we copy that. This field name is function name. With mint passed in. And then because mint accepts one input parameter, we now have to specify that with a field name called args. So this will be args, which is an array of the input parameters. And in this context, it's just the two address, which is passed in here as address. And that should be it for encoding call data. So at this point, we have a fully set up RIP 7755 request. Now we need to set up the function call to submit the request. So what does that look like? To give you a little bit more context on how this class is set up, we have something called a wallet client as a local variable here. And if you're unfamiliar with Veeam as a Web3 library, it uses things called public clients and wallet clients. The public client is for reading state from the chain, and wallet client is for writing state to the chain. This wallet client is already defined in the constructor, so we can just use it as is for the target chain that we are submitting the request to, which is mock base here. So this is going to be an asynchronous call, so we'll start it with a wait. This.walletClient. And the function name that we care about here is writeContract. So under the hood, this, this creates your, um, your, your, your transaction and signs it and sends it to the network. This will return a transaction hash if it's successful. So we can store that as hash. And then we need to set up the configuration for like what contract are writing to, what function we're writing to, what parameters are needed. So that can be a single object in here where we define for reference I'm going to pull up the constants file again. We need to define the address that we're sending the transaction to. So address is going to be coming from mock base because we're submitting the request to mock base. The contract we care about is the outbox contract for the standard. So this will be mock base.contracts.outbox. Next we need to specify the ABI for the outbox. So this can be, we're going to have to import this at the top. So this will be import outbox ABI from dot dot slash ABI slash outbox. We already have that populated in this directory. So we define that as the ABI here. We then need to define the function name that we're requesting or sending the transaction to. So if we pull up the RIP 7755 outbox contract over here, the entry point, as I said earlier, for the standard is this request cross chain call function. So we can copy that into function name. We then need to define the input args. So as you see here, it's just one argument, which is the cross chain request. This is going to be the request that we just defined here. And then last but not least, we need to define any value that should be submitted with this transaction. So that's going to be a field called value. And in this context, it's not actually going to be mint price because if you remember, we added a 2% buffer to the mint price as the reward amount for this cross-chain call. So the value here should be request. setting up the contract send. The last piece here is using a public client from Veeam, we can wait for a transaction receipt to make sure that the request is confirmed or the transaction is confirmed. So we can do that with another await call. In the constants file for the chain configurations, one of the fields for mock base is a public client, so we can use that directly. So this will be await mock base dot public client dot the function name that we're going to use here is called wait for transaction receipt. And that takes in just one input object, which is the transaction hash. And in JavaScript, if the field name and the value name that you're setting it as are the same name, you can omit the colon and the value. So like this is the same thing as this. So I'll leave it like this for clarity. And if we get through this wait for transaction receipt call, then the transaction should be successful. So then we'll log something, console.log, transaction success. Okay. So that should be our full request. It's pretty straightforward. I just wanted to kind of give you a walk through on all the different field names and how they should be assigned and what the values mean and how they're fitting together. This should be enough in place to run the system end to end. So if we take a look in the root of the directory, the root readme here, there's a list of commands for spinning up all of these, all the infra here to deploy the contracts on top of and all of the services that are needed to get everything to work together. So we can start by running these commands. The first thing we need to do is spin up our local chains. So if we open new terminals that are each at the root of the project, we can open three in split screen side by side. And this is going to be meant to represent the mock L1 and then the two mock L2 chains. So we can start that with, there's a make file defining all these commands. If you want to see what they're actually doing under the hood, we can run make start mock L1 in the first terminal. So this is like our mock L1 chain. We can then run make start mock base. So we have our mock base chain in the middle. And then lastly, we can do make start mock arbitrum to run our mock arbitrum chain in the terminal on the right here. So now with these three chains running, we have our local chains running, so we can deploy our contracts on top of them. There's another make file command set up for deploying the contracts in the correct order that is required by the off-chain services here. So we can open a new terminal. The three block chains are still running in the background here. I just have a new terminal running on top of them. We can run make setup contracts, and that's going to compile and deploy all of the contracts to the local network. So that will just take a second. While that's going, we can copy the make start sinker command. So what this is going to do is start the off chain service for sharing state representations bidirectionally between the mock L1 and the mock L2s. And this one's pretty chatty. So we can now open a new terminal for the final two terminals that are needed here. It's a ton of terminals, I know. We need to start the off chain fulfiller to listen to in response to requests. So that's makechain fulfiller to listen to in response to requests. So that's make start fulfiller. So now we have the fulfiller listening. We can do a split screen here to now run the demo. So to run the demo app and mint the NFT, we would just run make demo. So if we type that in. Not too pretty, but logs to the console, what it's doing here, current state. So we have welcome to the demo, mint your NFT, the devcon NFT on mock arbitrum. We currently have zero in the wallet address that's being used for the demo here. The price is one ETH and the current base balance is almost 10,000 ETH because this is one of the default accounts on the local Anvil nodes. So now if we press enter, this is going to send that mint request to the local network and we can see if everything worked successfully. And it did. So the transaction went through. We actually, if we run this again, we already have the current NFT on the destination chain because the filler, if you saw over here, picked up on the request almost right away and submitted it because it validated that the incentive was sufficient. And then now in a second, we should see something else happen because the finality delay of 10 seconds has gone past. And what the fulfiller just did is it picked up on the fact that it waited long enough and it's now allowed to claim the reward for that request. And it generated this massive storage proof here and submitted that storage proof to the outbox contract on the mock base chain and everything was successful. So this was validated against the prover contract that we just implemented. And so if you take a look in the Fulfiller directory here, after that ran, it logged the proof into a JSON file. So you can see exactly what the proof was that it used to verify that the call was submitted. So that's what this file is. And then inside of the SRC directory, there's a database directory that is storing db.json, which this is the representation of the request that it picked up on that we just submitted. And then it has a rewards file that it's tracking how much ETH that it's claimed in rewards. So we have a locally running, like fully working system end to end. Woo. Woo. Yeah. And I'm realizing right now that the reward tracking is not accounting for gas cost on the destination chain, but nonetheless, you get the idea. So that just about does it. If you take a look here, yeah, like I said, if we run the demo again, we now see that the current NFT balance is one because the NFT was actually minted on the destination chain as defined by the encoded call data that we set up for the target calls. And then so if we run it again, now it would be just incrementing from there. So that does it for the demo. Thank you, everyone, for coming. And I'll be hanging around for a little bit if anyone has Questions or if you want to chat the standard a little bit more. Like I said, we have an open source repo for proof of concept Here, so if anyone feels compelled to contribute, we Fully invite you to contribute. So thank you. Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:30:00.000Z",
      "slot_end": "2024-11-15T06:30:00.000Z",
      "slot_roomId": "classroom-e",
      "resources_presentation": "https://docs.google.com/presentation/d/1R-pN3is6_qjmy7k7gl3hHECFG1O_ZDuH33K5B6JQmGc",
      "resources_slides": "https://drive.google.com/file/d/1X8YPz2wz4f6cXPeTKfzi0PXsw2DkxHd4/view",
      "slot_room": {
        "id": "classroom-e",
        "name": "Classroom E",
        "description": "Classroom (Round Tables)",
        "info": "1",
        "capacity": 150,
        "youtubeStreamUrl_1": "https://youtube.com/embed/d060E6Hre5Q",
        "youtubeStreamUrl_2": "https://youtube.com/embed/R9Y7Ih_tS9w",
        "youtubeStreamUrl_3": "https://youtube.com/embed/bI6U17Km9HU",
        "youtubeStreamUrl_4": "https://youtube.com/embed/eTvKRYVk6p4",
        "translationUrl": "https://stm.live/Classroom-E"
      }
    },
    {
      "id": "the-age-of-aggregation",
      "sourceId": "VVTWM7",
      "title": "The Age Of AGGREGATION",
      "description": "Aggregation plays a critical role in enhancing the usability and scalability of blockchain technology. In this session, we will explore the fundamental concepts of aggregation, debunk common myths, and discuss the necessity of aggregated blockchain systems for achieving real-world usage. Current scalability boundaries limit blockchain's potential, but through aggregation, we can optimize performance and usability, making blockchain technology accessible to a broader audience",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Product",
      "featured": false,
      "doNotRecord": false,
      "tags": "Protocol Design,Scalability,Token bridging,User Experience,Protocol Design,Token bridging,User Experience",
      "keywords": "Blockchain optimization,performance enhancement,scalability",
      "duration": 1566,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "8zTdw5WjBmI",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736e8da1b0f83434d32eac5.vtt",
      "transcript_text": " of Polygon Labs. For those of you who don't know Polygon Labs, or specifically who don't know what Polygon Labs does, because we do many things, we are core contributor to the AgLayer along with many others. We've also developed multiple blockchains being Polygon POS that pretty much everyone knows, as well as Polygon ZKVM. So I'm going to go through a little bit of history before actually attacking what exactly we mean by aggregation, and specifically what I want to look at. And people talk a lot about not looking at analogies, but if we think of what we want Web 3 to be, we really want it to eat as much of Web 2 up as possible. We should be able to scale Web 3 to the size of Web 2. And so the question then becomes, well, how did the Internet become the Internet? And if you think about it, the Internet actually started very fragmented. It was very disconnected with multiple networks, unable to communicate. And then new protocols came along. And specifically, things like TCP, IP came along. And what you suddenly had was this diversity in what could be built on the internet and the infrastructure underlying it, but with some common protocols. And specifically what you get there is TCP IP. And it's very important to actually understand this because a lot of people say, like, how are we going to bring together all of Web 3 when you've got so much disparate technology in Web3? You've got different stacks, you've got different VMs, like, everything looks different. But we actually have an example of bringing together different technology in a way that, like, unifies it and actually feels like one. So let's talk about, like, specific problems we see in Web3 and why this is actually very relevant. We've got fragmentation as an issue. I think this is an obvious one for everyone. Scalability, security, and frankly, just poor UX. If we look at each of these one by one, what do we have right now? We've got a bunch of chains, and liquidity is fragmented across all of these chains. We really have no good way of bringing them together at all. Similarly, we've got a scalability problem. If we want Web3 to scale to the size of the internet, then we should probably compare to what currently is using the internet. And when we look at that, what we see things is MasterCard, Visa, Nasdaq. And I do not believe we actually have a network that can handle the load of any of those. I think we've heard many state that they want blockchains to be able to handle, you know, a load like Nasdaq. And that is 250,000 transactions per second, which no network is even close to being able to handle in any way whatsoever. I actually just as a little side note, I'll give you a few side notes during this presentation, but this is a little interesting fact is let's just assume that we could actually scale a blockchain to the size of Nasdaq. Nasdaq can execute transactions with a latency of 50 microseconds. You are not going to execute any transaction on a blockchain in 50 microseconds. So I think our dreams of bringing Nasdaq on-chain should probably recognize the fact that there's some things that are actually better in a centralized state, and there's some things that are better in a decentralized state. Obviously, this does not mean that we don't want decentralized exchanges. It means that there's specific technologies that serve specific purposes, and we should probably focus on more long-tail assets in a decentralized way when we want perfect performance execution. And then when we just want censorship resistance, then we should probably bring that on chain. Third one is security. Right now, when you look at many different networks and how they come together, we talk about concepts like shared sequencers and intents and bridges, and most of that is not actually very secure, even though it actually works decently in some cases. And lastly, the UX challenge. We've got a UX problem, whether it's across wallets, bridges and intents, chains. Basically, what you get is this fragmented feeling across the board. And so then the question becomes, what do you do about this? And the first answer that we tried to give, or many do still give is let's just use a monolithic chain Solana can solve this for you because something something something well, you know most of the time that something isn't going to be good enough because Literally we have seen that we can't even scale to some of the the smallest Networks that are good for payments in the world or the biggest ones. And so the idea that we're going to throw everything on one chain is not happening. And so we need to go look at another solution. The other solution was, why don't we just scale through modular chains, right? But the problem is modular chains increase fragmentation. We see it today. You've got, you know, Polygon ZKVM, Polygon POS. You've got ZK Sync Era. You've got, you know, Arbitrum 1. You've got OpenMainnet. You could keep going on and on. And, like, great, this is nice. Like, being able to choose how you want to spin up a chain is great. It actually solves the scalability issue, right? Because now when you need more block space, what do you do? You spin up a new chain. And with time, this is actually going to get easier and easier, right? Like right now, people want to sell you on the idea that spinning up a new chain is easy. Like frankly, it's actually not that easy. But if you fast forward 12 months from now and 24 months from now and 36 months from now, you're going to get to a point where spinning up a new chain is actually incredibly easy. And when you need to increase throughput and increase block space, then you're going to spin up a new chain to actually be able to do that. And so through kind of the modular approach, we can actually solve scalability. The problem is we still don't have one unified Web3 if we do that. And so the answer to this is just aggregate all the chains. And so what does aggregation actually allow for? It actually allows us to scale Web3 to the size of the internet. What are the things that we need to do this? First, we need practically infinite scalability. And that's what you get when you can spin up new chains very easily when more demand is needed. Because there's... When there is more demand, and so more block space is needed. And then the second one is you need to be able to unify state liquidity users. And if you really think of the North Star of what we should be targeting, we should really want to be able to have a Web3 and an Ethereum that scales as much as you need in a unified way, making you believe that you are using one chain while you are using 100,000 different chains at any point in time, and that that 100,000 chains can become 125,000 chains at any point in time, and it still feels like one internet. So when we've thought about this problem and how we want to solve this problem exactly, we started thinking about it and we realized that we needed to create a neutral platform. And so, this is the concept of the Ag layer. Polygon Labs is a core contributor, along with many others that I will show you a little bit later. But the general idea was that we need some kind of neutral cross-chain settlement platform that unifies liquidity users in state, and that ultimately has finality on Ethereum. Now, very importantly, the AgLayer is actually not intended to scale only Ethereum. And there's a reason for this, which is that as amazing as Ethereum is, what do people actually value in Web3? They value assets. And there's always going to be assets outside of Ethereum. And we need to be able to get people to come to Ethereum and stay on Ethereum. And the only way to do that is to allow them to use or to receive or send assets to another chain that is not Ethereum. And the only way to do that is to aggregate those chains along with Ethereum and all chains on them. And so that is a big part of kind of the AgL layer is really bringing everything together and ultimately having finality on Ethereum. Maybe let me talk to you about kind of like the different components of the Ag layer. Really think about it in like four different parts. A lot of times when people hear the Ag layer or anything that has to do with interoperability, they immediately default to it's a bridge. And it's nothing but a bridge. And this is simply because when we think of the forms of interoperability that we have had to go cross-chain over the last many years, we default to bridges. Layer zeros, the wormholes, things like that. And then more recently, people start thinking of cross-chain transactions as being things driven by intents. But people don't really think holistically about everything that you need to get safety when there's a fragmented Web3 that you want to bring together. So let me talk through these four different components of the ag layer. First one is the pessimistic proof. So the most important thing, if you want to bring chains together, is to ensure that no one chain can rug another chain. Right? Like, if I have gone to a chain, I have decided this is a chain that I trust. Or like many, frankly, haven't paid attention, but I'm still assuming trust on that chain. And when I send a transaction cross-chain or I'm receiving an asset from another chain, I need to be 100% sure that the only thing I am trusting is that chain that I am on. And this is basically what the pessimistic proof does. What it does is it looks at all assets that come into a chain. It looks at all assets that are leaving a chain. And we call this chain level accounting. And basically what the pessimistic proof does, it then ensures that when an asset wants to leave a chain, it is not an asset that is in excess of any assets that have come into it. So if you've had, you know had 10 ETH come into a chain, then you can't have more than 10 ETH leave that chain. And this is what the pessimistic proof enforces. Now, the interesting thing about this pessimistic proof is that it is not just enforcing this for chains that have proofs. So a lot of times people think, okay, the ag layer or anything that Polygon is working on requires ZK technology. So you look at Polygon ZK VM or X layer and you're like, hey, this uses execution proofs. And so the ag layer requires execution proofs. The reality is the ag layer is actually completely agnostic to that. You could have execution proofs, you could have fraud proofs, you can have some form of consensus that can be proven, or you can straight up have a database. Pessimistic proof doesn't care. It's going to look at what assets came in, what assets have left, and whether an asset that wants to leave or be removed from there can. And so that's kind of like the guarantee you get with pessimistic proof, is that regardless of what it chain, like the form of a chain, you can prove facts about that chain. And specifically on something like a Polygon POS that has a validator set, you can prove the consensus of Polygon POS so that you can confidently actually have transactions go cross-chain. Next one is proof aggregation. A lot of people think the proof aggregation is what allows the interoperability in the AgLayer. All proof aggregation does is it lowers the cost of transacting on chains connected to the AgLayer and using the AgLayer. Specifically what it does is you get a ZK proof, and this could be the pessimistic proof or it could be an execution proof on a chain. And what it does is it wraps those proofs together and then wraps those proofs together and then wraps those proofs together and then submits them to Ethereum. If you were to submit each of those proofs to Ethereum on their own, it would be incredibly expensive. And so this is how you're able to lower the cost so that it is incredibly cheap to execute these cross-chain transactions. Next thing is there's a unified bridge. This is a bridge on Ethereum, and that bridge is the bridge used to create canonical assets. That allows for all transactions going across the ag layer to be done in fungible assets, meaning that you don't have wrapped assets and then unwrapped assets. And then lastly, there's this concept of fast interrupt. The ag layer with nothing else going between two chains, especially two L2s, is going to settle at the speed of Ethereum settlement, and so we're going to be looking at 15-minute cross-chain transactions, which isn't good enough. And so the goal with fast interrupt is to lower that latency to just a few seconds rather than minutes So like what are the benefits of this? First of all is you get these native tokens Look, I think cosmos has done many correct things. But when you go when you use cosmos the experience of having many, many unwrapped tokens and moving those around and figuring out what these assets are, and the same has become true within the Ethereum ecosystem as well, that is not a good user experience. Second is we need safe cross-chain transactions. As of right now, when we use bridges that we use, most of the time these are not actually very safe for interoperability, and we actually need something to add safety. Sometimes we talk about shared sequencers as providing atomic composability. Again, you need something to actually make that safe and trustless. And importantly, this is where you get the concept of compatibility. The ag layer often is viewed as competitive with all of these different options that people hear about for interoperability purposes. But it actually really isn't. It's a very low-level base layer that is compatible with most of these. So if you want to do an asynchronous transaction, you use the ag layer. But if you want a synchronous transaction, then you would actually use a shared sequencer with the added security of the AgLayer. If you want to use intents, those are expensive. They require capital in two different pools. You need to rebalance that capital. Rebalancing that capital has risk and time associated with it. You can lower that risk and time, allowing for rebalancing much quicker, allowing for users to get transactions at a cheaper cost. And so good experience there. And so the goal when we're thinking about building the AgLayer with the other contributors is really like how do we make this the best experience possible? And that includes from a cost perspective when working with other solutions out there. Next, like the AgLayer, if you think about it, is really like an asset-first protocol. You're actually passing assets from one chain to another. And this is different from other interoperability solutions that are generally passing messages. But the AgLayer does allow for passing messages as well, which is necessary and something that can be done. Next is it actually enables the concept of, like, chain abstraction. I think chain abstraction is a very nebulous concept that if you ask 10 people what is chain abstraction, 10 people will give you different answers. The way that I often think about it is like the Ag layer allows for easy use of chain abstraction. And like one example of this is a library that we've built that we refer to as Bridge and Call. And so this is a library that allows for users to basically execute one transaction themselves, but they're actually executing multiple transactions across chains. So imagine that you wanted to bridge funds from Ethereum to Polygon, ZKVM, and then you wanted to swap those assets into different assets and then transfer them over to X layer and then buy an NFT. With the bridge and call function, you can do that. And so you can imagine a wallet that abstracts that all the way and basically says, hey, do you want to do this bridge transfer and then this swap and then transfer it and then buy this NFT, click one button and it swap, and then transfer it, and then buy this NFT, click one button, and it's calling multiple functions in the background, and you've abstracted away the entire kind of cross-chain experience. And then lastly, it provides for this low interoperability, low latency in terms of these cross-chain transactions. Again, it doesn't provide synchronous interoperability. That's something that by working with shared sequencers secured by the AgLayer, you can receive, but it's not something that the AgLayer gives natively. This is the AgLayer ecosystem, multiple contributors. We're always looking to work with more folks on this. One thing you'll notice is the AgLayer is not called the Polygon AgLayer. This is very intentional. We've talked to every big team in the Ethereum ecosystem and most outside of it. The goal is to get everybody to contribute so that we can have a non-fragmented Web3 as a whole, allowing for Ethereum to grow and be used without actually needing to leave Ethereum. A very important point is that AgLayer is not rent-seeking. If you look at a lot of different interoperability solutions and a lot of different other ecosystems building out interoperability, what you end up seeing is some form of fee that is placed on every transaction. And it really looks like a middleman form of rent seeking. The AgLayer has no fee for joining the AgLayer. There's actually no fee per transaction on the AgLayer. Instead, what it is is that chains are going to create with as many transactions as they can include in their consensus or that's part of the consensus being satisfied or as part of an execution proof, and they're going to submit that to the AgLayer. They could choose to submit that once a month. They could choose to submit that every two seconds. They get to choose how often they actually want to submit a proof to the AgLayer. And that allows for chains to continue to remain sovereign. For example, you could think of like a gaming chain. Gaming chain is going to say, hey, our users don't actually care how often we finalize this chain, or they're going to accept lower security requirements for some period of time. And so we're going to submit proofs much less frequently than like a DeFi chain that's going to say, hey, I want to submit proofs every two seconds. And so by having this flexibility in the ag layer, it allows for people to be able to execute these transactions basically freely. And so basically what we see here is that we've solved fragmented liquidity. You've got fungible assets across the ecosystem. You've solved scalability because you have chains that can be spun up at any point in time. You've solved security with the pessimistic proofs that you can safely go between chains. And you've solved the UX issue because you can now seamlessly interoperate between these chains with some of the greatest technologies around shared sequencing and intents that are going to be part of this. And what you end up with is really a unified kind of Web3 allowing for users to stay in Ethereum, benefit from assets across every ecosystem. And what you see is a Web3 that ends up actually being united under some common protocol. Thank you. Thank you, Mark. That was quite insightful. We do have a few questions here. So who will aggregate the aggregators? That's a good question. I don't know. We see this all the time, of course. I have not been able to have anyone point to me to another team that is actually trying to aggregate all of Web3. And so there is nothing that prevents the Ag layer from being connected to a chain in the super chain. You will actually see that happen. There is nothing that prevents the Ag layer from being connected to a chain on the elastic chain. There is nothing that prevents it from connecting to an orbit chain. And so given that it's a low- level solution, you can call it the aggregator of aggregators. And so that's what the Agilera provides. All right. And what is the difference with CCIP? Yeah, that's a good question. So I would think of CCIP more on the messaging side of things than on the asset transfer side of things. And so specifically, like when you think about the bridges that I was referring to, that's kind of what currently exists is CCIP. One of the things that we're actually working on right now on the Ag layer is trying to bring kind of bridge standards within the Ag layer. There's actually nothing that prevents the AgLayer from using some of the bridge standards that we currently see and adding to it a level of security that currently doesn't actually exist. And so the goal is actually to work with something like CCIP rather than compete with it. All right. Where does AgLayer run and who are the actors? Yeah, that's a good question. So the AgLayer is going to be live in three to four weeks. And in its initial form, it's run in a centralized way. The nice thing, though, is that with ZK technology, you can be running a centralized system in a trustless manner. And that's kind of what the AgLayer is going to look like in its initial form. But I fundamentally believe that notwithstanding how centralized we're seeing everything in the space right now, pretty much everything is going to decentralize. And this is either going to happen because it's going to be forced by some government actors in some way, or it's going to happen because critical issues are going to happen in centralized systems, and we're all going to be reminded why it is that we actually have decentralized systems. And so our goal, and it's on the roadmap, and it's actually a lot of work that's already being done, is to decentralize the Agilator and not keep it in a centralized form. All right. Is there any hope of optimistic networks to get aggregated into the Ag layer? Their optimistic nature, that is waiting periods, etc., seems to be fundamentally incompatible with composability. Yes. This is a good question and something we've spent a lot of time about. So I got two answers for you on that. Answer number one is all optimistic roll-ups will be ZK roll-ups. It's just a matter of time. Everybody knows it. All optimistic roll-ups are working on that already. It's just a question of time. And that, frankly, is what will create the ideal user experience. Another alternative would be to actually just prove the fraud proof. Okay, that's the exact question. Like, you can't wait seven days to go cross-train. That's horrible. But what I was also saying is that we don't just need to prove proofs. We can prove other things. For example, you know, the centralized sequencers that we have right now do reach consensus in a centralized way, and you can actually prove that consensus using the pessimistic proof. That's why I was saying that even in this current state where you've got fraud proofs on chains, that we will still see them on the Ag layer because we can protect against the risk of anything happening within the fraud-proof window using the pessimistic proof. All right. We only have time for one more question. Is my understanding correct that existing L2s need to migrate over all their asset strings in their native bridges to the unified bridge? Yeah, that is a very good question. It's something we spend a lot of time on. So I would say there's the ideal state and there's the less the ideal state. As I've mentioned a few times, we can currently connect any existing chain and you will see existing chains get connected without migrating assets over to the Ag layer. And what will happen when that happens is basically you'll start issuing new assets on that new canonical bridge being the AgLayer bridge. For any chains that want the what I'll call ideal user experience, you would actually want to have them migrate all the assets over. This is actually something that you will see happening with Polygon POS. We have 6,500 or so assets on Polygon POS. They will all get migrated over to the unified bridge. And therefore, we're going to be the first example of probably one of the biggest chains, definitely the biggest chain in the world from an assets perspective, actually migrating all of those assets over. All right. Thank you again so much. Can we get a round of applause for Mark, please?",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:30:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/19GjAOPnXoMBNpAerM--poOFpPMM-IeprVNBtTrgK-UA",
      "resources_slides": "https://drive.google.com/file/d/1upW5srNCAMB9Wvg7J5vx9Kck4ZM8Yem9/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "thailands-talents-attraction-initiatives-ltr-smart-visa-program",
      "sourceId": "ENXEC9",
      "title": "Thailand’s talents attraction initiatives; LTR / Smart visa program",
      "description": "In this session, we'll explore Thailand’s talent attraction initiatives, including the Long-Term Resident (LTR) Visa and Smart Visa, crafted to draw global talent and investment. Expect a comprehensive overview of visa categories, eligibility criteria, and exclusive benefits like reduced personal income tax rates for highly skilled professionals, streamlined work permissions, and extended reporting requirements.",
      "track": "Real World Ethereum",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": true,
      "tags": "",
      "keywords": "Visa,work,permit",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:40:00.000Z",
      "slot_end": "2024-11-15T05:10:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1azLdMUNgvh3r_j9J8CLf4Esqc2Konko-l9riMWwdpNU",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "based-preconfirmations-with-mr-mev-boost",
      "sourceId": "79SYDU",
      "title": "Based Preconfirmations with MR-MEV-Boost",
      "description": "In this talk, we will analyze a simple strawman setup of based preconfirmations to highlight its challenges, such as supply chain centralization, the lack of pricing mechanisms, and latency/spam races. We will then introduce MR-MEV-Boost, a preconfirmation solution that runs multiple rounds of MEV-Boost auctions within a single slot. This solution addresses the mentioned challenges by preconfirming batches instead of individual transactions and better integrating with the L1 PBS pipeline.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Rollups,User Experience,Mechanism design,sequencer,based,preconfs,pre-confirmations,Mechanism design,Rollups,User Experience",
      "keywords": "Based preconfirmations,Based sequencing",
      "duration": 591,
      "language": "en",
      "sources_swarmHash": "ab3acd65bb89bf470bbbc617c45fd35a84fe8c1ec59ba577e4b22c917ce1ac5b",
      "sources_youtubeId": "fo2xDLSst_M",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67383b881b0f83434d1c38e4",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67383b881b0f83434d1c38e4.vtt",
      "transcript_text": " Hello everyone. Today I'm gonna talk about base pre-confirmation with Mr. Meb Boost. I'm Lino Shitaini from Nethermind Research. Okay, so most rollups, let's talk about most rollups today. So most rollups today build their blocks using centralized sequencer. So users send their transaction to centralized sequencer, the L2 transactions. The centralized sequ sequencer, the L2 transactions. The centralized sequencer will sequence the L2 transactions, eventually construct these L2 blocks, submit it to the L1 block building pipeline, and then the L1 block will include these L2 blocks. And centralized sequencer are nice. They can provide pre-confirmations. And pre-confirmations, what are pre-confirmations? Basically, users, for example, user submits their transaction C to the centralized sequencer. The centralized sequencer will return a pre-conf, which will say, I will promise to include your transaction C at the third position of the L2 block after transaction A and transaction B. And then some time passes, and then the centralized sequencer will actually submit this L2 block. But the thing here is this preconf will happen way ahead of time before the L2 block is actually submitted to the L1. So it provides good UX. So centralized sequencer are nice, they can provide facts to UX via preconfs, but however they're centralized. So big question here is how do we decentralize? So one idea here is what if we move the sequencer role entirely? And this brings us to based rollups. The based rollups idea is to use the L1 block building pipeline to sequence not only the L1 transactions, but also the L2 transactions. So the user submits their transactions, not only L1, but also the L2, and then the L1 block building pipeline will sequence not only the L1, but also the L2 blocks. So let's expand this L1 block building pipeline part. So this is like what the L1 block building pipeline looks like right now. The user will submit their transaction, and then there are going to be builders who are going to collect these L1 transactions, and then they're gonna be builders who are gonna collect these L1 transactions and then they're gonna construct blocks. And then there are gonna be many builders and then there are gonna be many blocks and then each are gonna submit a bid to the proposer. The proposer is gonna select the most profitable block and then propose it. So the idea of base rollups is that let's use this same pipeline that was used for the L1 which is used in blue here. Also for the L1, which is used in blue here, also for the L2 transactions, which is using the red arrow here. And base follow-ups are great. They inherit the L1 censorship resistance and liveness, because there is no external entity other than the, and we only have the L1 actors that we already have right now. And it also enables L1 composability, because these L1 builders, they will have full sequencing rights over the L1 and the L2. So they can act basically as a shared sequencer between the L1 and the L2, providing composability guarantees. But there's a problem, which is base rollups are slow, because there is no pre-confirmations here. So to the address of this problem, there is base pre-confirmations here. To the address problem, there is base pre-confirmations. Base pre-confirmations idea is this. Let's let the L1 proposer opt in to provide pre-confirmation during their slot. The user can directly send their L2 transactions to the L1 proposer. The L1 proposer can directly respond with a pre-confirmation. And then later on, the L1 block building pipeline runs, and then the proposer will ensure that these L2 blocks are included in the L1 blocks that they propose. So one observation here is that providing pre-confirmation requires sophistication. Because like the proposer here, you have to have some API or some like RPC endpoint to provide the pre-confirmations and you also need to run the L2 full nodes to actually like sequence these L2 blocks. So they would probably want to delegate and they will want to delegate to what we call gateways. So the idea of gateways is this. Proposer can delegate their pre-conf duties to gateways ahead of their slot. So the proposer can say, hey, this is my gateway. Please send your pre-conf request, not to me, but to this gateway. And the gateway will be responsible for responding with pre-confirmations. And then eventually the gateway will sequence L2 blocks, submit it to proposer, proposer will include it in their L1 blocks. So gateways are nice, but they have some problems, which is first, the inheritance of L1 censorship resistance and liveness is degraded. Because now you have this gateway entity that didn't exist in the L1 blockbinding pipeline that is now sequencing the L2 transactions. And now L1 composability is also made more complicated because you no longer have an entity that's sequencing both the L1 and the L2. Because the gateway is sequencing the L2, the builder is sequencing the L1. So the question here is, can we introduce base pre-conformation while retaining the good properties of base rollups? And here comes our proposal, which is called Mr. MevBoost. So it's for multi-round MevBoost. And the idea is this. So let's split the slots into multiple sub-slots, aka rounds, and in each round let's run a MevBoost auction to pre-confirm a partial block. So this whole L1 block pipeline that's sequencing both L1 and L2, let's just run this whole thing many times during the slot. So instead of running it once in the 12 second duration, run it every three seconds, for example. This pipeline is nice because there is no additional entity added to the pipeline. So there is no gateway here. And as a result, we inherit the L1 sensor resistance and liveness much better. Because there is no additional choke point to the system. And also enables L1 composability. Because the L1 builder now is sequencing both L1 and L2. Or they're pre-confirming both L1 and L2 at the same time. That it is, thank you very much. Yeah, thank you a lot. So just a quick reminder to the audience, you can ask questions by scanning the QR code. Do we have any questions from the audience, you can ask questions by scanning the QR code. Do we have any questions from the audience? Yeah, I was wondering myself if you can explain how long you've been working on this idea and if you published any papers already. Yes, there's an E3 search post titled",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:50:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1KG1sgWfc3v4CHrdbeyiwz6GNGMfkJySEhfCERsBjwxA",
      "resources_slides": "https://drive.google.com/file/d/1Ec3xTKgVAOjj9pL5WIV6II0OSwDm7ucS/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "logs-for-you-anon",
      "sourceId": "RRYVNW",
      "title": "Logs for you anon",
      "description": "The removal of log events has sparked a discussion about its implications for apps that rely on events to display information. Without logs, developers would need to use specialized software to index the chain and search for specific actions, which is costly, not friendly with privacy and requires a case-by-case approach. This is in contrast to the current system, where logs provide developers with the freedom to query the chain anonymously, without limits, and without sacrificing any detail.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Privacy,Decentralization,indexing,Decentralization,DevEx,Privacy",
      "keywords": "logs,local apps,indexing",
      "duration": 526,
      "language": "en",
      "sources_swarmHash": "e4e9a3d1779f291acbf4d359d30f7beb6e8e538ba5f374c13ef4402fa8d29873",
      "sources_youtubeId": "e0HJbXgdl-g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d90d74749a4b8935f22a",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d90d74749a4b8935f22a.vtt",
      "transcript_text": " . Hello everyone. Thanks for coming. Thanks to the organization for this event, it has been great so far. So for today, my goal is logs for you, Anu. I will be talking about logs. So first, let's see what logs are. They are a piece of information emitted by contracts. They don't have a limit on size more than the block size, and they have three important parts. One is the address of the contract emitting the log. The other is topics that are indexed information so it can be retrieved fast. And then we have data that is decimal data field, and it can be whatever you want. So this is a BlockScout screenshot, and this is a transfer from USDT. And you can see that they present, like, a very human-friendly UI, like they could input data. But here below, you can see the topics and the data. The first one is the signature, always, the signature of the event. In this case, the transfer has a from and a to. And these are the two other topics in the logs. So the importance of topics is, for example, this is a screenshot from Rodkey, and we can use logs to provide insightful information to the user. From transfer, for example, we can obtain what tokens were swapped in a transaction, and it works in any EVM chain. So for example, these are information obtained from Arbitrum and Optimist, and we can see that the user swapped some tokens in Lama Thief. And all this is thank you to logs. So what is the problem with them? There are two big complaints that the community has about them. One is that the user has to pay for emitting logs, so every time the contract emits information, the user pays for it. And the network as a whole needs to store this information in the database. To see about the first problem, I took this Dune dashboard, and as you can see, it's a bit old information, but in the 30 days period, it was over 1,000 Ethereum spent only on gas, and this is around 15 to 20% of the total spent in transactions. So what can we do about this? One of the proposals that I saw, the first company that I saw was Shadow, is to completely remove the locks. So if they are a problem, we get rid of them. And the proposal is that you can have the chain without locks, and then afterwards, you off-chain, emit them, like reprocess the transaction and emit them, modifying the bytecode of the contract. This was from a company, and then it came the proposal, the standard proposal. It is this one, and it's useful to have a standard because everyone can implement it. But it has a conversation and it didn't have much feedback. It stopped at some point last year. So is this good, is this bad? We need to consider a few things first. The user pays for the emitting logs, but they also benefit from them. So as you can see in RodKey, we display information, and it's thanks to the logs. Also, if you use Sirion or any other application, Aave, Frontend, whatever, they all use logs. So you are paying for them, but they are useful for you always. Also, regarding the Ethereum scalability, GoEthereum, the node is 14 terabytes in archive mode. I was told that it's around 600 gigabytes the locks in the system so it's a small proportion of the 14 terabytes and if at any point there are problems, they can be completely removed from the node. Also, they can be sharded in a different node or whatever. And it's not such a big deal. And if it ever becomes a problem, we can manage it. And also data ownership. The problem is that if you put a company like Shadow or Ghost or any other company doing Shadow Logs, you are not sovereign of your data. You can't use your node anymore to get this information that it is useful for you. So you are using it. So this is a big problem. And, of course, we are cyberpunk, so we want to have this data and use it. There are also good things about shadow logs. For example, liquidity liquidations in the contracts were not indexed properly. So you can use shadow logs to properly detect when you were liquidated. And also there is a similar issue in the Gnosis bridge because the address is not indexed properly. Also, for example, Uniswap created a beautiful dashboard and Pendel improved the routing using shadow logs, so there are good use cases for them. So my feedback here is that we need balance and there are two main takeaways. One is that we need the logs for everyone to use them. We don't need everything in there. You can add it later, but we need a minimum. And it's good to have extra information in Shadow Logs. So that is all. Thank you for coming. Let's see if you have any questions. Okay. Now, if somebody has any questions, please raise your hand and I will throw you this. Don't worry, it doesn't hurt. It's soft. Also, it's a five-minute talk. Yeah, and afterwards if there's some more deep questions, you can discuss it outside. It's okay. Okay, Miko has a question. Don't worry, it doesn't break. Mikko on käsitellyt. Ei huomaa, se ei rikki. Onko tämä mikrofoni? Se on. Laita sen lähelle ja puhutaan siitä. Se ei ole orffi. Miksi ei laiteta ne melkein ilmaiseksi, jotta protokollit käyttäisivät niitä enemmän? them basically almost free so that the protocols would use them more and that way we don't need set of logs because I don't think the impact on the node performance is that bad. As far as I know there is no such big impact on using the logs. The main problem will be, I guess I'm not an expert on that part, but I guess it will be DDoS to the node. So if you can expand as much logs as you want, someone could DDoS and it's not zero cost but it's almost free, the logs in the disk, but if you can reduce the amount that you spend on them, you can DDoS nodes and also you have to careful measure because everything is like an equilibrium in the Ethereum ecosystem so if you modify one constant here in the price it can affect to everything because as far as I know, locks were supposed to be like for real-time notification so user could get real-time feedback and then developers use them to store information so they change it the use case of the lock. So I guess it needs an equilibrium and careful review for that. Hey. Do you think that it's fair to say that the reason that Shadow wants to just eliminate the logs is in order to make the users pay less, but in the end, the users end up using the logs from like Rodkey, from Aave, from every front end. So shouldn't the users actually end up paying for the logs? I mean, doesn't it make sense in the end? For users to pay? Yeah. Yeah, for me, it makes use of what I said a little bit. The user is paying for this information, but they directly have a positive impact from it. So shadow logs are good in cases that you need extra information, like adding more things as a developer if you need analytics or anything. But the user has a real positive impact for having logs. They pay for something that really benefits. And it's not that expensive. For a single user, the whole network can have a high cost but the single user is not paying everything, just paying a fraction of that. So this is positive for them. Okay. Thank you very much, Javier. Time is up. Thank you. Bye. . So now please...",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T04:50:00.000Z",
      "slot_end": "2024-11-15T05:00:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/19tr5hJbHHcDFcMqxEDdnvWaK2uCU2yR2HV12bhQ1NTQ",
      "resources_slides": "https://drive.google.com/file/d/1viCgTcgFxwThmUw7wqFjIOMDYZN2yGRx/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "a-proposers-perspective-on-preconfirmations-a-new-game-in-town",
      "sourceId": "8LYGCY",
      "title": "A proposer's perspective on preconfirmations: a new game in town?",
      "description": "This talk will provide a node operator's perspective on the preconf transaction pipeline by examining incentives and infrastructure best practices. \r\nSpecifically, ways node operators may attempt to earn higher revenues than peers through custom optimizations will be discussed alongside, related risks, and potential mitigations.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Validator Experience,PBS,Transaction fees mechanisms,preconfs,PBS,Transaction fees mechanisms,Validator Experience",
      "keywords": "Preconfirmations,Preconfs",
      "duration": 1213,
      "language": "en",
      "sources_swarmHash": "23102edd79f1d2ffc04dbd4136e229f30b73f5d2cba2cb4541edbc562ee181f3",
      "sources_youtubeId": "Wa5O4TMEdwE",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67383bf21b0f83434d24292b",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67383bf21b0f83434d24292b.vtt",
      "transcript_text": " Node Operator's perspective on prep confirmation transaction pipeline. Please welcome our speaker, Michael Moser. Hi everybody. Let me just calibrate this. So I'm Michael. I'm head of research at ChorusOne. We are Node Operator with just about 3 billion in stake. Some of you guys might know us from some of our research papers, actually. For example, we've designed MEV mitigation on DYDXV4, which is a system that's currently in production. We were also quite early to time in games. We started kind of experimenting with this type of thing early last year and then and then we think it's always important to strike a balance between rational competition and ecosystem health so we've published quite a bit about our approach right and we took a very transparent angle to that and I think philosophically this presentation is an extension of that I just want to talk about pre confirmations from a proposal perspective and maybe a more spicy way of framing this presentation could have been timing games in pre-confirmations, what do they look like? And then secondly, what could an earlier approach to pricing pre-confirmations look like? I think first of all, it's important to note that it's very early days. I think there's just one public pricing model for pre-confirmations. And so what I'll describe here is more of a general approach rather than a specific model, because this will be very parameter dependent. And in the spirit of rational competition, we'll talk about what it is that we do, but we might not have all the parameters public naturally. Yeah, so here's the agenda. I will quickly walk through the types of pre-confirmations. Then I'll talk about the pre-confirmations pipeline from a proposal POV. Game A is kind of like a timing game, right, what that would look like. And then game B is pricing and optimal inclusion. So let's dive in. Yeah, I think there's a lot of jargon flying around. I think it's important to just bring these things back to what they actually are. They are basically inclusion pre-confirmations and then the execution pre-confirmations. I think the key difference between those, you can really boil it down to the two points I give at the top of the slide. An inclusion pre-confirmation is just a credible commitment that your transaction will be present in a future block, but it's not a commitment of the transaction actually executing, right? The transaction may fail. So what the validator does is it tells you, I commit to putting your transaction in there, but then again, you know, if it touches contentious state, there's a chance that it will fail. An execution pre-confirmation, by contrast, actually commits to the transaction being one present, of course, and then two subset executed in the block. Therefore, it should not fail. So what does this mean in practice? Implication one, execution pre-confirmations can touch contentious state, right? They also require the block to be simulated because otherwise you cannot commit to a transaction actually landing, right? You need to simulate the block out as a whole to understand whether this is a commitment that can be credibly given. And then conclusion one from that is execution pre-confirmations are best issued by builders and I think there's two really simple reasons for that right if you just demystify it reason a is that builders have private flow and you know as it like a proposal you don't see that private flow so you a priori do not have the option of simulating a block out fully simply because not all the infrastructure, and the other thing is that the block out fully simply because not all transactions are available to you. And then B, builders also have kind of sophisticated pricing. This is something that they do in terms of part of their competitive edge. For a proposer, if I'm talking about running infrastructure really well. I see the light went out, but hopefully the rest of my talk will be enlightening. Maybe I'll do my best. Inclusion pre-confirmations on the reverse, don't touch contentious state, because otherwise it's not something that you could commit to without giving the execution pre-confirmation, which means they are typically best issued by proposers. And the reason for that is just that the proposer is certain to propose. With a builder, builder A may win the auction, but builder B may also win the auction. With the proposer, you know for sure that the proposer is going to propose to the next block. And therefore, you do not have to probability gate it, right? If you get a pre-confirmation from a builder, you always have to expect that that builder might not win the block, or it must solicit downstream agreement from the proposer. But with the proposer, there is no such uncertainty, right? The proposer is quite certain, unless something really terrible happens on the infraside of things, which is unlikely. Okay, so let's zoom in on the latter point. Let's zoom in on inclusion pre-confirmation transaction pipeline for a proposer. Yeah, I kind of sketched it out a little bit there. There's, of course, a lot of nuance here, but I think to understand the general concept, it's enough to understand the general transaction flow. And then I think we can dive deeper into more granular detail. The generalized proposal inclusion pre-confirmation transaction pipeline, you basically have a transaction originator, would be, you know, maybe it's a base rollup, maybe it's a user, and that transaction originator would send the transaction to some kind of transaction aggregator, right? You do not want transactions to be sent to the validator directly, because then what functionally happens is it can be, you know, it opens up a DDoS attack vector, and that would very badly reflect, for example, on the home stakers. It also generally wouldn't be the way that people are set up in the firm. On Solana, your proposers expect a certain amount of traffic. On Ethereum, that's really not the case. So there would need to be some kind of proxy in the middle. You can conceptualize a little bit like a relay. And this proxy can do two further tasks. Either it just takes the transactions and then forwards a set of transactions to the proposer for the proposer to choose, pick and choose, or it does pricing itself. And when it does pricing itself, frequently the lingo that's being used is we are talking about a gateway here. So really that's kind of where the path diverges. Either this type of proxy does pricing or it doesn't do pricing. And then for the proposer, if you want to optimize, you have to be opinionated about two things. First, you must source transactions optimally. This is basically how do you interface with the relay gateway proxy, whatever you want to call it in an optimal way. And then B, there's another scenario where this is like a subset of scenarios where you can engage in pricing. And then three, there's also maybe another game where you can strategically play inclusion and exclusion. And we've actually published about this, right? But this is more relevant for execution pre-cons. So if you're interested in that, you can find it on Eve Research. It's by Umberto, who's a colleague of mine. And maybe I just want to mention here, this is a co-op with two other team members, Umberto and Benoit, who are very, very active on this type of more quantitative research okay really let's take it back right either transactions will be sent to the electronic will be sent to a proxy or a transaction pricing can be delegated to a third party and then and then to kind of close the slide I want to just like structure out these two optimization axes. Optimal transaction sourcing, what that would mean in practice, there may be something like a timing game, but in actuality we would argue that it's a reverse type timing game. And then there's a sub game where some transaction types you might want to strategically include or exclude, but I won't talk about this today. And then secondly for secondly for pricing yes if there's no gateway design and the validator does pricing then you need to have an in-house an in-house opinion on that right i think in general it's important to say that all of these things always exist on a risk reverse axis for example even in normal pbs timing games if you push them really far, what you do is you simply delay into a flat curve, and you're actually on a very bad point on the risk-reverse axis. So I think it's important to always frame that in terms of expected returns. Let's talk a little bit about optimally sourcing transactions. So this is a busy slide. Therefore, I'll take it step by step. You see two kind of axes of graphs here. There's two graphs at the top and then there's two graphs at the bottom. And I think that the way to read this best is the two graphs at the top correspond to the first large point on the slide and then the two graphs to the bottom correspond to the second large point on the slide, and then the two graphs to the bottom correspond to the second large point on the slide. I will explain. I think the first thing to notice here, to set up kind of the timing game, is that gas use is dynamical, right? I mean it varies a lot between blocks, but then it also varies a lot within blocks. And if you look at the graph there at the top right, that's basically the gas per transaction distribution and it's of course extremely skewed. And then if you look at the gas usage versus position, which is on the top left here, you will notice that the higher a transaction position is in the block, the more gas it consumes. So let's just keep that in mind. The earlier a transaction is in the block, the more gas it consumes. So let's just keep that in mind. The earlier a transaction is in the block, the more gas it tends to consume with some edge cases around background specifically and the gas per transaction is distributed in a manner which is super skewed. And then I think, and then the second information we need to set up our case here is that PBS timing games capture value by soliciting the block very late, right? So you send a get header request really late. You give the builder a lot of time to gather transactions to kind of combine them, right, for 6-dex arbitrage to come in. And this is what you see at the bottom here, right? This is the way it works in PBS. So the bottom left graph is the number of transactions and kind of when they come in. And I think here the point to take away is just the longer you wait, the more transactions are available, so the combinatorial space is higher. And then the second graph at the bottom, which is on the right, is the longer you wait, the higher the ratio of the bit to the max bit is. So basically the way you would read this is simply the expected value of a block goes up over time. And now let's look at the game that we can deduce from these two categories of information. So conclusion one here is, PBS timing games benefit from expected transaction gas use going up over slot time. And PBS timing games also profit from winning over transactions with mainland in the next block. Now, for pre-confirmations, I would expect this to be quite different. And the reason for that is because the premium that you're willing to pay for pre-confirmation, it corresponds to the service level you're getting. And let's say you can land anywhere within a 12-second time frame. If you land at 11 seconds, then you're getting a certain service level. But then if you land at one second to the next block, do you really need a pre-confirmation? So there's definitely some like an effect here where instead of the expected value going up over time, it might go down over time, simply because the service level that you provide where pre-confirmation, this is earlier, this is better the earlier in the block, you're just doing more. And what this would mean to us is that the pre-confirmation inclusion premium scales with the expected wait time, right? And what this would mean to us is, on the one hand, we would expect pre-confirmation value to cluster early. So if you look at the PPS graph here, we wouldn't expect it to look like that for pre-confirmations. In actuality, the highest expected value per transaction, that would happen a lot earlier. And then you really probably wouldn't want to wait super long. Instead, what you want to do is you want to wrap the pre-confirmation auction up and you want to give the builder more optimization time. And then if you take it really practical, we think there is an optimal early end to the auction that is a function of the transaction arrival and distribution and the price decay. Right. And we have some we have some very specific thoughts on this, but unfortunately there's no time. So we'll need to dive into the next game. And this is pricing and optimal inclusion. So previously I mentioned that either a gateway can price it or potentially a validator can price it, right? If you asked me for a hypothesis, I would say it's probably more likely for the gateway to price it because I think node operators on the whole, as I mentioned earlier, are more set up to run good infrastructure, right? Pricing is not something that typically comes up. But yeah, that being said, let's go ahead and sketch out a quick model, right? I think the first thing to consider here is earlier I mentioned that the transaction, the gas usage of a transaction, that varies depending on where the transaction is in the block, okay? And so let's approach this heuristically. And there are kind of three tiers of transactions in the block, right? There's like tier one, which is super early, which is top of block, which is typically your sex-tax arbitrage. There's tier two, which is kind of like mid-block, and then there's tier three, which is anywhere else in the block, right? Can say like mid-block to bottom of block. And then, okay, so we have this heuristic, is it valid? Well, we ran the statistics on it, and we, you know, like this p-value, which is pretty small. If I read that out now, that would take the end of my presentation time. So these tiers are not the same in terms of transaction usage, right? In terms of gas usage. So it's a heuristic which is kind of, you know, like if you rigorously look at it from a statistical POV, it's fair. And then one conclusion we can draw from that is that validators compete for tier 2 and tier 3 transactions, because tier 1 transactions, they are just super tight to being at the top of the block, right? The sex tax arbitrage, you know, like there is no real discretion about maybe including it at a later point of time. It has to happen at that point in time because it is tied to a certain price delta between a centralized exchange and the DEX. Okay, yeah, that being said, again, take away one, there can be, you know, we can separate out transactions in the block into three tiers in a statistically valid manner. Takeaway two, tier one transactions here really aren't, you know, as useful for our pricing model because we are talking about inclusion pre-confirmations, which don't really touch contentious state in this way. That being said, let's sketch out the model, right? So the first statement that we can make is well what do we need to build a model here right can we even do that? Yeah I think we can do it right because inclusion pre confirmations do not touch contentious state you can build a model solely on public flow okay so the model can be built that's premise A. Two, how would the model look like if we dig in a little bit deeper? Well, okay, so we have a base fee component and we have a priority fee estimation. Let's look at some of the attributes of these two, right? The base fee is easier to estimate because it's discrete and then the priority fee is a lot harder to estimate because it is it is continuous right so just there's a there's a much much larger space the base fee we can generally frame as the the opportunity cost of pre-conferring transactions right because if you if you do not pre-confer a transaction, you can always get a future transaction which, you know, as a minimum incurs the base fee. And then the priority fee can be seen as something that's more akin to like a premium, right? Which it is in actuality, even in a spot market, the priority fee would basically correspond to a premium. Okay, so how can we schedule a toy model? Well, first of all, we need a way to forecast the base fee, which is something that can be reasonably done. And then we finalize a distribution of the priority fee over tier 2 and tier 3 transactions, because again, tier 1 transactions are not as relevant for inclusion pre-confirmations. And then what we think is a good first way of approaching it is you take the base fee baseline and then you add a percentile of the priority fee distribution as the pre-confirmation premium to it. And actually which percentile you add to it, I think this is largely a function of you can see it as I think this is largely a function of, you know, you can see it as a confidence interval of how well you understand transaction pricing in a future slot. I think this is a good way of approaching it. We hope to publish a little bit more on it. I think for both the last game with timing and for this, it's important to look at things empirically. For that reason, this is something that will mature over time. But in general, I think this is an approach that is valid for either an old operator, pre-confirmation protocol, or whoever else is interested in that. So I think I'm ahead of time. Thanks for attending, and this is it. Thank you a lot. We have two questions. I will give you a chance to choose which one you want to answer first? Sure, sure. I'll answer the question with the one vote first. Besides based roll-ups and MEV searches, we also might be using pre-confirmations. Okay, let's take the taxonomy that I mentioned earlier in the presentation between inclusion and execution pre-confirmations. I think for inclusion pre-confirmations, it's going to be two cases. One is based roll-ups and then the second case would be UX, right? Okay, so you can look at that and you can say well, you know, waiting 12 seconds is that really so bad? But then if you frame it differently and you say, you know, speeds that's comparable to Solana, then that actually that actually sounds a lot better, right? So for, I think it's UX, let's say wallets and based rollups. And then for execution pre-confirmations, I think that's kind of where searches start coming in. And not only like atomic searches, but then there's a point where it gets relevant for sex tax arbitrage as well. Yeah. Okay. Thank you. We still have a bit of time. If you have any questions, we have microphones available. Okay, thank you a lot. I think it was a very interesting talk. I also, not a predator.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T05:30:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/10kKWaC4imyMLa4e4mR8BSiSrX6pmWjzQCOwVryP2ff8",
      "resources_slides": "https://drive.google.com/file/d/1C_oze48ywGg4cDMuBMHzVdoWnBshnSBP/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "debunking-myths-about-building-out-of-sea",
      "sourceId": "CDVQ7R",
      "title": "Debunking Myths about Building out of SEA",
      "description": "South East Asia is home to a burgeoning community of builders and some of the most influential projects in Ethereum. Listen in as SEA founders share their experiences and answer your questions about building out of this corner of the world.",
      "track": "Real World Ethereum",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Local/SEA",
      "featured": false,
      "doNotRecord": false,
      "tags": "Local Impact,SEA,myths,SEA",
      "keywords": "Build,Myth,Local",
      "duration": 3420,
      "language": "en",
      "sources_swarmHash": "35f38b842a091df2c8086f5f7478de01d7f7f1e73b14d309315211776470463c",
      "sources_youtubeId": "JqwV9bFPrXQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673828c920d1f9ac4878622b",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T06:00:00.000Z",
      "slot_roomId": "stage-6",
      "resources_presentation": "https://docs.google.com/presentation/d/1SpHoMINj55MzEUWqqO7ToaiDbowMedsSM9tKnMWMaSY",
      "resources_slides": "",
      "slot_room": {
        "id": "stage-6",
        "name": "Stage 6",
        "description": "Kites",
        "info": "1",
        "capacity": 450,
        "youtubeStreamUrl_1": "https://youtube.com/embed/YeUgxdhkXqY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/1udf88Uh6UM",
        "youtubeStreamUrl_3": "https://youtube.com/embed/XeulPyx5FuQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/TWOSgF5ciLE",
        "translationUrl": "https://stm.live/Stage-6"
      }
    },
    {
      "id": "dj-mayu",
      "sourceId": "XV779L",
      "title": "DJ MAYU",
      "description": "Join us at the Music Stage in the social area on Floor G for an unforgettable experience with the Open Source Orchestra! Dive into the beats and vibes curated by talented musicians from the Ethereum ecosystem, bringing together community, creativity, and rhythm. Let’s groove and connect through the universal language of music!",
      "track": "Entertainment",
      "type": "Music",
      "expertise": "",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "",
      "keywords": "",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": null,
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T06:00:00.000Z",
      "slot_roomId": "music-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1t2zQdmj0AJUDWbkdwI8GyR90vs3nQ_7TKvydOwUZYYk",
      "resources_slides": "",
      "slot_room": {
        "id": "music-stage",
        "name": "Music stage",
        "description": "",
        "info": "G",
        "capacity": null,
        "youtubeStreamUrl_1": null,
        "youtubeStreamUrl_2": null,
        "youtubeStreamUrl_3": null,
        "youtubeStreamUrl_4": null,
        "translationUrl": null
      }
    },
    {
      "id": "ethereum-citizen-embracing-self-sovereign-digital-identity",
      "sourceId": "ATKWT8",
      "title": "Ethereum Citizen: Embracing Self-Sovereign Digital Identity",
      "description": "The world is changing. Everything is becoming digital. As we seek to extract more from digital services, we are giving them more and more of our personal data.\r\n\r\nBut it doesn't have to be this way. Just as we gained self-sovereignty and ownership over our digital assets and money, we can achieve the same for our digital identities and data using similar and new technologies.\r\n\r\nThis presentation will explain what self-sovereign identity is, why we need it, and where we stand today.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Privacy,Identity,Social,data,Identity,Privacy,Social",
      "keywords": "Attestations,data",
      "duration": 600,
      "language": "en",
      "sources_swarmHash": "a65e59ff5884726a3266ba344b24a0d68991446dba97d23df9cb9d39182fb641",
      "sources_youtubeId": "jK5uGFCH9HY",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736d9ad74749a4b8937d5e4",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736d9ad74749a4b8937d5e4.vtt",
      "transcript_text": " Hi, welcome from my site as well. I'm a researcher and founder of Lutrolabs and today I'm going to talk about how Ethereum citizens are embracing self-sourced identity. So who is a citizen based on the dictionary, a person who is a member of a particular country or a person who lives in a particular town or city. We extend this definition to the 21st century, we can just state the network state or any part of the digital community. But who is an Ethereum citizen? So Ethereum citizen is a person who is participating in the Ethereum ecosystem or using the Ethereum technology to achieve its goals while promoting the Ethereum's values. So I just want to say there is really no hard definition similar to Ethereum enlightenment, but you can pretty much tell by feeling what I mean by that. So what are the Ethereum values? So from the technology perspective, this is decentralization, security, and scalability. But from the user perspective, is in decentralization security and scalability but from the user perspective this is self-certainty censorship resistance ownership permissionless and privacy so where are we today so for financial assets uh we are pretty much there uh for smart contracts we are somewhat there so smart contracts can be permissionally deployed to the ethereum network but usually they can be upgraded through the governance process of a decentralized community, but most smart contracts have multisig today. So websites. Websites can be hosted in a decentralized way on a decentralized storage network, but we are also somewhat there. And for identity and data, identity and data should be fully owned and controlled by the users. And we are not there yet, but things are getting better in the last years. So as an Ethereum citizen, I want to control my identifiers so no one can take my identity. So here we have many options. So basically identifiers are Ethereum addresses. ENS handles DIDs or semaphore identifiers in the ZOOPAS. But currently the most adopted thing we have are EOAs, so blockchain addresses with ENS, but this doesn't give us any privacy. And for the data part, we want to control our data and share this closet only to the people we want and when we want. So that means that the data should be stored in our wallets, on our personal servers, or encrypted on the third parties. But for that, if we want to make it that work, we need verifiable data. So what is verifiable data? This is like the famous triangle of trust in the self-sourced identity community. So we have the data issuers, we have users, and we have verifiers. So how the verifiable data works is that the issuers issues the data to the users and they sign over the data. And when users disclose the data to the verifiers, the verifiers can verify the digital signature so they know the data came from the issuer. But here I also want to mention that the data issuer and the user can be the same person because some data can be self-signed or self-issued if the data is not that important, or maybe it is important, but for example, what is my favorite color? This data can be signed by myself. So what's the reality? So financial assets in the decentralized applications are in our control but the data is usually stored in the centralized databases for example list of my favorite favorite nfts or tokens on trading devs this is usually stored on each platform in their centralized databases on every platform you have to connect your social accounts and prove that you're owning them and you have to repeat this on every platform and the kyc also you have to connect your social accounts and prove that you're owning them and you have to repeat this on every platform and the KYC also you have to repeat that on each platform but it's not all that bad things have drastically improved in the last two years so for example just look at your tickets for the DEF CON you can you store them in the zoo pass so the tickets for the DEF CON are one kind of the verifiable data, and we also have other kinds, for example, forecaster and NAND protocol posts. DIDs and verifiable credentials are getting more used in the last years. The proof-carrying data is another type of the verifiable data, and we also have zero-null solutions like ZKIT-TLS. So how would the perfect world look like? User would visit her favorite app. The user needs to fill out some information, for example social profiles or any other data that this DAP needs, and the DAP would be able to fetch the data or from the user or from some other third-party provider where the user is storing his encrypted data, and the DAP could use this data without requiring the data to be created or filled out again. So what are some of the challenges? First challenge is user-friendliness. So data sharing in this way takes a long time. Proof generation time for zero knowledge proofs also takes a long time. Use the developer experience so we don't have any unified SDKs and tools for different solutions. And we have many standards for the data and identity but we also don't have enough standards on the wallet side and how to actually use these standards in the applications. So Ethereum citizens will become self-sovereign when not only their assets, but also their data and identities will be owned by them without any centralized point of failure. And that's it from my side. Thank you. Thank you very much, Vít. So, Q&A, remember, just raise your hand, and I will toss this to you. Are there some questions for Vít? Yep, let's go. Hey, quick question on your view of government data. So because all of our identities, unfortunately, it's tied to some sort of government credential. How does that integrate into the schema that you presented for Ethereum citizens? Yeah, so maybe I can talk about how the idea is currently in Europe, where I'm from. So the European government and governments talk about how the idea is currently in the Europe where I'm from so the the European government and governments are also pushing for the similar concept so you have issuers you have users you have very files verifiers and you also have the data signatures basically the whole system is the same only the issuers are the government entities which are issuing the data. You're saying to integrate with the IDAS 2.0 wallets, that's the plan? Excuse me? So is the plan then to kind of integrate with IDAS 2.0 identity wallets that governments are providing? I think that depends on the applications. For example, if you have some decentralized applications, you don't need for everything the government IDs. But I think that the best approach is to merge on the similar technologies. So basically, if you are working on a wallet, which works in the Ethereum ecosystem, it's not so much work to also support other government use cases or whatever. Are there any more questions? Don't be shy, guys. Yep, we have one. Don't be shy. put it close to your... I'm bad at volleyball. So the question is, like, what do you think holds us back? Is it just the governments and KYC providers being not flexible enough? Or, like, what's the real... I think there are many challenges. So, for example, if you want to develop your application in a way that the users own the data, I mean, like, the user experience in Web3 is already not so great. And if you want to do everything like I said, it gets even worse. So I think that we need more tools, just like the standard technical things that we need to figure out first before we can give it to the user so they can use it more. Thanks. Yep, we have one question. Will be the last question. Yeah, thanks. Do you think there's going to be, is there somehow a bit of a race between governments rolling out citizen identification systems which are mandatory and a version of decentralised identity which you've outlined here? and a version of decentralized identity, which you've outlined here. Are you optimistic that the idealized, decentralized, self-sovereign identity model will prevail in time, or do you think they're at odds with the centralized sort of government-issued IDs? I think that in the short term or midterm, we will have both. So basically, like, I don't see that, for example, getting a passport or anything like that, we will be able to do it with decentralized identity. I think that some kind of documents will always be tied to the government identities. But inside the, like, Ethereum ecosystem or there, I don't think there is be tied to the government identities, but inside the Ethereum ecosystem or there, I don't think there is a need for the government ideas. I think we can go fully with decentralized identity. Yep. So that's all. Thank you very much. Thank you. Pete.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T05:10:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1JzCvRvtEDW6bmL33pf1kIydVAzlZM-tN5p_XZlUg02I",
      "resources_slides": "https://drive.google.com/file/d/1OnLej0P4QDJF5LDYcZgAfrwkM4Sxr8VA/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "ethersjs-api-hidden-gems",
      "sourceId": "EG8ML8",
      "title": "Ethers.js - API Hidden Gems",
      "description": "There are many shortcuts and powerful API features in Ethers.js which go unnoticed or under-exploited. The goal of this talk is to raise awareness, provide examples and encourage usage of some of these useful APIs to unlock features which can improve user experience, user security and be more transparent to users.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Developer",
      "featured": false,
      "doNotRecord": false,
      "tags": "DevEx,Testing,UI/UX,api,DevEx,Testing,UI/UX",
      "keywords": "Ethers,API",
      "duration": 1192,
      "language": "en",
      "sources_swarmHash": "6ee306a0ea634950b19333665c6e0f1af4c15ef985147faa92f247be8e7ba05d",
      "sources_youtubeId": "zjApYb3mtAg",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673d954917a97b4f4dd733aa",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T05:30:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1B_Zxh9JTKekXGn74kLQf28CCReGTzSYFG5ED2_8egac",
      "resources_slides": "https://drive.google.com/file/d/1a2bYjTpqhMlCvM5mpJ7OdBG8g1HxQdkY/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    },
    {
      "id": "how-i-audit",
      "sourceId": "3NRXP9",
      "title": "How I Audit",
      "description": "Dom, a former security researcher at Trail of Bits, is going to give a peek of what it's like to be an auditor in 2024. Some of the techniques and tools discussed:\r\n\r\n* How to prepare for an audit?\r\n* How to hand over the resources?\r\n* What is the first thing auditors do?\r\n* How to communicate with auditors?\r\n* How I use the following tools, and their evaluation:\r\n  * Codebase visualization with Wake and Neovim\r\n  * Static analysis tools\r\n  * Fuzzing (and debugging)\r\n  * Manual review",
      "track": "Security",
      "type": "Workshop",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Tooling,Security,Auditing,analysis,static,Auditing,Security,Tooling",
      "keywords": "Solidity,Frameworks,Program analysis,Static Analysis",
      "duration": 5302,
      "language": "en",
      "sources_swarmHash": "4427212cb7b20f8a44407a4b5401b5dbe9d985867ab725d409048614bac75dfd",
      "sources_youtubeId": "8uC1QvsYeu0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673863e020d1f9ac48bd215e",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T06:30:00.000Z",
      "slot_roomId": "classroom-b",
      "resources_presentation": "https://docs.google.com/presentation/d/1cJm-toCXN2UU4rbGe04A5r8Ki0Mu2kurnbC7eBJWsbQ",
      "resources_slides": "",
      "slot_room": {
        "id": "classroom-b",
        "name": "Classroom B",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://youtube.com/embed/PHapjgAvMXg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ABP72AYXX_k",
        "youtubeStreamUrl_3": "https://youtube.com/embed/hddUwQvvFq8",
        "youtubeStreamUrl_4": "https://youtube.com/embed/9dAUhOYrmXI",
        "translationUrl": "https://stm.live/Classroom-B"
      }
    },
    {
      "id": "l2-specific-mev-mitigation-strategies",
      "sourceId": "FFWJAV",
      "title": "L2 Specific MEV Mitigation Strategies",
      "description": "MEV mitigation and prevention has primarily been researched in the base L1 Ethereum layer. This talk explores L2 specific strategies, including the future in the event of decentralized sequencing. We explore emerging EIP proposals and drafts (EIP-7640), the use of intents in L2s and other new constructions.",
      "track": "Layer 2",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Layer 2s,Rollups,MEV,defi,Layer 2s,MEV,Rollups",
      "keywords": "DeFi",
      "duration": 1490,
      "language": "en",
      "sources_swarmHash": "bb61f06d6177b7bc13e9366cbeba4e4fdf1d238ef84a4852919c8def46558a39",
      "sources_youtubeId": "IXV3yFXHlfo",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f8ee1b0f83434dc12123",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T05:30:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1WzPEAvLhXYIe49IEB4HEC3EgI2OglZ-ElusjYDJG2QY",
      "resources_slides": "https://drive.google.com/file/d/1WtGRPj67rCdioc1DzmL2tfqoFP-WKPls/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "protocol-guild-funding-the-ethereum-commons",
      "sourceId": "EJVT7E",
      "title": "Protocol Guild: funding the Ethereum commons",
      "description": "Ethereum produces shared resources within the commons frame.\r\n\r\nProtocol Guild is a way collectively fund the people maintaining the crucial underlying software, while rebalancing the incentives to do this work relative to the broader industry context.\r\n\r\nThe entire ecosystem benefits when there is consistent incentives to recognize this work.",
      "track": "Core Protocol",
      "type": "Talk",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Gaming,theory",
      "keywords": "ACD,Core Protocol,DAO,Onchain Organization,Game Theory",
      "duration": 1534,
      "language": "en",
      "sources_swarmHash": "0f113065ce2a6b6c41f92bfdfadafdcb1c4e2b703debb37ffe0e52577bedd617",
      "sources_youtubeId": "4Hc664qQkV0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736ea1d1b0f83434d40f7be",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:00:00.000Z",
      "slot_end": "2024-11-15T05:30:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1X-IkjzbaZoye8kj19czZe1suKsBA9C7jL4gsmxYI5ko",
      "resources_slides": "https://drive.google.com/file/d/1CL3uBIiEScBDHDe3k-Nlz6Cy3qoLzB_b/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "building-resilient-teams-navigating-the-web3-ecosystem",
      "sourceId": "YDRT7T",
      "title": "Building Resilient Teams: Navigating the Web3 Ecosystem",
      "description": "The panel will consist of Evin McMullen (CEO, Disco), Olivia Smith (COO, Moonsong Labs), Rachel Brissenden (UETH, EDCON), and moderator Bridget Hearst (Director of Marketing, Moonsong Labs) as they explore strategies for building resilient teams across various Web3 sectors. Discuss the importance of resiliency in Web3, its unique challenges, and effective practices for fostering strong, adaptable teams in a decentralized landscape.",
      "track": "Coordination",
      "type": "Panel",
      "expertise": "Beginner",
      "audience": "Community",
      "featured": false,
      "doNotRecord": false,
      "tags": "Coordination,Values,Best Practices,adaptability,Best Practices,Coordination,Values",
      "keywords": "team building,strategy,culture,adaptability",
      "duration": 3319,
      "language": "en",
      "sources_swarmHash": "445c2341a4f3e783b9f2e569f2f3ffb18ffbb93c1a3fb2263484eee0f0c56eea",
      "sources_youtubeId": "payFlKyZ_tw",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f8c074749a4b899c4c0f",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736f8c074749a4b899c4c0f.vtt",
      "transcript_text": " Hello, hello. How is everybody doing today? Last day of DevCon. We are super excited to be here talking to you about building resilient teams, navigating the Web3 ecosystem. And we have three incredible speakers today. Our first panelist, we have Evan McMullen. She's a visionary in the Web3 space with such a robust background in both blockchain and enterprise tech. She's the co-founder of Pravado ID, where they're focusing on transforming identity verification across digital and real-world environments. And she was previously the founder of Disco.xyz. With leadership experience at Berkshire Hathaway and ConsenSys, Evan has really been at the forefront of advancing decentralized identity and data solutions. And fun fact, Evan and I met while we were students at Yale University, and here we are years later together. So, and our next guest, we have Rachel. And Rachel's been working in the blockchain space for years as a founder, best-selling author, and global public speaker. She's really passionate about sharing knowledge and building community. She has been very active with the base and Coinbase ecosystems, founding Onchain Media, where she runs a podcast and Twitter spaces. And she's co-founded Incentives with Perlabs with Mr. Miggles on base team. And additionally, she works with the city of Berkeley to promote blockchain use cases at a municipal level. And our final panelist, Olivia Smith, is the COO of Moonsong Labs, where she directs operational strategy and scales a company's impact in Web3 innovation. She was formerly the VP of strategy at PureStake, where she played a key role in launching Moonbeam. And she also has a background in finance finance having spent four years in TradFi prior to blockchain and she also holds a joint JD MBA from Columbia University. So welcome to these panelists. Let's, why don't we kick it off with, you know, let's start simple. To each of you in a sentence, what does resilience mean? To each of you in a sentence, what does resilience mean? I think of resilience both in a very sort of literal technical sense, as well as the human sense, the ability to withstand friction, disruption, to maintain integrity, and to maintain process and success despite shifting conditions around you. When I think of resilience, I don't know, I was doing a Twitter space with this MMA fighter who was talking about, you know, getting knocked down repeatedly over and over. And I feel like in our space, we are really fighting this good fight for the greater good. And after getting knocked down time and time again, I think resilience is, you know, the ability to pick yourself back up and realize, okay, we are working towards a greater good, and this is a fight worth fighting. So I think it's just getting up, you know, and if you get knocked down, just get up and keep trying. I feel like that's resilience. I like that image of just keep getting up. For me, it's like, you know, it's obviously persistence, but it's this like mental fortitude. I think when I think of it, it's like as simple as being able to sustain so you can succeed. And so it's something that's consistent, that's always there that you have to keep cultivating, but is key to success. So looking at these definitions that you all just provided, how do you instill these sort of core values of resilience that you just defined within your teams? So the only thing that is certain about the blockchain industry is that we can guarantee uncertainty. I'm sure that all of you here have experienced various different swings of market cycles, introductions of new political influences, social influences that change the dynamics in which we operate every day, both from our technical resilience and the challenges of new adoption or new capabilities or even errors that we find along the way, as well as the human beings coordinating these systems who need to be able to accommodate themselves. At Privato ID, in our practice, we have an incredibly diverse team. Our teammates represent nations across the world, as well as backgrounds that are incredibly diverse from more traditional to pretty degenerate. And it is through that range of experiences and perspectives that I think we find resiliency and strength. The opportunity to invite points of view that might not occur to you. In fact, the invitation to disagree rigorously and respectfully. You know, raising one's voice because you love your users is never a bad thing on my team. And so I think that through open communication, deep consideration for what success looks like, and a shared understanding that the ground under us will always shift. These principles, you know, this way of being has been working so far. But of course, as I say, with all technologies and all processes, we will work for as long as this serves us. And when we find something else, we will do that instead. Mic drop. Yeah. When we find something else, we will do that instead. Mic drop. Yeah, so I feel like what creates a resilient team in my eyes is, I don't know, I'm a big proponent of really understanding your why and the purpose behind what you're doing. When you have a strong enough why, you figure out the how. And the teams that I've seen make the most waves and make the most progress, they're tapped into this greater mission and greater purpose. progress, they're tapped into this greater mission and greater purpose. And they just do it with such conviction like Evan here, you know, just constantly showing up and realizing, okay, what we're doing is so important for the greater good. What we're doing is, you know, what I'm doing is aligned with my purpose every day. And I think that excitement and that joy to be a part of this positive change globally, I think is what is behind the energy of resilience. Yeah, I agree. And I think, Evan, what you spoke to is like psychological safety in a work environment, being able to speak up, being able to have debate. I think that's crucial to a part of a culture. I think it really comes down to people and purpose. And you talked about that of like you have to have a greater good that you're solving for. And you have to give people purpose for the work that they do day in and day out. And then I think a key part is you've got to invest in people. And investment is not just paying them. I think that's really key. It's recognition and appreciation and not just accepting work, but actually talking about all the work they put into it, talking about what they learned from it. And I think, you know, to add to the psychological safety that you mentioned, Evan, I think there's this aspect that everything should be a learning opportunity, the long-term purpose of what you're building for, but then you also want to have, like, a personal purpose of I am developing myself. And as a leader, you want to invest in that, bringing in outside speakers, investing in them learning stuff that's not even just like related to their direct work, but cross-cutting across the company. I think a lot of times we get really focused and siloed on our direct work, and we don't realize that people can learn a ton from whether it's the marketing department, the business development department. There's just a lot that we can take advantage of. So investing in people, I think, is the key to resilience. Can I add one thing there? Okay, so another big component that I was just thinking of, and I feel like Evan has always understood this, but it's also having fun. I actually, I went to Doge Day in Tokyo recently in Japan, and, you know, they were actually honoring the life of a big community member in member in the community named Julia Love. A quote from her is if it's not fun, it's not sustainable. I feel like we're seeing the rise of all these meme communities and people are realizing, okay, this is fun. We can rally around this and celebrate community. So I feel like fun is a big component, too. Being connected to a greater purpose and having fun. And Evan's always got that. She's got a disco ball on stage with us right now. Head of fun. So, you know, I think that a lot of things like this with, like, resiliency, there's sort of these kind of higher level, almost, like, philosophical questions of how we approach this and discussions about this. But if you were to, like, measure resilience and look at metrics, as we're in a very metric-driven industry, what would you say are some things that you could look at for that? Burnout, attrition, uptime, transactions per second. Mic drop. No, but like, for real though, when we talk about metrics, it's a challenging thing to try to apply across the board, because every team is different, every human experience is different. But what I can tell in terms of measuring the health and resilience of our community is that the number of chairs in this room far exceeds the number of chairs in the room in 2018 in Prague at DevCon when I first learned about the work of my now co-founders, the Iden 3 protocol. The ecosystem around us has scaled significantly such that there are a lot of new faces and many that remain. But whether it's through the difficulty of the work that we do, the pressure of the dynamics and constantly changing ecosystem, it's a challenging place for everyone to build resilience, for every team to persist. And so through the natural emergent progress of Ethereum, I think it is organic that not everything will stay the same. And in fact, the ability to adapt and change and to jettison what no longer serves us I think is part of what makes that resilience possible. I know I am definitely guilty of hanging on to some ideas for a little too long even when it was clear that they may no longer be viable whether that's a signature scheme or a you know particular way like state channels. I thought that was gonna happen turns out we went with layer twos. So, you know, the ability to receive that new information and iterate, I think, is also, you know, important in the ways that we measure who's here and how it's working. So I think team by team, it's gonna be, the metrics are gonna be individual to what the purpose is, but I think kind of a universal metric",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:10:00.000Z",
      "slot_end": "2024-11-15T06:10:00.000Z",
      "slot_roomId": "stage-5",
      "resources_presentation": "https://docs.google.com/presentation/d/1mYc9MY0LQHKBFJ4LpR5hSBWXay5VfaXFYGB_2fn3Tk4",
      "resources_slides": "https://drive.google.com/file/d/1B6GMhVAOEs4E6b8rFdzodZ4yNtp1iOTh/view",
      "slot_room": {
        "id": "stage-5",
        "name": "Stage 5",
        "description": "Hats",
        "info": "1",
        "capacity": 400,
        "youtubeStreamUrl_1": "https://youtube.com/embed/LQlvKgDmAhE",
        "youtubeStreamUrl_2": "https://youtube.com/embed/IWGNmihdnb4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/_m81piFG5WY",
        "youtubeStreamUrl_4": "https://youtube.com/embed/mfVJ2Ba9z6E",
        "translationUrl": "https://stm.live/Stage-5"
      }
    },
    {
      "id": "the-end-of-self-custodial-wallets",
      "sourceId": "KDUNLM",
      "title": "The end of self-custodial wallets",
      "description": "This talk provides a quick overview of how countries worldwide restrict or plan to ban the self-custodial ownership model, which is the foundation of cryptocurrencies.\r\n\r\n- What kind of laws, regulations and guidance countries have passed to restrict self-custodial\r\n- What kind of areas are being targeted: ownership of cryptocurrencies, wallets, developers, interfaces\r\n- Who are the driving forces behind opposing self-custodial\r\n- How to counteract this development",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Beginner",
      "audience": "Business",
      "featured": false,
      "doNotRecord": false,
      "tags": "Free Speech,Censorship Resistance,Regulation,fatf,Censorship Resistance,Free Speech,Regulation",
      "keywords": "Self custodial,FATF,wallet",
      "duration": 594,
      "language": "en",
      "sources_swarmHash": "9a46ddf422d25723e072d0ee0fd70a3bb5735f6e1210c767aaa352e247b6d9ff",
      "sources_youtubeId": "Cwn42afQZ3k",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736db4874749a4b8945b981",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736db4874749a4b8945b981.vtt",
      "transcript_text": " Tämä video on tehty Yleisölle. Yleisölle on tehty Yleisölle. Yleisölle on tehty Yleisölle. Yleisölle on tehty Yleisölle. Yleisölle on tehty Yleisölle. Yleisölle on tehty Yleisölle. Yleisölle on tehty Yleisölle. Hyvä. Tänään olen täällä puhumaan selviämisestä ja miten hallitsevat ja ystävät yrittävät heitä pannaan. Tämä on järjestelmä, joten jos haluat nähdä täysin esityksen, niin seuraa minua Twitterissä, koska laitan lisää. Why does this matter? If and when the government are going to restrict the use of self-custodial wallets, it's so called bad ending. At that point it doesn't matter what you do there in the audience, because you can all go home and work in a bank or a McDonald's. So it should be obvious for you why this is important. pankissa tai McDonald'sissa. Se on tärkeää, että se on tärkeää. Ensimmäisenä omaa kastoria-korttia. Ensimmäinen on, että me olemme asettamme. Se on meidän kontroliimme. Ei ole riskejä, että pankissa tai keskusteluissa he rikkovat sen ja sinä menet. Jos menet, se on vain o screw it up and you lose. If you lose, it's only your own fault. The other important point is no vendor login, so you can change wallets. You are not locked in a single wallet provider, but you can have a competition between those, and it's very good for cost efficiency, because the cost will be very, very low for us, and you all know that we are not Kosko-efficiönsä, koska kosteus on hyvin vähäinen meille. Ja kaikki tiedätte, että emme palaa valitsemme. Ja valitsema on ei valitsema, vaan krypto, joka vahvistaa kryptovaluutteiden luonnon luonnon. Ja nyt ei ole vielä mahdollista syntyä kryptovaluutteita itse. It's no longer in this point feasible to ban the crypto themselves. Instead, what governments and regulators work is that they will restrict its use. And there are mainly three ways to do it. Don't allow to transfer in and out from wallets. Regulate developers or regulate interactions with the wallets like a front-end Uniswap. And this is how the regulation in the world is done. ja sillä voidaan säädellä interaktioita valitseihin, kuten Uniswap. Ja tällä tavalla maailman säädellä on tehty. Meidän huolimme USA-politiikasta on se, että kaikki ekonomiset rikot ovat menneet pysyvään, mutta tämä ei ole rahaa, vaan se on vitsi. Joten kaikki kansalliset säädellä jatkuvat, mitä FITF is doing and they are doing what the US says them to do. And there's also compliance industry who is making money out of this. So for them it's very beneficial that we will restrict the use of the wallets. So they will vote for more regulation every time. And the first case we have in Estonia. So 2021 it was already almost there that they banned this. Ja ensimmäinen tapauksena on Estonia. Eli vuonna 2021 se oli jo melkein siellä, että he pahasivat tämän. Siinä oli yksityiskumppani, joka lobaoi hallituksen, että me ei pitäisi oikeastaan pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä pystyä. ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja aloittaa, ja Seuraava asia on Euroopan unionin. Suomalaisen antimainojen lausunnon edistämisessä sanotaan, että käsittelyt ja käsittelyt itse käsittelyt ovat edistettyä, mutta niiden määrä on vähentävä. Siinä on vain käsittelyt, joita on vähentävä, luulen, että se on 1iminnassa ensi vuonna tai vuonna 2026. Sitten Euroopan unionin kansalaiset eivät voi käyttää itsevastuullisia koneita, jos asetuksissa on enemmän kuin 1 000 euroa. Sitten on Seychelles. Se oli ensimmäinen maa, jossa ei ole edes ohjausta, vaan sen omaa lakia. Joku, en tiedä kuka, I don't know who, wrote in the law that non-custodial services, including wallets, should be regulated. And it means that the developers should sign up there with the local regulator if you are developing a wallet. It didn't make to the final law text, but it was explicitly also said that this request came from FATF. And they wanted to close this so-called loophole for the users. And then there's Denmark where they told that Uniswap and others can be identified, so that's why developers like these should be also registered in Denmark asen VASP-palveluun, koska ne voivat antaa palveluita danilaisille. Ja mitä voit tehdä? Organisaatioita, jotka ovat pro-kryptoita. Uskon, että Yhdysvalloissa on mennyt melko hyvin. Ja tärkeintä on, että puhut lokalaista mediasta oman maanasi. Ja yritetään saada luonnollisia ihmisiä siellä, jotka voivat lobbyoida privatiivisesta ja oikeuksista omaa asettasi. Kiitos. Ja seuraavat kysymykset. Kiitos paljon. and the next questions. Thank you very much. So again, if somebody has any questions, please raise your hand. I will toss the mic to you. Yep, we have one. Yeah, better. It's a bit far. So, of course, you can ban alcohol, as they tried to in the US, obviously, but it's an enforcement issue. Do you think that's going to give us any sort of ‑‑ that example, does that give us any reason to have hopium? I think your talk is great, by the way. I think it's so important. I'm not trying to dumb this down. It's really important. But is there hopium out there that this isn't going to be enforceable? I think we have to work a bit harder than that. I mean, it depends, because when they banned alcohol, it was like 100 years ago, and the serverless state didn't exist. Today, governments can follow your actions, and especially the actions of the software companies and developers very closely, so they can knock your door if you have a GitHub account. So that's why I'm quite pessimistic that this can fly under radar, so to speak. So there are bad people out there who do bad things. And while you can argue with how the regulations are laid out, there's not much argument that terrorist financing and money laundering shouldn't be allowed. So how do we, I think we all in this here, we believe in self-sovereignty, but we also, many of us probably think that zero knowledge is a potential solution. How do we get the governments to try it? Because it's not even in the Overton window of discussing these new technologies. How do we get them to actually approve a test case or some way of trying it? I mean, I think we already fixed it in one country by having our friends in the U.S. elect Trump. So that's a step forward. But also, I think the correct way to go about it is to talk with media, so that we have a proof, we have a real scientific research that for the anti-money laundering work, the current regulations restricting centralized exchanges is enough, and it's already stopping most of terrorist funding. We know that the terrorists, yes, they use USDT and Bitcoin and so on and especially North Korea is kind of a pain in the butt but it's not very significant still. Crypto has grown but still it's super small compared to you know all the other gas and oil and whatever there is going around. Yeah, we have one there. Last question. What do you think about companies like Chain Analysis and other days? Do you think they make a positive impact? Some yes, some no. Some of them are pro-crypto, but some of them are part of this compliance industry complex, and they only see how they could seek rents. nähdä, miten he voisivat saada lupaa. Ja usein voi tietää, onko yritys Pro Crypto tai onko se täällä vain rahaa, jos lukee raportteja. Ja jos he tekevät niin sanottua fear-saleja, kuten he tekevät raportteja, joissa sanotaan, että niin moni teröristi tulee ja blockchain on vain käyttöön rahallisuus ja rauhoja ja niin edelleen. chain is only used for money laundering and drugs and so on, and they show the curves like how it's bad usage is growing, which by the way is not anymore growing. It means that they are selling the fare to the governments who then buy their services. But some good companies, like I think chain analysis is one of the good ones. They create reports that are balanced and they show that yeah, some things are bad, but some things are actually very good. Thank you very much.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:10:00.000Z",
      "slot_end": "2024-11-15T05:20:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Ap05BLrc25kR-WdwGvInSGF6oehwIIAg82A0vs0Krrg",
      "resources_slides": "https://drive.google.com/file/d/1ckKhhAIZuV4WVwkWwUdDUpbYfGiRsmUD/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "satellite-based-cryptographic-layer-extra-terrestial-extension-to-ethereum",
      "sourceId": "SZBQLK",
      "title": "Satellite based Cryptographic Layer - Extra-terrestial Extension to Ethereum",
      "description": "Using nano-satellites with edge compute units we will show how we intend to build an orbital compute layer with unique properties. We will propose a novel cryptographic applications layer built with vision to space explorations.\r\n\r\nTypically public blockchains enable cryptographic primitives for the digital commons on earth, we will share novel implementation of cryptographic applications that will extend the digital commons into Low Earth Orbit (LEO) and import cryptographic resources from LEO.",
      "track": "Cypherpunk & Privacy",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Network State,Use cases of cryptography,DePIN,space,frontier,DePIN,Network State,Use cases of cryptography",
      "keywords": "space,frontier",
      "duration": 593,
      "language": "en",
      "sources_swarmHash": "7f0a508bddf0c6ec3d9a6daf5707ab211837a4da389a507c6bc1e223da4741bd",
      "sources_youtubeId": "AJq7z4eLYm0",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736dec174749a4b89942272",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736dec174749a4b89942272.vtt",
      "transcript_text": " Leonardo Silva Reviewer:\" Elisabeth Buffard Hello, Earthlings. So, Ethereum is great. We want to take it to more planets than only Earth. And when we think about space frontier, often nerds think about the space frontier as some sci-fi or alien technologies. Or for some other people, it's just an empty void, which is slightly less inspiring. The reality is that if anything of value is going to come out of it, we need to get nuanced and work with the complexities. So what we have seen is that in the last two years alone, more satellites have been launched into orbit than in the previous five decades combined. So finally, after decades of stagnation, we're entering a space acceleration era. And this is the time where democratization of the space industry is happening and plenty of startups with wild ideas are finally taking a stab at implementing their stuff. With SpaceCoin, what we're building is orbital route of trust. It's literally out- this world secure hardware, out of reach of physical attacks. It's space-based trusted execution environment, or we call it Space TE. And this serves as infrastructure for Celestial Compute Marketplace, payment layer for satellite services, and from Earth, we'll be able to access it via RPC nodes for EVM and other networks. This is a real photo from a launch not long ago. What we've built with the Space T is infrastructure with our aerospace partner, CryptoSat. We basically designed, built, and launched satellites into orbit since 2021. Last year we contributed to the EIP-4844 with the trusted setup ceremony, the KZG, aboard the satellites. And the latest satellites are equipped with the Iridium connection, which is reachable from any place in the planet. And now we're preparing a set of nodes that will be equipped with more iridium antennas, C-edge nodes. And essentially this is immune to any kind of attacks there. The architecture is Yang Wang's session. Okay, thank you for mic. Thanks, Laj. So this is actually what you see is like the network architecture which actually is like network resilient. So we are using the Iridium satellites, which have like the 66 pieces distributed across the planet with the nine in the backup. So there's like the first line of our communication path we are using. Then we are using the Aptos Orbital, which is like the, provides us the computation infrastructure currently. And thanks to this, actually, we are building the space coin on the top. So what actually we can build on thanks to this is actually like the protocol with specific hardware capabilities. And it will be like resistant to any kind of the DDoS attacks and the censorship and in the future we believe there will be like much more a lot of communication the paths we can use and the computational capacity we can get. So this is basically like a brief diagram where actually the satellites are reachable any single moment so you can see like in any kind of the intervals some satellites are actually reachable where actually the ground stations are the proposing and then actually the electing and the voting back and the broadcasting back on the ground stations. So what we are using this, because there's a lot of constraints in the environment, is actually we used the modified HotStuff1 with the instant finality. And thanks to this, we actually, we've retrofitted for our lower bandwidth with the least amount of the messages needed between the satellites. So and actually opens us the requirement like we just need for the connectivity only one node designed as the leader. So this is briefly like the diagram. So how actually it will be like working in the future and what we are looking forward to implement currently. So we have like the pre-finality on the L2 with the pre-finality with L2 as with the L1 backup whereas the state is posted on the L1 which is actually good for the low value transactions and for full finality we'll be using the L1 where actually it's good for the high value transaction and actually the state will be fully committed to the L1 on the satellites. So for the security services thanks to this we can like offer like the space T the copcessors, which enable FHG cases. For that, actually, we are looking forward to offer the Secure Custodian for the various key management systems and schemes. Then actually, we are looking for the Cosmic Entropy and the VRF. And thanks to this, actually, we can offer some kind of the security deletion because the hardware is inaccessible for the inspection and the eternal bulletin board. So what we're actually looking forward up there is that we'll be implementing the support the EVM compatibility and we'll be looking forward to the initial deployment focusing on the bridge contracts and for future extensibility through the WASM risk and the VM based stuff. So we are looking like for the L1, L2 foundations, MPC wallets, and the validators, and especially entropy providers as well. So thank you for the attention. Please give us the follow on the Twitter, and I think now it's time for the questions. Thank you. It is, it is. Man. Peace among worlds. So, we have any questions? Remember to stand up your hand and I will toss this to you. Yeah, we can have any kind of questions. We can also have some physics. Oh. We'll be delivered to you. Don't worry, it doesn't break. So, this is fascinating to see, so hats off for building it. What happens, is it harder to patch because it's in satellites? Like on Earth, I guess you could upgrade the hardware or switch over to a different operating system. If there's a vulnerability in the OS that's running on your satellite, I imagine that's a lot harder to go and fix. So how did those challenges come in? And the second one was, what happens if one of the satellites fails? And can you still maintain connection between your L1 nodes? So there's redundancy, right? You have actually on each of the units, you have two Linux, like hardened Linux machines, and then eventually both you have the redundancy in case one satellite goes off. But with regard to patching, ignoring for a moment the hardware-related stuff, which is a longer conversation, but for software, we're actually looking at potentially implementing some kind of committee sign-off. So, essentially, you have it both peer-vetted and also it's not just like a centralized entity that patches and pushes potentially man-in-the-middle unnecessary things. If I understand correctly from your slides, you use Iridium's satellite? Correct. Okay. I understand SpaceX has a lot of low orbital satellite. Why do you choose Iridium over SpaceX? Okay. Yeah. So for currently, the Iridium is like this. It's most oldest, but it's actually most real you can get right now It's also Expensive, but it's actually thanks to it where is actually the constellation is actually quite have a long Sustainability of the networks, but yes the Starling and other providers. They're actually coming in the path So we believe there is actually in the market of the optical terminals And but we are just wait the satellites being up and ready. So now Starlink will offer for the satellite providers, but it takes time to implement in orbit. So once we have more viable and, of course, cheaper options, we will switch to that or so. And I like the redundancy. Thank you. Any more questions? Last question. Thanks for the presentation. What are some first use cases you envisage and what are some really moonshot or literally moonshot use cases that you can see potentially or what could be the coolest use cases you can foresee? It's a bit of a long story, but I reckon just some of the use cases, cosmic randomness for VRF, secure deletion, MPC application with co-signing, secure co-processing, these are the things that are applicable already today. What's maybe moonshot and very interesting is, as we showed, there's so many launches now. There's an entire new space-native economy. So we foresee a scenario where there's going an entire new space of like space native economy right so we foresee a scenario where there's going to be ai agents in space making decisions without having to download um data to earth so eventually like there's a lot of security that is required for for this to happen like we've seen with planet labs how it influenced uh security Earth. And with more assets in space, it's becoming much more sensitive. So we foresee it as the backbone for expansion of Ethereum into more planets eventually, or at least outside of pure bounds of Earth. I would like to say the Moon cases is like the payment network between the satellites. So that's how we see it. All right. Please give a big applause to these guys. That's a wonderful presentation.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:20:00.000Z",
      "slot_end": "2024-11-15T05:30:00.000Z",
      "slot_roomId": "stage-4",
      "resources_presentation": "https://docs.google.com/presentation/d/1Net_UwG69ncJlQvHg5qG_nefAW16HDrDDKf-9OaDpsw",
      "resources_slides": "https://drive.google.com/file/d/1Wctzat6Ivk2OKGoqk6Wk6gX6JTd82uz5/view",
      "slot_room": {
        "id": "stage-4",
        "name": "Stage 4",
        "description": "Leafs",
        "info": "1",
        "capacity": 250,
        "youtubeStreamUrl_1": "https://youtube.com/embed/n8fREU3UZgA",
        "youtubeStreamUrl_2": "https://youtube.com/embed/uJyMPGA5VS0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/wFae3SkzvDA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/dNR5TuYtQpc",
        "translationUrl": "https://stm.live/Stage-4"
      }
    },
    {
      "id": "conditional-recall",
      "sourceId": "XTQUDR",
      "title": "Conditional Recall",
      "description": "In the neon-lit nights of 2025, Johnson & Johnson unveiled X. A pill, not larger than a snowflake, but it promised a tempest of change. This miraculous drug didn't just allow people to cherry-pick memories to erase from their minds, it also can credibly leave a reminder in people's minds that those memories has been vanished, forever.\r\nWe explore the game theoretic implications of a technology (such as TEEs) that allows players to commit to forget information and discuss several applications.",
      "track": "Cryptoeconomics",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Research",
      "featured": false,
      "doNotRecord": false,
      "tags": "Mechanism design,Game Theory,Economics,commitment,Economics,Game Theory,Mechanism design",
      "keywords": "TEE,Commitment",
      "duration": 1513,
      "language": "en",
      "sources_swarmHash": "aee6df134fb5a9461bd2b460f55ad9332c8b4c900f495e9be865a132e73b1c79",
      "sources_youtubeId": "rO1amHaKH2U",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736f9bf1b0f83434ddf1bf6",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736f9bf1b0f83434ddf1bf6.vtt",
      "transcript_text": " Tanya Cushman Reviewer:\"Petra Petrovic' Hello. I didn't know that I was famous. I doubt that very much. Chin is famous. Ooh, yes. So, we will talk first about X. Here's a story, Anon. So, in the neon-lit nights of 2025, Johnson & Johnson unveiled X. A pill not larger than a snowflake, but it promised a tempest of change. The miraculous drug didn't just allow people to cherry epic memories to erase from their minds, it also can credibly leave a reminder in people's minds that those memories have been vanished forever. Amidst the iconic red-bricked walls of Harvard Law School, you with books in one hand and dreams in the other are on a mission. You're not just another student, You carry the hope of revolutionizing the archaic chambers of the legal world. Each night, as you pour over the terms of law, you wonder what greatness society could have achieved. On a cold evening, your phone buzzes. It's Dax, your old college friend, turned underground dealer. His message is simple. Got X? Special price for you. The temptation swirls around you. Would you trade the lessons of the past for a clearer yet incomplete future? The decision rests in your hands and on. Okay. What is conditional recall? First, let me tell you about perfect recall. So perfect recall is the assumption that we usually make in human cognition and game theory and all kinds of strategic interactions, right? We assume that we remember what we have experienced and we cannot choose what we remember or what we don't remember. So we assume that humans just remember. Or at least don't have control what we remember. All of that will change in 2025. But then you have a technology that allows you to conditionally recall information. And in particular to credibly forget. And that is the exploration we do in this project which will be a paper soon we want to understand what are the strategic implications or social implications of a technology that would allow you to credibly forget information and you can do that in such a way that you forget information you commit to forgetting information, if certain kind of conditions are met in the environment. So Johnson & Johnson Expo will be 2025. Before that, we already have artificial agents who have access to conditional recall. So we are very bullish on TEEs sort of as a commitment device because that is a technology where you can use an artificial agent put it inside of a TE and then you can generate attestations or proofs that that agent has erased a certain kind of memory or has sent some send a request to remove memory from an external storage device. And so if you have these artificial agents having access to conditionally called information, then we can do magical things with it. And we want to present some magical things that you can do with that. I talk more about philosophical ideas, but in all of them, I think there is the potential for an actual application in them. And then Shin will talk about already existing applications that use the power of conditionally forgetting information. Okay. First application. that use the power of conditionally forgetting information. Okay, first application. That's a classical problem from this guy, Ken Arrow. And he described for the first time this very famous Arrow information paradox, which is a very basic but fundamental idea. So you want to sell a piece of intellectual property. You have an idea. You want to sell it to somebody. But in order to sell it to somebody, you need to convince somebody that that is actually a good idea. And so if you start telling that person about the idea more and more and more and more and more, at some point, that other party has learned the idea. And then contracting breaks down down because he already knows the idea and then he doesn't need to pay you for it. Conventional workarounds for this contracting problem usually is IP law like you hire some lawyers and they protect your intellectual property. You have patent law and you need to rely on the legal system. Or you can do the classical contracting stuff. You can contract only on the outputs of the technology, but not actually on the actual technology. Then you have all the problems of asymmetric information and contracting. So that's no good. But if you have X, then you can conditionally recall information. You can solve this transacting problem. You commit before the actual selling of the idea that if you don't like it as a buyer, you commit to forgetting the content of that idea. And then the only thing that after the negotiation you remember in case you don't buy the content of that idea. And then the only thing that after the negotiation you remember, in case you don't buy the information, is that you didn't like it, but nothing else. Here's another broad class of ideas that you can do with this technology. One-time use of information. So previously I was telling about selling information. Now we can also think about renting out information, using it only once or twice or conditionally or five times, six times. And Shin has beautiful examples of that as well. Another quote. I don't believe that this story is true, but I would like it to be true. So one of my favorite writers, Jorge Luis Borges, wrote this about 15th century Italian warfare. So in 15th century Italy, he claims war has reached a perfection that many would call ridiculous. Once the armies were assembled, the generals compared the numbers, strength, and position of the troops and decided who among them must suffer defeat, they decided on the outcome of that war. Chance and bloodshed were eliminated. That is an instance of the power of reducing asymmetric information and bargaining. If we are perfectly informed about each other's capabilities",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:30:00.000Z",
      "slot_end": "2024-11-15T06:00:00.000Z",
      "slot_roomId": "stage-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1jXv2kbSefJjmF8zhC0ibw1x1paaVWwuDjrd37N_Nzj8",
      "resources_slides": "https://drive.google.com/file/d/1MBGkwol6aPlJeQrf3L5YVddZYWwvWy0r/view",
      "slot_room": {
        "id": "stage-2",
        "name": "Stage 2",
        "description": "Lantern",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/pTUsI72zKOY",
        "youtubeStreamUrl_2": "https://youtube.com/embed/ZriZGSANJHg",
        "youtubeStreamUrl_3": "https://youtube.com/embed/0SISgw_37kc",
        "youtubeStreamUrl_4": "https://youtube.com/embed/seC5wuDogPc",
        "translationUrl": "https://stm.live/Stage-2"
      }
    },
    {
      "id": "keynote-glass-houses-and-tornados",
      "sourceId": "K9A8EG",
      "title": "Keynote: Glass Houses and Tornados",
      "description": "The Tornado Cash sanctions and criminal prosecutions have challenged longstanding assumptions within crypto about the limits of money transmission licensing, money laundering statutes, and sanctions laws. They've also revealed a longstanding assumption from some in policy and law enforcement circles: that blockchains have always been and must remain transparent. Neither assumption has served us well and the time has come for legal certainty. This talk is about how we get there.",
      "track": "Cypherpunk & Privacy",
      "type": "Talk",
      "expertise": "Intermediate",
      "audience": "Lobby",
      "featured": true,
      "doNotRecord": false,
      "tags": "Governance,Mixers,Open Source Software,Privacy",
      "keywords": "Legal,Government,Regulation",
      "duration": 904,
      "language": "en",
      "sources_swarmHash": "",
      "sources_youtubeId": "haxfI3A-E4g",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "",
      "transcript_vtt": "",
      "transcript_text": "",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:30:00.000Z",
      "slot_end": "2024-11-15T05:50:00.000Z",
      "slot_roomId": "main-stage",
      "resources_presentation": "https://docs.google.com/presentation/d/1Xs3Tvj3iPf9ArWjPRjf3e7zXu_JG8R-eXuI5yEgHV6c",
      "resources_slides": "https://drive.google.com/file/d/1ldvlW_g6g6BiOKBxdEhHXr9TprztsMKM/view",
      "slot_room": {
        "id": "main-stage",
        "name": "Main Stage",
        "description": "Masks",
        "info": "1",
        "capacity": 900,
        "youtubeStreamUrl_1": "https://youtube.com/embed/rGE_RDumZGg",
        "youtubeStreamUrl_2": "https://youtube.com/embed/JMZcRMy-Oow",
        "youtubeStreamUrl_3": "https://youtube.com/embed/-HAh09a5Qec",
        "youtubeStreamUrl_4": "https://youtube.com/embed/onV-iTNM3sM",
        "translationUrl": "https://stm.live/Mainstage"
      }
    },
    {
      "id": "multi-party-fully-homomorphic-encryption-mp-fhe-in-practice",
      "sourceId": "QC7FH7",
      "title": "Multi-Party Fully Homomorphic Encryption (MP-FHE) in Practice",
      "description": "In this session, we will break down the FHE game Frogzone, which required advancements at every layer of the cryptographic software stack: cryptography libraries and tooling, circuits, software infrastructure, and even DevOps. We will also cover additional use cases for FHE at a technical level.",
      "track": "[CLS] Programmable / Frogrammable Cryptography, by 0xPARC",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography,Homomorphic Encryption",
      "keywords": "Programmable,Cryptography",
      "duration": 5029,
      "language": "en",
      "sources_swarmHash": "e81444659e448d17e4a0dca6fd3e287c5ef8c78525a0ead1b46362bc631cc1e3",
      "sources_youtubeId": "uNDFmC4NHkM",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "673d8e0f17a97b4f4de7517d",
      "transcript_vtt": "No VTT link provided",
      "transcript_text": "No transcript text provided",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:30:00.000Z",
      "slot_end": "2024-11-15T07:00:00.000Z",
      "slot_roomId": "breakout-3",
      "resources_presentation": "https://docs.google.com/presentation/d/15m-ipmgu4kmVNAWtsY-5mdugROSn_lIoAK6AY-lB8wM",
      "resources_slides": "https://drive.google.com/file/d/1fWnCKp6dZczMJ1YtHroSxcFWUKVYMF7F/view",
      "slot_room": {
        "id": "breakout-3",
        "name": "Breakout 3",
        "description": "Community-Led-Sessions",
        "info": "2",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/qehhamVaAHM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/n0WwuAUCdo4",
        "youtubeStreamUrl_3": "https://youtube.com/embed/JBi3DwfFwto",
        "youtubeStreamUrl_4": "https://youtube.com/embed/G_ey_WenFH8",
        "translationUrl": "https://stm.live/Breakout-3"
      }
    },
    {
      "id": "programmable-cryptography-from-a-software-engineering-lens",
      "sourceId": "SWD9LD",
      "title": "Programmable Cryptography from a Software Engineering Lens",
      "description": "Different cryptographic primitives have different affordances, especially when using them in practice, and especially together. In this session, we explore a new way of interacting with PCs at a software engineering level via a LISP like programming language. This language enables creating self-verifying graphs of computation.",
      "track": "[CLS] Programmable / Frogrammable Cryptography, by 0xPARC",
      "type": "Workshop",
      "expertise": "Intermediate",
      "audience": "",
      "featured": false,
      "doNotRecord": false,
      "tags": "Cryptography",
      "keywords": "Programmable,Cryptography",
      "duration": null,
      "language": "en",
      "sources_swarmHash": null,
      "sources_youtubeId": "5C8SovQZnqY",
      "sources_ipfsHash": null,
      "sources_livepeerId": null,
      "sources_streamethId": null,
      "transcript_vtt": null,
      "transcript_text": null,
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:30:00.000Z",
      "slot_end": "2024-11-15T07:00:00.000Z",
      "slot_roomId": "breakout-2",
      "resources_presentation": "https://docs.google.com/presentation/d/1yWVJ6yTEFsI9WxcM3wmAe6YClRLfYGGhGBGYB8pv2Sg",
      "resources_slides": "",
      "slot_room": {
        "id": "breakout-2",
        "name": "Breakout 2",
        "description": "",
        "info": "2",
        "capacity": 100,
        "youtubeStreamUrl_1": "",
        "youtubeStreamUrl_2": "",
        "youtubeStreamUrl_3": "",
        "youtubeStreamUrl_4": "",
        "translationUrl": "https://stm.live/Breakout-2"
      }
    },
    {
      "id": "searcher-competition-in-block-building",
      "sourceId": "MHRYV9",
      "title": "Searcher Competition in Block Building",
      "description": "We study the amount of MEV captured by validators, as a function of searcher competition. The core is a suitable solution concept in this context that makes robust predictions independent of implementation details or specific mechanisms chosen. The surplus share of validators is a function of searcher competition. Searchers can obtain at most the marginal value increase of the winning block relative to the best block that can be built without them. We validate the theory empirically.",
      "track": "Cryptoeconomics",
      "type": "Lightning Talk",
      "expertise": "Intermediate",
      "audience": "Design",
      "featured": false,
      "doNotRecord": false,
      "tags": "Core Protocol,Gaming,Mechanism design,MEV,theory,cooperative,Core Protocol,Mechanism design,MEV",
      "keywords": "Cooperative,Game,Theory;",
      "duration": 599,
      "language": "en",
      "sources_swarmHash": "2549f66fc5a9634575f6d89f41afb1dc348f3a5b89c4267992a25578642d3491",
      "sources_youtubeId": "T_I7HYBIxZQ",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "67383c451b0f83434d2a7a78",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67383c451b0f83434d2a7a78.vtt",
      "transcript_text": " Hello everyone, welcome to the presentation. Today I will talk about searcher competition in block building. So this is the paper title and it was co-authored with Christoph Schlegel and Danik Sui from Flashpots and Benny Sudakov from ETH Zurich and I'm a researcher at off-chain labs. So motivation was MEV and we know that there are many forms of MEV extraction and I'm listing only the most popular ones here. So there's this sex-sex arbitrage, background, sandwiches, and liquidation. And in each form, there are also many different specializations that you can take as a searcher. And here, the main point is that there are many different types, so searchers are different in their specializations. So it creates some kind of heterogeneity among searchers. So we tried to model this with the tools of game theory. And for that, we need to identify who are the players. And the main player is validator or a proposer that proposes the next block. And then there are these specialized arbitrageurs that in the Ethereum setting are called searchers. And in current market, there are also builders that aggregate searchers and also mempool, public mempool transactions. But we are ignoring it for this simplified game because for us, the fundamental players are searchers and the proposer. We denote the set of searchers by S that we can identify by their addresses. Then searchers submit their bundles of transactions to include in the block. Typically now they send it to block builder, but in principle they could have sent it to a proposer then from these bundles there are some in these bundles there are some conflicts so you cannot build the block that is the union of all bundles so you can build many different blocks. For a build block, searcher generates some value, and validator or the proposer also generates a value. So, these are the smallest building blocks of our game. So, we are abstracting away from all particular mechanisms, how they interact, and that includes builders and we only try to understand who will get how much among these players depending on their bundles. So for this we are using tools from cooperative game theory and for that we need to define a value of a coalition and value of a coalition is the best block that the searchers in this coalition build. If there is no proposer in the coalition, then the value is zero. So proposer or validator can block any, or veto any block. So you need to agree with the proposer, the searchers. And this already gives a coalitional transferable utility game. And in these games, the most natural solution, we think, is the core. And let me define what is core. It's very simple and intuitive. The core solution gives payoffs to all players so that no coalition of players prefer to deviate from their location and create their own block together. Because if they can, they will. And so we need to specify the payments to all searchers and the validator of the global game. So there are nice properties that Core has. First of all, it's always non-empty. You always have one solution. In particular, you can give all the value to the validator. But, of course, this would be very unfair to searchers. And there are other core solutions too, often, but not always. So, for example, there are cores such that searchers capture all the value. And if we have additive value over the bundles, then we actually have nice characterization of the core. So to make it more interesting, we look at the stochastic setting where we have a number of opportunities denoted by M and number of searchers denoted by N. And each searcher generates some value for each opportunity, so we have this matrix. And by P we denote the probability that each searcher finds each opportunity. Then we have very nice simple result that as soon as probability slightly high so it's larger than this tool log n divided by n and there are not too many opportunities so m is less than n and with high probability the validator captures everything and this can be in particular empirically checked in the data. So thank you for your attention. Happy to answer your questions. Thank you very much. So I posted one question. I wonder if you know how many search providers are currently in ethereum ecosystem so currently today i don't know but it's in the order of hundreds hundreds yes okay and we have one more question how do you know that the core is non-empty and this is usually requires strong assumptions so we know because if you give all the value to validator, it satisfies all these inequalities that core requires. It's very easy to check and strong assumption is that what you may refer as strong assumption is that validator can block any or veto any coalition. So that has a veto power and that's what that's why core is non-empty. And in particular we give exact example of a core. Okay, thank you very much. Please give a round of applause to our speaker. So we're gonna to have, I think, a break and the next session will start at 1.50. Thank you. Terima kasih telah menonton! Kampung Kampung Kampung Thank you.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:30:00.000Z",
      "slot_end": "2024-11-15T05:40:00.000Z",
      "slot_roomId": "classroom-a",
      "resources_presentation": "https://docs.google.com/presentation/d/1oRDP1vAH4P88oiBLEXOsJco7KgtJbQmYvKAeAkMug6Y",
      "resources_slides": "https://drive.google.com/file/d/182J1xnWr7i92dwkT_5wohH-6ATft1TNi/view",
      "slot_room": {
        "id": "classroom-a",
        "name": "Classroom A",
        "description": "",
        "info": "1",
        "capacity": 172,
        "youtubeStreamUrl_1": "https://streameth.org/embed/?stage=6720fe2a24af22d0ca96c99c",
        "youtubeStreamUrl_2": "https://youtube.com/embed/K5jLZJfX2a0",
        "youtubeStreamUrl_3": "https://youtube.com/embed/NDWSsJRLqFQ",
        "youtubeStreamUrl_4": "https://youtube.com/embed/BI5XEDZAezs",
        "translationUrl": "https://stm.live/Classroom-A"
      }
    },
    {
      "id": "slangs-query-api-a-better-way-to-analyse-solidity-code",
      "sourceId": "8PYLB7",
      "title": "Slang’s Query API: a better way to analyse Solidity code",
      "description": "Slang is Nomic Foundation’s modular set of Solidity compiler APIs. This presentation will review Slang’s query engine approach to analysing Solidity code, and explain why it makes building tools that support multiple Solidity versions significantly easier than existing solutions, leading overall to higher quality tools.",
      "track": "Developer Experience",
      "type": "Talk",
      "expertise": "Expert",
      "audience": "Engineering",
      "featured": false,
      "doNotRecord": false,
      "tags": "Developer Infrastructure,Tooling,Languages,compilers,Developer Infrastructure,Languages,Tooling",
      "keywords": "Parsing,Compiling",
      "duration": 1573,
      "language": "en",
      "sources_swarmHash": "43fe979794664aaea8f19c8d9b6da6366feea49e50b444c8a89c69179314f148",
      "sources_youtubeId": "ScMhFA5Jnhk",
      "sources_ipfsHash": "",
      "sources_livepeerId": "",
      "sources_streamethId": "6736e51074749a4b8997dc40",
      "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/6736e51074749a4b8997dc40.vtt",
      "transcript_text": " Okay, good day. My name is Tony. I work at the NOMIC Foundation in Applied Research. I was previously a co-lead on the SLANG project and it's in this capacity that I'm going to speak to you today. I have a fantastic job working with excellent people at NOMIC. NOMIC is a non-profit and our number one core value is kindness and we live this every day. And we are hiring. There's meant to be a QR code at the end of my slides but I'm afraid it didn't make it. And I want to give heartfelt thanks to the co-founders of Nomec, Pato and Fran for creating and maintaining such an amazing place to work. So today we're going to explore the slang query API which is a new feature which makes working with solidity code surprisingly straightforward. Now this is not a tutorial so some of the details have been omitted. You can't copy and paste any of the code that I'm going to be providing. It's just so that you can, well I'm hoping to inspire you, so that you go and find out some more details. Okay, so 20 minutes is a bit of a sprint, so I'm going to speed up. Let's go. Okay, so what is Slang? So Slang's our Solidity compiler library. It's basically a developer tool that enables you to write better tools. It parses and analyzes solidity code from version 0.4.11 to the latest 0.8, which is basically all of the solidity code that is live. Our focus is on correctness and convenience. We're about to release version 1 which covers the typical front-end features of a compiler and our main objective as I said is to enable you to develop better tools. Slang has a radical open to clarity of meta architecture. Slang's an error-correcting compiler, so it always produces output, even if you've got source code errors. A key feature of Slang, which is what we're here to talk about, is its query API for code analysis. So compiler front ends fall into two main categories. There are those that produce what's called concrete syntax trees, and then produce an abstract syntax tree from that. And there are those that produce abstract syntax trees directly. So what's the difference between concrete syntax and abstract syntax? Well, concrete syntax, as you can see on this slide, is a complete representation of the source code. It includes every character, every bit of white space, every comment. It's like having, as I say, like having the full book, like a PDF. Whereas an abstract syntax tree is a simplified tree representation. So it doesn't include white space and equal signs and punctuation. It's just got the essential details. It's like if you were reading the plot of a book but you didn't actually have the text. So here's an example of a concrete syntax tree. So we've got a simple variable declaration here and every single element of the original source is represented. Nothing is removed or simplified. Now, this is different from an AST, which will eliminate some of these details. So if we look what we've got here, it's a simple variable declaration. Now, it consists of a type name, which is uint, and then some whitespace, an identifier, some whitespace punctuation, the equal sign, some whitespace, punctuation, equal sign, some whitespace, a number literal, one, and then the semicolon. Now, what's crucial to understand is that we're preserving every single character. In fact, even if there's an error in the source code, you'll find it in the concrete syntax tree. So why is this important? Well, when we're building developer tools or transforming code, we often need the complete information so we can round trip. If we were just concerned with the meaning of the code, then we'd use an abstract syntax tree. But for many tools, like formatters, linters, or refactoring tools, we often want the full syntactic details. You don't want to lose comments when you're refactoring and sometimes you want to know if you've left two blank lines and you want to preserve that. This level of detail is great but it's also challenging. It's quite difficult to process this. On the one hand we have complete information but working with detailed trees can be complex and that's the challenge that the query API solves. So from now on I'll admit trivia nodes and whitespace from the examples. They're still there but I don't need to show them. So let's look at a slightly more complex example. This definition shows how different elements nest within each other. And once again, notice how everything is preserved here. We've got a function definition, we've got the keyword function, the identifier, we've got a parameter list, which in this case is empty, but we still preserve it. And then nested in that is a block and a return statement and so on. Now, this level of detail allows us to maintain complete source fidelity. As I said, if we're going to round trip, we can track precise code locations. So if you want to do error reporting, it handles formatting preservation. And when you're building development tools this detailed structure is essential for certain class of development tools. So to emphasize this point, I know I'm being repetitive here I really want to make this point though, why do we need a concrete syntax tree? Well we really want to in a lot of tools, for example in Pretia, which is now using slang by the way, using this technology, Pretia Solidity, we want to make sure that we can preserve every single character, especially comments. For example, you might have a tool that wants to deal with comments, wants to encode validation information in a special comment format. Not like the documentation comment, but your own special comment format, so you want to preserve those comments. An AST on the other hand is focusing on the essential structure which is good for semantic processing, doesn't contain the formatting information, simplifies expressions and emits all comments in white space. So now that we understand what a CST is for, let's look at how you might traditionally process a CST. So this is the kind of code that you often see a lot of. know what it's like to traverse a tree especially a very deep and complex tree Which is what you typically get out of a compiler? And it's not pretty What we're trying to do is actually quite simple We just want to find all the variable declarations in some solidity code, but look at what we have to do So first of all we're writing a recursive function. We need recursion because we don't know how deep in the tree it is. Could be at the contract, inside a function, inside a block, anywhere. And then when we find each node we've got to manually check its kind, we've got to search through the children, We've got to handle all the edge cases, and we've got to manage the recursion stack, especially if you're threading state. And this is a simple example. In real-life code, you'd have to handle error cases, deal with unexpected node types, which is what you can get if you've got a source code error, for example. Manage your state during traversal, and if you've written a visitor pattern, you know that it's a pain in the ass to manage your state. You've got to handle parent references, and you've got to manage the position in the source code. Now, the problems with this approach are numerous. First, it's incredibly verbose. The amount of code you need to write to do even simple analysis is substantial, and the more code you write, the more you have to maintain, and the more places there are for bugs to hide. Second, it's error-prone, not only because of the volume of the code, but also it's quite tricky to write these when you're dealing with complicated trees. For example, in Solidity, variable declarations can appear in for loop initialisers. I don't know if this code would handle that. It's a question for the reader. It's hard to maintain. When the language evolves, and as you know, solidity evolves significantly in all sorts of interesting ways, like in one of the 0.5s where the exponentiation operator changed its associativity. So when the language evolves, you need to update your traversal code, and that's because the traversal code is mixed with the analysis code. So what you want to do for analysis is mixed in with the mechanics of traversal. To make changes, you're often going to break things. Finally, and more importantly, this forces you to think about what or how you want to do things rather than what you want to do. And the intent of this code, finding variable declarations, is buried in here. You could look at a big bit of traversal code and you wouldn't know what it was meant to be doing. We need something better. So we'll have a look at some specific challenges with manual traversal. So recursion management. You've got to handle deep nesting efficiently, avoiding stack overflow. You've got to make sure you don't have any infinite loops. You've got to maintain your context, state management, and you've got to know when to stop recursing. Once again, if you use a visitor pattern where you've got an external code driving your visitor, you often want to stop or you don't want to go into the children of this node. How do you do that? That means you end up with a more complicated API. State tracking is quite complex. You need to keep track of the parent node when you're walking a tree. You need to maintain scope information in the case of a compiler, build up your composite results, handle cross-node relationships, i.e. you want to link this node which you visited 30 minutes ago with this other node which you're visiting now. Now error handling just multiplies the complexity and edge cases keep appearing. For example, parenthesised expressions, nested structures, optional elements and language specific quirks of which there are plenty in Solidity. And finally you've got a big maintenance budget. Now I know I'm repeating this point but this is significant as you'll see when we get to queries, which I think is the next slide. So this is where queries come in. So let's look at how we solve some of those same problems using queries. So this is our first example. We want to find all the unit variable, all the uint variable declarations. It's as simple as saying, in variable declarations where the type name is uint and it's got an identifier, find that and return it. No traversal. No edge cases. Hardly any maintenance. No efficiency problems. So the second example shows something a bit more powerful. In this case we want to find immediately nested function declarations, i.e. a function inside another function. So here we're saying in every contract definition there are some contract members. In the contract members there's a contract member. For every function definition in there find the block and look for a statement in there that is itself a function definition, bind that to a variable called nested and return it. So why is this so powerful? Well first of all it's declarative, where it's saying what we want, not how we want to do it. It makes our code easier to understand and maintain, and when you look at this two months later it's obvious what it's doing. Whereas I'd suggest that you'd have a couple of pages of code traversing your tree if you were using manual traversal. And it wouldn't be obvious what it was actually meant to do. So it's structure aware. So the query understands Solidity's syntax structure. It knows about scope and it knows about nesting. And it also knows where type name, for example, in the first case here, appears in the tree. These patterns are composable. So we can build complex pattern matching from simple pattern matching. So if we wanted to find uint variables within these nested function definitions it would be as simple as putting that, appending that first query below the second query. But most importantly this is focusing once again on what we're trying to achieve. We can think about code patterns and structure design without getting bogged down in the implementation details. So let's have a look at some more advanced query patterns. This one isn't that advanced, but for example in an unchecked block you want to find all function calls. It's easy as that and it's obvious what it does. You could write this on one line. The second example is a bit more complicated but here we're looking for variable declarations, state variable declarations that have a type that is either a mapping or an array. And think about what this would take if you're doing manual traversal. You'd need scope tracking, type checking logic, complex conditional logic, and you'd need to carefully handle the nested structure here. So what are the use cases for this? Some use cases, because I'm hoping that people come up with far more use cases than we know. We want emergent benefits from this. Well, if you wanted to have custom coding standards, you want to enforce custom coding standards. You've got project-specific restrictions that you want to check. Or you want to do version compatibility checks, so you want to make sure that you don't use certain features of solidity. Style checking. Well, if you've got specific naming patterns you want to enforce specific structural conventions and you can have context aware rules now you might be thinking well can't I use a linter to do this well yes but this is meant to write the linter this is designed for writing linters so that's why these are the use cases. This is not an end user tool. This is a tool for developers to write tools. So you can do pattern detection. Anti-patterns, for example. Optimisation opportunities, which may not be immediately obvious if you've got a big library of things. Detecting complex structural patterns that you might want to simplify or mark. So once again, this is a kind of an ESLinter equivalent. Code transformation. Automated refactoring. Refactoring is transformations that preserve the semantics. Code modernisation. It's very easy to write patterns that then transform into more modern code. Automated modifications, i.e. things that aren't refactorings that actually change the code. And formatting. And as I said, Prettier Solidity is already using slang. One important point I want to make here is that we support WASM and we support specifically the component spec and wit. So for those who know what that means, this means that all of this technology works in the browser. It's also available as a Rust API or as TypeScript, which is our first target because most people are using TypeScript, which is our first target, because most people are using TypeScript. So, some more applications, code transfer, I've already done that. Documentation generation, it's easy to generate automatic documents by extracting function signatures, doing structure analysis, checking for particular usage patterns and documenting them, and processing comments. As I mentioned before, you might have validation information in a certain specialised comment format. Now, you can also use this technology in combination with AI tooling, for example, to produce diagrams from your code. You can use this by encoding the slang API as a rag. So what are the key benefits? Well, structure-aware queries. Your queries naturally align with the structure of the code that you're trying to match. You don't have to think about trees and nodes and traversal. Complete syntax preservation. You never lose information. Perform transformations without losing the formatting. And you can round trip from a CST back to source. Efficient pattern matching. So we can spend all the effort required to make this efficient. If you know anything about the state of the art in terms of tree pattern matching, and in fact we're not a search engine, we are a unifier. So we use a prologue slash datalog based mechanism which returns you all the potential matching results, not just the first one. You can avoid unnecessary traversals. We can cache results. We can index the syntax tree. Make it very efficient. It would take a lot of effort to get the same result if you were doing it yourself. Composable syntax rules, you can combine simple rules to make bigger ones, you can have a big library of these patterns and you can reuse them over your code, share queries between your projects. You've got maintainable analysis code. So the queries express your intent, not how you want to do it, but what you want to do. Your code is shorter, it's easier to understand. Any changes to the language, we take care of that. Slang takes care of that, so you don't have to. Less code means fewer bugs. And these benefits compound. It's not one plus one plus one. It's the whole is far greater than the sum of the parts. So what impact does this have on your development? Well, before using this query API, you'd spend significant time writing and debugging the traversal code. You'd struggle with maintaining the analysis logic, and you'd face challenges when adding new features. After it, you end up with clear, focused code. It's easy to maintain. You've got a robust implementation, and it's highly extensible. And that is it. Any questions? Fantastic. Thank you, Anthony. All right, let's get with the questions. So, folks, remember, you can scan that QR code, add questions to that list, smash that upvote button so that the most interesting question gets asked. Let's go with the first one. What are our advantages to using slang compared to Semgrep? Well, I must admit I'm not familiar with Semgrep, but I know the general concept. The specific thing here that this is a programmatic tool that you use to build other tools. Now, of course, you can combine things like Semgrep, I imagine, but I don't know if that is something that you would include in a tool for analysing lots of different versions of solidity. That makes sense. Thank you. How does slang differ from a fine-tuned LLM? Could slang's modularity allow to use LLMs in the future? Oh, this is... I'm in applied research, and this is something that is very actively under research. I'm not making any commitment, but if you've used fine-tuned LLMs, and I've used them a lot, for example, a lot of the code that I write is actually written by Claude. A lot of this presentation was written by Claude. Amazing tool, but you always need a human in the loop. a lot of this presentation was written by Claude. Amazing tool, but you always need a human in the loop. So, yes, you can do this using LLMs, but the thing with LLMs is that they are a human augmentation tool, whereas the approach that we're talking about here is exact and precise. If you've used LLMs to do this, you know that you spend a lot of time on prompt engineering, and then you cross your fingers, and something comes back, and it doesn't quite work, and you prompt it again, and so on and so forth. So, yes, this is certainly something that could be included in the future. There's a chance, but maybe not now. I mean, not immediately. Certainly not now. Okay. Are the limitations of a tree visitor-based approach not solved by using an API with CFG and a terminated representation like Slither provides? Well, if by CFG, well, you're either meaning two things. You're either meaning the control flow graph, which is typically something you'd find in an abstract syntax tree, or if you mean control flow as exposed by rust for example and this is something where you reify your control flow. So you return a token in your visitor which says do you want to continue, do you want to go down the trees. Now this tree visitor, that are, these solution, these slither may well solve the same kind of problem. We have a different set of constraints. For example, our query language is intended to be extended with semantic predicates and with the ability to do arbitrary recursion by skipping over arbitrary subtrees. So there's a lot that we want to do there. So I wouldn't say that in Slither you can or cannot do that. I'm not familiar enough with Slither. Perfect, thank you. Is there a question you'd like Moose in there? Well will this work with different EVM versions? Yes, it will, because we're dealing with source. We're well before EVM. Slang is a compiler, and it'll produce bytecode. So that's a question about the compiler, not about querying. Have we published the grammar for slang? Yes, we have. Slang is actually a declarative meta project. So you can use it for any programming language, not just Solidity. So we have a very good open source, we have a very good declarative meta project, so you can use it for any programming language, not just solidity. We have published that. Everything we do is open source. Nomic, foundation on GitHub, all of our development is in the open. You can reuse all of this. Awesome. How important is it to maintain code with different Solidity versions? Why not just use one version? Because if you want to be able to provide tools that analyse a large number of contracts, which may be an earlier version of Solidity than the latest one, for example, 0411. There's a contract on Mainnet that is 0411. You want to get the source to that. You want to provide analysis tools for it. That's why we want to support all of these versions. Now we don't go back to 001 or whatever the first version was. So we're pragmatic about it. But these are the versions that are live, and our goal is to support all the versions that are live. How do you determine the cut-off version? What's pragmatic? Analysis of Mainnet and all the contracts that are live. Okay, is it the number of contracts? Is it the value stored in those? Is it the amount of transactions? No, the earliest. I didn't do this analysis, so I must admit I don't know what process we used to determine that, but it is the case that the earliest contract in use, apparently, is 0411. Fantastic, thank you. What's the best practice for using slang to analyze a large number of programs? Where should the data be stored and in what format? Imagine this is something you run locally, correct? Yes, it is. Perfect. Thank you. All right. What update in Solidity was the hardest to adapt for? You mentioned quirks and weird updates in Solidity. Which one was the hardest? None of them was particularly hard. The volume of quirks is what is a challenge. I mean, we've got hundreds of edge cases. We had to analyse Sol-C because that's the only definition of the language and go through the code and then test it and then go to Sanctuary and look at massive numbers of contracts, run our compiler over them or run our parser over them. What breaks? What doesn't? We've got extensive test cases. So that was the challenge. It's the number, not any particular difficulty. Fantastic. Thank you. Do you dream in bytecode? No, I don't dream in... I dream compilers, actually. All right. Anthony, thank you a lot for your time and all the best to you. Thank you a lot for your time and all the best to you. Thank you. People, our next session will start in a few minutes. I am off for today. Thank you for those who spent a bit of time with me this morning. And I'll see you soon.",
      "eventId": "devcon-7",
      "slot_start": "2024-11-15T05:30:00.000Z",
      "slot_end": "2024-11-15T06:00:00.000Z",
      "slot_roomId": "stage-3",
      "resources_presentation": "https://docs.google.com/presentation/d/1y7kvxWFxGZ-TBTEld48n6Dz0MGYoIGHria1lhFAdTZo",
      "resources_slides": "https://drive.google.com/file/d/1VukfZOfvmTCPi9Zgla8IjhhjDFLyuxPp/view",
      "slot_room": {
        "id": "stage-3",
        "name": "Stage 3",
        "description": "Fabrics",
        "info": "1",
        "capacity": 300,
        "youtubeStreamUrl_1": "https://youtube.com/embed/Ta1s31vUsvM",
        "youtubeStreamUrl_2": "https://youtube.com/embed/te5u0tM6-9g",
        "youtubeStreamUrl_3": "https://youtube.com/embed/Q5jTqAov5bA",
        "youtubeStreamUrl_4": "https://youtube.com/embed/_znDO7PVoBI",
        "translationUrl": "https://stm.live/Stage-3"
      }
    }
  ]
}